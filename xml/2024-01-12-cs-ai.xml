<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03991" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.04732">
<title>A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation. (arXiv:2401.04732v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.04732</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we design a real-time question-answering system specifically
targeted for helping sellers get relevant material/documentation they can share
live with their customers or refer to during a call. Taking the Seismic content
repository as a relatively large scale example of a diverse dataset of sales
material, we demonstrate how LLM embeddings of sellers&apos; queries can be matched
with the relevant content. We achieve this by engineering prompts in an
elaborate fashion that makes use of the rich set of meta-features available for
documents and sellers. Using a bi-encoder with cross-encoder re-ranker
architecture, we show how the solution returns the most relevant content
recommendations in just a few seconds even for large datasets. Our recommender
system is deployed as an AML endpoint for real-time inferencing and has been
integrated into a Copilot interface that is now deployed in the production
version of the Dynamics CRM, known as MSX, used daily by Microsoft sellers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Manpreet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasricha_R/0/1/0/all/0/1&quot;&gt;Ravdeep Pasricha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1&quot;&gt;Nitish Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1&quot;&gt;Ravi Prasad Kondapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_M/0/1/0/all/0/1&quot;&gt;Manoj R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1&quot;&gt;Kiran R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boue_L/0/1/0/all/0/1&quot;&gt;Laurent Bou&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04736">
<title>Exploring Attack Resilience in Distributed Platoon Controllers with Model Predictive Control. (arXiv:2401.04736v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.04736</link>
<description rdf:parseType="Literal">&lt;p&gt;The extensive use of distributed vehicle platoon controllers has resulted in
several benefits for transportation systems, such as increased traffic flow,
fuel efficiency, and decreased pollution. The rising reliance on interconnected
systems and communication networks, on the other hand, exposes these
controllers to potential cyber-attacks, which may compromise their safety and
functionality. This thesis aims to improve the security of distributed vehicle
platoon controllers by investigating attack scenarios and assessing their
influence on system performance. Various attack techniques, including
man-in-the-middle (MITM) and false data injection (FDI), are simulated using
Model Predictive Control (MPC) controller to identify vulnerabilities and
weaknesses of the platoon controller. Countermeasures are offered and tested,
that includes attack analysis and reinforced communication protocols using
Machine Learning techniques for detection. The findings emphasize the
significance of integrating security issues into their design and
implementation, which helps to construct safe and resilient distributed platoon
controllers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Choudhury_T/0/1/0/all/0/1&quot;&gt;Tashfique Hasnine Choudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04737">
<title>Music Genre Classification: A Comparative Analysis of CNN and XGBoost Approaches with Mel-frequency cepstral coefficients and Mel Spectrograms. (arXiv:2401.04737v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.04737</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, various well-designed algorithms have empowered music
platforms to provide content based on one&apos;s preferences. Music genres are
defined through various aspects, including acoustic features and cultural
considerations. Music genre classification works well with content-based
filtering, which recommends content based on music similarity to users. Given a
considerable dataset, one premise is automatic annotation using machine
learning or deep learning methods that can effectively classify audio files.
The effectiveness of systems largely depends on feature and model selection, as
different architectures and features can facilitate each other and yield
different results. In this study, we conduct a comparative study investigating
the performances of three models: a proposed convolutional neural network
(CNN), the VGG16 with fully connected layers (FC), and an eXtreme Gradient
Boosting (XGBoost) approach on different features: 30-second Mel spectrogram
and 3-second Mel-frequency cepstral coefficients (MFCCs). The results show that
the MFCC XGBoost model outperformed the others. Furthermore, applying data
segmentation in the data preprocessing phase can significantly enhance the
performance of the CNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yigang Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04739">
<title>Content-Conditioned Generation of Stylized Free hand Sketches. (arXiv:2401.04739v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04739</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the recognition of free-hand sketches has remained a popular
task. However, in some special fields such as the military field, free-hand
sketches are difficult to sample on a large scale. Common data augmentation and
image generation techniques are difficult to produce images with various
free-hand sketching styles. Therefore, the recognition and segmentation tasks
in related fields are limited. In this paper, we propose a novel adversarial
generative network that can accurately generate realistic free-hand sketches
with various styles. We explore the performance of the model, including using
styles randomly sampled from a prior normal distribution to generate images
with various free-hand sketching styles, disentangling the painters&apos; styles
from known free-hand sketches to generate images with specific styles, and
generating images of unknown classes that are not in the training set. We
further demonstrate with qualitative and quantitative evaluations our
advantages in visual quality, content accuracy, and style imitation on
SketchIME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guangming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1&quot;&gt;Ning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_E/0/1/0/all/0/1&quot;&gt;Eryang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04747">
<title>DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation. (arXiv:2401.04747v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.04747</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose DiffSHEG, a Diffusion-based approach for Speech-driven Holistic 3D
Expression and Gesture generation with arbitrary length. While previous works
focused on co-speech gesture or expression generation individually, the joint
generation of synchronized expressions and gestures remains barely explored. To
address this, our diffusion-based co-speech motion generation transformer
enables uni-directional information flow from expression to gesture,
facilitating improved matching of joint expression-gesture distributions.
Furthermore, we introduce an outpainting-based sampling strategy for arbitrary
long sequence generation in diffusion models, offering flexibility and
computational efficiency. Our method provides a practical solution that
produces high-quality synchronized expression and gesture generation driven by
speech. Evaluated on two public datasets, our approach achieves
state-of-the-art performance both quantitatively and qualitatively.
Additionally, a user study confirms the superiority of DiffSHEG over prior
approaches. By enabling the real-time generation of expressive and synchronized
motions, DiffSHEG showcases its potential for various applications in the
development of digital humans and embodied agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04748">
<title>Convolutional Neural Network Ensemble Learning for Hyperspectral Imaging-based Blackberry Fruit Ripeness Detection in Uncontrolled Farm Environment. (arXiv:2401.04748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04748</link>
<description rdf:parseType="Literal">&lt;p&gt;Fruit ripeness estimation models have for decades depended on spectral index
features or colour-based features, such as mean, standard deviation, skewness,
colour moments, and/or histograms for learning traits of fruit ripeness.
Recently, few studies have explored the use of deep learning techniques to
extract features from images of fruits with visible ripeness cues. However, the
blackberry (Rubus fruticosus) fruit does not show obvious and reliable visible
traits of ripeness when mature and therefore poses great difficulty to fruit
pickers. The mature blackberry, to the human eye, is black before, during, and
post-ripening. To address this engineering application challenge, this paper
proposes a novel multi-input convolutional neural network (CNN) ensemble
classifier for detecting subtle traits of ripeness in blackberry fruits. The
multi-input CNN was created from a pre-trained visual geometry group 16-layer
deep convolutional network (VGG16) model trained on the ImageNet dataset. The
fully connected layers were optimized for learning traits of ripeness of mature
blackberry fruits. The resulting model served as the base for building
homogeneous ensemble learners that were ensemble using the stack generalization
ensemble (SGE) framework. The input to the network is images acquired with a
stereo sensor using visible and near-infrared (VIS-NIR) spectral filters at
wavelengths of 700 nm and 770 nm. Through experiments, the proposed model
achieved 95.1% accuracy on unseen sets and 90.2% accuracy with in-field
conditions. Further experiments reveal that machine sensory is highly and
positively correlated to human sensory over blackberry fruit skin texture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olisah_C/0/1/0/all/0/1&quot;&gt;Chollette C. Olisah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trewhella_B/0/1/0/all/0/1&quot;&gt;Ben Trewhella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_M/0/1/0/all/0/1&quot;&gt;Melvyn L. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winstone_B/0/1/0/all/0/1&quot;&gt;Benjamin Winstone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitfield_E/0/1/0/all/0/1&quot;&gt;E. Charles Whitfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_F/0/1/0/all/0/1&quot;&gt;Felicidad Fern&amp;#xe1;ndez Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncalfe_H/0/1/0/all/0/1&quot;&gt;Harriet Duncalfe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04749">
<title>LogFormer: A Pre-train and Tuning Pipeline for Log Anomaly Detection. (arXiv:2401.04749v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;Log anomaly detection is a key component in the field of artificial
intelligence for IT operations (AIOps). Considering log data of variant
domains, retraining the whole network for unknown domains is inefficient in
real industrial scenarios. However, previous deep models merely focused on
extracting the semantics of log sequences in the same domain, leading to poor
generalization on multi-domain logs. To alleviate this issue, we propose a
unified Transformer-based framework for Log anomaly detection (LogFormer) to
improve the generalization ability across different domains, where we establish
a two-stage process including the pre-training and adapter-based tuning stage.
Specifically, our model is first pre-trained on the source domain to obtain
shared semantic knowledge of log data. Then, we transfer such knowledge to the
target domain via shared parameters. Besides, the Log-Attention module is
proposed to supplement the information ignored by the log-paring. The proposed
method is evaluated on three public and one real-world datasets. Experimental
results on multiple benchmarks demonstrate the effectiveness of our LogFormer
with fewer trainable parameters and lower training costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongcheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jiaqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoujun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tieqiao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+peng_J/0/1/0/all/0/1&quot;&gt;Junran peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04757">
<title>How predictable is language model benchmark performance?. (arXiv:2401.04757v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04757</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate large language model performance across five orders of
magnitude of compute scaling in eleven recent model architectures. We show that
average benchmark performance, aggregating over many individual tasks and
evaluations as in the commonly-used BIG-Bench dataset, is decently predictable
as a function of training compute scale. Specifically, when extrapolating
BIG-Bench Hard performance across one order of magnitude in compute, we observe
average absolute errors of 6 percentage points (pp). By contrast, extrapolation
for individual BIG-Bench tasks across an order of magnitude in compute yields
higher average errors of 18pp. Nonetheless, individual task performance remains
significantly more predictable than chance. Overall, our work suggests compute
scaling provides a promising basis to forecast AI capabilities in diverse
benchmarks, though predicting performance in specific tasks poses challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owen_D/0/1/0/all/0/1&quot;&gt;David Owen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04812">
<title>Sample-and-Bound for Non-Convex Optimization. (arXiv:2401.04812v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.04812</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard approaches for global optimization of non-convex functions, such as
branch-and-bound, maintain partition trees to systematically prune the domain.
The tree size grows exponentially in the number of dimensions. We propose new
sampling-based methods for non-convex optimization that adapts Monte Carlo Tree
Search (MCTS) to improve efficiency. Instead of the standard use of visitation
count in Upper Confidence Bounds, we utilize numerical overapproximations of
the objective as an uncertainty metric, and also take into account of sampled
estimates of first-order and second-order information. The Monte Carlo tree in
our approach avoids the usual fixed combinatorial patterns in growing the tree,
and aggressively zooms into the promising regions, while still balancing
exploration and exploitation. We evaluate the proposed algorithms on
high-dimensional non-convex optimization benchmarks against competitive
baselines and analyze the effects of the hyper parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yaoguang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Sicun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04820">
<title>Phishing Website Detection through Multi-Model Analysis of HTML Content. (arXiv:2401.04820v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.04820</link>
<description rdf:parseType="Literal">&lt;p&gt;The way we communicate and work has changed significantly with the rise of
the Internet. While it has opened up new opportunities, it has also brought
about an increase in cyber threats. One common and serious threat is phishing,
where cybercriminals employ deceptive methods to steal sensitive
information.This study addresses the pressing issue of phishing by introducing
an advanced detection model that meticulously focuses on HTML content. Our
proposed approach integrates a specialized Multi-Layer Perceptron (MLP) model
for structured tabular data and two pretrained Natural Language Processing
(NLP) models for analyzing textual features such as page titles and content.
The embeddings from these models are harmoniously combined through a novel
fusion process. The resulting fused embeddings are then input into a linear
classifier. Recognizing the scarcity of recent datasets for comprehensive
phishing research, our contribution extends to the creation of an up-to-date
dataset, which we openly share with the community. The dataset is meticulously
curated to reflect real-life phishing conditions, ensuring relevance and
applicability. The research findings highlight the effectiveness of the
proposed approach, with the CANINE demonstrating superior performance in
analyzing page titles and the RoBERTa excelling in evaluating page content. The
fusion of two NLP and one MLP model,termed MultiText-LP, achieves impressive
results, yielding a 96.80 F1 score and a 97.18 accuracy score on our research
dataset. Furthermore, our approach outperforms existing methods on the
CatchPhish HTML dataset, showcasing its efficacies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colhak_F/0/1/0/all/0/1&quot;&gt;Furkan &amp;#xc7;olhak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecevit_M/0/1/0/all/0/1&quot;&gt;Mert &amp;#x130;lhan Ecevit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ucar_B/0/1/0/all/0/1&quot;&gt;Bilal Emir U&amp;#xe7;ar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creutzburg_R/0/1/0/all/0/1&quot;&gt;Reiner Creutzburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dag_H/0/1/0/all/0/1&quot;&gt;Hasan Da&amp;#x11f;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04821">
<title>MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer. (arXiv:2401.04821v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04821</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pre-trained language models (PLMs) have achieved remarkable
performance in various natural language processing (NLP) tasks. However,
pre-training such models can take considerable resources that are almost only
available to high-resource languages. On the contrary, static word embeddings
are easier to train in terms of computing resources and the amount of data
required. In this paper, we introduce MoSECroT Model Stitching with Static Word
Embeddings for Crosslingual Zero-shot Transfer), a novel and challenging task
that is especially relevant to low-resource languages for which static word
embeddings are available. To tackle the task, we present the first framework
that leverages relative representations to construct a common space for the
embeddings of a source language PLM and the static word embeddings of a target
language. In this way, we can train the PLM on source-language training data
and perform zero-shot transfer to the target language by simply swapping the
embedding layer. However, through extensive experiments on two classification
datasets, we show that although our proposed framework is competitive with weak
baselines when addressing MoSECroT, it fails to achieve competitive results
compared with some strong baselines. In this paper, we attempt to explain this
negative result and provide several thoughts on possible improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haotian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chunlan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1&quot;&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04846">
<title>The inherent goodness of well educated intelligence. (arXiv:2401.04846v1 [econ.TH])</title>
<link>http://arxiv.org/abs/2401.04846</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper will examine what makes a being intelligent, whether that be a
biological being or an artificial silicon being on a computer. Special
attention will be paid to the being having the ability to characterize and
control a collective system of many identical conservative sub-systems
conservatively interacting. The essence of intelligence will be found to be the
golden rule -- &quot;the collective acts as one&quot; or &quot;knowing the global consequences
of local actions&quot;. The flow of the collective is a small set of twinkling
textures, that are governed by a puppeteer who is pulling a small number of
strings according to a geodesic motion of least action, determined by the
symmetries. Controlling collective conservative systems is difficult and has
historically been done by adding significant viscosity to the system to
stabilize the desirable meta stable equilibriums of maximum performance, but it
degrades or destroys them in the process. There is an alternative. Once the
optimum twinkling textures of the meta stable equilibriums are identified by
the intelligent being (that is the collective system is characterized), the
collective system can be moved by the intelligent being to the optimum
twinkling textures, then quickly vibrated by the intelligent being according to
the textures so that the collective system remains at the meta stable
equilibrium. Well educated intelligence knows the global consequences of its
local actions so that it will not take short term actions that will lead to
poor long term outcomes. In contrast, trained intelligence or trained stupidity
will optimize its short term actions, leading to poor long term outcomes. Well
educated intelligence is inherently good, but trained stupidity is inherently
evil and should be feared. Particular attention is paid to the control and
optimization of economic and social collectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Glinsky_M/0/1/0/all/0/1&quot;&gt;Michael E. Glinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Sievert_S/0/1/0/all/0/1&quot;&gt;Sharon Sievert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04849">
<title>A Deep Learning Representation of Spatial Interaction Model for Resilient Spatial Planning of Community Business Clusters. (arXiv:2401.04849v1 [econ.EM])</title>
<link>http://arxiv.org/abs/2401.04849</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Spatial Interaction Models (SIMs) are limited in capturing the
complex and context-aware interactions between business clusters and trade
areas. To address the limitation, we propose a SIM-GAT model to predict
spatiotemporal visitation flows between community business clusters and their
trade areas. The model innovatively represents the integrated system of
business clusters, trade areas, and transportation infrastructure within an
urban region using a connected graph. Then, a graph-based deep learning model,
i.e., Graph AttenTion network (GAT), is used to capture the complexity and
interdependencies of business clusters. We developed this model with data
collected from the Miami metropolitan area in Florida. We then demonstrated its
effectiveness in capturing varying attractiveness of business clusters to
different residential neighborhoods and across scenarios with an eXplainable AI
approach. We contribute a novel method supplementing conventional SIMs to
predict and analyze the dynamics of inter-connected community business
clusters. The analysis results can inform data-evidenced and place-specific
planning strategies helping community business clusters better accommodate
their customers across scenarios, and hence improve the resilience of community
businesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Hao_H/0/1/0/all/0/1&quot;&gt;Haiyan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04851">
<title>Graph Learning-based Fleet Scheduling for Urban Air Mobility under Operational Constraints, Varying Demand &amp; Uncertainties. (arXiv:2401.04851v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.04851</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops a graph reinforcement learning approach to online
planning of the schedule and destinations of electric aircraft that comprise an
urban air mobility (UAM) fleet operating across multiple vertiports. This fleet
scheduling problem is formulated to consider time-varying demand, constraints
related to vertiport capacity, aircraft capacity and airspace safety
guidelines, uncertainties related to take-off delay, weather-induced route
closures, and unanticipated aircraft downtime. Collectively, such a formulation
presents greater complexity, and potentially increased realism, than in
existing UAM fleet planning implementations. To address these complexities, a
new policy architecture is constructed, primary components of which include:
graph capsule conv-nets for encoding vertiport and aircraft-fleet states both
abstracted as graphs; transformer layers encoding time series information on
demand and passenger fare; and a Multi-head Attention-based decoder that uses
the encoded information to compute the probability of selecting each available
destination for an aircraft. Trained with Proximal Policy Optimization, this
policy architecture shows significantly better performance in terms of daily
averaged profits on unseen test scenarios involving 8 vertiports and 40
aircraft, when compared to a random baseline and genetic algorithm-derived
optimal solutions, while being nearly 1000 times faster in execution than the
latter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1&quot;&gt;Steve Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witter_J/0/1/0/all/0/1&quot;&gt;Jhoel Witter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Souma Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04858">
<title>User Embedding Model for Personalized Language Prompting. (arXiv:2401.04858v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04858</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling long histories plays a pivotal role in enhancing recommendation
systems, allowing to capture user&apos;s evolving preferences, resulting in more
precise and personalized recommendations. In this study we tackle the
challenges of modeling long user histories for preference understanding in
natural language. Specifically, we introduce a new User Embedding Module (UEM)
that efficiently processes user history in free-form text by compressing and
representing them as embeddings, to use them as soft prompts to a LM. Our
experiments demonstrate the superior capability of this approach in handling
significantly longer histories compared to conventional text based prompting
methods, yielding substantial improvements in predictive performance. The main
contribution of this research is to demonstrate the ability to bias language
models with user signals represented as embeddings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1&quot;&gt;Sumanth Doddapaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayana_K/0/1/0/all/0/1&quot;&gt;Krishna Sayana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jash_A/0/1/0/all/0/1&quot;&gt;Ambarish Jash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sodhi_S/0/1/0/all/0/1&quot;&gt;Sukhdeep Sodhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuzmin_D/0/1/0/all/0/1&quot;&gt;Dima Kuzmin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04867">
<title>An Analysis of User Behaviours for Objectively Evaluating Spoken Dialogue Systems. (arXiv:2401.04867v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04867</link>
<description rdf:parseType="Literal">&lt;p&gt;Establishing evaluation schemes for spoken dialogue systems is important, but
it can also be challenging. While subjective evaluations are commonly used in
user experiments, objective evaluations are necessary for research comparison
and reproducibility. To address this issue, we propose a framework for
indirectly but objectively evaluating systems based on users&apos; behaviours. In
this paper, to this end, we investigate the relationship between user
behaviours and subjective evaluation scores in social dialogue tasks: attentive
listening, job interview, and first-meeting conversation. The results reveal
that in dialogue tasks where user utterances are primary, such as attentive
listening and job interview, indicators like the number of utterances and words
play a significant role in evaluation. Observing disfluency also can indicate
the effectiveness of formal tasks, such as job interview. On the other hand, in
dialogue tasks with high interactivity, such as first-meeting conversation,
behaviours related to turn-taking, like average switch pause length, become
more important. These findings suggest that selecting appropriate user
behaviours can provide valuable insights for objective evaluation in each
social dialogue task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1&quot;&gt;Koji Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1&quot;&gt;Divesh Lala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochi_K/0/1/0/all/0/1&quot;&gt;Keiko Ochi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1&quot;&gt;Tatsuya Kawahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1&quot;&gt;Gabriel Skantze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04898">
<title>ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language Models In Chinese Domain. (arXiv:2401.04898v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04898</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, various Large Language Models (LLMs) evaluation datasets have
emerged, but most of them have issues with distorted rankings and difficulty in
model capabilities analysis. Addressing these concerns, this paper introduces
ANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes
\textit{Keypoint} categorization standard for the first time, each question in
ANGO can correspond to multiple keypoints, effectively enhancing
interpretability of evaluation results. Base on performance of real humans, we
build a quantifiable question difficulty standard and divide ANGO questions
into 9 difficulty levels, which provide more precise guidance for model
training. To minimize data leakage impact and fully leverage ANGO&apos;s innovative
features, we have engineered exclusive sampling strategies and a new evaluation
framework that support swift testset iteration. Our experiments demonstrate
that ANGO poses a stronger challenge to models and reveals more details in
evaluation result compared to existing benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bingchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04925">
<title>The Impact of Reasoning Step Length on Large Language Models. (arXiv:2401.04925v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.04925</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain of Thought (CoT) is significant in improving the reasoning abilities of
large language models (LLMs). However, the correlation between the
effectiveness of CoT and the length of reasoning steps in prompts remains
largely unknown. To shed light on this, we have conducted several empirical
experiments to explore the relations. Specifically, we design experiments that
expand and compress the rationale reasoning steps within CoT demonstrations,
while keeping all other factors constant. We have the following key findings.
First, the results indicate that lengthening the reasoning steps in prompts,
even without adding new information into the prompt, considerably enhances
LLMs&apos; reasoning abilities across multiple datasets. Alternatively, shortening
the reasoning steps, even while preserving the key information, significantly
diminishes the reasoning abilities of models. This finding highlights the
importance of the number of steps in CoT prompts and provides practical
guidance to make better use of LLMs&apos; potential in complex problem-solving
scenarios. Second, we also investigated the relationship between the
performance of CoT and the rationales used in demonstrations. Surprisingly, the
result shows that even incorrect rationales can yield favorable outcomes if
they maintain the requisite length of inference. Third, we observed that the
advantages of increasing reasoning steps are task-dependent: simpler tasks
require fewer steps, whereas complex tasks gain significantly from longer
inference sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Mingyu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinkai Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+shu_D/0/1/0/all/0/1&quot;&gt;Dong shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yanda Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04929">
<title>Learning-Based Difficulty Calibration for Enhanced Membership Inference Attacks. (arXiv:2401.04929v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.04929</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models, in particular deep neural networks, are currently an
integral part of various applications, from healthcare to finance. However,
using sensitive data to train these models raises concerns about privacy and
security. One method that has emerged to verify if the trained models are
privacy-preserving is Membership Inference Attacks (MIA), which allows
adversaries to determine whether a specific data point was part of a model&apos;s
training dataset. While a series of MIAs have been proposed in the literature,
only a few can achieve high True Positive Rates (TPR) in the low False Positive
Rate (FPR) region (0.01%~1%). This is a crucial factor to consider for an MIA
to be practically useful in real-world settings. In this paper, we present a
novel approach to MIA that is aimed at significantly improving TPR at low FPRs.
Our method, named learning-based difficulty calibration for MIA(LDC-MIA),
characterizes data records by their hardness levels using a neural network
classifier to determine membership. The experiment results show that LDC-MIA
can improve TPR at low FPR by up to 4x compared to the other difficulty
calibration based MIAs. It also has the highest Area Under ROC curve (AUC)
across all datasets. Our method&apos;s cost is comparable with most of the existing
MIAs, but is orders of magnitude more efficient than one of the
state-of-the-art methods, LiRA, while achieving similar performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haonan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_T/0/1/0/all/0/1&quot;&gt;Tu Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;An Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04934">
<title>Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey. (arXiv:2401.04934v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative multi-agent reinforcement learning is a powerful tool to solve
many real-world cooperative tasks, but restrictions of real-world applications
may require training the agents in a fully decentralized manner. Due to the
lack of information about other agents, it is challenging to derive algorithms
that can converge to the optimal joint policy in a fully decentralized setting.
Thus, this research area has not been thoroughly studied. In this paper, we
seek to systematically review the fully decentralized methods in two settings:
maximizing a shared reward of all agents and maximizing the sum of individual
rewards of all agents, and discuss open questions and future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiechuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kefan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zongqing Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04950">
<title>Information Flow Rate for Cross-Correlated Stochastic Processes. (arXiv:2401.04950v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/2401.04950</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference seeks to identify cause-and-effect interactions in coupled
systems. A recently proposed method by Liang detects causal relations by
quantifying the direction and magnitude of information flow between time
series. The theoretical formulation of information flow for stochastic
dynamical systems provides a general expression and a data-driven statistic for
the rate of entropy transfer between different system units. To advance
understanding of information flow rate in terms of intuitive concepts and
physically meaningful parameters, we investigate statistical properties of the
data-driven information flow rate between coupled stochastic processes. We
derive relations between the expectation of the information flow rate statistic
and properties of the auto- and cross-correlation functions. Thus, we elucidate
the dependence of the information flow rate on the analytical properties and
characteristic times of the correlation functions. Our analysis provides
insight into the influence of the sampling step, the strength of
cross-correlations, and the temporal delay of correlations on information flow
rate. We support the theoretical results with numerical simulations of
correlated Gaussian processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hristopulos_D/0/1/0/all/0/1&quot;&gt;Dionissios T. Hristopulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04978">
<title>Closed-Form Interpretation of Neural Network Classifiers with Symbolic Regression Gradients. (arXiv:2401.04978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04978</link>
<description rdf:parseType="Literal">&lt;p&gt;I introduce a unified framework for interpreting neural network classifiers
tailored toward automated scientific discovery. In contrast to neural
network-based regression, for classification, it is in general impossible to
find a one-to-one mapping from the neural network to a symbolic equation even
if the neural network itself bases its classification on a quantity that can be
written as a closed-form equation. In this paper, I embed a trained neural
network into an equivalence class of classifying functions that base their
decisions on the same quantity. I interpret neural networks by finding an
intersection between this equivalence class and human-readable equations
defined by the search space of symbolic regression. The approach is not limited
to classifiers or full neural networks and can be applied to arbitrary neurons
in hidden layers or latent spaces or to simplify the process of interpreting
neural network regressors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzel_S/0/1/0/all/0/1&quot;&gt;Sebastian Johann Wetzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04979">
<title>Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04979</link>
<description rdf:parseType="Literal">&lt;p&gt;To handle the complexities of irregular and incomplete time series data, we
propose an invertible solution of Neural Differential Equations (NDE)-based
method. While NDE-based methods are a powerful method for analyzing
irregularly-sampled time series, they typically do not guarantee reversible
transformations in their standard form. Our method suggests the variation of
Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which
ensures invertibility while maintaining a lower computational burden.
Additionally, it enables the training of a dual latent space, enhancing the
modeling of dynamic temporal dynamics. Our research presents an advanced
framework that excels in both classification and interpolation tasks. At the
core of our approach is an enhanced dual latent states architecture, carefully
designed for high precision across various time series tasks. Empirical
analysis demonstrates that our method significantly outperforms existing
models. This work significantly advances irregular time series analysis,
introducing innovative techniques and offering a versatile tool for diverse
practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;YongKyung Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_D/0/1/0/all/0/1&quot;&gt;Dongyoung Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungil Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04980">
<title>Autonomous Navigation of Tractor-Trailer Vehicles through Roundabout Intersections. (arXiv:2401.04980v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.04980</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, significant advancements have been made in the field of
autonomous driving with the aim of increasing safety and efficiency. However,
research that focuses on tractor-trailer vehicles is relatively sparse. Due to
the physical characteristics and articulated joints, such vehicles require
tailored models. While turning, the back wheels of the trailer turn at a
tighter radius and the truck often has to deviate from the centre of the lane
to accommodate this. Due to the lack of publicly available models, this work
develops truck and trailer models using the high-fidelity simulation software
CARLA, together with several roundabout scenarios, to establish a baseline
dataset for benchmarks. Using a twin-q soft actor-critic algorithm, we train a
quasi-end-to-end autonomous driving model which is able to achieve a 73%
success rate on different roundabouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attard_D/0/1/0/all/0/1&quot;&gt;Daniel Attard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajada_J/0/1/0/all/0/1&quot;&gt;Josef Bajada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04993">
<title>AdaFed: Fair Federated Learning via Adaptive Common Descent Direction. (arXiv:2401.04993v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.04993</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a promising technology via which some edge
devices/clients collaboratively train a machine learning model orchestrated by
a server. Learning an unfair model is known as a critical problem in federated
learning, where the trained model may unfairly advantage or disadvantage some
of the devices. To tackle this problem, in this work, we propose AdaFed. The
goal of AdaFed is to find an updating direction for the server along which (i)
all the clients&apos; loss functions are decreasing; and (ii) more importantly, the
loss functions for the clients with larger values decrease with a higher rate.
AdaFed adaptively tunes this common direction based on the values of local
gradients and loss functions. We validate the effectiveness of AdaFed on a
suite of federated datasets, and demonstrate that AdaFed outperforms
state-of-the-art fair FL methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Shayan Mohajer Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1&quot;&gt;En-Hui Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05010">
<title>Less is More : A Closer Look at Multi-Modal Few-Shot Learning. (arXiv:2401.05010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05010</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot Learning aims to learn and distinguish new categories with a very
limited number of available images, presenting a significant challenge in the
realm of deep learning. Recent researchers have sought to leverage the
additional textual or linguistic information of these rare categories with a
pre-trained language model to facilitate learning, thus partially alleviating
the problem of insufficient supervision signals. However, the full potential of
the textual information and pre-trained language model have been underestimated
in the few-shot learning till now, resulting in limited performance
enhancements. To address this, we propose a simple but effective framework for
few-shot learning tasks, specifically designed to exploit the textual
information and language model. In more detail, we explicitly exploit the
zero-shot capability of the pre-trained language model with the learnable
prompt. And we just add the visual feature with the textual feature for
inference directly without the intricate designed fusion modules in previous
works. Additionally, we apply the self-ensemble and distillation to further
enhance these components. Our extensive experiments conducted across four
widely used few-shot datasets demonstrate that our simple framework achieves
impressive results. Particularly noteworthy is its outstanding performance in
the 1-shot learning task, surpassing state-of-the-art methods by an average of
3.0\% in classification accuracy. \footnote{We will make the source codes of
the proposed framework publicly available upon acceptance. }.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xilu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05014">
<title>Source-Free Cross-Modal Knowledge Transfer by Unleashing the Potential of Task-Irrelevant Data. (arXiv:2401.05014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free cross-modal knowledge transfer is a crucial yet challenging task,
which aims to transfer knowledge from one source modality (e.g., RGB) to the
target modality (e.g., depth or infrared) with no access to the task-relevant
(TR) source data due to memory and privacy concerns. A recent attempt leverages
the paired task-irrelevant (TI) data and directly matches the features from
them to eliminate the modality gap. However, it ignores a pivotal clue that the
paired TI data could be utilized to effectively estimate the source data
distribution and better facilitate knowledge transfer to the target modality.
To this end, we propose a novel yet concise framework to unlock the potential
of paired TI data for enhancing source-free cross-modal knowledge transfer. Our
work is buttressed by two key technical components. Firstly, to better estimate
the source data distribution, we introduce a Task-irrelevant data-Guided
Modality Bridging (TGMB) module. It translates the target modality data (e.g.,
infrared) into the source-like RGB images based on paired TI data and the
guidance of the available source model to alleviate two key gaps: 1)
inter-modality gap between the paired TI data; 2) intra-modality gap between TI
and TR target data. We then propose a Task-irrelevant data-Guided Knowledge
Transfer (TGKT) module that transfers knowledge from the source model to the
target model by leveraging the paired TI data. Notably, due to the
unavailability of labels for the TR target data and its less reliable
prediction from the source model, our TGKT model incorporates a self-supervised
pseudo-labeling approach to enable the target model to learn from its
predictions. Extensive experiments show that our method achieves
state-of-the-art performance on three datasets (RGB-to-depth and
RGB-to-infrared).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinjing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yucheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05033">
<title>Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk. (arXiv:2401.05033v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05033</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are powerful dialogue agents, but specializing
them towards fulfilling a specific function can be challenging. Instructing
tuning, i.e. tuning models on instruction and sample responses generated by
humans (Ouyang et al., 2022), has proven as an effective method to do so, yet
requires a number of data samples that a) might not be available or b) costly
to generate. Furthermore, this cost increases when the goal is to make the LLM
follow a specific workflow within a dialogue instead of single instructions.
Inspired by the self-play technique in reinforcement learning and the use of
LLMs to simulate human agents, we propose a more effective method for data
collection through LLMs engaging in a conversation in various roles. This
approach generates a training data via &quot;self-talk&quot; of LLMs that can be refined
and utilized for supervised fine-tuning. We introduce an automated way to
measure the (partial) success of a dialogue. This metric is used to filter the
generated conversational data that is fed back in LLM for training. Based on
our automated and human evaluations of conversation quality, we demonstrate
that such self-talk data improves results. In addition, we examine the various
characteristics that showcase the quality of generated dialogues and how they
can be connected to their potential utility as training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1&quot;&gt;Dennis Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1&quot;&gt;Elman Mansimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kaixiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Justin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xibin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05043">
<title>CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.05043</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty estimation is increasingly attractive for improving the
reliability of neural networks. In this work, we present novel credal-set
interval neural networks (CreINNs) designed for classification tasks. CreINNs
preserve the traditional interval neural network structure, capturing weight
uncertainty through deterministic intervals, while forecasting credal sets
using the mathematical framework of probability intervals. Experimental
validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN)
showcase that CreINNs outperform epistemic uncertainty estimation when compared
to variational Bayesian neural networks (BNNs) and deep ensembles (DEs).
Furthermore, CreINNs exhibit a notable reduction in computational complexity
compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariatmadar_K/0/1/0/all/0/1&quot;&gt;Keivan Shariatmadar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchingal_S/0/1/0/all/0/1&quot;&gt;Shireen Kudukkil Manchingal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_D/0/1/0/all/0/1&quot;&gt;David Moens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hallez_H/0/1/0/all/0/1&quot;&gt;Hans Hallez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05054">
<title>Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding. (arXiv:2401.05054v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05054</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most important challenges in text generation systems is to produce
outputs that are not only correct but also diverse. Recently, Minimum
Bayes-Risk (MBR) decoding has gained prominence for generating sentences of the
highest quality among the decoding algorithms. However, existing algorithms
proposed for generating diverse outputs are predominantly based on beam search
or random sampling, thus their output quality is capped by these underlying
methods. In this paper, we investigate an alternative approach -- we develop
diversity-promoting decoding algorithms by enforcing diversity objectives to
MBR decoding. We propose two variants of MBR, Diverse MBR (DMBR) and
$k$-medoids MBR (KMBR), methods to generate a set of sentences with high
quality and diversity. We evaluate DMBR and KMBR on a variety of directed text
generation tasks using encoder-decoder models and a large language model with
prompting. The experimental results show that the proposed method achieves a
better trade-off than the diverse beam search and sampling algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinnai_Y/0/1/0/all/0/1&quot;&gt;Yuu Jinnai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1&quot;&gt;Ukyo Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morimura_T/0/1/0/all/0/1&quot;&gt;Tetsuro Morimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05097">
<title>Any-Way Meta Learning. (arXiv:2401.05097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.05097</link>
<description rdf:parseType="Literal">&lt;p&gt;Although meta-learning seems promising performance in the realm of rapid
adaptability, it is constrained by fixed cardinality. When faced with tasks of
varying cardinalities that were unseen during training, the model lacks its
ability. In this paper, we address and resolve this challenge by harnessing
`label equivalence&apos; emerged from stochastic numeric label assignments during
episodic task sampling. Questioning what defines ``true&quot; meta-learning, we
introduce the ``any-way&quot; learning paradigm, an innovative model training
approach that liberates model from fixed cardinality constraints. Surprisingly,
this model not only matches but often outperforms traditional fixed-way models
in terms of performance, convergence speed, and stability. This disrupts
established notions about domain generalization. Furthermore, we argue that the
inherent label equivalence naturally lacks semantic information. To bridge this
semantic information gap arising from label equivalence, we further propose a
mechanism for infusing semantic class information into the model. This would
enhance the model&apos;s comprehension and functionality. Experiments conducted on
renowned architectures like MAML and ProtoNet affirm the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junhoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yearim Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05115">
<title>Unpacking Human-AI interactions: From interaction primitives to a design space. (arXiv:2401.05115v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.05115</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to develop a semi-formal design space for Human-AI
interactions, by building a set of interaction primitives which specify the
communication between users and AI systems during their interaction. We show
how these primitives can be combined into a set of interaction patterns which
can provide an abstract specification for exchanging messages between humans
and AI/ML models to carry out purposeful interactions. The motivation behind
this is twofold: firstly, to provide a compact generalisation of existing
practices, that highlights the similarities and differences between systems in
terms of their interaction behaviours; and secondly, to support the creation of
new systems, in particular by opening the space of possibilities for
interactions with models. We present a short literature review on frameworks,
guidelines and taxonomies related to the design and implementation of HAI
interactions, including human-in-the-loop, explainable AI, as well as hybrid
intelligence and collaborative learning approaches. From the literature review,
we define a vocabulary for describing information exchanges in terms of
providing and requesting particular model-specific data types. Based on this
vocabulary, a message passing model for interactions between humans and models
is presented, which we demonstrate can account for existing systems and
approaches. Finally, we build this into design patterns as mid-level constructs
that capture common interactional structures. We discuss how this approach can
be used towards a design space for Human-AI interactions that creates new
possibilities for designs as well as keeping track of implementation issues and
concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsiakas_K/0/1/0/all/0/1&quot;&gt;Kostas Tsiakas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_Rust_D/0/1/0/all/0/1&quot;&gt;Dave Murray-Rust&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05133">
<title>Neural Population Learning beyond Symmetric Zero-sum Games. (arXiv:2401.05133v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.05133</link>
<description rdf:parseType="Literal">&lt;p&gt;We study computationally efficient methods for finding equilibria in n-player
general-sum games, specifically ones that afford complex visuomotor skills. We
show how existing methods would struggle in this setting, either
computationally or in theory. We then introduce NeuPL-JPSRO, a neural
population learning algorithm that benefits from transfer learning of skills
and converges to a Coarse Correlated Equilibrium (CCE) of the game. We show
empirical convergence in a suite of OpenSpiel games, validated rigorously by
exact game solvers. We then deploy NeuPL-JPSRO to complex domains, where our
approach enables adaptive coordination in a MuJoCo control domain and skill
transfer in capture-the-flag. Our work shows that equilibrium convergent
population learning can be implemented at scale and in generality, paving the
way towards solving real-world games between heterogeneous players with mixed
motives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marris_L/0/1/0/all/0/1&quot;&gt;Luke Marris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1&quot;&gt;Marc Lanctot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1&quot;&gt;Georgios Piliouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z. Leibo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1&quot;&gt;Nicolas Heess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05134">
<title>Yes, this is what I was looking for! Towards Multi-modal Medical Consultation Concern Summary Generation. (arXiv:2401.05134v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.05134</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, the use of the Internet for healthcare-related tasks
has grown by leaps and bounds, posing a challenge in effectively managing and
processing information to ensure its efficient utilization. During moments of
emotional turmoil and psychological challenges, we frequently turn to the
internet as our initial source of support, choosing this over discussing our
feelings with others due to the associated social stigma. In this paper, we
propose a new task of multi-modal medical concern summary (MMCS) generation,
which provides a short and precise summary of patients&apos; major concerns brought
up during the consultation. Nonverbal cues, such as patients&apos; gestures and
facial expressions, aid in accurately identifying patients&apos; concerns. Doctors
also consider patients&apos; personal information, such as age and gender, in order
to describe the medical condition appropriately. Motivated by the potential
efficacy of patients&apos; personal context and visual gestures, we propose a
transformer-based multi-task, multi-modal intent-recognition, and medical
concern summary generation (IR-MMCSG) system. Furthermore, we propose a
multitasking framework for intent recognition and medical concern summary
generation for doctor-patient consultations. We construct the first multi-modal
medical concern summary generation (MM-MediConSummation) corpus, which includes
patient-doctor consultations annotated with medical concern summaries, intents,
patient personal information, doctor&apos;s recommendations, and keywords. Our
experiments and analysis demonstrate (a) the significant role of patients&apos;
expressions/gestures and their personal information in intent identification
and medical concern summary generation, and (b) the strong correlation between
intent recognition and patients&apos; medical concern summary generation
&lt;/p&gt;
&lt;p&gt;The dataset and source code are available at https://github.com/NLP-RL/MMCSG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1&quot;&gt;Abhisek Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_S/0/1/0/all/0/1&quot;&gt;Shreyangshu Bera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Sriparna Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1&quot;&gt;Pushpak Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Samrat Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05159">
<title>Derm-T2IM: Harnessing Synthetic Skin Lesion Data via Stable Diffusion Models for Enhanced Skin Disease Classification using ViT and CNN. (arXiv:2401.05159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05159</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the utilization of Dermatoscopic synthetic data generated
through stable diffusion models as a strategy for enhancing the robustness of
machine learning model training. Synthetic data generation plays a pivotal role
in mitigating challenges associated with limited labeled datasets, thereby
facilitating more effective model training. In this context, we aim to
incorporate enhanced data transformation techniques by extending the recent
success of few-shot learning and a small amount of data representation in
text-to-image latent diffusion models. The optimally tuned model is further
used for rendering high-quality skin lesion synthetic data with diverse and
realistic characteristics, providing a valuable supplement and diversity to the
existing training data. We investigate the impact of incorporating newly
generated synthetic data into the training pipeline of state-of-art machine
learning models, assessing its effectiveness in enhancing model performance and
generalization to unseen real-world data. Our experimental results demonstrate
the efficacy of the synthetic data generated through stable diffusion models
helps in improving the robustness and adaptability of end-to-end CNN and vision
transformer models on two different real-world skin lesion datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schukat_M/0/1/0/all/0/1&quot;&gt;Michael Schukat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_M/0/1/0/all/0/1&quot;&gt;Mark A Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05163">
<title>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical visual question answering (VQA) is a challenging multimodal task,
where Vision-Language Pre-training (VLP) models can effectively improve the
generalization performance. However, most methods in the medical field treat
VQA as an answer classification task which is difficult to transfer to
practical application scenarios. Additionally, due to the privacy of medical
images and the expensive annotation process, large-scale medical image-text
pairs datasets for pretraining are severely lacking. In this paper, we propose
a large-scale MultI-task Self-Supervised learning based framework (MISS) for
medical VQA tasks. Unlike existing methods, we treat medical VQA as a
generative task. We unify the text encoder and multimodal encoder and align
image-text features through multi-task learning. Furthermore, we propose a
Transfer-and-Caption method that extends the feature space of single-modal
image datasets using large language models (LLMs), enabling those traditional
medical vision field task data to be applied to VLP. Experiments show that our
method achieves excellent results with fewer multimodal datasets and
demonstrates the advantages of generative VQA models. The code and model
weights will be released upon the paper&apos;s acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05176">
<title>Can ChatGPT Rival Neural Machine Translation? A Comparative Study. (arXiv:2401.05176v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05176</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the increasing interest in leveraging large language models for
translation, this paper evaluates the capabilities of large language models
(LLMs) represented by ChatGPT in comparison to the mainstream neural machine
translation (NMT) engines in translating Chinese diplomatic texts into English.
Specifically, we examine the translation quality of ChatGPT and NMT engines as
measured by four automated metrics and human evaluation based on an
error-typology and six analytic rubrics. Our findings show that automated
metrics yield similar results for ChatGPT under different prompts and NMT
systems, while human annotators tend to assign noticeably higher scores to
ChatGPT when it is provided an example or contextual information about the
translation task. Pairwise correlation between automated metrics and dimensions
of human evaluation produces weak and non-significant results, suggesting the
divergence between the two methods of translation quality assessment. These
findings provide valuable insights into the potential of ChatGPT as a capable
machine translator, and the influence of prompt engineering on its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhaokun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05193">
<title>Experiment Planning with Function Approximation. (arXiv:2401.05193v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.05193</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of experiment planning with function approximation in
contextual bandit problems. In settings where there is a significant overhead
to deploying adaptive algorithms -- for example, when the execution of the data
collection policies is required to be distributed, or a human in the loop is
needed to implement these policies -- producing in advance a set of policies
for data collection is paramount. We study the setting where a large dataset of
contexts but not rewards is available and may be used by the learner to design
an effective data collection strategy. Although when rewards are linear this
problem has been well studied, results are still missing for more complex
reward models. In this work we propose two experiment planning strategies
compatible with function approximation. The first is an eluder planning and
sampling procedure that can recover optimality guarantees depending on the
eluder dimension of the reward function class. For the second, we show that a
uniform sampler achieves competitive optimality rates in the setting where the
number of actions is small. We finalize our results introducing a statistical
gap fleshing out the fundamental differences between planning and adaptive
learning and provide results for planning with model selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1&quot;&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jonathan N. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brunskill_E/0/1/0/all/0/1&quot;&gt;Emma Brunskill&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05194">
<title>Modelling, Positioning, and Deep Reinforcement Learning Path Tracking Control of Scaled Robotic Vehicles: Design and Experimental Validation. (arXiv:2401.05194v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.05194</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile robotic systems are becoming increasingly popular. These systems are
used in various indoor applications, raging from warehousing and manufacturing
to test benches for assessment of advanced control strategies, such as
artificial intelligence (AI)-based control solutions, just to name a few.
Scaled robotic cars are commonly equipped with a hierarchical control
acthiecture that includes tasks dedicated to vehicle state estimation and
control. This paper covers both aspects by proposing (i) a federeted extended
Kalman filter (FEKF), and (ii) a novel deep reinforcement learning (DRL) path
tracking controller trained via an expert demonstrator to expedite the learning
phase and increase robustess to the simulation-to-reality gap. The paper also
presents the formulation of a vehicle model along with an effective yet simple
procedure for identifying tis paramters. The experimentally validated model is
used for (i) supporting the design of the FEKF and (ii) serving as a digital
twin for training the proposed DRL-based path tracking algorithm. Experimental
results confirm the ability of the FEKF to improve the estimate of the mobile
robot&apos;s position. Furthermore, the effectiveness of the DRL path tracking
strateguy is experimentally tested along manoeuvres not considered during
training, showing also the ability of the AI-based solution to outpeform
model-based control strategies and the demonstrator. The comparison with
benchmraking controllers is quantitavely evalueted through a set of key
performance indicators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caponio_C/0/1/0/all/0/1&quot;&gt;Carmine Caponio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stano_P/0/1/0/all/0/1&quot;&gt;Pietro Stano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carli_R/0/1/0/all/0/1&quot;&gt;Raffaele Carli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivieri_I/0/1/0/all/0/1&quot;&gt;Ignazio Olivieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragone_D/0/1/0/all/0/1&quot;&gt;Daniele Ragone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorniotti_A/0/1/0/all/0/1&quot;&gt;Aldo Sorniotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montanaro_U/0/1/0/all/0/1&quot;&gt;Umberto Montanaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05199">
<title>Monte Carlo Tree Search for Recipe Generation using GPT-2. (arXiv:2401.05199v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05199</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic food recipe generation methods provide a creative tool for chefs to
explore and to create new, and interesting culinary delights. Given the recent
success of large language models (LLMs), they have the potential to create new
recipes that can meet individual preferences, dietary constraints, and adapt to
what is in your refrigerator. Existing research on using LLMs to generate
recipes has shown that LLMs can be finetuned to generate realistic-sounding
recipes. However, on close examination, these generated recipes often fail to
meet basic requirements like including chicken as an ingredient in chicken
dishes. In this paper, we propose RecipeMC, a text generation method using
GPT-2 that relies on Monte Carlo Tree Search (MCTS). RecipeMC allows us to
define reward functions to put soft constraints on text generation and thus
improve the credibility of the generated recipes. Our results show that human
evaluators prefer recipes generated with RecipeMC more often than recipes
generated with other baseline methods when compared with real recipes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taneja_K/0/1/0/all/0/1&quot;&gt;Karan Taneja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segal_R/0/1/0/all/0/1&quot;&gt;Richard Segal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_R/0/1/0/all/0/1&quot;&gt;Richard Goodwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05200">
<title>Knowledge Sharing in Manufacturing using Large Language Models: User Evaluation and Model Benchmarking. (arXiv:2401.05200v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.05200</link>
<description rdf:parseType="Literal">&lt;p&gt;Managing knowledge efficiently is crucial for organizational success. In
manufacturing, operating factories has become increasing knowledge-intensive
putting strain on the factory&apos;s capacity to train and support new operators. In
this paper, we introduce a Large Language Model (LLM)-based system designed to
use the extensive knowledge contained in factory documentation. The system aims
to efficiently answer queries from operators and facilitate the sharing of new
knowledge. To assess its effectiveness, we conducted an evaluation in a factory
setting. The results of this evaluation demonstrated the system&apos;s benefits;
namely, in enabling quicker information retrieval and more efficient resolution
of issues. However, the study also highlighted a preference for learning from a
human expert when such an option is available. Furthermore, we benchmarked
several closed and open-sourced LLMs for this system. GPT-4 consistently
outperformed its counterparts, with open-source models like StableBeluga2
trailing closely, presenting an attractive option given its data privacy and
customization benefits. Overall, this work offers preliminary insights for
factories considering using LLM-tools for knowledge management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freire_S/0/1/0/all/0/1&quot;&gt;Samuel Kernan Freire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaofan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foosherian_M/0/1/0/all/0/1&quot;&gt;Mina Foosherian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wellsandt_S/0/1/0/all/0/1&quot;&gt;Stefan Wellsandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_Arenas_S/0/1/0/all/0/1&quot;&gt;Santiago Ruiz-Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niforatos_E/0/1/0/all/0/1&quot;&gt;Evangelos Niforatos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05204">
<title>A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts into a Verbalizer. (arXiv:2401.05204v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05204</link>
<description rdf:parseType="Literal">&lt;p&gt;The verbalizer, which serves to map label words to class labels, is an
essential component of prompt-tuning. In this paper, we present a novel
approach to constructing verbalizers. While existing methods for verbalizer
construction mainly rely on augmenting and refining sets of synonyms or related
words based on class names, this paradigm suffers from a narrow perspective and
lack of abstraction, resulting in limited coverage and high bias in the
label-word space. To address this issue, we propose a label-word construction
process that incorporates scenario-specific concepts. Specifically, we extract
rich concepts from task-specific scenarios as label-word candidates and then
develop a novel cascade calibration module to refine the candidates into a set
of label words for each class. We evaluate the effectiveness of our proposed
approach through extensive experiments on {five} widely used datasets for
zero-shot text classification. The results demonstrate that our method
outperforms existing methods and achieves state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Senlin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1&quot;&gt;Yu-Ming Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05215">
<title>Pre-trained Large Language Models for Financial Sentiment Analysis. (arXiv:2401.05215v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05215</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial sentiment analysis refers to classifying financial text contents
into sentiment categories (e.g. positive, negative, and neutral). In this
paper, we focus on the classification of financial news title, which is a
challenging task due to a lack of large amount of training samples. To overcome
this difficulty, we propose to adapt the pretrained large language models
(LLMs) [1, 2, 3] to solve this problem. The LLMs, which are trained from huge
amount of text corpora,have an advantage in text understanding and can be
effectively adapted to domain-specific task while requiring very few amount of
training samples. In particular, we adapt the open-source Llama2-7B model
(2023) with the supervised fine-tuning (SFT) technique [4]. Experimental
evaluation shows that even with the 7B model (which is relatively small for
LLMs), our approach significantly outperforms the previous state-of-the-art
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Wei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1&quot;&gt;Dihong Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05219">
<title>Distributed Monitoring for Data Distribution Shifts in Edge-ML Fraud Detection. (arXiv:2401.05219v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2401.05219</link>
<description rdf:parseType="Literal">&lt;p&gt;The digital era has seen a marked increase in financial fraud. edge ML
emerged as a promising solution for smartphone payment services fraud
detection, enabling the deployment of ML models directly on edge devices. This
approach enables a more personalized real-time fraud detection. However, a
significant gap in current research is the lack of a robust system for
monitoring data distribution shifts in these distributed edge ML applications.
Our work bridges this gap by introducing a novel open-source framework designed
for continuous monitoring of data distribution shifts on a network of edge
devices. Our system includes an innovative calculation of the
Kolmogorov-Smirnov (KS) test over a distributed network of edge devices,
enabling efficient and accurate monitoring of users behavior shifts. We
comprehensively evaluate the proposed framework employing both real-world and
synthetic financial transaction datasets and demonstrate the framework&apos;s
effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karayanni_N/0/1/0/all/0/1&quot;&gt;Nader Karayanni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahla_R/0/1/0/all/0/1&quot;&gt;Robert J. Shahla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1&quot;&gt;Chieh-Lien Hsiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05224">
<title>Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.05224</link>
<description rdf:parseType="Literal">&lt;p&gt;Aligned text-image encoders such as CLIP have become the de facto model for
vision-language tasks. Furthermore, modality-specific encoders achieve
impressive performances in their respective domains. This raises a central
question: does an alignment exist between uni-modal vision and language
encoders since they fundamentally represent the same physical world? Analyzing
the latent spaces structure of vision and language models on image-caption
benchmarks using the Centered Kernel Alignment (CKA), we find that the
representation spaces of unaligned and aligned encoders are semantically
similar. In the absence of statistical similarity in aligned encoders like
CLIP, we show that a possible matching of unaligned encoders exists without any
training. We frame this as a seeded graph-matching problem exploiting the
semantic similarity between graphs and propose two methods - a Fast Quadratic
Assignment Problem optimization, and a novel localized CKA metric-based
matching/retrieval. We demonstrate the effectiveness of this on several
downstream tasks including cross-lingual, cross-domain caption matching and
image classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniparambil_M/0/1/0/all/0/1&quot;&gt;Mayug Maniparambil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akshulakov_R/0/1/0/all/0/1&quot;&gt;Raiymbek Akshulakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djilali_Y/0/1/0/all/0/1&quot;&gt;Yasser Abdelaziz Dahou Djilali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1&quot;&gt;Sanath Narayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seddik_M/0/1/0/all/0/1&quot;&gt;Mohamed El Amine Seddik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangalam_K/0/1/0/all/0/1&quot;&gt;Karttikeya Mangalam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1&quot;&gt;Noel E. O&amp;#x27;Connor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05251">
<title>ReACT: Reinforcement Learning for Controller Parametrization using B-Spline Geometries. (arXiv:2401.05251v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.05251</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust and performant controllers are essential for industrial applications.
However, deriving controller parameters for complex and nonlinear systems is
challenging and time-consuming. To facilitate automatic controller
parametrization, this work presents a novel approach using deep reinforcement
learning (DRL) with N-dimensional B-spline geometries (BSGs). We focus on the
control of parameter-variant systems, a class of systems with complex behavior
which depends on the operating conditions. For this system class,
gain-scheduling control structures are widely used in applications across
industries due to well-known design principles. Facilitating the expensive
controller parametrization task regarding these control structures, we deploy
an DRL agent. Based on control system observations, the agent autonomously
decides how to adapt the controller parameters. We make the adaptation process
more efficient by introducing BSGs to map the controller parameters which may
depend on numerous operating conditions. To preprocess time-series data and
extract a fixed-length feature vector, we use a long short-term memory (LSTM)
neural networks. Furthermore, this work contributes actor regularizations that
are relevant to real-world environments which differ from training.
Accordingly, we apply dropout layer normalization to the actor and critic
networks of the truncated quantile critic (TQC) algorithm. To show our
approach&apos;s working principle and effectiveness, we train and evaluate the DRL
agent on the parametrization task of an industrial control structure with
parameter lookup tables.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolf_T/0/1/0/all/0/1&quot;&gt;Thomas Rudolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flogel_D/0/1/0/all/0/1&quot;&gt;Daniel Fl&amp;#xf6;gel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schurmann_T/0/1/0/all/0/1&quot;&gt;Tobias Sch&amp;#xfc;rmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suss_S/0/1/0/all/0/1&quot;&gt;Simon S&amp;#xfc;&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_S/0/1/0/all/0/1&quot;&gt;Stefan Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohmann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Hohmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05268">
<title>AUTOACT: Automatic Agent Learning from Scratch via Self-Planning. (arXiv:2401.05268v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05268</link>
<description rdf:parseType="Literal">&lt;p&gt;Language agents have achieved considerable performance on various complex
tasks. Despite the incessant exploration in this field, existing language agent
systems still struggle with costly, non-reproducible data reliance and face the
challenge of compelling a single model for multiple functions. To this end, we
introduce AutoAct, an automatic agent learning framework that does not rely on
large-scale annotated data and synthetic trajectories from closed-source models
(e.g., GPT-4). Given limited data with a tool library, AutoAct first
automatically synthesizes planning trajectories without any assistance from
humans or strong closed-source models. Then, AutoAct leverages a
division-of-labor strategy to automatically differentiate based on the target
task information and synthesized trajectories, producing a sub-agent group to
complete the task. We conduct comprehensive experiments with different LLMs,
which demonstrates that AutoAct yields better or parallel performance compared
to various strong baselines. We even notice that AutoAct, when using the
Llama-2-13b model, can achieve performance comparable to that of the
GPT-3.5-Turbo agent. Code will be available at
https://github.com/zjunlp/AutoAct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Shuofei Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1&quot;&gt;Runnan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yujie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wangchunshu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Eleanor Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengfei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05273">
<title>INACIA: Integrating Large Language Models in Brazilian Audit Courts: Opportunities and Challenges. (arXiv:2401.05273v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05273</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces INACIA (Instru\c{c}\~ao Assistida com Intelig\^encia
Artificial), a groundbreaking system designed to integrate Large Language
Models (LLMs) into the operational framework of Brazilian Federal Court of
Accounts (TCU). The system automates various stages of case analysis, including
basic information extraction, admissibility examination, Periculum in mora and
Fumus boni iuris analyses, and recommendations generation. Through a series of
experiments, we demonstrate INACIA&apos;s potential in extracting relevant
information from case documents, evaluating its legal plausibility, and
generating judicial recommendations. Utilizing a validation dataset alongside
LLMs, our evaluation methodology presents an innovative approach to assessing
system performance, correlating highly with human judgment. The results
highlight INACIA&apos;s proficiency in handling complex legal tasks, indicating its
suitability for augmenting efficiency and judicial fairness within legal
systems. The paper also discusses potential enhancements and future
applications, positioning INACIA as a model for worldwide AI integration in
legal domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1&quot;&gt;Jayr Pereira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assumpcao_A/0/1/0/all/0/1&quot;&gt;Andre Assumpcao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trecenti_J/0/1/0/all/0/1&quot;&gt;Julio Trecenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Airosa_L/0/1/0/all/0/1&quot;&gt;Luiz Airosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lente_C/0/1/0/all/0/1&quot;&gt;Caio Lente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleto_J/0/1/0/all/0/1&quot;&gt;Jhonatan Cl&amp;#xe9;to&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobins_G/0/1/0/all/0/1&quot;&gt;Guilherme Dobins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Nogueira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_L/0/1/0/all/0/1&quot;&gt;Luis Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1&quot;&gt;Roberto Lotufo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05300">
<title>I am a Strange Dataset: Metalinguistic Tests for Language Models. (arXiv:2401.05300v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Statements involving metalinguistic self-reference (&quot;This paper has six
sections.&quot;) are prevalent in many domains. Can large language models (LLMs)
handle such language? In this paper, we present &quot;I am a Strange Dataset&quot;, a new
dataset for addressing this question. There are two subtasks: generation and
verification. In generation, models continue statements like &quot;The penultimate
word in this sentence is&quot; (where a correct continuation is &quot;is&quot;). In
verification, models judge the truth of statements like &quot;The penultimate word
in this sentence is sentence.&quot; (false). We also provide minimally different
metalinguistic non-self-reference examples to complement the main dataset by
probing for whether models can handle metalinguistic language at all. The
dataset is hand-crafted by experts and validated by non-expert annotators. We
test a variety of open-source LLMs (7B to 70B parameters) as well as
closed-source LLMs through APIs. All models perform close to chance across both
subtasks and even on the non-self-referential metalinguistic control data,
though we find some steady improvement with model scale. GPT 4 is the only
model to consistently do significantly better than chance, and it is still only
in the 60% range, while our untrained human annotators score well in the 89-93%
range. The dataset and evaluation toolkit are available at
https://github.com/TristanThrush/i-am-a-strange-dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1&quot;&gt;Tristan Thrush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Jared Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monares_M/0/1/0/all/0/1&quot;&gt;Miguel Monares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1&quot;&gt;Douwe Kiela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05302">
<title>Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?. (arXiv:2401.05302v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.05302</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have shown exceptional generative abilities in various
natural language and generation tasks. However, possible anthropomorphization
and leniency towards failure cases have propelled discussions on emergent
abilities of Large Language Models especially on Theory of Mind (ToM) abilities
in Large Language Models. While several false-belief tests exists to verify the
ability to infer and maintain mental models of another entity, we study a
special application of ToM abilities that has higher stakes and possibly
irreversible consequences : Human Robot Interaction. In this work, we explore
the task of Perceived Behavior Recognition, where a robot employs a Large
Language Model (LLM) to assess the robot&apos;s generated behavior in a manner
similar to human observer. We focus on four behavior types, namely -
explicable, legible, predictable, and obfuscatory behavior which have been
extensively used to synthesize interpretable robot behaviors. The LLMs goal is,
therefore to be a human proxy to the agent, and to answer how a certain agent
behavior would be perceived by the human in the loop, for example &quot;Given a
robot&apos;s behavior X, would the human observer find it explicable?&quot;. We conduct a
human subject study to verify that the users are able to correctly answer such
a question in the curated situations (robot setting and plan) across five
domains. A first analysis of the belief test yields extremely positive results
inflating ones expectations of LLMs possessing ToM abilities. We then propose
and perform a suite of perturbation tests which breaks this illusion, i.e.
Inconsistent Belief, Uninformative Context and Conviction Test. We conclude
that, the high score of LLMs on vanilla prompts showcases its potential use in
HRI settings, however to possess ToM demands invariance to trivial or
irrelevant perturbations in the context which LLMs lack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1&quot;&gt;Mudit Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1&quot;&gt;Siddhant Bhambri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07802">
<title>Improving generalization by mimicking the human visual diet. (arXiv:2206.07802v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07802</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new perspective on bridging the generalization gap between
biological and computer vision -- mimicking the human visual diet. While
computer vision models rely on internet-scraped datasets, humans learn from
limited 3D scenes under diverse real-world transformations with objects in
natural context. Our results demonstrate that incorporating variations and
contextual cues ubiquitous in the human visual training data (visual diet)
significantly improves generalization to real-world transformations such as
lighting, viewpoint, and material changes. This improvement also extends to
generalizing from synthetic to real-world data -- all models trained with a
human-like visual diet outperform specialized architectures by large margins
when tested on natural image data. These experiments are enabled by our two key
contributions: a novel dataset capturing scene context and diverse real-world
transformations to mimic the human visual diet, and a transformer model
tailored to leverage these aspects of the human visual diet. All data and
source code can be accessed at
https://github.com/Spandan-Madan/human_visual_diet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1&quot;&gt;Spandan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1&quot;&gt;Gabriel Kreiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14741">
<title>Cluster-based Sampling in Hindsight Experience Replay for Robotic Tasks (Student Abstract). (arXiv:2208.14741v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14741</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-goal reinforcement learning with a sparse binary reward, training
agents is particularly challenging, due to a lack of successful experiences. To
solve this problem, hindsight experience replay (HER) generates successful
experiences even from unsuccessful ones. However, generating successful
experiences from uniformly sampled ones is not an efficient process. In this
paper, the impact of exploiting the property of achieved goals in generating
successful experiences is investigated and a novel cluster-based sampling
strategy is proposed. The proposed sampling strategy groups episodes with
different achieved goals by using a cluster model and samples experiences in
the manner of HER to create the training batch. The proposed method is
validated by experiments with three robotic control tasks of the OpenAI Gym.
The results of experiments demonstrate that the proposed method is
substantially sample efficient and achieves better performance than baseline
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Har_D/0/1/0/all/0/1&quot;&gt;Dongsoo Har&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01829">
<title>t-SMILES: A Scalable Fragment-based Molecular Representation Framework for De Novo Molecule Generation. (arXiv:2301.01829v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01829</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective representation of molecules is a crucial factor affecting the
performance of artificial intelligence models. This study introduces a
flexible, fragment-based, multiscale molecular representation framework called
t-SMILES (tree-based SMILES) with three code algorithms: TSSA (t-SMILES with
Shared Atom), TSDY (t-SMILES with Dummy Atom) and TSID (t-SMILES with ID). It
describes molecules using SMILES-type strings obtained by performing a
breadth-first search on a full binary tree formed from a fragmented molecular
graph. Systematic evaluations using JTVAE, BRICS, MMPA, and Scaffold show the
feasibility to construct a multi-code molecular description system, where
various descriptions complement each other, enhancing the overall performance.
Additionally, it exhibits impressive performance on low-resource datasets,
whether the model is original, data augmented, or pre-training fine-tuned. It
significantly outperforms classical SMILES, DeepSMILES, SELFIES and baseline
models in goal-directed tasks. Furthermore, it surpasses start-of-the-art
fragment, graph and SMILES based approaches on ChEMBL, Zinc, and QM9.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Juan-Ni Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li-Juan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hai-Long Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ru-Qin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05158">
<title>SemPPL: Predicting pseudo-labels for better contrastive representations. (arXiv:2301.05158v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05158</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from large amounts of unsupervised data and a small amount of
supervision is an important open problem in computer vision. We propose a new
semi-supervised learning method, Semantic Positives via Pseudo-Labels (SemPPL),
that combines labelled and unlabelled data to learn informative
representations. Our method extends self-supervised contrastive learning --
where representations are shaped by distinguishing whether two samples
represent the same underlying datum (positives) or not (negatives) -- with a
novel approach to selecting positives. To enrich the set of positives, we
leverage the few existing ground-truth labels to predict the missing ones
through a $k$-nearest neighbours classifier by using the learned embeddings of
the labelled data. We thus extend the set of positives with datapoints having
the same pseudo-label and call these semantic positives. We jointly learn the
representation and predict bootstrapped pseudo-labels. This creates a
reinforcing cycle. Strong initial representations enable better pseudo-label
predictions which then improve the selection of semantic positives and lead to
even better representations. SemPPL outperforms competing semi-supervised
methods setting new state-of-the-art performance of $68.5\%$ and $76\%$ top-$1$
accuracy when using a ResNet-$50$ and training on $1\%$ and $10\%$ of labels on
ImageNet, respectively. Furthermore, when using selective kernels, SemPPL
significantly outperforms previous state-of-the-art achieving $72.3\%$ and
$78.3\%$ top-$1$ accuracy on ImageNet with $1\%$ and $10\%$ labels,
respectively, which improves absolute $+7.8\%$ and $+6.2\%$ over previous work.
SemPPL also exhibits state-of-the-art performance over larger ResNet models as
well as strong robustness, out-of-distribution and transfer performance. We
release the checkpoints and the evaluation code at
https://github.com/deepmind/semppl .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richemond_P/0/1/0/all/0/1&quot;&gt;Pierre H. Richemond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1&quot;&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strub_F/0/1/0/all/0/1&quot;&gt;Florian Strub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1&quot;&gt;Jacob C. Walker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1&quot;&gt;Felix Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buesing_L/0/1/0/all/0/1&quot;&gt;Lars Holger Buesing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09480">
<title>Cross-Gate MLP with Protein Complex Invariant Embedding is A One-Shot Antibody Designer. (arXiv:2305.09480v5 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09480</link>
<description rdf:parseType="Literal">&lt;p&gt;Antibodies are crucial proteins produced by the immune system in response to
foreign substances or antigens. The specificity of an antibody is determined by
its complementarity-determining regions (CDRs), which are located in the
variable domains of the antibody chains and form the antigen-binding site.
Previous studies have utilized complex techniques to generate CDRs, but they
suffer from inadequate geometric modeling. Moreover, the common iterative
refinement strategies lead to an inefficient inference. In this paper, we
propose a \textit{simple yet effective} model that can co-design 1D sequences
and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple
the antibody CDR design problem into two stages: (i) geometric modeling of
protein complex structures and (ii) sequence-structure co-learning. We develop
a novel macromolecular structure invariant embedding, typically for protein
complexes, that captures both intra- and inter-component interactions among the
backbone atoms, including C$\alpha$, N, C, and O atoms, to achieve
comprehensive geometric modeling. Then, we introduce a simple cross-gate MLP
for sequence-structure co-learning, allowing sequence and structure
representations to implicitly refine each other. This enables our model to
design desired sequences and structures in a one-shot manner. Extensive
experiments are conducted to evaluate our results at both the sequence and
structure levels, which demonstrate that our model achieves superior
performance compared to the state-of-the-art antibody CDR design methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiangbin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bozhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11731">
<title>Persian Typographical Error Type Detection Using Deep Neural Networks on Algorithmically-Generated Misspellings. (arXiv:2305.11731v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11731</link>
<description rdf:parseType="Literal">&lt;p&gt;Spelling correction is a remarkable challenge in the field of natural
language processing. The objective of spelling correction tasks is to recognize
and rectify spelling errors automatically. The development of applications that
can effectually diagnose and correct Persian spelling and grammatical errors
has become more important in order to improve the quality of Persian text. The
Typographical Error Type Detection in Persian is a relatively understudied
area. Therefore, this paper presents a compelling approach for detecting
typographical errors in Persian texts. Our work includes the presentation of a
publicly available dataset called FarsTypo, which comprises 3.4 million words
arranged in chronological order and tagged with their corresponding
part-of-speech. These words cover a wide range of topics and linguistic styles.
We develop an algorithm designed to apply Persian-specific errors to a scalable
portion of these words, resulting in a parallel dataset of correct and
incorrect words. By leveraging FarsTypo, we establish a strong foundation and
conduct a thorough comparison of various methodologies employing different
architectures. Additionally, we introduce a groundbreaking Deep Sequential
Neural Network that utilizes both word and character embeddings, along with
bidirectional LSTM layers, for token classification aimed at detecting
typographical errors across 51 distinct classes. Our approach is contrasted
with highly advanced industrial systems that, unlike this study, have been
developed using a diverse range of resources. The outcomes of our final method
proved to be highly competitive, achieving an accuracy of 97.62%, precision of
98.83%, recall of 98.61%, and surpassing others in terms of speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mohammad Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1&quot;&gt;Heshaam Faili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12520">
<title>SLaDe: A Portable Small Language Model Decompiler for Optimized Assembly. (arXiv:2305.12520v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12520</link>
<description rdf:parseType="Literal">&lt;p&gt;Decompilation is a well-studied area with numerous high-quality tools
available. These are frequently used for security tasks and to port legacy
code. However, they regularly generate difficult-to-read programs and require a
large amount of engineering effort to support new programming languages and
ISAs. Recent interest in neural approaches has produced portable tools that
generate readable code. However, to-date such techniques are usually restricted
to synthetic programs without optimization, and no models have evaluated their
portability. Furthermore, while the code generated may be more readable, it is
usually incorrect. This paper presents SLaDe, a Small Language model Decompiler
based on a sequence-to-sequence transformer trained over real-world code. We
develop a novel tokenizer and exploit no-dropout training to produce
high-quality code. We utilize type-inference to generate programs that are more
readable and accurate than standard analytic and recent neural approaches.
Unlike standard approaches, SLaDe can infer out-of-context types and unlike
neural approaches, it generates correct code. We evaluate SLaDe on over 4,000
functions from AnghaBench on two ISAs and at two optimizations levels. SLaDe is
up to 6 times more accurate than Ghidra, a state-of-the-art,
industrial-strength decompiler and up to 4 times more accurate than the large
language model ChatGPT and generates significantly more readable code than
both.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1&quot;&gt;Jordi Armengol-Estap&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_J/0/1/0/all/0/1&quot;&gt;Jackson Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1&quot;&gt;Chris Cummins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBoyle_M/0/1/0/all/0/1&quot;&gt;Michael F.P. O&amp;#x27;Boyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17100">
<title>BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks. (arXiv:2305.17100v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17100</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional task- and modality-specific artificial intelligence (AI) models
are inflexible in real-world deployment and maintenance for biomedicine. At the
same time, the growing availability of biomedical data, coupled with the
advancements in modern multi-modal multi-task AI techniques, has paved the way
for the emergence of generalist biomedical AI solutions. These solutions hold
the potential to interpret different medical modalities and produce expressive
outputs such as free-text reports or disease diagnosis. Here, we propose
BiomedGPT, the first open-source and generalist visual language AI for diverse
biomedical tasks. BiomedGPT achieved 16 state-of-the-art results across five
clinically significant tasks on 26 datasets. Notably, it outperformed OpenAI&apos;s
GPT-4 with vision (GPT-4V) in radiology human evaluation and surpassed Google&apos;s
Med-PaLM M (12B) in breast cancer diagnosis and medical visual question
answering. Moreover, BiomedGPT facilitates zero-shot transfer learning, greatly
enhancing its utility as a biomedical assistant, similar to ChatGPT. Our method
demonstrates effective training with diverse datasets can lead to more
practical biomedical AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikarla_E/0/1/0/all/0/1&quot;&gt;Eashan Adhikarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhiling Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Lifang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davison_B/0/1/0/all/0/1&quot;&gt;Brian Davison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1&quot;&gt;Sunyang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongfang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02560">
<title>Tensorized Hypergraph Neural Networks. (arXiv:2306.02560v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02560</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypergraph neural networks (HGNN) have recently become attractive and
received significant attention due to their excellent performance in various
domains. However, most existing HGNNs rely on first-order approximations of
hypergraph connectivity patterns, which ignores important high-order
information. To address this issue, we propose a novel adjacency-tensor-based
\textbf{T}ensorized \textbf{H}ypergraph \textbf{N}eural \textbf{N}etwork
(THNN). THNN is a faithful hypergraph modeling framework through high-order
outer product feature message passing and is a natural tensor extension of the
adjacency-matrix-based graph neural networks. The proposed THNN is equivalent
to a high-order polynomial regression scheme, which enables THNN with the
ability to efficiently extract high-order information from uniform hypergraphs.
Moreover, in consideration of the exponential complexity of directly processing
high-order outer product features, we propose using a partially symmetric CP
decomposition approach to reduce model complexity to a linear degree.
Additionally, we propose two simple yet effective extensions of our method for
non-uniform hypergraphs commonly found in real-world applications. Results from
experiments on two widely used {hypergraph datasets for 3-D visual object
classification} show the model&apos;s promising performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Maolin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_Y/0/1/0/all/0/1&quot;&gt;Yaoming Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Ruocheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11565">
<title>HomeRobot: Open-Vocabulary Mobile Manipulation. (arXiv:2306.11565v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11565</link>
<description rdf:parseType="Literal">&lt;p&gt;HomeRobot (noun): An affordable compliant robot that navigates homes and
manipulates a wide range of objects in order to complete everyday tasks.
Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object
in any unseen environment, and placing it in a commanded location. This is a
foundational challenge for robots to be useful assistants in human
environments, because it involves tackling sub-problems from across robotics:
perception, language understanding, navigation, and manipulation are all
essential to OVMM. In addition, integration of the solutions to these
sub-problems poses its own substantial challenges. To drive research in this
area, we introduce the HomeRobot OVMM benchmark, where an agent navigates
household environments to grasp novel objects and place them on target
receptacles. HomeRobot has two components: a simulation component, which uses a
large and diverse curated object set in new, high-quality multi-room home
environments; and a real-world component, providing a software stack for the
low-cost Hello Robot Stretch to encourage replication of real-world experiments
across labs. We implement both reinforcement learning and heuristic
(model-based) baselines and show evidence of sim-to-real transfer. Our
baselines achieve a 20% success rate in the real world; our experiments
identify ways future research work improve performance. See videos on our
website: https://ovmm.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenamandra_S/0/1/0/all/0/1&quot;&gt;Sriram Yenamandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_A/0/1/0/all/0/1&quot;&gt;Arun Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1&quot;&gt;Karmesh Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Austin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_M/0/1/0/all/0/1&quot;&gt;Mukul Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervet_T/0/1/0/all/0/1&quot;&gt;Theophile Gervet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1&quot;&gt;Alexander William Clegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1&quot;&gt;John Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1&quot;&gt;Manolis Savva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Angel Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Chaplot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1&quot;&gt;Chris Paxton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13339">
<title>TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13339</link>
<description rdf:parseType="Literal">&lt;p&gt;Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamic nature of trust, overlook the adverse effects of
trust-related attacks, and cannot provide convincing explanations on evaluation
results. To address these problems, we propose TrustGuard, a GNN-based accurate
trust evaluation model that supports trust dynamicity, is robust against
typical attacks, and provides explanations through visualization. Specifically,
TrustGuard is designed with a layered architecture that contains a snapshot
input layer, a spatial aggregation layer, a temporal aggregation layer, and a
prediction layer. Among them, the spatial aggregation layer adopts a defense
mechanism to robustly aggregate local trust, and the temporal aggregation layer
applies an attention mechanism for effective learning of temporal patterns.
Extensive experiments on two real-world datasets show that TrustGuard
outperforms state-of-the-art GNN-based trust evaluation models with respect to
trust prediction across single-timeslot and multi-timeslot, even in the
presence of attacks. In addition, TrustGuard can explain its evaluation results
by visualizing both spatial and temporal views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jiahe Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1&quot;&gt;Elisa Bertino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1&quot;&gt;Witold Pedrycz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10763">
<title>Actor-agnostic Multi-label Action Recognition with Multi-modal Query. (arXiv:2307.10763v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called &apos;actor-agnostic multi-modal
multi-label action recognition,&apos; which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code is made available at
https://github.com/mondalanindya/MSQNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Anindya Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1&quot;&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07520">
<title>Nonlinearity, Feedback and Uniform Consistency in Causal Structural Learning. (arXiv:2308.07520v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07520</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Causal Discovery is to find automated search methods for learning
causal structures from observational data. In some cases all variables of the
interested causal mechanism are measured, and the task is to predict the
effects one measured variable has on another. In contrast, sometimes the
variables of primary interest are not directly observable but instead inferred
from their manifestations in the data. These are referred to as latent
variables. One commonly known example is the psychological construct of
intelligence, which cannot directly measured so researchers try to assess
through various indicators such as IQ tests. In this case, casual discovery
algorithms can uncover underlying patterns and structures to reveal the causal
connections between the latent variables and between the latent and observed
variables. This thesis focuses on two questions in causal discovery: providing
an alternative definition of k-Triangle Faithfulness that (i) is weaker than
strong faithfulness when applied to the Gaussian family of distributions, (ii)
can be applied to non-Gaussian families of distributions, and (iii) under the
assumption that the modified version of Strong Faithfulness holds, can be used
to show the uniform consistency of a modified causal discovery algorithm;
relaxing the sufficiency assumption to learn causal structures with latent
variables. Given the importance of inferring cause-and-effect relationships for
understanding and forecasting complex systems, the work in this thesis of
relaxing various simplification assumptions is expected to extend the causal
discovery method to be applicable in a wider range with diversified causal
mechanism and statistical phenomena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09737">
<title>RaTrack: Moving Object Detection and Tracking with 4D Radar Point Cloud. (arXiv:2309.09737v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09737</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile autonomy relies on the precise perception of dynamic environments.
Robustly tracking moving objects in 3D world thus plays a pivotal role for
applications like trajectory prediction, obstacle avoidance, and path planning.
While most current methods utilize LiDARs or cameras for Multiple Object
Tracking (MOT), the capabilities of 4D imaging radars remain largely
unexplored. Recognizing the challenges posed by radar noise and point sparsity
in 4D radar data, we introduce RaTrack, an innovative solution tailored for
radar-based tracking. Bypassing the typical reliance on specific object types
and 3D bounding boxes, our method focuses on motion segmentation and
clustering, enriched by a motion estimation module. Evaluated on the
View-of-Delft dataset, RaTrack showcases superior tracking precision of moving
objects, largely surpassing the performance of the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhijun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1&quot;&gt;Hantao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10149">
<title>Analysis of the Memorization and Generalization Capabilities of AI Agents: Are Continual Learners Robust?. (arXiv:2309.10149v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10149</link>
<description rdf:parseType="Literal">&lt;p&gt;In continual learning (CL), an AI agent (e.g., autonomous vehicles or
robotics) learns from non-stationary data streams under dynamic environments.
For the practical deployment of such applications, it is important to guarantee
robustness to unseen environments while maintaining past experiences. In this
paper, a novel CL framework is proposed to achieve robust generalization to
dynamic environments while retaining past knowledge. The considered CL agent
uses a capacity-limited memory to save previously observed environmental
information to mitigate forgetting issues. Then, data points are sampled from
the memory to estimate the distribution of risks over environmental change so
as to obtain predictors that are robust with unseen changes. The generalization
and memorization performance of the proposed framework are theoretically
analyzed. This analysis showcases the tradeoff between memorization and
generalization with the memory size. Experiments show that the proposed
algorithm outperforms memory-based CL baselines across all environments while
significantly improving the generalization performance on unseen target
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10744">
<title>Evaluating large language models&apos; ability to understand metaphor and sarcasm using a screening test for Asperger syndrome. (arXiv:2309.10744v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10744</link>
<description rdf:parseType="Literal">&lt;p&gt;Metaphors and sarcasm are precious fruits of our highly-evolved social
communication skills. However, children with Asperger syndrome are known to
have difficulties in comprehending sarcasm, even if they possess a certain
level of verbal IQ sufficient for understanding metaphors. Given that, a
screening test that scores the ability to understand metaphor and sarcasm has
been used to differentiate Asperger syndrome from other symptoms exhibiting
akin external behaviors (e.g., attention-deficit/hyperactivity disorder). This
study uses the standardized test to examine the capability of recent large
language models (LLMs) in understanding human nuanced communication. The
results divulged that, whereas their ability to comprehend metaphors has been
improved with the increase of the number of model parameters, the improvement
in sarcasm understanding was not observed. This implies that an alternative
approach is imperative to imbue LLMs with the capacity to grasp sarcasm, which
has been associated with the amygdala, a pivotal cerebral region for emotional
learning, in the case of humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakura_H/0/1/0/all/0/1&quot;&gt;Hiromu Yakura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13500">
<title>Enhancing Student Performance Prediction on Learnersourced Questions with SGNN-LLM Synergy. (arXiv:2309.13500v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13500</link>
<description rdf:parseType="Literal">&lt;p&gt;Learnersourcing offers great potential for scalable education through student
content creation. However, predicting student performance on learnersourced
questions, which is essential for personalizing the learning experience, is
challenging due to the inherent noise in student-generated data. Moreover,
while conventional graph-based methods can capture the complex network of
student and question interactions, they often fall short under cold start
conditions where limited student engagement with questions yields sparse data.
To address both challenges, we introduce an innovative strategy that synergizes
the potential of integrating Signed Graph Neural Networks (SGNNs) and Large
Language Model (LLM) embeddings. Our methodology employs a signed bipartite
graph to comprehensively model student answers, complemented by a contrastive
learning framework that enhances noise resilience. Furthermore, LLM&apos;s
contribution lies in generating foundational question embeddings, proving
especially advantageous in addressing cold start scenarios characterized by
limited graph data. Validation across five real-world datasets sourced from the
PeerWise platform underscores our approach&apos;s effectiveness. Our method
outperforms baselines, showcasing enhanced predictive accuracy and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1&quot;&gt;Lin Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xianda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiamou Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02567">
<title>Improving Automatic VQA Evaluation Using Large Language Models. (arXiv:2310.02567v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02567</link>
<description rdf:parseType="Literal">&lt;p&gt;8 years after the visual question answering (VQA) task was proposed, accuracy
remains the primary metric for automatic evaluation. VQA Accuracy has been
effective so far in the IID evaluation setting. However, our community is
undergoing a shift towards open-ended generative models and OOD evaluation. In
this new paradigm, the existing VQA Accuracy metric is overly stringent and
underestimates the performance of VQA systems. Thus, there is a need to develop
more robust automatic VQA metrics that serve as a proxy for human judgment. In
this work, we propose to leverage the in-context learning capabilities of
instruction-tuned large language models (LLMs) to build a better VQA metric. We
formulate VQA evaluation as an answer-rating task where the LLM is instructed
to score the accuracy of a candidate answer given a set of reference answers.
We demonstrate the proposed metric better correlates with human judgment
compared to existing metrics across several VQA models and benchmarks. We hope
wide adoption of our metric will contribute to better estimating the research
progress on the VQA task. We plan to release the evaluation code and collected
human judgments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manas_O/0/1/0/all/0/1&quot;&gt;Oscar Ma&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1&quot;&gt;Benno Krojer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Aishwarya Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05365">
<title>Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05365</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce a method to fine-tune a Transformer-based
generative model for molecular de novo design. Leveraging the superior sequence
learning capacity of Transformers over Recurrent Neural Networks (RNNs), our
model can generate molecular structures with desired properties effectively. In
contrast to the traditional RNN-based models, our proposed method exhibits
superior performance in generating compounds predicted to be active against
various biological targets, capturing long-term dependencies in the molecular
structure sequence. The model&apos;s efficacy is demonstrated across numerous tasks,
including generating analogues to a query structure and producing compounds
with particular attributes, outperforming the baseline RNN-based methods. Our
approach can be used for scaffold hopping, library expansion starting from a
single molecule, and generating compounds with high predicted activity against
biological targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tianfan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laghuvarapu_S/0/1/0/all/0/1&quot;&gt;Siddhartha Laghuvarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15872">
<title>KirchhoffNet: A Circuit Bridging Message Passing and Continuous-Depth Models. (arXiv:2310.15872v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15872</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we exploit a fundamental principle of analog electronic
circuitry, Kirchhoff&apos;s current law, to introduce a unique class of neural
network models that we refer to as KirchhoffNet. KirchhoffNet establishes close
connections with message passing neural networks and continuous-depth networks.
We demonstrate that even in the absence of any traditional layers (such as
convolution, pooling, or linear layers), KirchhoffNet attains 98.86% test
accuracy on the MNIST dataset, comparable with state of the art (SOTA) results.
What makes KirchhoffNet more intriguing is its potential in the realm of
hardware. Contemporary deep neural networks are conventionally deployed on
GPUs. In contrast, KirchhoffNet can be physically realized by an analog
electronic circuit. Moreover, we justify that irrespective of the number of
parameters within a KirchhoffNet, its forward calculation can always be
completed within 1/f seconds, with f representing the hardware&apos;s clock
frequency. This characteristic introduces a promising technology for
implementing ultra-large-scale neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhengqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fan-Keng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boning_D/0/1/0/all/0/1&quot;&gt;Duane S. Boning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04666">
<title>Pre-training LLMs using human-like development data corpus. (arXiv:2311.04666v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Large Language Models (LLMs) have shown success in a diverse set
of language inference and understanding tasks. The pre-training stage of LLMs
looks at a large corpus of raw textual data. The BabyLM shared task compares
LLM pre-training to human language acquisition, where the number of tokens seen
by 13-year-old kids is magnitudes smaller than the number of tokens seen by
LLMs. In this work, we pre-train and evaluate LLMs on their ability to learn
contextual word representations using roughly the same number of tokens as seen
by children. We provide a strong set of baselines; with different
architectures, evaluation of changes in performance across epochs, and reported
pre-training metrics for the strict small and strict tracks of the task. We
also try to loosely replicate the RoBERTa baseline given by the task organizers
to observe the training robustness to hyperparameter selection and
replicability. We provide the submission details to the strict and strict-small
tracks in this report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Khushi Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Raj Sanjay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varma_S/0/1/0/all/0/1&quot;&gt;Sashank Varma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13538">
<title>Speak Like a Native: Prompting Large Language Models in a Native Style. (arXiv:2311.13538v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13538</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) with large language models (LLMs) has become the
modern tools of choice for many natural language processing tasks. However, how
the text style of in-context examples influences the performance of LLMs still
remains under-explored. This paper presents a novel and effective approach,
named \textbf{AlignedCoT}, to improve the reasoning capability of LLMs by
aligning the in-context examples with the native style of LLMs.&apos;&apos;Native&apos;&apos;
refers to the inherent characteristic of LLMs which can be probed by zero-shot
scenarios.AlignedCoT is widely applicable to ICL methods, making it easy to
combine with state-of-the-art techniques to further improve the LLMs&apos;
performance. We conduct extensive and comprehensive experiments on several
benchmarks on mathematical question-answering, common-sense reasoning, and text
understanding. The empirical results demonstrate that our AlignedCoT
significantly improves performance over the carefully handcrafted
demonstrations. Specifically, with AlignedCoT, we observe an average +3.2\%
improvement for \texttt{gpt-3.5-turbo} compared to the carefully handcrafted
CoT on multi-step reasoning benchmarks.Furthermore, we use AlignedCoT to
rewrite the CoT text style in the training set, which improves the performance
of Retrieval Augmented Generation by 3.6\%.The source code and dataset is
available at https://github.com/yangzhch6/AlignedCoT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yinya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jing Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jing Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14115">
<title>A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14115</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator&apos;s implicit preference distribution.
Finally, we discuss and present findings on &quot;annotator misspecification&quot; --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1&quot;&gt;Daniel D. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1&quot;&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1&quot;&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1&quot;&gt;Yann Dauphin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00102">
<title>FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging paradigm for decentralized training of
machine learning models on distributed clients, without revealing the data to
the central server. The learning scheme may be horizontal, vertical or hybrid
(both vertical and horizontal). Most existing research work with deep neural
network (DNN) modelling is focused on horizontal data distributions, while
vertical and hybrid schemes are much less studied. In this paper, we propose a
generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based
learning. The idea of our algorithm is characterised by higher inference
accuracy, stronger privacy-preserving properties, and lower client-server
communication bandwidth demands as compared with existing work. The
experimental results show that FedEmb is an effective method to tackle both
split feature &amp;amp; subject space decentralized problems, shows 0.3% to 4.2%
inference accuracy improvement with limited privacy revealing for datasets
stored in local clients, and reduces 88.9 % time complexity over vertical
baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lele Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03038">
<title>Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03038</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer requires a fixed number of layers and heads which makes them
inflexible to the complexity of individual samples and expensive in training
and inference. To address this, we propose a sample-based Dynamic Hierarchical
Transformer (DHT) model whose layers and heads can be dynamically configured
with single data samples via solving contextual bandit problems. To determine
the number of layers and heads, we use the Uniform Confidence Bound while we
deploy combinatorial Thompson Sampling in order to select specific head
combinations given their number. Different from previous work that focuses on
compressing trained networks for inference only, DHT is not only advantageous
for adaptively optimizing the underlying network architecture during training
but also has a flexible network for efficient inference. To the best of our
knowledge, this is the first comprehensive data-driven dynamic transformer
without any additional auxiliary neural networks that implement the dynamic
system. According to the experiment results, we achieve up to 74% computational
savings for both training and inference with a minimal loss of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lele Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04889">
<title>KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04889</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user&apos;s query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system&apos;s performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haojie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zepeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yaojia Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruiji Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07213">
<title>Human-computer Interaction for Brain-inspired Computing Based on Machine Learning And Deep Learning:A Review. (arXiv:2312.07213v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07213</link>
<description rdf:parseType="Literal">&lt;p&gt;The continuous development of artificial intelligence has a profound impact
on biomedical research and other fields.Brain-inspired computing is an
important intersection of multimodal technology and biomedical field. This
paper presents a comprehensive review of machine learning (ML) and deep
learning (DL) models applied in human-computer interaction for brain-inspired
computing, tracking their evolution, application value, challenges, and
potential research trajectories. First, the basic concepts and development
history are reviewed, and their evolution is divided into two stages: recent
machine learning and current deep learning, emphasizing the importance of each
stage in the research state of human-computer interaction for brain-inspired
computing. In addition, the latest progress and key techniques of deep learning
in different tasks of human-computer interaction for brain-inspired computing
are introduced from six perspectives. Despite significant progress, challenges
remain in making full use of its capabilities. This paper aims to provide a
comprehensive review of human-computer interaction for brain-inspired computing
models based on machine learning and deep learning, highlighting their
potential in various applications and providing a valuable reference for future
academic research. It can be accessed through the following url:
https://github.com/ultracoolHub/brain-inspired-computing
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sibo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingxuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Linzhuang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1&quot;&gt;Liping Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10808">
<title>Non-Euclidean Spatial Graph Neural Network. (arXiv:2312.10808v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10808</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial networks are networks whose graph topology is constrained by their
embedded spatial space. Understanding the coupled spatial-graph properties is
crucial for extracting powerful representations from spatial networks.
Therefore, merely combining individual spatial and network representations
cannot reveal the underlying interaction mechanism of spatial networks.
Besides, existing spatial network representation learning methods can only
consider networks embedded in Euclidean space, and can not well exploit the
rich geometric information carried by irregular and non-uniform non-Euclidean
space. In order to address this issue, in this paper we propose a novel generic
framework to learn the representation of spatial networks that are embedded in
non-Euclidean manifold space. Specifically, a novel message-passing-based
neural network is proposed to combine graph topology and spatial geometry,
where spatial geometry is extracted as messages on the edges. We theoretically
guarantee that the learned representations are provably invariant to important
symmetries such as rotation or translation, and simultaneously maintain
sufficient ability in distinguishing different geometric structures. The
strength of our proposed method is demonstrated through extensive experiments
on both synthetic and real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sirui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingcheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angirekula_A/0/1/0/all/0/1&quot;&gt;Abhinav Angirekula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Allen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12102">
<title>I-CEE: Tailoring Explanations of Image Classification Models to User Expertise. (arXiv:2312.12102v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12102</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively explaining decisions of black-box machine learning models is
critical to responsible deployment of AI systems that rely on them. Recognizing
their importance, the field of explainable AI (XAI) provides several techniques
to generate these explanations. Yet, there is relatively little emphasis on the
user (the explainee) in this growing body of work and most XAI techniques
generate &quot;one-size-fits-all&quot; explanations. To bridge this gap and achieve a
step closer towards human-centered XAI, we present I-CEE, a framework that
provides Image Classification Explanations tailored to User Expertise. Informed
by existing work, I-CEE explains the decisions of image classification models
by providing the user with an informative subset of training data (i.e.,
example images), corresponding local explanations, and model decisions.
However, unlike prior work, I-CEE models the informativeness of the example
images to depend on user expertise, resulting in different examples for
different users. We posit that by tailoring the example set to user expertise,
I-CEE can better facilitate users&apos; understanding and simulatability of the
model. To evaluate our approach, we conduct detailed experiments in both
simulation and with human participants (N = 100) on multiple datasets.
Experiments with simulated users show that I-CEE improves users&apos; ability to
accurately predict the model&apos;s decisions (simulatability) compared to
baselines, providing promising preliminary results. Experiments with human
participants demonstrate that our method significantly improves user
simulatability accuracy, highlighting the importance of human-centered XAI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Yao Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1&quot;&gt;Peizhu Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unhelkar_V/0/1/0/all/0/1&quot;&gt;Vaibhav Unhelkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1&quot;&gt;Enkelejda Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15667">
<title>TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy Gradient. (arXiv:2312.15667v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15667</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Agent Policy Gradient (MAPG) has made significant progress in recent
years. However, centralized critics in state-of-the-art MAPG methods still face
the centralized-decentralized mismatch (CDM) issue, which means sub-optimal
actions by some agents will affect other agent&apos;s policy learning. While using
individual critics for policy updates can avoid this issue, they severely limit
cooperation among agents. To address this issue, we propose an agent topology
framework, which decides whether other agents should be considered in policy
gradient and achieves compromise between facilitating cooperation and
alleviating the CDM issue. The agent topology allows agents to use coalition
utility as learning objective instead of global utility by centralized critics
or local utility by individual critics. To constitute the agent topology,
various models are studied. We propose Topology-based multi-Agent Policy
gradiEnt (TAPE) for both stochastic and deterministic MAPG methods. We prove
the policy improvement theorem for stochastic TAPE and give a theoretical
explanation for the improved cooperation among agents. Experiment results on
several benchmarks show the agent topology is able to facilitate agent
cooperation and alleviate CDM issue respectively to improve performance of
TAPE. Finally, multiple ablation studies and a heuristic graph search algorithm
are devised to show the efficacy of the agent topology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_X/0/1/0/all/0/1&quot;&gt;Xingzhou Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norman_T/0/1/0/all/0/1&quot;&gt;Timothy J. Norman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16805">
<title>DarkShot: Lighting Dark Images with Low-Compute and High-Quality. (arXiv:2312.16805v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16805</link>
<description rdf:parseType="Literal">&lt;p&gt;Nighttime photography encounters escalating challenges in extremely low-light
conditions, primarily attributable to the ultra-low signal-to-noise ratio. For
real-world deployment, a practical solution must not only produce visually
appealing results but also require minimal computation. However, most existing
methods are either focused on improving restoration performance or employ
lightweight models at the cost of quality. This paper proposes a lightweight
network that outperforms existing state-of-the-art (SOTA) methods in low-light
enhancement tasks while minimizing computation. The proposed network
incorporates Siamese Self-Attention Block (SSAB) and Skip-Channel Attention
(SCA) modules, which enhance the model&apos;s capacity to aggregate global
information and are well-suited for high-resolution images. Additionally, based
on our analysis of the low-light image restoration process, we propose a
Two-Stage Framework that achieves superior results. Our model can restore a UHD
4K resolution image with minimal computation while keeping SOTA restoration
quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiazhang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1&quot;&gt;Qiuping Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yangxing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00926">
<title>Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level Feature Fusion for Aiding Diagnosis of Blood Diseases. (arXiv:2401.00926v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients&apos; blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model&apos;s feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Ben Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changmiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianjun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuxing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1&quot;&gt;Feiwei Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yu Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01227">
<title>IdentiFace : A VGG Based Multimodal Facial Biometric System. (arXiv:2401.01227v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there&apos;s always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce &quot;IdentiFace&quot; which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabea_M/0/1/0/all/0/1&quot;&gt;Mahmoud Rabea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_H/0/1/0/all/0/1&quot;&gt;Hanya Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmoud_S/0/1/0/all/0/1&quot;&gt;Sohaila Mahmoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_N/0/1/0/all/0/1&quot;&gt;Nourhan Sayed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02705">
<title>XUAT-Copilot: Multi-Agent Collaborative System for Automated User Acceptance Testing with Large Language Model. (arXiv:2401.02705v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02705</link>
<description rdf:parseType="Literal">&lt;p&gt;In past years, we have been dedicated to automating user acceptance testing
(UAT) process of WeChat Pay, one of the most influential mobile payment
applications in China. A system titled XUAT has been developed for this
purpose. However, there is still a human-labor-intensive stage, i.e, test
scripts generation, in the current system. Therefore, in this paper, we
concentrate on methods of boosting the automation level of the current system,
particularly the stage of test scripts generation. With recent notable
successes, large language models (LLMs) demonstrate significant potential in
attaining human-like intelligence and there has been a growing research area
that employs LLMs as autonomous agents to obtain human-like decision-making
capabilities. Inspired by these works, we propose an LLM-powered multi-agent
collaborative system, named XUAT-Copilot, for automated UAT. The proposed
system mainly consists of three LLM-based agents responsible for action
planning, state checking and parameter selecting, respectively, and two
additional modules for state sensing and case rewriting. The agents interact
with testing device, make human-like decision and generate action command in a
collaborative way. The proposed multi-agent system achieves a close
effectiveness to human testers in our experimental studies and gains a
significant improvement of Pass@1 accuracy compared with single-agent
architecture. More importantly, the proposed system has launched in the formal
testing environment of WeChat Pay mobile app, which saves a considerable amount
of manpower in the daily development work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhitao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zirao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Long Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1&quot;&gt;Can Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Luyang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hanjing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shouzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02987">
<title>Has Your Pretrained Model Improved? A Multi-head Posterior Based Approach. (arXiv:2401.02987v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02987</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of pretrained models has significantly impacted Natural
Language Processing (NLP) and Computer Vision to relational datasets.
Traditionally, these models are assessed through fine-tuned downstream tasks.
However, this raises the question of how to evaluate these models more
efficiently and more effectively. In this study, we explore a novel approach
where we leverage the meta features associated with each entity as a source of
worldly knowledge and employ entity representations from the models. We propose
using the consistency between these representations and the meta features as a
metric for evaluating pretrained models. Our method&apos;s effectiveness is
demonstrated across various domains, including models with relational datasets,
large language models and image models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboagye_P/0/1/0/all/0/1&quot;&gt;Prince Aboagye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_U/0/1/0/all/0/1&quot;&gt;Uday Singh Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xin Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_M/0/1/0/all/0/1&quot;&gt;Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yujie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Shubham Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03154">
<title>Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent multi-target tracking has a wide range of applications, including
wildlife patrolling, security surveillance or environment monitoring. Such
algorithms often make restrictive assumptions: the number of targets and/or
their initial locations may be assumed known, or agents may be pre-assigned to
monitor disjoint partitions of the environment, reducing the burden of
exploration. This also limits applicability when there are fewer agents than
targets, since agents are unable to continuously follow the targets in their
fields of view. Multi-agent tracking algorithms additionally assume inter-agent
synchronization of observations, or the presence of a central controller to
coordinate joint actions. Instead, we focus on the setting of decentralized
multi-agent, multi-target, simultaneous active search-and-tracking with
asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a
sequential monte carlo implementation of the probability hypothesis density
filter for posterior inference combined with Thompson sampling for
decentralized multi-agent decision making. We compare different action
selection policies, focusing on scenarios where targets outnumber agents. In
simulation, we demonstrate that DecSTER is robust to unreliable inter-agent
communication and outperforms information-greedy baselines in terms of the
Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets
and varying teamsizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arundhati Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03160">
<title>Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03160</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant progress in autonomous vehicles (AVs), the development of
driving policies that ensure both the safety of AVs and traffic flow efficiency
has not yet been fully explored. In this paper, we propose an enhanced
human-in-the-loop reinforcement learning method, termed the Human as AI
mentor-based deep reinforcement learning (HAIM-DRL) framework, which
facilitates safe and efficient autonomous driving in mixed traffic platoon.
Drawing inspiration from the human learning process, we first introduce an
innovative learning paradigm that effectively injects human intelligence into
AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves
as a mentor to the AI agent. While allowing the agent to sufficiently explore
uncertain environments, the human expert can take control in dangerous
situations and demonstrate correct actions to avoid potential accidents. On the
other hand, the agent could be guided to minimize traffic flow disturbance,
thereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data
collected from free exploration and partial human demonstrations as its two
training sources. Remarkably, we circumvent the intricate process of manually
designing reward functions; instead, we directly derive proxy state-action
values from partial human demonstrations to guide the agents&apos; policy learning.
Additionally, we employ a minimal intervention technique to reduce the human
mentor&apos;s cognitive load. Comparative results show that HAIM-DRL outperforms
traditional methods in driving safety, sampling efficiency, mitigation of
traffic flow disturbance, and generalizability to unseen traffic scenarios. The
code and demo videos for this paper can be accessed at:
https://zilin-huang.github.io/HAIM-DRL-website/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zihao Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chengyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sikai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03188">
<title>A Survey on Verification and Validation, Testing and Evaluations of Neurosymbolic Artificial Intelligence. (arXiv:2401.03188v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03188</link>
<description rdf:parseType="Literal">&lt;p&gt;Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that
combines the strengths of symbolic AI and sub-symbolic AI. A major drawback of
sub-symbolic AI is that it acts as a &quot;black box&quot;, meaning that predictions are
difficult to explain, making the testing &amp;amp; evaluation (T&amp;amp;E) and validation &amp;amp;
verification (V&amp;amp;V) processes of a system that uses sub-symbolic AI a challenge.
Since neurosymbolic AI combines the advantages of both symbolic and
sub-symbolic AI, this survey explores how neurosymbolic applications can ease
the V&amp;amp;V process. This survey considers two taxonomies of neurosymbolic AI,
evaluates them, and analyzes which algorithms are commonly used as the symbolic
and sub-symbolic components in current applications. Additionally, an overview
of current techniques for the T&amp;amp;E and V&amp;amp;V processes of these components is
provided. Furthermore, it is investigated how the symbolic part is used for T&amp;amp;E
and V&amp;amp;V purposes in current neurosymbolic applications. Our research shows that
neurosymbolic AI as great potential to ease the T&amp;amp;E and V&amp;amp;V processes of
sub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,
the applicability of current T&amp;amp;E and V&amp;amp;V methods to neurosymbolic AI is
assessed, and how different neurosymbolic architectures can impact these
methods is explored. It is found that current T&amp;amp;E and V&amp;amp;V techniques are partly
sufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic
part of neurosymbolic applications independently, while some of them use
approaches where current T&amp;amp;E and V&amp;amp;V methods are not applicable by default, and
adjustments or even new approaches are needed. Our research shows that there is
great potential in using symbolic AI to test, evaluate, verify, or validate the
predictions of a sub-symbolic model, making neurosymbolic AI an interesting
research direction for safe, secure, and trustworthy AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renkhoff_J/0/1/0/all/0/1&quot;&gt;Justus Renkhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Ke Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_Doernberg_M/0/1/0/all/0/1&quot;&gt;Marc Meier-Doernberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1&quot;&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Houbing Herbert Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03397">
<title>Predicting the Skies: A Novel Model for Flight-Level Passenger Traffic Forecasting. (arXiv:2401.03397v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03397</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate prediction of flight-level passenger traffic is of paramount
importance in airline operations, influencing key decisions from pricing to
route optimization. This study introduces a novel, multimodal deep learning
approach to the challenge of predicting flight-level passenger traffic,
yielding substantial accuracy improvements compared to traditional models.
Leveraging an extensive dataset from American Airlines, our model ingests
historical traffic data, fare closure information, and seasonality attributes
specific to each flight. Our proposed neural network integrates the strengths
of Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN),
exploiting the temporal patterns and spatial relationships within the data to
enhance prediction performance. Crucial to the success of our model is a
comprehensive data processing strategy. We construct 3D tensors to represent
data, apply careful masking strategies to mirror real-world dynamics, and
employ data augmentation techniques to enrich the diversity of our training
set. The efficacy of our approach is borne out in the results: our model
demonstrates an approximate 33\% improvement in Mean Squared Error (MSE)
compared to traditional benchmarks. This study, therefore, highlights the
significant potential of deep learning techniques and meticulous data
processing in advancing the field of flight traffic prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_S/0/1/0/all/0/1&quot;&gt;Sina Ehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sergeeva_E/0/1/0/all/0/1&quot;&gt;Elina Sergeeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murdy_W/0/1/0/all/0/1&quot;&gt;Wendy Murdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_B/0/1/0/all/0/1&quot;&gt;Benjamin Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03756">
<title>Adaptive Experimental Design for Policy Learning. (arXiv:2401.03756v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03756</link>
<description rdf:parseType="Literal">&lt;p&gt;Evidence-based targeting has been a topic of growing interest among the
practitioners of policy and business. Formulating decision-maker&apos;s policy
learning as a fixed-budget best arm identification (BAI) problem with
contextual information, we study an optimal adaptive experimental design for
policy learning with multiple treatment arms. In the sampling stage, the
planner assigns treatment arms adaptively over sequentially arriving
experimental units upon observing their contextual information (covariates).
After the experiment, the planner recommends an individualized assignment rule
to the population. Setting the worst-case expected regret as the performance
criterion of adaptive sampling and recommended policies, we derive its
asymptotic lower bounds, and propose a strategy, Adaptive Sampling-Policy
Learning strategy (PLAS), whose leading factor of the regret upper bound aligns
with the lower bound as the size of experimental units increases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okumura_K/0/1/0/all/0/1&quot;&gt;Kyohei Okumura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishihara_T/0/1/0/all/0/1&quot;&gt;Takuya Ishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitagawa_T/0/1/0/all/0/1&quot;&gt;Toru Kitagawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04130">
<title>Plug-and-Play Transformer Modules for Test-Time Adaptation. (arXiv:2401.04130v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04130</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient tuning (PET) methods such as LoRA, Adapter, and Visual
Prompt Tuning (VPT) have found success in enabling adaptation to new domains by
tuning small modules within a transformer model. However, the number of domains
encountered during test time can be very large, and the data is usually
unlabeled. Thus, adaptation to new domains is challenging; it is also
impractical to generate customized tuned modules for each such domain. Toward
addressing these challenges, this work introduces PLUTO: a Plug-and-pLay
modUlar Test-time domain adaptatiOn strategy. We pre-train a large set of
modules, each specialized for different source domains, effectively creating a
``module store&apos;&apos;. Given a target domain with few-shot unlabeled data, we
introduce an unsupervised test-time adaptation (TTA) method to (1) select a
sparse subset of relevant modules from this store and (2) create a weighted
combination of selected modules without tuning their weights. This
plug-and-play nature enables us to harness multiple most-relevant source
domains in a single inference call. Comprehensive evaluations demonstrate that
PLUTO uniformly outperforms alternative TTA methods and that selecting $\leq$5
modules suffice to extract most of the benefit. At a high level, our method
equips pre-trained transformers with the capability to dynamically adapt to new
domains, motivating a new paradigm for efficient and scalable domain
adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Sk Miraj Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1&quot;&gt;Srikanth V. Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_B/0/1/0/all/0/1&quot;&gt;Basak Guler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swami_A/0/1/0/all/0/1&quot;&gt;Ananthram Swami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1&quot;&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04336">
<title>Deep Efficient Private Neighbor Generation for Subgraph Federated Learning. (arXiv:2401.04336v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04336</link>
<description rdf:parseType="Literal">&lt;p&gt;Behemoth graphs are often fragmented and separately stored by multiple data
owners as distributed subgraphs in many realistic applications. Without harming
data privacy, it is natural to consider the subgraph federated learning
(subgraph FL) scenario, where each local client holds a subgraph of the entire
global graph, to obtain globally generalized graph mining models. To overcome
the unique challenge of incomplete information propagation on local subgraphs
due to missing cross-subgraph neighbors, previous works resort to the
augmentation of local neighborhoods through the joint FL of missing neighbor
generators and GNNs. Yet their technical designs have profound limitations
regarding the utility, efficiency, and privacy goals of FL. In this work, we
propose FedDEP to comprehensively tackle these challenges in subgraph FL.
FedDEP consists of a series of novel technical designs: (1) Deep neighbor
generation through leveraging the GNN embeddings of potential missing
neighbors; (2) Efficient pseudo-FL for neighbor generation through embedding
prototyping; and (3) Privacy protection through noise-less
edge-local-differential-privacy. We analyze the correctness and efficiency of
FedDEP, and provide theoretical guarantees on its privacy. Empirical results on
four real-world datasets justify the clear benefits of proposed techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yiu_S/0/1/0/all/0/1&quot;&gt;Siu Ming Yiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Carl Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04620">
<title>Agent Alignment in Evolving Social Norms. (arXiv:2401.04620v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04620</link>
<description rdf:parseType="Literal">&lt;p&gt;Agents based on Large Language Models (LLMs) are increasingly permeating
various domains of human production and life, highlighting the importance of
aligning them with human values. The current alignment of AI systems primarily
focuses on passively aligning LLMs through human intervention. However, agents
possess characteristics like receiving environmental feedback and
self-evolution, rendering the LLM alignment methods inadequate. In response, we
propose an evolutionary framework for agent evolution and alignment, named
EvolutionaryAgent, which transforms agent alignment into a process of evolution
and selection under the principle of survival of the fittest. In an environment
where social norms continuously evolve, agents better adapted to the current
social norms will have a higher probability of survival and proliferation,
while those inadequately aligned dwindle over time. Experimental results
assessing the agents from multiple perspectives in aligning with social norms
demonstrate that EvolutionaryAgent possesses the capability to align
progressively better with the evolving social norms while maintaining its
proficiency in general tasks. Effectiveness tests conducted on various open and
closed-source LLMs as the foundation for agents also prove the applicability of
our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tianxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04679">
<title>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04679</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1&quot;&gt;Mahdi Nikdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1&quot;&gt;Soroush Tabesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06067">
<title>Attributes Grouping and Mining Hashing for Fine-Grained Image Retrieval. (arXiv:2311.06067v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.06067</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, hashing methods have been popular in the large-scale media
search for low storage and strong representation capabilities. To describe
objects with similar overall appearance but subtle differences, more and more
studies focus on hashing-based fine-grained image retrieval. Existing hashing
networks usually generate both local and global features through attention
guidance on the same deep activation tensor, which limits the diversity of
feature representations. To handle this limitation, we substitute convolutional
descriptors for attention-guided features and propose an Attributes Grouping
and Mining Hashing (AGMH), which groups and embeds the category-specific visual
attributes in multiple descriptors to generate a comprehensive feature
representation for efficient fine-grained image retrieval. Specifically, an
Attention Dispersion Loss (ADL) is designed to force the descriptors to attend
to various local regions and capture diverse subtle details. Moreover, we
propose a Stepwise Interactive External Attention (SIEA) to mine critical
attributes in each descriptor and construct correlations between fine-grained
attributes and objects. The attention mechanism is dedicated to learning
discrete attributes, which will not cost additional computations in hash codes
generation. Finally, the compact binary codes are learned by preserving
pairwise similarities. Experimental results demonstrate that AGMH consistently
yields the best performance against state-of-the-art methods on fine-grained
benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shikun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yichao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17286">
<title>Comparative study of clustering models for multivariate time series from connected medical devices. (arXiv:2312.17286v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.17286</link>
<description rdf:parseType="Literal">&lt;p&gt;In healthcare, patient data is often collected as multivariate time series,
providing a comprehensive view of a patient&apos;s health status over time. While
this data can be sparse, connected devices may enhance its frequency. The goal
is to create patient profiles from these time series. In the absence of labels,
a predictive model can be used to predict future values while forming a latent
cluster space, evaluated based on predictive performance. We compare two models
on Withing&apos;s datasets, M AGMAC LUST which clusters entire time series and
DGM${}^2$ which allows the group affiliation of an individual to change over
time (dynamic clustering).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courrier_V/0/1/0/all/0/1&quot;&gt;Violaine Courrier&lt;/a&gt; (MODAL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biernacki_C/0/1/0/all/0/1&quot;&gt;Christophe Biernacki&lt;/a&gt; (MODAL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preda_C/0/1/0/all/0/1&quot;&gt;Cristian Preda&lt;/a&gt; (MODAL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vittrant_B/0/1/0/all/0/1&quot;&gt;Benjamin Vittrant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03991">
<title>Advancing Spatial Reasoning in Large Language Models: An In-Depth Evaluation and Enhancement Using the StepGame Benchmark. (arXiv:2401.03991v1 [cs.AI] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.03991</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has made remarkable progress across various
domains, with large language models like ChatGPT gaining substantial attention
for their human-like text-generation capabilities. Despite these achievements,
spatial reasoning remains a significant challenge for these models. Benchmarks
like StepGame evaluate AI spatial reasoning, where ChatGPT has shown
unsatisfactory performance. However, the presence of template errors in the
benchmark has an impact on the evaluation results. Thus there is potential for
ChatGPT to perform better if these template errors are addressed, leading to
more accurate assessments of its spatial reasoning capabilities. In this study,
we refine the StepGame benchmark, providing a more accurate dataset for model
evaluation. We analyze GPT&apos;s spatial reasoning performance on the rectified
benchmark, identifying proficiency in mapping natural language text to spatial
relations but limitations in multi-hop reasoning. We provide a flawless
solution to the benchmark by combining template-to-relation mapping with
logic-based reasoning. This combination demonstrates proficiency in performing
qualitative reasoning on StepGame without encountering any errors. We then
address the limitations of GPT models in spatial reasoning. We deploy
Chain-of-thought and Tree-of-thoughts prompting strategies, offering insights
into GPT&apos;s ``cognitive process&quot;, and achieving remarkable improvements in
accuracy. Our investigation not only sheds light on model deficiencies but also
proposes enhancements, contributing to the advancement of AI with more robust
spatial reasoning capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogg_D/0/1/0/all/0/1&quot;&gt;David C. Hogg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohn_A/0/1/0/all/0/1&quot;&gt;Anthony G. Cohn&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>