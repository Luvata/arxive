<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07315" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.07611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.10629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06841" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.07129">
<title>Ontologies for increasing the FAIRness of plant research data. (arXiv:2309.07129v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2309.07129</link>
<description rdf:parseType="Literal">&lt;p&gt;The importance of improving the FAIRness (findability, accessibility,
interoperability, reusability) of research data is undeniable, especially in
the face of large, complex datasets currently being produced by omics
technologies. Facilitating the integration of a dataset with other types of
data increases the likelihood of reuse, and the potential of answering novel
research questions. Ontologies are a useful tool for semantically tagging
datasets as adding relevant metadata increases the understanding of how data
was produced and increases its interoperability. Ontologies provide concepts
for a particular domain as well as the relationships between concepts. By
tagging data with ontology terms, data becomes both human and machine
interpretable, allowing for increased reuse and interoperability. However, the
task of identifying ontologies relevant to a particular research domain or
technology is challenging, especially within the diverse realm of fundamental
plant research. In this review, we outline the ontologies most relevant to the
fundamental plant sciences and how they can be used to annotate data related to
plant-specific experiments within metadata frameworks, such as
Investigation-Study-Assay (ISA). We also outline repositories and platforms
most useful for identifying applicable ontologies or finding ontology terms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumschott_K/0/1/0/all/0/1&quot;&gt;Kathryn Dumschott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorpholz_H/0/1/0/all/0/1&quot;&gt;Hannah D&amp;#xf6;rpholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laporte_M/0/1/0/all/0/1&quot;&gt;Marie-Ang&amp;#xe9;lique Laporte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brilhaus_D/0/1/0/all/0/1&quot;&gt;Dominik Brilhaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrader_A/0/1/0/all/0/1&quot;&gt;Andrea Schrader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usadel_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Usadel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_S/0/1/0/all/0/1&quot;&gt;Steffen Neumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaud_E/0/1/0/all/0/1&quot;&gt;Elizabeth Arnaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kranz_A/0/1/0/all/0/1&quot;&gt;Angela Kranz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07136">
<title>Masked Transformer for Electrocardiogram Classification. (arXiv:2309.07136v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07136</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiogram (ECG) is one of the most important diagnostic tools in
clinical applications. With the advent of advanced algorithms, various deep
learning models have been adopted for ECG tasks. However, the potential of
Transformers for ECG data is not yet realized, despite their widespread success
in computer vision and natural language processing. In this work, we present a
useful masked Transformer method for ECG classification referred to as MTECG,
which expands the application of masked autoencoders to ECG time series. We
construct a dataset comprising 220,251 ECG recordings with a broad range of
diagnoses annoated by medical experts to explore the properties of MTECG. Under
the proposed training strategies, a lightweight model with 5.7M parameters
performs stably well on a broad range of masking ratios (5%-75%). The ablation
studies highlight the importance of fluctuated reconstruction targets, training
schedule length, layer-wise LR decay and DropPath rate. The experiments on both
private and public ECG datasets demonstrate that MTECG-T significantly
outperforms the recent state-of-the-art algorithms in ECG classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Ya Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xiaolin Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yanni Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaohan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07141">
<title>Design of Recognition and Evaluation System for Table Tennis Players&apos; Motor Skills Based on Artificial Intelligence. (arXiv:2309.07141v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07141</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of electronic science and technology, the research
on wearable devices is constantly updated, but for now, it is not comprehensive
for wearable devices to recognize and analyze the movement of specific sports.
Based on this, this paper improves wearable devices of table tennis sport, and
realizes the pattern recognition and evaluation of table tennis players&apos; motor
skills through artificial intelligence. Firstly, a device is designed to
collect the movement information of table tennis players and the actual
movement data is processed. Secondly, a sliding window is made to divide the
collected motion data into a characteristic database of six table tennis
benchmark movements. Thirdly, motion features were constructed based on feature
engineering, and motor skills were identified for different models after
dimensionality reduction. Finally, the hierarchical evaluation system of motor
skills is established with the loss functions of different evaluation indexes.
The results show that in the recognition of table tennis players&apos; motor skills,
the feature-based BP neural network proposed in this paper has higher
recognition accuracy and stronger generalization ability than the traditional
convolutional neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhuo-yong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Ye-tao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Ke-xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Ding-han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Long-meng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07145">
<title>ETP: Learning Transferable ECG Representations via ECG-Text Pre-training. (arXiv:2309.07145v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07145</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of cardiovascular healthcare, the Electrocardiogram (ECG)
serves as a critical, non-invasive diagnostic tool. Although recent strides in
self-supervised learning (SSL) have been promising for ECG representation
learning, these techniques often require annotated samples and struggle with
classes not present in the fine-tuning stages. To address these limitations, we
introduce ECG-Text Pre-training (ETP), an innovative framework designed to
learn cross-modal representations that link ECG signals with textual reports.
For the first time, this framework leverages the zero-shot classification task
in the ECG domain. ETP employs an ECG encoder along with a pre-trained language
model to align ECG signals with their corresponding textual reports. The
proposed framework excels in both linear evaluation and zero-shot
classification tasks, as demonstrated on the PTB-XL and CPSC2018 datasets,
showcasing its ability for robust and generalizable cross-modal ECG feature
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sibo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arcucci_R/0/1/0/all/0/1&quot;&gt;Rossella Arcucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07149">
<title>Decoding visual brain representations from electroencephalography through Knowledge Distillation and latent diffusion models. (arXiv:2309.07149v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07149</link>
<description rdf:parseType="Literal">&lt;p&gt;Decoding visual representations from human brain activity has emerged as a
thriving research domain, particularly in the context of brain-computer
interfaces. Our study presents an innovative method that employs to classify
and reconstruct images from the ImageNet dataset using electroencephalography
(EEG) data from subjects that had viewed the images themselves (i.e. &quot;brain
decoding&quot;). We analyzed EEG recordings from 6 participants, each exposed to 50
images spanning 40 unique semantic categories. These EEG readings were
converted into spectrograms, which were then used to train a convolutional
neural network (CNN), integrated with a knowledge distillation procedure based
on a pre-trained Contrastive Language-Image Pre-Training (CLIP)-based image
classification teacher network. This strategy allowed our model to attain a
top-5 accuracy of 80%, significantly outperforming a standard CNN and various
RNN-based benchmarks. Additionally, we incorporated an image reconstruction
mechanism based on pre-trained latent diffusion models, which allowed us to
generate an estimate of the images which had elicited EEG activity. Therefore,
our architecture not only decodes images from neural activity but also offers a
credible image reconstruction from EEG only, paving the way for e.g. swift,
individualized feedback experiments. Our research represents a significant step
forward in connecting neural signals with visual cognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferrante_M/0/1/0/all/0/1&quot;&gt;Matteo Ferrante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boccato_T/0/1/0/all/0/1&quot;&gt;Tommaso Boccato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bargione_S/0/1/0/all/0/1&quot;&gt;Stefano Bargione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Toschi_N/0/1/0/all/0/1&quot;&gt;Nicola Toschi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07153">
<title>Finding Influencers in Complex Networks: An Effective Deep Reinforcement Learning Approach. (arXiv:2309.07153v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2309.07153</link>
<description rdf:parseType="Literal">&lt;p&gt;Maximizing influences in complex networks is a practically important but
computationally challenging task for social network analysis, due to its NP-
hard nature. Most current approximation or heuristic methods either require
tremendous human design efforts or achieve unsatisfying balances between
effectiveness and efficiency. Recent machine learning attempts only focus on
speed but lack performance enhancement. In this paper, different from previous
attempts, we propose an effective deep reinforcement learning model that
achieves superior performances over traditional best influence maximization
algorithms. Specifically, we design an end-to-end learning framework that
combines graph neural network as the encoder and reinforcement learning as the
decoder, named DREIM. Trough extensive training on small synthetic graphs,
DREIM outperforms the state-of-the-art baseline methods on very large synthetic
and real-world networks on solution quality, and we also empirically show its
linear scalability with regard to the network size, which demonstrates its
superiority in solving this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Changjun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07154">
<title>Recall-driven Precision Refinement: Unveiling Accurate Fall Detection using LSTM. (arXiv:2309.07154v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07154</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an innovative approach to address the pressing concern of
fall incidents among the elderly by developing an accurate fall detection
system. Our proposed system combines state-of-the-art technologies, including
accelerometer and gyroscope sensors, with deep learning models, specifically
Long Short-Term Memory (LSTM) networks. Real-time execution capabilities are
achieved through the integration of Raspberry Pi hardware. We introduce pruning
techniques that strategically fine-tune the LSTM model&apos;s architecture and
parameters to optimize the system&apos;s performance. We prioritize recall over
precision, aiming to accurately identify falls and minimize false negatives for
timely intervention. Extensive experimentation and meticulous evaluation
demonstrate remarkable performance metrics, emphasizing a high recall rate
while maintaining a specificity of 96\%. Our research culminates in a
state-of-the-art fall detection system that promptly sends notifications,
ensuring vulnerable individuals receive timely assistance and improve their
overall well-being. Applying LSTM models and incorporating pruning techniques
represent a significant advancement in fall detection technology, offering an
effective and reliable fall prevention and intervention solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mondal_R/0/1/0/all/0/1&quot;&gt;Rishabh Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosal_P/0/1/0/all/0/1&quot;&gt;Prasun Ghosal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07164">
<title>Hybrid ASR for Resource-Constrained Robots: HMM - Deep Learning Fusion. (arXiv:2309.07164v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2309.07164</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel hybrid Automatic Speech Recognition (ASR) system
designed specifically for resource-constrained robots. The proposed approach
combines Hidden Markov Models (HMMs) with deep learning models and leverages
socket programming to distribute processing tasks effectively. In this
architecture, the HMM-based processing takes place within the robot, while a
separate PC handles the deep learning model. This synergy between HMMs and deep
learning enhances speech recognition accuracy significantly. We conducted
experiments across various robotic platforms, demonstrating real-time and
precise speech recognition capabilities. Notably, the system exhibits
adaptability to changing acoustic conditions and compatibility with low-power
hardware, making it highly effective in environments with limited computational
resources. This hybrid ASR paradigm opens up promising possibilities for
seamless human-robot interaction. In conclusion, our research introduces a
pioneering dimension to ASR techniques tailored for robotics. By employing
socket programming to distribute processing tasks across distinct devices and
strategically combining HMMs with deep learning models, our hybrid ASR system
showcases its potential to enable robots to comprehend and respond to spoken
language adeptly, even in environments with restricted computational resources.
This paradigm sets a innovative course for enhancing human-robot interaction
across a wide range of real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ranjan_A/0/1/0/all/0/1&quot;&gt;Anshul Ranjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jegadeesan_K/0/1/0/all/0/1&quot;&gt;Kaushik Jegadeesan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07168">
<title>Goal Space Abstraction in Hierarchical Reinforcement Learning via Reachability Analysis. (arXiv:2309.07168v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07168</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-ended learning benefits immensely from the use of symbolic methods for
goal representation as they offer ways to structure knowledge for efficient and
transferable learning. However, the existing Hierarchical Reinforcement
Learning (HRL) approaches relying on symbolic reasoning are often limited as
they require a manual goal representation. The challenge in autonomously
discovering a symbolic goal representation is that it must preserve critical
information, such as the environment dynamics. In this work, we propose a
developmental mechanism for subgoal discovery via an emergent representation
that abstracts (i.e., groups together) sets of environment states that have
similar roles in the task. We create a HRL algorithm that gradually learns this
representation along with the policies and evaluate it on navigation tasks to
show the learned representation is interpretable and results in data
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1&quot;&gt;Mehdi Zadem&lt;/a&gt; (LIX, U2IS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1&quot;&gt;Sergio Mover&lt;/a&gt; (LIX), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Sao Mai Nguyen&lt;/a&gt; (U2IS, Flowers, IMT Atlantique - INFO, Lab-STICC_RAMBO)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07172">
<title>Exploring Large Language Models for Ontology Alignment. (arXiv:2309.07172v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the applicability of recent generative Large Language
Models (LLMs), such as the GPT series and Flan-T5, to ontology alignment for
identifying concept equivalence mappings across ontologies. To test the
zero-shot performance of Flan-T5-XXL and GPT-3.5-turbo, we leverage challenging
subsets from two equivalence matching datasets of the OAEI Bio-ML track, taking
into account concept labels and structural contexts. Preliminary findings
suggest that LLMs have the potential to outperform existing ontology alignment
systems like BERTMap, given careful framework and prompt design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1&quot;&gt;Ian Horrocks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07174">
<title>HurriCast: An Automatic Framework Using Machine Learning and Statistical Modeling for Hurricane Forecasting. (arXiv:2309.07174v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07174</link>
<description rdf:parseType="Literal">&lt;p&gt;Hurricanes present major challenges in the U.S. due to their devastating
impacts. Mitigating these risks is important, and the insurance industry is
central in this effort, using intricate statistical models for risk assessment.
However, these models often neglect key temporal and spatial hurricane patterns
and are limited by data scarcity. This study introduces a refined approach
combining the ARIMA model and K-MEANS to better capture hurricane trends, and
an Autoencoder for enhanced hurricane simulations. Our experiments show that
this hybrid methodology effectively simulate historical hurricane behaviors
while providing detailed projections of potential future trajectories and
intensities. Moreover, by leveraging a comprehensive yet selective dataset, our
simulations enrich the current understanding of hurricane patterns and offer
actionable insights for risk management strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shouwei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Meiyan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuepeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wenqian Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07178">
<title>CloudBrain-NMR: An Intelligent Cloud Computing Platform for NMR Spectroscopy Processing, Reconstruction and Analysis. (arXiv:2309.07178v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2309.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclear Magnetic Resonance (NMR) spectroscopy has served as a powerful
analytical tool for studying molecular structure and dynamics in chemistry and
biology. However, the processing of raw data acquired from NMR spectrometers
and subsequent quantitative analysis involves various specialized tools, which
necessitates comprehensive knowledge in programming and NMR. Particularly, the
emerging deep learning tools is hard to be widely used in NMR due to the
sophisticated setup of computation. Thus, NMR processing is not an easy task
for chemist and biologists. In this work, we present CloudBrain-NMR, an
intelligent online cloud computing platform designed for NMR data reading,
processing, reconstruction, and quantitative analysis. The platform is
conveniently accessed through a web browser, eliminating the need for any
program installation on the user side. CloudBrain-NMR uses parallel computing
with graphics processing units and central processing units, resulting in
significantly shortened computation time. Furthermore, it incorporates
state-of-the-art deep learning-based algorithms offering comprehensive
functionalities that allow users to complete the entire processing procedure
without relying on additional software. This platform has empowered NMR
applications with advanced artificial intelligence processing. CloudBrain-NMR
is openly accessible for free usage at https://csrc.xmu.edu.cn/CloudBrain.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sijin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhangren Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tianyu Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Liubin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Donghai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hong_Q/0/1/0/all/0/1&quot;&gt;Qing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Meijin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yanqin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07185">
<title>A Health Monitoring System Based on Flexible Triboelectric Sensors for Intelligence Medical Internet of Things and its Applications in Virtual Reality. (arXiv:2309.07185v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07185</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of Medical Things (IoMT) is a platform that combines Internet of
Things (IoT) technology with medical applications, enabling the realization of
precision medicine, intelligent healthcare, and telemedicine in the era of
digitalization and intelligence. However, the IoMT faces various challenges,
including sustainable power supply, human adaptability of sensors and the
intelligence of sensors. In this study, we designed a robust and intelligent
IoMT system through the synergistic integration of flexible wearable
triboelectric sensors and deep learning-assisted data analytics. We embedded
four triboelectric sensors into a wristband to detect and analyze limb
movements in patients suffering from Parkinson&apos;s Disease (PD). By further
integrating deep learning-assisted data analytics, we actualized an intelligent
healthcare monitoring system for the surveillance and interaction of PD
patients, which includes location/trajectory tracking, heart monitoring and
identity recognition. This innovative approach enabled us to accurately capture
and scrutinize the subtle movements and fine motor of PD patients, thus
providing insightful feedback and comprehensive assessment of the patients
conditions. This monitoring system is cost-effective, easily fabricated, highly
sensitive, and intelligent, consequently underscores the immense potential of
human body sensing technology in a Health 4.0 society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Junqi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Puen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Hongbo Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Liuyang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiqiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ban_D/0/1/0/all/0/1&quot;&gt;Dayan Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haiwu Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07188">
<title>Predicting Survival Time of Ball Bearings in the Presence of Censoring. (arXiv:2309.07188v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2309.07188</link>
<description rdf:parseType="Literal">&lt;p&gt;Ball bearings find widespread use in various manufacturing and mechanical
domains, and methods based on machine learning have been widely adopted in the
field to monitor wear and spot defects before they lead to failures. Few
studies, however, have addressed the problem of censored data, in which failure
is not observed. In this paper, we propose a novel approach to predict the time
to failure in ball bearings using survival analysis. First, we analyze bearing
data in the frequency domain and annotate when a bearing fails by comparing the
Kullback-Leibler divergence and the standard deviation between its break-in
frequency bins and its break-out frequency bins. Second, we train several
survival models to estimate the time to failure based on the annotated data and
covariates extracted from the time domain, such as skewness, kurtosis and
entropy. The models give a probabilistic prediction of risk over time and allow
us to compare the survival function between groups of bearings. We demonstrate
our approach on the XJTU and PRONOSTIA datasets. On XJTU, the best result is a
0.70 concordance-index and 0.21 integrated Brier score. On PRONOSTIA, the best
is a 0.76 concordance-index and 0.19 integrated Brier score. Our work motivates
further work on incorporating censored data in models for predictive
maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lillelund_C/0/1/0/all/0/1&quot;&gt;Christian Marius Lillelund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pannullo_F/0/1/0/all/0/1&quot;&gt;Fernando Pannullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jakobsen_M/0/1/0/all/0/1&quot;&gt;Morten Opprud Jakobsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pedersen_C/0/1/0/all/0/1&quot;&gt;Christian Fischer Pedersen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07196">
<title>Attention-based Dynamic Graph Convolutional Recurrent Neural Network for Traffic Flow Prediction in Highway Transportation. (arXiv:2309.07196v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07196</link>
<description rdf:parseType="Literal">&lt;p&gt;As one of the important tools for spatial feature extraction, graph
convolution has been applied in a wide range of fields such as traffic flow
prediction. However, current popular works of graph convolution cannot
guarantee spatio-temporal consistency in a long period. The ignorance of
correlational dynamics, convolutional locality and temporal comprehensiveness
would limit predictive accuracy. In this paper, a novel Attention-based Dynamic
Graph Convolutional Recurrent Neural Network (ADGCRNN) is proposed to improve
traffic flow prediction in highway transportation. Three temporal resolutions
of data sequence are effectively integrated by self-attention to extract
characteristics; multi-dynamic graphs and their weights are dynamically created
to compliantly combine the varying characteristics; a dedicated gated kernel
emphasizing highly relative nodes is introduced on these complete graphs to
reduce overfitting for graph convolution operations. Experiments on two public
datasets show our work better than state-of-the-art baselines, and case studies
of a real Web system prove practical benefit in highway transportation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianpu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Weilong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_M/0/1/0/all/0/1&quot;&gt;Mengda Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07200">
<title>Latent Representation and Simulation of Markov Processes via Time-Lagged Information Bottleneck. (arXiv:2309.07200v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov processes are widely used mathematical models for describing dynamic
systems in various fields. However, accurately simulating large-scale systems
at long time scales is computationally expensive due to the short time steps
required for accurate integration. In this paper, we introduce an inference
process that maps complex systems into a simplified representational space and
models large jumps in time. To achieve this, we propose Time-lagged Information
Bottleneck (T-IB), a principled objective rooted in information theory, which
aims to capture relevant temporal features while discarding high-frequency
information to simplify the simulation task and minimize the inference error.
Our experiments demonstrate that T-IB learns information-optimal
representations for accurately modeling the statistical properties and dynamics
of the original process at a selected time lag, outperforming existing
time-lagged dimensionality reduction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Federici_M/0/1/0/all/0/1&quot;&gt;Marco Federici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forre_P/0/1/0/all/0/1&quot;&gt;Patrick Forr&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomioka_R/0/1/0/all/0/1&quot;&gt;Ryota Tomioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeling_B/0/1/0/all/0/1&quot;&gt;Bastiaan S. Veeling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07235">
<title>Autotuning Apache TVM-based Scientific Applications Using Bayesian Optimization. (arXiv:2309.07235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07235</link>
<description rdf:parseType="Literal">&lt;p&gt;Apache TVM (Tensor Virtual Machine), an open source machine learning compiler
framework designed to optimize computations across various hardware platforms,
provides an opportunity to improve the performance of dense matrix
factorizations such as LU (Lower Upper) decomposition and Cholesky
decomposition on GPUs and AI (Artificial Intelligence) accelerators. In this
paper, we propose a new TVM autotuning framework using Bayesian Optimization
and use the TVM tensor expression language to implement linear algebra kernels
such as LU, Cholesky, and 3mm. We use these scientific computation kernels to
evaluate the effectiveness of our methods on a GPU cluster, called Swing, at
Argonne National Laboratory. We compare the proposed autotuning framework with
the TVM autotuning framework AutoTVM with four tuners and find that our
framework outperforms AutoTVM in most cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingfu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paramasivam_P/0/1/0/all/0/1&quot;&gt;Praveen Paramasivam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_V/0/1/0/all/0/1&quot;&gt;Valerie Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07265">
<title>Safe and Accelerated Deep Reinforcement Learning-based O-RAN Slicing: A Hybrid Transfer Learning Approach. (arXiv:2309.07265v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2309.07265</link>
<description rdf:parseType="Literal">&lt;p&gt;The open radio access network (O-RAN) architecture supports intelligent
network control algorithms as one of its core capabilities. Data-driven
applications incorporate such algorithms to optimize radio access network (RAN)
functions via RAN intelligent controllers (RICs). Deep reinforcement learning
(DRL) algorithms are among the main approaches adopted in the O-RAN literature
to solve dynamic radio resource management problems. However, despite the
benefits introduced by the O-RAN RICs, the practical adoption of DRL algorithms
in real network deployments falls behind. This is primarily due to the slow
convergence and unstable performance exhibited by DRL agents upon deployment
and when facing previously unseen network conditions. In this paper, we address
these challenges by proposing transfer learning (TL) as a core component of the
training and deployment workflows for the DRL-based closed-loop control of
O-RAN functionalities. To this end, we propose and design a hybrid TL-aided
approach that leverages the advantages of both policy reuse and distillation TL
methods to provide safe and accelerated convergence in DRL-based O-RAN slicing.
We conduct a thorough experiment that accommodates multiple services, including
real VR gaming traffic to reflect practical scenarios of O-RAN slicing. We also
propose and implement policy reuse and distillation-aided DRL and non-TL-aided
DRL as three separate baselines. The proposed hybrid approach shows at least:
7.7% and 20.7% improvements in the average initial reward value and the
percentage of converged scenarios, and a 64.6% decrease in reward variance
while maintaining fast convergence and enhancing the generalizability compared
with the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagib_A/0/1/0/all/0/1&quot;&gt;Ahmad M. Nagib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abou_Zeid_H/0/1/0/all/0/1&quot;&gt;Hatem Abou-Zeid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanein_H/0/1/0/all/0/1&quot;&gt;Hossam S. Hassanein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07276">
<title>Language-Conditioned Observation Models for Visual Object Search. (arXiv:2309.07276v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07276</link>
<description rdf:parseType="Literal">&lt;p&gt;Object search is a challenging task because when given complex language
descriptions (e.g., &quot;find the white cup on the table&quot;), the robot must move its
camera through the environment and recognize the described object. Previous
works map language descriptions to a set of fixed object detectors with
predetermined noise models, but these approaches are challenging to scale
because new detectors need to be made for each object. In this work, we bridge
the gap in realistic object search by posing the search problem as a partially
observable Markov decision process (POMDP) where the object detector and visual
sensor noise in the observation model is determined by a single Deep Neural
Network conditioned on complex language descriptions. We incorporate the neural
network&apos;s outputs into our language-conditioned observation model (LCOM) to
represent dynamically changing sensor noise. With an LCOM, any language
description of an object can be used to generate an appropriate object detector
and noise model, and training an LCOM only requires readily available
supervised image-caption datasets. We empirically evaluate our method by
comparing against a state-of-the-art object search algorithm in simulation, and
demonstrate that planning with our observation model yields a significantly
higher average task completion rate (from 0.46 to 0.66) and more efficient and
quicker object search than with a fixed-noise model. We demonstrate our method
on a Boston Dynamics Spot robot, enabling it to handle complex natural language
object descriptions and efficiently find objects in a room-scale environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hrosinkov_V/0/1/0/all/0/1&quot;&gt;Vladislav Hrosinkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosen_E/0/1/0/all/0/1&quot;&gt;Eric Rosen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1&quot;&gt;Stefanie Tellex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07314">
<title>AudioSR: Versatile Audio Super-resolution at Scale. (arXiv:2309.07314v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.07314</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio super-resolution is a fundamental task that predicts high-frequency
components for low-resolution audio, enhancing audio quality in digital
applications. Previous methods have limitations such as the limited scope of
audio types (e.g., music, speech) and specific bandwidth settings they can
handle (e.g., 4kHz to 8kHz). In this paper, we introduce a diffusion-based
generative model, AudioSR, that is capable of performing robust audio
super-resolution on versatile audio types, including sound effects, music, and
speech. Specifically, AudioSR can upsample any input audio signal within the
bandwidth range of 2kHz to 16kHz to a high-resolution audio signal at 24kHz
bandwidth with a sampling rate of 48kHz. Extensive objective evaluation on
various audio super-resolution benchmarks demonstrates the strong result
achieved by the proposed model. In addition, our subjective evaluation shows
that AudioSR can acts as a plug-and-play module to enhance the generation
quality of a wide range of audio generative models, including AudioLDM,
Fastspeech2, and MusicGen. Our code and demo are available at
https://audioldm.github.io/audiosr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qiao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenwu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1&quot;&gt;Mark D. Plumbley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07315">
<title>Traveling Words: A Geometric Interpretation of Transformers. (arXiv:2309.07315v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07315</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have significantly advanced the field of natural language
processing, but comprehending their internal mechanisms remains a challenge. In
this paper, we introduce a novel geometric perspective that elucidates the
inner mechanisms of transformer operations. Our primary contribution is
illustrating how layer normalization confines the latent features to a
hyper-sphere, subsequently enabling attention to mold the semantic
representation of words on this surface. This geometric viewpoint seamlessly
connects established properties such as iterative refinement and contextual
embeddings. We validate our insights by probing a pre-trained 124M parameter
GPT-2 model. Our findings reveal clear query-key attention patterns in early
layers and build upon prior observations regarding the subject-specific nature
of attention heads at deeper layers. Harnessing these geometric insights, we
present an intuitive understanding of transformers, depicting them as processes
that model the trajectory of word particles along the hyper-sphere.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_R/0/1/0/all/0/1&quot;&gt;Raul Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07332">
<title>Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining. (arXiv:2309.07332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07332</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately labeling biomedical data presents a challenge. Traditional
semi-supervised learning methods often under-utilize available unlabeled data.
To address this, we propose a novel reliability-based training data cleaning
method employing inductive conformal prediction (ICP). This method capitalizes
on a small set of accurately labeled training data and leverages ICP-calculated
reliability metrics to rectify mislabeled data and outliers within vast
quantities of noisy training data. The efficacy of the method is validated
across three classification tasks within distinct modalities: filtering
drug-induced-liver-injury (DILI) literature with title and abstract, predicting
ICU admission of COVID-19 patients through CT radiomics and electronic health
records, and subtyping breast cancer using RNA-sequencing data. Varying levels
of noise to the training labels were introduced through label permutation.
Results show significant enhancements in classification performance: accuracy
enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC
enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and
accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing
experiments (up to 74.6% and 89.0%). Our method offers the potential to
substantially boost classification performance in multi-modal biomedical
machine learning tasks. Importantly, it accomplishes this without necessitating
an excessive volume of meticulously curated training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xianghao Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qinmei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yuanning Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1&quot;&gt;Olivier Gevaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07334">
<title>Learning from Auxiliary Sources in Argumentative Revision Classification. (arXiv:2309.07334v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07334</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop models to classify desirable reasoning revisions in argumentative
writing. We explore two approaches -- multi-task learning and transfer learning
-- to take advantage of auxiliary sources of revision data for similar tasks.
Results of intrinsic and extrinsic evaluations show that both approaches can
indeed improve classifier performance over baselines. While multi-task learning
shows that training on different sources of data at the same time may improve
performance, transfer-learning better represents the relationship between the
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afrin_T/0/1/0/all/0/1&quot;&gt;Tazin Afrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1&quot;&gt;Diane Litman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07339">
<title>Efficient quantum recurrent reinforcement learning via quantum reservoir computing. (arXiv:2309.07339v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2309.07339</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum reinforcement learning (QRL) has emerged as a framework to solve
sequential decision-making tasks, showcasing empirical quantum advantages. A
notable development is through quantum recurrent neural networks (QRNNs) for
memory-intensive tasks such as partially observable environments. However, QRL
models incorporating QRNN encounter challenges such as inefficient training of
QRL with QRNN, given that the computation of gradients in QRNN is both
computationally expensive and time-consuming. This work presents a novel
approach to address this challenge by constructing QRL agents utilizing
QRNN-based reservoirs, specifically employing quantum long short-term memory
(QLSTM). QLSTM parameters are randomly initialized and fixed without training.
The model is trained using the asynchronous advantage actor-aritic (A3C)
algorithm. Through numerical simulations, we validate the efficacy of our
QLSTM-Reservoir RL framework. Its performance is assessed on standard
benchmarks, demonstrating comparable results to a fully trained QLSTM RL model
with identical architecture and training settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Samuel Yen-Chi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07364">
<title>Hodge-Aware Contrastive Learning. (arXiv:2309.07364v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07364</link>
<description rdf:parseType="Literal">&lt;p&gt;Simplicial complexes prove effective in modeling data with multiway
dependencies, such as data defined along the edges of networks or within other
higher-order structures. Their spectrum can be decomposed into three
interpretable subspaces via the Hodge decomposition, resulting foundational in
numerous applications. We leverage this decomposition to develop a contrastive
self-supervised learning approach for processing simplicial data and generating
embeddings that encapsulate specific spectral information.Specifically, we
encode the pertinent data invariances through simplicial neural networks and
devise augmentations that yield positive contrastive examples with suitable
spectral properties for downstream tasks. Additionally, we reweight the
significance of negative examples in the contrastive loss, considering the
similarity of their Hodge components to the anchor. By encouraging a stronger
separation among less similar instances, we obtain an embedding space that
reflects the spectral properties of the data. The numerical results on two
standard edge flow classification tasks show a superior performance even when
compared to supervised learning techniques. Our findings underscore the
importance of adopting a spectral perspective for contrastive learning with
higher-order data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollers_A/0/1/0/all/0/1&quot;&gt;Alexander M&amp;#xf6;llers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1&quot;&gt;Alexander Immer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1&quot;&gt;Elvin Isufi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07367">
<title>The kernel-balanced equation for deep neural networks. (arXiv:2309.07367v1 [cond-mat.dis-nn])</title>
<link>http://arxiv.org/abs/2309.07367</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown many fruitful applications in this decade. A
network can get the generalized function through training with a finite
dataset. The degree of generalization is a realization of the proximity scale
in the data space. Specifically, the scale is not clear if the dataset is
complicated. Here we consider a network for the distribution estimation of the
dataset. We show the estimation is unstable and the instability depends on the
data density and training duration. We derive the kernel-balanced equation,
which gives a short phenomenological description of the solution. The equation
tells us the reason for the instability and the mechanism of the scale. The
network outputs a local average of the dataset as a prediction and the scale of
averaging is determined along the equation. The scale gradually decreases along
training and finally results in instability in our case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Nakazato_K/0/1/0/all/0/1&quot;&gt;Kenichi Nakazato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07390">
<title>Unleashing the Power of Depth and Pose Estimation Neural Networks by Designing Compatible Endoscopic Images. (arXiv:2309.07390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07390</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have witnessed depth and pose estimation framework on
unannotated datasets as a effective pathway to succeed in endoscopic
navigation. Most current techniques are dedicated to developing more advanced
neural networks to improve the accuracy. However, existing methods ignore the
special properties of endoscopic images, resulting in an inability to fully
unleash the power of neural networks. In this study, we conduct a detail
analysis of the properties of endoscopic images and improve the compatibility
of images and neural networks, to unleash the power of current neural networks.
First, we introcude the Mask Image Modelling (MIM) module, which inputs partial
image information instead of complete image information, allowing the network
to recover global information from partial pixel information. This enhances the
network&apos; s ability to perceive global information and alleviates the phenomenon
of local overfitting in convolutional neural networks due to local artifacts.
Second, we propose a lightweight neural network to enhance the endoscopic
images, to explicitly improve the compatibility between images and neural
networks. Extensive experiments are conducted on the three public datasets and
one inhouse dataset, and the proposed modules improve baselines by a large
margin. Furthermore, the enhanced images we proposed, which have higher network
compatibility, can serve as an effective data augmentation method and they are
able to extract more stable feature points in traditional feature point
matching tasks and achieve outstanding performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yun Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07396">
<title>DebCSE: Rethinking Unsupervised Contrastive Sentence Embedding Learning in the Debiasing Perspective. (arXiv:2309.07396v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07396</link>
<description rdf:parseType="Literal">&lt;p&gt;Several prior studies have suggested that word frequency biases can cause the
Bert model to learn indistinguishable sentence embeddings. Contrastive learning
schemes such as SimCSE and ConSERT have already been adopted successfully in
unsupervised sentence embedding to improve the quality of embeddings by
reducing this bias. However, these methods still introduce new biases such as
sentence length bias and false negative sample bias, that hinders model&apos;s
ability to learn more fine-grained semantics. In this paper, we reexamine the
challenges of contrastive sentence embedding learning from a debiasing
perspective and argue that effectively eliminating the influence of various
biases is crucial for learning high-quality sentence embeddings. We think all
those biases are introduced by simple rules for constructing training data in
contrastive learning and the key for contrastive learning sentence embedding is
to mimic the distribution of training data in supervised machine learning in
unsupervised way. We propose a novel contrastive framework for sentence
embedding, termed DebCSE, which can eliminate the impact of these biases by an
inverse propensity weighted sampling method to select high-quality positive and
negative pairs according to both the surface and semantic similarity between
sentences. Extensive experiments on semantic textual similarity (STS)
benchmarks reveal that DebCSE significantly outperforms the latest
state-of-the-art models with an average Spearman&apos;s correlation coefficient of
80.33% on BERTbase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_P/0/1/0/all/0/1&quot;&gt;Pu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zeyao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junlin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07398">
<title>Semantic Adversarial Attacks via Diffusion Models. (arXiv:2309.07398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07398</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional adversarial attacks concentrate on manipulating clean examples in
the pixel space by adding adversarial perturbations. By contrast, semantic
adversarial attacks focus on changing semantic attributes of clean examples,
such as color, context, and features, which are more feasible in the real
world. In this paper, we propose a framework to quickly generate a semantic
adversarial attack by leveraging recent diffusion models since semantic
information is included in the latent space of well-trained diffusion models.
Then there are two variants of this framework: 1) the Semantic Transformation
(ST) approach fine-tunes the latent space of the generated image and/or the
diffusion model itself; 2) the Latent Masking (LM) approach masks the latent
space with another target image and local backpropagation-based interpretation
methods. Additionally, the ST approach can be applied in either white-box or
black-box settings. Extensive experiments are conducted on CelebA-HQ and AFHQ
datasets, and our framework demonstrates great fidelity, generalizability, and
transferability compared to other baselines. Our approaches achieve
approximately 100% attack success rate in multiple settings with the best FID
as 36.61. Code is available at
https://github.com/steven202/semantic_adv_via_dm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1&quot;&gt;Edward Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamm_M/0/1/0/all/0/1&quot;&gt;Matthew Stamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07401">
<title>Multi-Grade Deep Learning for Partial Differential Equations with Applications to the Burgers Equation. (arXiv:2309.07401v1 [math.NA])</title>
<link>http://arxiv.org/abs/2309.07401</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop in this paper a multi-grade deep learning method for solving
nonlinear partial differential equations (PDEs). Deep neural networks (DNNs)
have received super performance in solving PDEs in addition to their
outstanding success in areas such as natural language processing, computer
vision, and robotics. However, training a very deep network is often a
challenging task. As the number of layers of a DNN increases, solving a
large-scale non-convex optimization problem that results in the DNN solution of
PDEs becomes more and more difficult, which may lead to a decrease rather than
an increase in predictive accuracy. To overcome this challenge, we propose a
two-stage multi-grade deep learning (TS-MGDL) method that breaks down the task
of learning a DNN into several neural networks stacked on top of each other in
a staircase-like manner. This approach allows us to mitigate the complexity of
solving the non-convex optimization problem with large number of parameters and
learn residual components left over from previous grades efficiently. We prove
that each grade/stage of the proposed TS-MGDL method can reduce the value of
the loss function and further validate this fact through numerical experiments.
Although the proposed method is applicable to general PDEs, implementation in
this paper focuses only on the 1D, 2D, and 3D viscous Burgers equations.
Experimental results show that the proposed two-stage multi-grade deep learning
method enables efficient learning of solutions of the equations and outperforms
existing single-grade deep learning methods in predictive accuracy.
Specifically, the predictive errors of the single-grade deep learning are
larger than those of the TS-MGDL method in 26-60, 4-31 and 3-12 times, for the
1D, 2D, and 3D equations, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuesheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Taishan Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07405">
<title>FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec. (arXiv:2309.07405v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.07405</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents FunCodec, a fundamental neural speech codec toolkit,
which is an extension of the open-source speech processing toolkit FunASR.
FunCodec provides reproducible training recipes and inference scripts for the
latest neural speech codec models, such as SoundStream and Encodec. Thanks to
the unified design with FunASR, FunCodec can be easily integrated into
downstream tasks, such as speech recognition. Along with FunCodec, pre-trained
models are also provided, which can be used for academic or generalized
purposes. Based on the toolkit, we further propose the frequency-domain codec
models, FreqCodec, which can achieve comparable speech quality with much lower
computation and parameter complexity. Experimental results show that, under the
same compression ratio, FunCodec can achieve better reconstruction quality
compared with other toolkits and released models. We also demonstrate that the
pre-trained models are suitable for downstream tasks, including automatic
speech recognition and personalized text-to-speech synthesis. This toolkit is
publicly available at https://github.com/alibaba-damo-academy/FunCodec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zhihao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Siqi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07415">
<title>Client-side Gradient Inversion Against Federated Learning from Poisoning. (arXiv:2309.07415v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.07415</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) enables distributed participants (e.g., mobile
devices) to train a global model without sharing data directly to a central
server. Recent studies have revealed that FL is vulnerable to gradient
inversion attack (GIA), which aims to reconstruct the original training samples
and poses high risk against the privacy of clients in FL. However, most
existing GIAs necessitate control over the server and rely on strong prior
knowledge including batch normalization and data distribution information. In
this work, we propose Client-side poisoning Gradient Inversion (CGI), which is
a novel attack method that can be launched from clients. For the first time, we
show the feasibility of a client-side adversary with limited knowledge being
able to recover the training samples from the aggregated global model. We take
a distinct approach in which the adversary utilizes a malicious model that
amplifies the loss of a specific targeted class of interest. When honest
clients employ the poisoned global model, the gradients of samples belonging to
the targeted class are magnified, making them the dominant factor in the
aggregated update. This enables the adversary to effectively reconstruct the
private input belonging to other clients using the aggregated update. In
addition, our CGI also features its ability to remain stealthy against
Byzantine-robust aggregation rules (AGRs). By optimizing malicious updates and
blending benign updates with a malicious replacement vector, our method remains
undetected by these defense mechanisms. To evaluate the performance of CGI, we
conduct experiments on various benchmark datasets, considering representative
Byzantine-robust AGRs, and exploring diverse FL settings with different levels
of adversary knowledge about the data. Our results demonstrate that CGI
consistently and successfully extracts training input in all tested scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1&quot;&gt;Kok-Leong Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yang Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07425">
<title>JSMNet Improving Indoor Point Cloud Semantic and Instance Segmentation through Self-Attention and Multiscale. (arXiv:2309.07425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07425</link>
<description rdf:parseType="Literal">&lt;p&gt;The semantic understanding of indoor 3D point cloud data is crucial for a
range of subsequent applications, including indoor service robots, navigation
systems, and digital twin engineering. Global features are crucial for
achieving high-quality semantic and instance segmentation of indoor point
clouds, as they provide essential long-range context information. To this end,
we propose JSMNet, which combines a multi-layer network with a global feature
self-attention module to jointly segment three-dimensional point cloud
semantics and instances. To better express the characteristics of indoor
targets, we have designed a multi-resolution feature adaptive fusion module
that takes into account the differences in point cloud density caused by
varying scanner distances from the target. Additionally, we propose a framework
for joint semantic and instance segmentation by integrating semantic and
instance features to achieve superior results. We conduct experiments on S3DIS,
which is a large three-dimensional indoor point cloud dataset. Our proposed
method is compared against other methods, and the results show that it
outperforms existing methods in semantic and instance segmentation and provides
better results in target local area segmentation. Specifically, our proposed
method outperforms PointNet (Qi et al., 2017a) by 16.0% and 26.3% in terms of
semantic segmentation mIoU in S3DIS (Area 5) and instance segmentation mPre,
respectively. Additionally, it surpasses ASIS (Wang et al., 2019) by 6.0% and
4.6%, respectively, as well as JSPNet (Chen et al., 2022) by a margin of 3.3%
for semantic segmentation mIoU and a slight improvement of 0.3% for instance
segmentation mPre.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuochen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenxin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07429">
<title>Semantic Parsing in Limited Resource Conditions. (arXiv:2309.07429v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07429</link>
<description rdf:parseType="Literal">&lt;p&gt;This thesis explores challenges in semantic parsing, specifically focusing on
scenarios with limited data and computational resources. It offers solutions
using techniques like automatic data curation, knowledge transfer, active
learning, and continual learning.
&lt;/p&gt;
&lt;p&gt;For tasks with no parallel training data, the thesis proposes generating
synthetic training examples from structured database schemas. When there is
abundant data in a source domain but limited parallel data in a target domain,
knowledge from the source is leveraged to improve parsing in the target domain.
&lt;/p&gt;
&lt;p&gt;For multilingual situations with limited data in the target languages, the
thesis introduces a method to adapt parsers using a limited human translation
budget. Active learning is applied to select source-language samples for manual
translation, maximizing parser performance in the target language. In addition,
an alternative method is also proposed to utilize machine translation services,
supplemented by human-translated data, to train a more effective parser.
&lt;/p&gt;
&lt;p&gt;When computational resources are limited, a continual learning approach is
introduced to minimize training time and computational memory. This maintains
the parser&apos;s efficiency in previously learned tasks while adapting it to new
tasks, mitigating the problem of catastrophic forgetting.
&lt;/p&gt;
&lt;p&gt;Overall, the thesis provides a comprehensive set of methods to improve
semantic parsing in resource-constrained conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07438">
<title>Towards Artificial General Intelligence (AGI) in the Internet of Things (IoT): Opportunities and Challenges. (arXiv:2309.07438v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.07438</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial General Intelligence (AGI), possessing the capacity to comprehend,
learn, and execute tasks with human cognitive abilities, engenders significant
anticipation and intrigue across scientific, commercial, and societal arenas.
This fascination extends particularly to the Internet of Things (IoT), a
landscape characterized by the interconnection of countless devices, sensors,
and systems, collectively gathering and sharing data to enable intelligent
decision-making and automation. This research embarks on an exploration of the
opportunities and challenges towards achieving AGI in the context of the IoT.
Specifically, it starts by outlining the fundamental principles of IoT and the
critical role of Artificial Intelligence (AI) in IoT systems. Subsequently, it
delves into AGI fundamentals, culminating in the formulation of a conceptual
framework for AGI&apos;s seamless integration within IoT. The application spectrum
for AGI-infused IoT is broad, encompassing domains ranging from smart grids,
residential environments, manufacturing, and transportation to environmental
monitoring, agriculture, healthcare, and education. However, adapting AGI to
resource-constrained IoT settings necessitates dedicated research efforts.
Furthermore, the paper addresses constraints imposed by limited computing
resources, intricacies associated with large-scale IoT communication, as well
as the critical concerns pertaining to security and privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_F/0/1/0/all/0/1&quot;&gt;Fei Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Geng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1&quot;&gt;Wei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haijian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_L/0/1/0/all/0/1&quot;&gt;Le Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guoyu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1&quot;&gt;Gengchen Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenjiao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaochen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xianqiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1&quot;&gt;Lilong Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hongyue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yunli Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Wenzhan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07461">
<title>Detecting Unknown Attacks in IoT Environments: An Open Set Classifier for Enhanced Network Intrusion Detection. (arXiv:2309.07461v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.07461</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread integration of Internet of Things (IoT) devices across all
facets of life has ushered in an era of interconnectedness, creating new
avenues for cybersecurity challenges and underscoring the need for robust
intrusion detection systems. However, traditional security systems are designed
with a closed-world perspective and often face challenges in dealing with the
ever-evolving threat landscape, where new and unfamiliar attacks are constantly
emerging. In this paper, we introduce a framework aimed at mitigating the open
set recognition (OSR) problem in the realm of Network Intrusion Detection
Systems (NIDS) tailored for IoT environments. Our framework capitalizes on
image-based representations of packet-level data, extracting spatial and
temporal patterns from network traffic. Additionally, we integrate stacking and
sub-clustering techniques, enabling the identification of unknown attacks by
effectively modeling the complex and diverse nature of benign behavior. The
empirical results prominently underscore the framework&apos;s efficacy, boasting an
impressive 88\% detection rate for previously unseen attacks when compared
against existing approaches and recent advancements. Future work will perform
extensive experimentation across various openness levels and attack scenarios,
further strengthening the adaptability and performance of our proposed solution
in safeguarding IoT environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrukh_Y/0/1/0/all/0/1&quot;&gt;Yasir Ali Farrukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wali_S/0/1/0/all/0/1&quot;&gt;Syed Wali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Irfan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_N/0/1/0/all/0/1&quot;&gt;Nathaniel D. Bastian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07473">
<title>Where2Explore: Few-shot Affordance Learning for Unseen Novel Categories of Articulated Objects. (arXiv:2309.07473v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07473</link>
<description rdf:parseType="Literal">&lt;p&gt;Articulated object manipulation is a fundamental yet challenging task in
robotics. Due to significant geometric and semantic variations across object
categories, previous manipulation models struggle to generalize to novel
categories. Few-shot learning is a promising solution for alleviating this
issue by allowing robots to perform a few interactions with unseen objects.
However, extant approaches often necessitate costly and inefficient test-time
interactions with each unseen instance. Recognizing this limitation, we observe
that despite their distinct shapes, different categories often share similar
local geometries essential for manipulation, such as pullable handles and
graspable edges - a factor typically underutilized in previous few-shot
learning works. To harness this commonality, we introduce &apos;Where2Explore&apos;, an
affordance learning framework that effectively explores novel categories with
minimal interactions on a limited number of instances. Our framework explicitly
estimates the geometric similarity across different categories, identifying
local areas that differ from shapes in the training categories for efficient
exploration while concurrently transferring affordance knowledge to similar
parts of the objects. Extensive experiments in simulated and real-world
environments demonstrate our framework&apos;s capacity for efficient few-shot
exploration and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haoran Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1&quot;&gt;Kaichun Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07495">
<title>HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods. (arXiv:2309.07495v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07495</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking Face Generation (TFG) aims to reconstruct facial movements to achieve
high natural lip movements from audio and facial features that are under
potential connections. Existing TFG methods have made significant advancements
to produce natural and realistic images. However, most work rarely takes visual
quality into consideration. It is challenging to ensure lip synchronization
while avoiding visual quality degradation in cross-modal generation methods. To
address this issue, we propose a universal High-Definition Teeth Restoration
Network, dubbed HDTR-Net, for arbitrary TFG methods. HDTR-Net can enhance teeth
regions at an extremely fast speed while maintaining synchronization, and
temporal consistency. In particular, we propose a Fine-Grained Feature Fusion
(FGFF) module to effectively capture fine texture feature information around
teeth and surrounding regions, and use these features to fine-grain the feature
map to enhance the clarity of teeth. Extensive experiments show that our method
can be adapted to arbitrary TFG methods without suffering from lip
synchronization and frame coherence. Another advantage of HDTR-Net is its
real-time generation ability. Also under the condition of high-definition
restoration of talking face video synthesis, its inference speed is $300\%$
faster than the current state-of-the-art face restoration based on
super-resolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mingqiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07504">
<title>Connected Autonomous Vehicle Motion Planning with Video Predictions from Smart, Self-Supervised Infrastructure. (arXiv:2309.07504v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07504</link>
<description rdf:parseType="Literal">&lt;p&gt;Connected autonomous vehicles (CAVs) promise to enhance safety, efficiency,
and sustainability in urban transportation. However, this is contingent upon a
CAV correctly predicting the motion of surrounding agents and planning its own
motion safely. Doing so is challenging in complex urban environments due to
frequent occlusions and interactions among many agents. One solution is to
leverage smart infrastructure to augment a CAV&apos;s situational awareness; the
present work leverages a recently proposed &quot;Self-Supervised Traffic Advisor&quot;
(SSTA) framework of smart sensors that teach themselves to generate and
broadcast useful video predictions of road users. In this work, SSTA
predictions are modified to predict future occupancy instead of raw video,
which reduces the data footprint of broadcast predictions. The resulting
predictions are used within a planning framework, demonstrating that this
design can effectively aid CAV motion planning. A variety of numerical
experiments study the key factors that make SSTA outputs useful for practical
CAV planning in crowded urban environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kousik_S/0/1/0/all/0/1&quot;&gt;Shreyas Kousik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fridovich_Keil_D/0/1/0/all/0/1&quot;&gt;David Fridovich-Keil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwager_M/0/1/0/all/0/1&quot;&gt;Mac Schwager&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07510">
<title>Learning Environment-Aware Affordance for 3D Articulated Object Manipulation under Occlusions. (arXiv:2309.07510v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceiving and manipulating 3D articulated objects in diverse environments is
essential for home-assistant robots. Recent studies have shown that point-level
affordance provides actionable priors for downstream manipulation tasks.
However, existing works primarily focus on single-object scenarios with
homogeneous agents, overlooking the realistic constraints imposed by the
environment and the agent&apos;s morphology, e.g., occlusions and physical
limitations. In this paper, we propose an environment-aware affordance
framework that incorporates both object-level actionable priors and environment
constraints. Unlike object-centric affordance approaches, learning
environment-aware affordance faces the challenge of combinatorial explosion due
to the complexity of various occlusions, characterized by their quantities,
geometries, positions and poses. To address this and enhance data efficiency,
we introduce a novel contrastive affordance learning framework capable of
training on scenes containing a single occluder and generalizing to scenes with
complex occluder combinations. Experiments demonstrate the effectiveness of our
proposed approach in learning affordance considering environment constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kai Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chuanruo Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_G/0/1/0/all/0/1&quot;&gt;Guanqi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07525">
<title>SingFake: Singing Voice Deepfake Detection. (arXiv:2309.07525v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of singing voice synthesis presents critical challenges to artists
and industry stakeholders over unauthorized voice usage. Unlike synthesized
speech, synthesized singing voices are typically released in songs containing
strong background music that may hide synthesis artifacts. Additionally,
singing voices present different acoustic and linguistic characteristics from
speech utterances. These unique properties make singing voice deepfake
detection a relevant but significantly different problem from synthetic speech
detection. In this work, we propose the singing voice deepfake detection task.
We first present SingFake, the first curated in-the-wild dataset consisting of
28.93 hours of bonafide and 29.40 hours of deepfake song clips in five
languages from 40 singers. We provide a train/val/test split where the test
sets include various scenarios. We then use SingFake to evaluate four
state-of-the-art speech countermeasure systems trained on speech utterances. We
find these systems lag significantly behind their performance on speech test
data. When trained on SingFake, either using separated vocal tracks or song
mixtures, these systems show substantial improvement. However, our evaluations
also identify challenges associated with unseen singers, communication codecs,
languages, and musical contexts, calling for dedicated research into singing
voice deepfake detection. The SingFake dataset and related resources are
available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yongyi Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;You Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heydari_M/0/1/0/all/0/1&quot;&gt;Mojtaba Heydari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1&quot;&gt;Zhiyao Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07566">
<title>Speech-to-Speech Translation with Discrete-Unit-Based Style Transfer. (arXiv:2309.07566v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.07566</link>
<description rdf:parseType="Literal">&lt;p&gt;Direct speech-to-speech translation (S2ST) with discrete self-supervised
representations has achieved remarkable accuracy, but is unable to preserve the
speaker timbre of the source speech during translation. Meanwhile, the scarcity
of high-quality speaker-parallel data poses a challenge for learning style
transfer between source and target speech. We propose an S2ST framework with an
acoustic language model based on discrete units from a self-supervised model
and a neural codec for style transfer. The acoustic language model leverages
self-supervised in-context learning, acquiring the ability for style transfer
without relying on any speaker-parallel data, thereby overcoming the issue of
data scarcity. By using extensive training data, our model achieves zero-shot
cross-lingual style transfer on previously unseen source languages. Experiments
show that our model generates translated speeches with high fidelity and style
similarity. Audio samples are available at &lt;a href=&quot;http://stylelm.github.io/&quot;&gt;this http URL&lt;/a&gt; .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jionghao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Rongjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhiqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07578">
<title>Equivariant Data Augmentation for Generalization in Offline Reinforcement Learning. (arXiv:2309.07578v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07578</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to address the challenge of generalization in
offline reinforcement learning (RL), where the agent learns from a fixed
dataset without any additional interaction with the environment. Specifically,
we aim to improve the agent&apos;s ability to generalize to out-of-distribution
goals. To achieve this, we propose to learn a dynamics model and check if it is
equivariant with respect to a fixed type of transformation, namely translations
in the state space. We then use an entropy regularizer to increase the
equivariant set and augment the dataset with the resulting transformed samples.
Finally, we learn a new policy offline based on the augmented dataset, with an
off-the-shelf offline RL algorithm. Our experimental results demonstrate that
our approach can greatly improve the test performance of the policy on the
considered environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinneri_C/0/1/0/all/0/1&quot;&gt;Cristina Pinneri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bechtle_S/0/1/0/all/0/1&quot;&gt;Sarah Bechtle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wulfmeier_M/0/1/0/all/0/1&quot;&gt;Markus Wulfmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byravan_A/0/1/0/all/0/1&quot;&gt;Arunkumar Byravan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1&quot;&gt;William F. Whitney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1&quot;&gt;Martin Riedmiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07593">
<title>Statistically Valid Variable Importance Assessment through Conditional Permutations. (arXiv:2309.07593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07593</link>
<description rdf:parseType="Literal">&lt;p&gt;Variable importance assessment has become a crucial step in machine-learning
applications when using complex learners, such as deep neural networks, on
large-scale data. Removal-based importance assessment is currently the
reference approach, particularly when statistical guarantees are sought to
justify variable inclusion. It is often implemented with variable permutation
schemes. On the flip side, these approaches risk misidentifying unimportant
variables as important in the presence of correlations among covariates. Here
we develop a systematic approach for studying Conditional Permutation
Importance (CPI) that is model agnostic and computationally lean, as well as
reusable benchmarks of state-of-the-art variable importance estimators. We show
theoretically and empirically that $\textit{CPI}$ overcomes the limitations of
standard permutation importance by providing accurate type-I error control.
When used with a deep neural network, $\textit{CPI}$ consistently showed top
accuracy across benchmarks. An empirical benchmark on real-world data analysis
in a large-scale medical dataset showed that $\textit{CPI}$ provides a more
parsimonious selection of statistically significant variables. Our results
suggest that $\textit{CPI}$ can be readily used as drop-in replacement for
permutation-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chamma_A/0/1/0/all/0/1&quot;&gt;Ahmad Chamma&lt;/a&gt; (1 and 2 and 3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engemann_D/0/1/0/all/0/1&quot;&gt;Denis A. Engemann&lt;/a&gt; (4), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thirion_B/0/1/0/all/0/1&quot;&gt;Bertrand Thirion&lt;/a&gt; (1 and 2 and 3) ((1) Inria, (2) Universite Paris Saclay, (3) CEA, (4) Roche Pharma Research and Early Development, Neuroscience and Rare Diseases, Roche Innovation Center Basel, F. Hoffmann-La Roche Ltd., Basel, Switzerland)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07594">
<title>Neuro-Symbolic Recommendation Model based on Logic Query. (arXiv:2309.07594v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.07594</link>
<description rdf:parseType="Literal">&lt;p&gt;A recommendation system assists users in finding items that are relevant to
them. Existing recommendation models are primarily based on predicting
relationships between users and items and use complex matching models or
incorporate extensive external information to capture association patterns in
data. However, recommendation is not only a problem of inductive statistics
using data; it is also a cognitive task of reasoning decisions based on
knowledge extracted from information. Hence, a logic system could naturally be
incorporated for the reasoning in a recommendation task. However, although
hard-rule approaches based on logic systems can provide powerful reasoning
ability, they struggle to cope with inconsistent and incomplete knowledge in
real-world tasks, especially for complex tasks such as recommendation.
Therefore, in this paper, we propose a neuro-symbolic recommendation model,
which transforms the user history interactions into a logic expression and then
transforms the recommendation prediction into a query task based on this logic
expression. The logic expressions are then computed based on the modular logic
operations of the neural network. We also construct an implicit logic encoder
to reasonably reduce the complexity of the logic computation. Finally, a user&apos;s
interest items can be queried in the vector space based on the computation
results. Experiments on three well-known datasets verified that our method
performs better compared to state of the art shallow, deep, session, and
reasoning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Maonian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shaojun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07597">
<title>C-Pack: Packaged Resources To Advance General Chinese Embedding. (arXiv:2309.07597v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07597</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce C-Pack, a package of resources that significantly advance the
field of general Chinese embeddings. C-Pack includes three critical resources.
1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6
tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated
from labeled and unlabeled Chinese corpora for training embedding models. 3)
C-TEM is a family of embedding models covering multiple sizes. Our models
outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the
time of the release. We also integrate and optimize the entire suite of
training methods for C-TEM. Along with our resources on general Chinese
embedding, we release our data and models for English text embeddings. The
English models achieve state-of-the-art performance on MTEB benchmark;
meanwhile, our released English data is 2 times larger than the Chinese data.
All these resources are made publicly available at
https://github.com/FlagOpen/FlagEmbedding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muennighof_N/0/1/0/all/0/1&quot;&gt;Niklas Muennighof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07601">
<title>Detecting Misinformation with LLM-Predicted Credibility Signals and Weak Supervision. (arXiv:2309.07601v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07601</link>
<description rdf:parseType="Literal">&lt;p&gt;Credibility signals represent a wide range of heuristics that are typically
used by journalists and fact-checkers to assess the veracity of online content.
Automating the task of credibility signal extraction, however, is very
challenging as it requires high-accuracy signal-specific extractors to be
trained, while there are currently no sufficiently large datasets annotated
with all credibility signals. This paper investigates whether large language
models (LLMs) can be prompted effectively with a set of 18 credibility signals
to produce weak labels for each signal. We then aggregate these potentially
noisy labels using weak supervision in order to predict content veracity. We
demonstrate that our approach, which combines zero-shot LLM credibility signal
labeling and weak supervision, outperforms state-of-the-art classifiers on two
misinformation datasets without using any ground-truth labels for training. We
also analyse the contribution of the individual credibility signals towards
predicting content veracity, which provides new valuable insights into their
role in misinformation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leite_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o A. Leite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razuvayevskaya_O/0/1/0/all/0/1&quot;&gt;Olesya Razuvayevskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1&quot;&gt;Kalina Bontcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1&quot;&gt;Carolina Scarton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07602">
<title>Turning Dross Into Gold Loss: is BERT4Rec really better than SASRec?. (arXiv:2309.07602v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2309.07602</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently sequential recommendations and next-item prediction task has become
increasingly popular in the field of recommender systems. Currently, two
state-of-the-art baselines are Transformer-based models SASRec and BERT4Rec.
Over the past few years, there have been quite a few publications comparing
these two algorithms and proposing new state-of-the-art models. In most of the
publications, BERT4Rec achieves better performance than SASRec. But BERT4Rec
uses cross-entropy over softmax for all items, while SASRec uses negative
sampling and calculates binary cross-entropy loss for one positive and one
negative item. In our work, we show that if both models are trained with the
same loss, which is used by BERT4Rec, then SASRec will significantly outperform
BERT4Rec both in terms of quality and training speed. In addition, we show that
SASRec could be effectively trained with negative sampling and still outperform
BERT4Rec, but the number of negative examples should be much larger than one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klenitskiy_A/0/1/0/all/0/1&quot;&gt;Anton Klenitskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilev_A/0/1/0/all/0/1&quot;&gt;Alexey Vasilev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07610">
<title>Feature Engineering in Learning-to-Rank for Community Question Answering Task. (arXiv:2309.07610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07610</link>
<description rdf:parseType="Literal">&lt;p&gt;Community question answering (CQA) forums are Internet-based platforms where
users ask questions about a topic and other expert users try to provide
solutions. Many CQA forums such as Quora, Stackoverflow, Yahoo!Answer,
StackExchange exist with a lot of user-generated data. These data are leveraged
in automated CQA ranking systems where similar questions (and answers) are
presented in response to the query of the user. In this work, we empirically
investigate a few aspects of this domain. Firstly, in addition to traditional
features like TF-IDF, BM25 etc., we introduce a BERT-based feature that
captures the semantic similarity between the question and answer. Secondly,
most of the existing research works have focused on features extracted only
from the question part; features extracted from answers have not been explored
extensively. We combine both types of features in a linear fashion. Thirdly,
using our proposed concepts, we conduct an empirical investigation with
different rank-learning algorithms, some of which have not been used so far in
CQA domain. On three standard CQA datasets, our proposed framework achieves
state-of-the-art performance. We also analyze importance of the features we use
in our investigation. This work is expected to guide the practitioners to
select a better set of features for the CQA retrieval task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajid_N/0/1/0/all/0/1&quot;&gt;Nafis Sajid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Rashidul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Muhammad Ibrahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07666">
<title>Multi-Source Domain Adaptation meets Dataset Distillation through Dataset Dictionary Learning. (arXiv:2309.07666v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07666</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the intersection of two problems in machine
learning: Multi-Source Domain Adaptation (MSDA) and Dataset Distillation (DD).
On the one hand, the first considers adapting multiple heterogeneous labeled
source domains to an unlabeled target domain. On the other hand, the second
attacks the problem of synthesizing a small summary containing all the
information about the datasets. We thus consider a new problem called MSDA-DD.
To solve it, we adapt previous works in the MSDA literature, such as
Wasserstein Barycenter Transport and Dataset Dictionary Learning, as well as DD
method Distribution Matching. We thoroughly experiment with this novel problem
on four benchmarks (Caltech-Office 10, Tennessee-Eastman Process, Continuous
Stirred Tank Reactor, and Case Western Reserve University), where we show that,
even with as little as 1 sample per class, one achieves state-of-the-art
adaptation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1&quot;&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1&quot;&gt;Antoine Souloumiac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07670">
<title>Federated Dataset Dictionary Learning for Multi-Source Domain Adaptation. (arXiv:2309.07670v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07670</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose an approach for federated domain adaptation, a
setting where distributional shift exists among clients and some have unlabeled
data. The proposed framework, FedDaDiL, tackles the resulting challenge through
dictionary learning of empirical distributions. In our setting, clients&apos;
distributions represent particular domains, and FedDaDiL collectively trains a
federated dictionary of empirical distributions. In particular, we build upon
the Dataset Dictionary Learning framework by designing collaborative
communication protocols and aggregation operations. The chosen protocols keep
clients&apos; data private, thus enhancing overall privacy compared to its
centralized counterpart. We empirically demonstrate that our approach
successfully generates labeled data on the target domain with extensive
experiments on (i) Caltech-Office, (ii) TEP, and (iii) CWRU benchmarks.
Furthermore, we compare our method to its centralized counterpart and other
benchmarks in federated domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellon_F/0/1/0/all/0/1&quot;&gt;Fabiola Espinosa Castellon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1&quot;&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayoue_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Mayoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1&quot;&gt;Antoine Souloumiac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouy_Pallier_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Gouy-Pallier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07683">
<title>Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.07683</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI models garnered a large amount of public attention and
speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion
camps exist: one excited about possibilities these models offer for fundamental
changes to human tasks, and another highly concerned about power these models
seem to have. To address these concerns, we assessed GPT3.5 using standard,
normed, and validated cognitive and personality measures. For this seedling
project, we developed a battery of tests that allowed us to estimate the
boundaries of some of these models capabilities, how stable those capabilities
are over a short period of time, and how they compare to humans.
&lt;/p&gt;
&lt;p&gt;Our results indicate that GPT 3.5 is unlikely to have developed sentience,
although its ability to respond to personality inventories is interesting. It
did display large variability in both cognitive and personality measures over
repeated observations, which is not expected if it had a human-like
personality. Variability notwithstanding, GPT3.5 displays what in a human would
be considered poor mental health, including low self-esteem and marked
dissociation from reality despite upbeat and helpful responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speed_A/0/1/0/all/0/1&quot;&gt;Ann Speed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07684">
<title>deepFDEnet: A Novel Neural Network Architecture for Solving Fractional Differential Equations. (arXiv:2309.07684v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07684</link>
<description rdf:parseType="Literal">&lt;p&gt;The primary goal of this research is to propose a novel architecture for a
deep neural network that can solve fractional differential equations
accurately. A Gaussian integration rule and a $L_1$ discretization technique
are used in the proposed design. In each equation, a deep neural network is
used to approximate the unknown function. Three forms of fractional
differential equations have been examined to highlight the method&apos;s
versatility: a fractional ordinary differential equation, a fractional order
integrodifferential equation, and a fractional order partial differential
equation. The results show that the proposed architecture solves different
forms of fractional differential equations with excellent precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firoozsalari_A/0/1/0/all/0/1&quot;&gt;Ali Nosrati Firoozsalari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazraeh_H/0/1/0/all/0/1&quot;&gt;Hassan Dana Mazraeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghaei_A/0/1/0/all/0/1&quot;&gt;Alireza Afzal Aghaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parand_K/0/1/0/all/0/1&quot;&gt;Kourosh Parand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07689">
<title>Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text. (arXiv:2309.07689v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07689</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent advancements in the capabilities and widespread accessibility of
generative language models, such as ChatGPT (OpenAI, 2022), have brought about
various benefits by generating fluent human-like text, the task of
distinguishing between human- and large language model (LLM) generated text has
emerged as a crucial problem. These models can potentially deceive by
generating artificial text that appears to be human-generated. This issue is
particularly significant in domains such as law, education, and science, where
ensuring the integrity of text is of the utmost importance. This survey
provides an overview of the current approaches employed to differentiate
between texts generated by humans and ChatGPT. We present an account of the
different datasets constructed for detecting ChatGPT-generated text, the
various methods utilized, what qualitative analyses into the characteristics of
human versus ChatGPT-generated text have been performed, and finally, summarize
our findings into general insights
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhaini_M/0/1/0/all/0/1&quot;&gt;Mahdi Dhaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poelman_W/0/1/0/all/0/1&quot;&gt;Wessel Poelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogan_E/0/1/0/all/0/1&quot;&gt;Ege Erdogan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07694">
<title>Tree of Uncertain Thoughts Reasoning for Large Language Models. (arXiv:2309.07694v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07694</link>
<description rdf:parseType="Literal">&lt;p&gt;While the recently introduced Tree of Thoughts (ToT) has heralded
advancements in allowing Large Language Models (LLMs) to reason through
foresight and backtracking for global decision-making, it has overlooked the
inherent local uncertainties in intermediate decision points or &quot;thoughts&quot;.
These local uncertainties, intrinsic to LLMs given their potential for diverse
responses, remain a significant concern in the reasoning process. Addressing
this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a
reasoning framework tailored for LLMs. Our TouT effectively leverages Monte
Carlo Dropout to quantify uncertainty scores associated with LLMs&apos; diverse
local responses at these intermediate steps. By marrying this local uncertainty
quantification with global search algorithms, TouT enhances the model&apos;s
precision in response generation. We substantiate our approach with rigorous
experiments on two demanding planning tasks: Game of 24 and Mini Crosswords.
The empirical evidence underscores TouT&apos;s superiority over both ToT and
chain-of-thought prompting methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1&quot;&gt;Shentong Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1&quot;&gt;Miao Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07704">
<title>NutritionVerse: Empirical Study of Various Dietary Intake Estimation Approaches. (arXiv:2309.07704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07704</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate dietary intake estimation is critical for informing policies and
programs to support healthy eating, as malnutrition has been directly linked to
decreased quality of life. However self-reporting methods such as food diaries
suffer from substantial bias. Other conventional dietary assessment techniques
and emerging alternative approaches such as mobile applications incur high time
costs and may necessitate trained personnel. Recent work has focused on using
computer vision and machine learning to automatically estimate dietary intake
from food images, but the lack of comprehensive datasets with diverse
viewpoints, modalities and food annotations hinders the accuracy and realism of
such methods. To address this limitation, we introduce NutritionVerse-Synth,
the first large-scale dataset of 84,984 photorealistic synthetic 2D food images
with associated dietary information and multimodal annotations (including depth
images, instance masks, and semantic masks). Additionally, we collect a real
image dataset, NutritionVerse-Real, containing 889 images of 251 dishes to
evaluate realism. Leveraging these novel datasets, we develop and benchmark
NutritionVerse, an empirical study of various dietary intake estimation
approaches, including indirect segmentation-based and direct prediction
networks. We further fine-tune models pretrained on synthetic data with real
images to provide insights into the fusion of synthetic and real data. Finally,
we release both datasets (NutritionVerse-Synth, NutritionVerse-Real) on
https://www.kaggle.com/nutritionverse/datasets as part of an open initiative to
accelerate machine learning for dietary sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1&quot;&gt;Chi-en Amy Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1&quot;&gt;Matthew Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_O/0/1/0/all/0/1&quot;&gt;Olivia Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_K/0/1/0/all/0/1&quot;&gt;Krish Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_P/0/1/0/all/0/1&quot;&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_H/0/1/0/all/0/1&quot;&gt;Heather Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkpatrick_S/0/1/0/all/0/1&quot;&gt;Sharon Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07730">
<title>AIDPS:Adaptive Intrusion Detection and Prevention System for Underwater Acoustic Sensor Networks. (arXiv:2309.07730v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.07730</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater Acoustic Sensor Networks (UW-ASNs) are predominantly used for
underwater environments and find applications in many areas. However, a lack of
security considerations, the unstable and challenging nature of the underwater
environment, and the resource-constrained nature of the sensor nodes used for
UW-ASNs (which makes them incapable of adopting security primitives) make the
UW-ASN prone to vulnerabilities. This paper proposes an Adaptive decentralised
Intrusion Detection and Prevention System called AIDPS for UW-ASNs. The
proposed AIDPS can improve the security of the UW-ASNs so that they can
efficiently detect underwater-related attacks (e.g., blackhole, grayhole and
flooding attacks). To determine the most effective configuration of the
proposed construction, we conduct a number of experiments using several
state-of-the-art machine learning algorithms (e.g., Adaptive Random Forest
(ARF), light gradient-boosting machine, and K-nearest neighbours) and concept
drift detection algorithms (e.g., ADWIN, kdqTree, and Page-Hinkley). Our
experimental results show that incremental ARF using ADWIN provides optimal
performance when implemented with One-class support vector machine (SVM)
anomaly-based detectors. Furthermore, our extensive evaluation results also
show that the proposed scheme outperforms state-of-the-art bench-marking
methods while providing a wider range of desirable features such as scalability
and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Soumadeep Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasikhani_A/0/1/0/all/0/1&quot;&gt;Aryan Mohammadi Pasikhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gope_P/0/1/0/all/0/1&quot;&gt;Prosanta Gope&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;John A. Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_C/0/1/0/all/0/1&quot;&gt;Chintan Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdar_B/0/1/0/all/0/1&quot;&gt;Biplab Sikdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07755">
<title>Generative AI Text Classification using Ensemble LLM Approaches. (arXiv:2309.07755v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07755</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown impressive performance across a
variety of Artificial Intelligence (AI) and natural language processing tasks,
such as content creation, report generation, etc. However, unregulated malign
application of these models can create undesirable consequences such as
generation of fake news, plagiarism, etc. As a result, accurate detection of
AI-generated language can be crucial in responsible usage of LLMs. In this
work, we explore 1) whether a certain body of text is AI generated or written
by human, and 2) attribution of a specific language model in generating a body
of text. Texts in both English and Spanish are considered. The datasets used in
this study are provided as part of the Automated Text Identification
(AuTexTification) shared task. For each of the research objectives stated
above, we propose an ensemble neural model that generates probabilities from
different pre-trained LLMs which are used as features to a Traditional Machine
Learning (TML) classifier following it. For the first task of distinguishing
between AI and human generated text, our model ranked in fifth and thirteenth
place (with macro $F1$ scores of 0.733 and 0.649) for English and Spanish
texts, respectively. For the second task on model attribution, our model ranked
in first place with macro $F1$ scores of 0.625 and 0.653 for English and
Spanish texts, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abburi_H/0/1/0/all/0/1&quot;&gt;Harika Abburi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suesserman_M/0/1/0/all/0/1&quot;&gt;Michael Suesserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pudota_N/0/1/0/all/0/1&quot;&gt;Nirmala Pudota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeramani_B/0/1/0/all/0/1&quot;&gt;Balaji Veeramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowen_E/0/1/0/all/0/1&quot;&gt;Edward Bowen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sanmitra Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07760">
<title>PRE: Vision-Language Prompt Learning with Reparameterization Encoder. (arXiv:2309.07760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07760</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pre-trained vision-language models such as CLIP have demonstrated great
potential in zero-shot transferability to downstream tasks. However, to attain
optimal performance, the manual selection of prompts is necessary to improve
alignment between the downstream image distribution and the textual class
descriptions. This manual prompt engineering is the major challenge for
deploying such models in practice since it requires domain expertise and is
extremely time-consuming. To avoid non-trivial prompt engineering, recent work
Context Optimization (CoOp) introduced the concept of prompt learning to the
vision domain using learnable textual tokens. While CoOp can achieve
substantial improvements over manual prompts, its learned context is worse
generalizable to wider unseen classes within the same dataset. In this work, we
present Prompt Learning with Reparameterization Encoder (PRE) - a simple and
efficient method that enhances the generalization ability of the learnable
prompt to unseen classes while maintaining the capacity to learn Base classes.
Instead of directly optimizing the prompts, PRE employs a prompt encoder to
reparameterize the input prompt embeddings, enhancing the exploration of
task-specific knowledge from few-shot samples. Experiments and extensive
ablation studies on 8 benchmarks demonstrate that our approach is an efficient
method for prompt learning. Specifically, PRE achieves a notable enhancement of
5.60% in average accuracy on New classes and 3% in Harmonic mean compared to
CoOp in the 16-shot setting, all achieved within a good training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minh_A/0/1/0/all/0/1&quot;&gt;Anh Pham Thi Minh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07770">
<title>Variational Quantum Linear Solver enhanced Quantum Support Vector Machine. (arXiv:2309.07770v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2309.07770</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum Support Vector Machines (QSVM) play a vital role in using quantum
resources for supervised machine learning tasks, such as classification.
However, current methods are strongly limited in terms of scalability on Noisy
Intermediate Scale Quantum (NISQ) devices. In this work, we propose a novel
approach called the Variational Quantum Linear Solver (VQLS) enhanced QSVM.
This is built upon our idea of utilizing the variational quantum linear solver
to solve system of linear equations of a least squares-SVM on a NISQ device.
The implementation of our approach is evaluated by an extensive series of
numerical experiments with the Iris dataset, which consists of three distinct
iris plant species. Based on this, we explore the practicality and
effectiveness of our algorithm by constructing a classifier capable of
classification in a feature space ranging from one to seven dimensions.
Furthermore, by strategically exploiting both classical and quantum computing
for various subroutines of our algorithm, we effectively mitigate practical
challenges associated with the implementation. These include significant
improvement in the trainability of the variational ansatz and notable
reductions in run-time for cost calculations. Based on the numerical
experiments, our approach exhibits the capability of identifying a separating
hyperplane in an 8-dimensional feature space. Moreover, it consistently
demonstrated strong performance across various instances with the same dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Jianming Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Suresh_K/0/1/0/all/0/1&quot;&gt;Kalyani Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Moghiseh_A/0/1/0/all/0/1&quot;&gt;Ali Moghiseh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wehn_N/0/1/0/all/0/1&quot;&gt;Norbert Wehn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07808">
<title>What Matters to Enhance Traffic Rule Compliance of Imitation Learning for Automated Driving. (arXiv:2309.07808v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07808</link>
<description rdf:parseType="Literal">&lt;p&gt;More research attention has recently been given to end-to-end autonomous
driving technologies where the entire driving pipeline is replaced with a
single neural network because of its simpler structure and faster inference
time. Despite this appealing approach largely reducing the components in
driving pipeline, its simplicity also leads to interpretability problems and
safety issues &lt;a href=&quot;/abs/2003.06404&quot;&gt;arXiv:2003.06404&lt;/a&gt;. The trained policy is not always compliant with
the traffic rules and it is also hard to discover the reason for the
misbehavior because of the lack of intermediate outputs. Meanwhile, Sensors are
also critical to autonomous driving&apos;s security and feasibility to perceive the
surrounding environment under complex driving scenarios. In this paper, we
proposed P-CSG, a novel penalty-based imitation learning approach with cross
semantics generation sensor fusion technologies to increase the overall
performance of End-to-End Autonomous Driving. We conducted an assessment of our
model&apos;s performance using the Town 05 Long benchmark, achieving an impressive
driving score improvement of over 15%. Furthermore, we conducted robustness
evaluations against adversarial attacks like FGSM and Dot attacks, revealing a
substantial increase in robustness compared to baseline models.More detailed
information, such as code-based resources, ablation studies and videos can be
found at https://hk-zh.github.io/p-csg-plus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_A/0/1/0/all/0/1&quot;&gt;Aifen Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Letian Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07823">
<title>Large-scale Weakly Supervised Learning for Road Extraction from Satellite Imagery. (arXiv:2309.07823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.07823</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic road extraction from satellite imagery using deep learning is a
viable alternative to traditional manual mapping. Therefore it has received
considerable attention recently. However, most of the existing methods are
supervised and require pixel-level labeling, which is tedious and error-prone.
To make matters worse, the earth has a diverse range of terrain, vegetation,
and man-made objects. It is well known that models trained in one area
generalize poorly to other areas. Various shooting conditions such as light and
angel, as well as different image processing techniques further complicate the
issue. It is impractical to develop training data to cover all image styles.
This paper proposes to leverage OpenStreetMap road data as weak labels and
large scale satellite imagery to pre-train semantic segmentation models. Our
extensive experimental results show that the prediction accuracy increases with
the amount of the weakly labeled data, as well as the road density in the areas
chosen for training. Using as much as 100 times more data than the widely used
DeepGlobe road dataset, our model with the D-LinkNet architecture and the
ResNet-50 backbone exceeds the top performer of the current DeepGlobe
leaderboard. Furthermore, due to large-scale pre-training, our model
generalizes much better than those trained with only the curated datasets,
implying great application potential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_S/0/1/0/all/0/1&quot;&gt;Shiqiao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1&quot;&gt;Zonglin Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Siwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07832">
<title>VAPOR: Holonomic Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning. (arXiv:2309.07832v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.07832</link>
<description rdf:parseType="Literal">&lt;p&gt;We present VAPOR, a novel method for autonomous legged robot navigation in
unstructured, densely vegetated outdoor environments using Offline
Reinforcement Learning (RL). Our method trains a novel RL policy from unlabeled
data collected in real outdoor vegetation. This policy uses height and
intensity-based cost maps derived from 3D LiDAR point clouds, a goal cost map,
and processed proprioception data as state inputs, and learns the physical and
geometric properties of the surrounding vegetation such as height, density, and
solidity/stiffness for navigation. Instead of using end-to-end policy actions,
the fully-trained RL policy&apos;s Q network is used to evaluate dynamically
feasible robot actions generated from a novel adaptive planner capable of
navigating through dense narrow passages and preventing entrapment in
vegetation such as tall grass and bushes. We demonstrate our method&apos;s
capabilities on a legged robot in complex outdoor vegetation. We observe an
improvement in success rates, a decrease in average power consumption, and
decrease in normalized trajectory length compared to both existing end-to-end
offline RL and outdoor navigation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerakoon_K/0/1/0/all/0/1&quot;&gt;Kasun Weerakoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1&quot;&gt;Adarsh Jagan Sathyamoorthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elnoor_M/0/1/0/all/0/1&quot;&gt;Mohamed Elnoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07841">
<title>Two Timin&apos;: Repairing Smart Contracts With A Two-Layered Approach. (arXiv:2309.07841v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.07841</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the modern relevance of blockchain technology, smart contracts present
both substantial risks and benefits. Vulnerabilities within them can trigger a
cascade of consequences, resulting in significant losses. Many current papers
primarily focus on classifying smart contracts for malicious intent, often
relying on limited contract characteristics, such as bytecode or opcode. This
paper proposes a novel, two-layered framework: 1) classifying and 2) directly
repairing malicious contracts. Slither&apos;s vulnerability report is combined with
source code and passed through a pre-trained RandomForestClassifier (RFC) and
Large Language Models (LLMs), classifying and repairing each suggested
vulnerability. Experiments demonstrate the effectiveness of fine-tuned and
prompt-engineered LLMs. The smart contract repair models, built from
pre-trained GPT-3.5-Turbo and fine-tuned Llama-2-7B models, reduced the overall
vulnerability count by 97.5% and 96.7% respectively. A manual inspection of
repaired contracts shows that all retain functionality, indicating that the
proposed method is appropriate for automatic batch classification and repair of
vulnerabilities in smart contracts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Abhinav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masud_E/0/1/0/all/0/1&quot;&gt;Ehan Masud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Michelle Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhillon_R/0/1/0/all/0/1&quot;&gt;Rohan Dhillon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1&quot;&gt;Sumukh Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Arya Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheema_S/0/1/0/all/0/1&quot;&gt;Salar Cheema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Saurav Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07843">
<title>Applying Deep Learning to Calibrate Stochastic Volatility Models. (arXiv:2309.07843v1 [q-fin.CP])</title>
<link>http://arxiv.org/abs/2309.07843</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic volatility models, where the volatility is a stochastic process,
can capture most of the essential stylized facts of implied volatility surfaces
and give more realistic dynamics of the volatility smile or skew. However, they
come with the significant issue that they take too long to calibrate.
&lt;/p&gt;
&lt;p&gt;Alternative calibration methods based on Deep Learning (DL) techniques have
been recently used to build fast and accurate solutions to the calibration
problem. Huge and Savine developed a Differential Deep Learning (DDL) approach,
where Machine Learning models are trained on samples of not only features and
labels but also differentials of labels to features. The present work aims to
apply the DDL technique to price vanilla European options (i.e. the calibration
instruments), more specifically, puts when the underlying asset follows a
Heston model and then calibrate the model on the trained network. DDL allows
for fast training and accurate pricing. The trained neural network dramatically
reduces Heston calibration&apos;s computation time.
&lt;/p&gt;
&lt;p&gt;In this work, we also introduce different regularisation techniques, and we
apply them notably in the case of the DDL. We compare their performance in
reducing overfitting and improving the generalisation error. The DDL
performance is also compared to the classical DL (without differentiation) one
in the case of Feed-Forward Neural Networks. We show that the DDL outperforms
the DL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sridi_A/0/1/0/all/0/1&quot;&gt;Abir Sridi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Bilokon_P/0/1/0/all/0/1&quot;&gt;Paul Bilokon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07852">
<title>ExpertQA: Expert-Curated Questions and Attributed Answers. (arXiv:2309.07852v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07852</link>
<description rdf:parseType="Literal">&lt;p&gt;As language models are adapted by a more sophisticated and diverse set of
users, the importance of guaranteeing that they provide factually correct
information supported by verifiable sources is critical across fields of study
&amp;amp; professions. This is especially the case for high-stakes fields, such as
medicine and law, where the risk of propagating false information is high and
can lead to undesirable societal consequences. Previous work studying
factuality and attribution has not focused on analyzing these characteristics
of language model outputs in domain-specific scenarios. In this work, we
present an evaluation study analyzing various axes of factuality and
attribution provided in responses from a few systems, by bringing domain
experts in the loop. Specifically, we first collect expert-curated questions
from 484 participants across 32 fields of study, and then ask the same experts
to evaluate generated responses to their own questions. We also ask experts to
revise answers produced by language models, which leads to ExpertQA, a
high-quality long-form QA dataset with 2177 questions spanning 32 fields, along
with verified answers and attributions for claims in the answers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malaviya_C/0/1/0/all/0/1&quot;&gt;Chaitanya Malaviya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Subin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sihao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieber_E/0/1/0/all/0/1&quot;&gt;Elizabeth Sieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1&quot;&gt;Mark Yatskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07861">
<title>CiwaGAN: Articulatory information exchange. (arXiv:2309.07861v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.07861</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans encode information into sounds by controlling articulators and decode
information from sounds using the auditory apparatus. This paper introduces
CiwaGAN, a model of human spoken language acquisition that combines
unsupervised articulatory modeling with an unsupervised model of information
exchange through the auditory modality. While prior research includes
unsupervised articulatory modeling and information exchange separately, our
model is the first to combine the two components. The paper also proposes an
improved articulatory model with more interpretable internal representations.
The proposed CiwaGAN model is the most realistic approximation of human spoken
language acquisition using deep learning. As such, it is useful for cognitively
plausible simulations of the human speech act.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Begus_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#x161;per Begu&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Thomas Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Alan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Peter Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1&quot;&gt;Gopala K. Anumanchipalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07864">
<title>The Rise and Potential of Large Language Model Based Agents: A Survey. (arXiv:2309.07864v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.07864</link>
<description rdf:parseType="Literal">&lt;p&gt;For a long time, humanity has pursued artificial intelligence (AI) equivalent
to or surpassing the human level, with AI agents considered a promising vehicle
for this pursuit. AI agents are artificial entities that sense their
environment, make decisions, and take actions. Many efforts have been made to
develop intelligent AI agents since the mid-20th century. However, these
efforts have mainly focused on advancement in algorithms or training strategies
to enhance specific capabilities or performance on particular tasks. Actually,
what the community lacks is a sufficiently general and powerful model to serve
as a starting point for designing AI agents that can adapt to diverse
scenarios. Due to the versatile and remarkable capabilities they demonstrate,
large language models (LLMs) are regarded as potential sparks for Artificial
General Intelligence (AGI), offering hope for building general AI agents. Many
research efforts have leveraged LLMs as the foundation to build AI agents and
have achieved significant progress. We start by tracing the concept of agents
from its philosophical origins to its development in AI, and explain why LLMs
are suitable foundations for AI agents. Building upon this, we present a
conceptual framework for LLM-based agents, comprising three main components:
brain, perception, and action, and the framework can be tailored to suit
different applications. Subsequently, we explore the extensive applications of
LLM-based agents in three aspects: single-agent scenarios, multi-agent
scenarios, and human-agent cooperation. Following this, we delve into agent
societies, exploring the behavior and personality of LLM-based agents, the
social phenomena that emerge when they form societies, and the insights they
offer for human society. Finally, we discuss a range of key topics and open
problems within the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenxiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1&quot;&gt;Wei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_B/0/1/0/all/0/1&quot;&gt;Boyang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junzhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Senjie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoran Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Limao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Changhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yicheng Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1&quot;&gt;Rongxiang Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wensen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_W/0/1/0/all/0/1&quot;&gt;Wenjuan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yongyan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huan_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07867">
<title>Beta Diffusion. (arXiv:2309.07867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07915">
<title>MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.07915</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting from the resurgence of deep learning, vision-language models (VLMs)
benefiting from large language models (LLMs) have never been so popular.
However, while LLMs can utilize extensive background knowledge and task
information with in-context learning, most VLMs still struggle with
understanding complex multi-modal prompts with multiple images. The issue can
traced back to the architectural design of VLMs or pre-training data.
Specifically, the current VLMs primarily emphasize utilizing multi-modal data
with a single image some, rather than multi-modal prompts with interleaved
multiple images and text. Even though some newly proposed VLMs could handle
user prompts with multiple images, pre-training data does not provide more
sophisticated multi-modal prompts than interleaved image and text crawled from
the web. We propose MMICL to address the issue by considering both the model
and data perspectives. We introduce a well-designed architecture capable of
seamlessly integrating visual and textual context in an interleaved manner and
MIC dataset to reduce the gap between the training data and the complex user
prompts in real-world applications, including: 1) multi-modal context with
interleaved images and text, 2) textual references for each image, and 3)
multi-image data with spatial, logical, or temporal relationships. Our
experiments confirm that MMICL achieves new stat-of-the-art zero-shot and
few-shot performance on a wide range of general vision-language tasks,
especially for complex reasoning benchmarks including MME and MMBench. Our
analysis demonstrates that MMICL effectively deals with the challenge of
complex multi-modal prompt understanding. The experiments on ScienceQA-IMG also
show that MMICL successfully alleviates the issue of language bias in VLMs,
which we believe is the reason behind the advanced performance of MMICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haozhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zefan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Shuzheng Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1&quot;&gt;Kaikai An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wenjuan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Baobao Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.07611">
<title>Speeding up Learning Quantum States through Group Equivariant Convolutional Quantum Ans\&quot;atze. (arXiv:2112.07611v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2112.07611</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a theoretical framework for $S_n$-equivariant convolutional
quantum circuits with SU$(d)$-symmetry, building on and significantly
generalizing Jordan&apos;s Permutational Quantum Computing (PQC) formalism based on
Schur-Weyl duality connecting both SU$(d)$ and $S_n$ actions on qudits. In
particular, we utilize the Okounkov-Vershik approach to prove Harrow&apos;s
statement (Ph.D. Thesis 2005 p.160) on the equivalence between
$\operatorname{SU}(d)$ and $S_n$ irrep bases and to establish the
$S_n$-equivariant Convolutional Quantum Alternating Ans\&quot;atze ($S_n$-CQA) using
Young-Jucys-Murphy (YJM) elements. We prove that $S_n$-CQA is able to generate
any unitary in any given $S_n$ irrep sector, which may serve as a universal
model for a wide array of quantum machine learning problems with the presence
of SU($d$) symmetry. Our method provides another way to prove the universality
of Quantum Approximate Optimization Algorithm (QAOA) and verifies that 4-local
SU($d$) symmetric unitaries are sufficient to build generic SU($d$) symmetric
quantum circuits up to relative phase factors. We present numerical simulations
to showcase the effectiveness of the ans\&quot;atze to find the ground state energy
of the $J_1$--$J_2$ antiferromagnetic Heisenberg model on the rectangular and
Kagome lattices. Our work provides the first application of the celebrated
Okounkov-Vershik&apos;s $S_n$ representation theory to quantum physics and machine
learning, from which to propose quantum variational ans\&quot;atze that strongly
suggests to be classically intractable tailored towards a specific optimization
problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Han Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zimu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Strelchuk_S/0/1/0/all/0/1&quot;&gt;Sergii Strelchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kondor_R/0/1/0/all/0/1&quot;&gt;Risi Kondor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03609">
<title>PolicyCleanse: Backdoor Detection and Mitigation in Reinforcement Learning. (arXiv:2202.03609v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03609</link>
<description rdf:parseType="Literal">&lt;p&gt;While real-world applications of reinforcement learning are becoming popular,
the security and robustness of RL systems are worthy of more attention and
exploration. In particular, recent works have revealed that, in a multi-agent
RL environment, backdoor trigger actions can be injected into a victim agent
(a.k.a. Trojan agent), which can result in a catastrophic failure as soon as it
sees the backdoor trigger action. To ensure the security of RL agents against
malicious backdoors, in this work, we propose the problem of Backdoor Detection
in a multi-agent competitive reinforcement learning system, with the objective
of detecting Trojan agents as well as the corresponding potential trigger
actions, and further trying to mitigate their Trojan behavior. In order to
solve this problem, we propose PolicyCleanse that is based on the property that
the activated Trojan agents accumulated rewards degrade noticeably after
several timesteps. Along with PolicyCleanse, we also design a machine
unlearning-based approach that can effectively mitigate the detected backdoor.
Extensive experiments demonstrate that the proposed methods can accurately
detect Trojan agents, and outperform existing backdoor mitigation baseline
approaches by at least 3% in winning rate across various types of agents and
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.10629">
<title>Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.10629</link>
<description rdf:parseType="Literal">&lt;p&gt;In data-rich domains such as vision, language, and speech, deep learning
prevails to deliver high-performance task-specific models and can even learn
general task-agnostic representations for efficient finetuning to downstream
tasks. However, deep learning in resource-limited domains still faces multiple
challenges including (i) limited data, (ii) constrained model development cost,
and (iii) lack of adequate pre-trained models for effective finetuning. This
paper provides an overview of model reprogramming to bridge this gap. Model
reprogramming enables resource-efficient cross-domain machine learning by
repurposing and reusing a well-developed pre-trained model from a source domain
to solve tasks in a target domain without model finetuning, where the source
and target domains can be vastly different. In many applications, model
reprogramming outperforms transfer learning and training from scratch. This
paper elucidates the methodology of model reprogramming, summarizes existing
use cases, provides a theoretical explanation of the success of model
reprogramming, and concludes with a discussion on open-ended research questions
and opportunities. A list of model reprogramming studies is actively maintained
and updated at https://github.com/IBM/model-reprogramming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11110">
<title>Meta-Learning Regrasping Strategies for Physical-Agnostic Objects. (arXiv:2205.11110v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11110</link>
<description rdf:parseType="Literal">&lt;p&gt;Grasping inhomogeneous objects in real-world applications remains a
challenging task due to the unknown physical properties such as mass
distribution and coefficient of friction. In this study, we propose a
meta-learning algorithm called ConDex, which incorporates Conditional Neural
Processes (CNP) with DexNet-2.0 to autonomously discern the underlying physical
properties of objects using depth images. ConDex efficiently acquires physical
embeddings from limited trials, enabling precise grasping point estimation.
Furthermore, ConDex is capable of updating the predicted grasping quality
iteratively from new trials in an online fashion. To the best of our knowledge,
we are the first who generate two object datasets focusing on inhomogeneous
physical properties with varying mass distributions and friction coefficients.
Extensive evaluations in simulation demonstrate ConDex&apos;s superior performance
over DexNet-2.0 and existing meta-learning-based grasping pipelines.
Furthermore, ConDex shows robust generalization to previously unseen real-world
objects despite training solely in the simulation. The synthetic and real-world
datasets will be published as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Ning Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03420">
<title>An Adaptive Federated Relevance Framework for Spatial Temporal Graph Learning. (arXiv:2206.03420v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03420</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial-temporal data contains rich information and has been widely studied
in recent years due to the rapid development of relevant applications in many
fields. For instance, medical institutions often use electrodes attached to
different parts of a patient to analyse the electorencephal data rich with
spatial and temporal features for health assessment and disease diagnosis.
Existing research has mainly used deep learning techniques such as
convolutional neural network (CNN) or recurrent neural network (RNN) to extract
hidden spatial-temporal features. Yet, it is challenging to incorporate both
inter-dependencies spatial information and dynamic temporal changes
simultaneously. In reality, for a model that leverages these spatial-temporal
features to fulfil complex prediction tasks, it often requires a colossal
amount of training data in order to obtain satisfactory model performance.
Considering the above-mentioned challenges, we propose an adaptive federated
relevance framework, namely FedRel, for spatial-temporal graph learning in this
paper. After transforming the raw spatial-temporal data into high quality
features, the core Dynamic Inter-Intra Graph (DIIG) module in the framework is
able to use these features to generate the spatial-temporal graphs capable of
capturing the hidden topological and long-term temporal correlation information
in these graphs. To improve the model generalization ability and performance
while preserving the local data privacy, we also design a relevance-driven
federated learning module in our framework to leverage diverse data
distributions from different participants with attentive aggregations of their
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tiehua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhishu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00085">
<title>Machine Learning and Computer Vision Techniques in Continuous Beehive Monitoring Applications: A survey. (arXiv:2208.00085v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00085</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide use and availability of the machine learning and computer vision
techniques allows development of relatively complex monitoring systems in many
domains. Besides the traditional industrial domain, new application appears
also in biology and agriculture, where we could speak about the detection of
infections, parasites and weeds, but also about automated monitoring and early
warning systems. This is also connected with the introduction of the easily
accessible hardware and development kits such as Arduino, or RaspberryPi
family. In this paper, we survey 50 existing papers focusing on the methods of
automated beehive monitoring methods using the computer vision techniques,
particularly on the pollen and Varroa mite detection together with the bee
traffic monitoring. Such systems could also be used for the monitoring of the
honeybee colonies and for the inspection of their health state, which could
identify potentially dangerous states before the situation is critical, or to
better plan periodic bee colony inspections and therefore save significant
costs. Later, we also include analysis of the research trends in this
application field and we outline the possible direction of the new
explorations. Our paper is aimed also at veterinary and apidology professionals
and experts, who might not be familiar with machine learning to introduce them
to its possibilities, therefore each family of applications is opened by a
brief theoretical introduction and motivation related to its base method. We
hope that this paper will inspire other scientists to use machine learning
techniques for other applications in beehive monitoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilik_S/0/1/0/all/0/1&quot;&gt;Simon Bilik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemcik_T/0/1/0/all/0/1&quot;&gt;Tomas Zemcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratochvila_L/0/1/0/all/0/1&quot;&gt;Lukas Kratochvila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricanek_D/0/1/0/all/0/1&quot;&gt;Dominik Ricanek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_M/0/1/0/all/0/1&quot;&gt;Milos Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambanini_S/0/1/0/all/0/1&quot;&gt;Sebastian Zambanini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horak_K/0/1/0/all/0/1&quot;&gt;Karel Horak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00305">
<title>LambdaKG: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings. (arXiv:2210.00305v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00305</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graphs (KGs) often have two characteristics: heterogeneous graph
structure and text-rich entity/relation information. Text-based KG embeddings
can represent entities by encoding descriptions with pre-trained language
models, but no open-sourced library is specifically designed for KGs with PLMs
at present. In this paper, we present LambdaKG, a library for KGE that equips
with many pre-trained language models (e.g., BERT, BART, T5, GPT-3), and
supports various tasks (e.g., knowledge graph completion, question answering,
recommendation, and knowledge probing). LambdaKG is publicly open-sourced at
https://github.com/zjunlp/PromptKG/tree/main/lambdaKG, with a demo video at
&lt;a href=&quot;http://deepke.zjukg.cn/lambdakg.mp4&quot;&gt;this http URL&lt;/a&gt; and long-term maintenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoubo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04688">
<title>BAFFLE: Backdoor Attack in Offline Reinforcement Learning. (arXiv:2210.04688v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04688</link>
<description rdf:parseType="Literal">&lt;p&gt;A growing body of research has focused on the Reinforcement Learning (RL)
methods which allow the agent to learn from trial-and-error experiences
gathered during the interaction with the environment. Recently, offline RL
becomes a popular RL paradigm because it saves the interactions with
environments. In offline RL, data providers share large pre-collected datasets,
and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. Baffle
modifies $10\%$ of the datasets for four tasks. Agents trained on the poisoned
datasets perform well in normal settings. However, when triggers are presented,
the agents&apos; performance decreases drastically by $63.2\%$, $53.9\%$, $64.7\%$,
and $47.4\%$ in the four tasks on average. The backdoor still persists after
fine-tuning poisoned agents on clean datasets. We further show that the
inserted backdoor is also hard to be detected by a popular defensive method.
This paper calls attention to developing more effective protection for the
open-source offline RL dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junda He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jieke Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kecen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Arunesh Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinwen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05845">
<title>ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;In real life, success is often contingent upon multiple critical steps that
are distant in time from each other and from the final reward. These critical
steps are challenging to identify with traditional reinforcement learning (RL)
methods that rely on the Bellman equation for credit assignment. Here, we
present a new RL algorithm that uses offline contrastive learning to hone in on
critical steps. This algorithm, which we call contrastive introspection
(ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of
prototypes for the critical steps in a task by a novel contrastive loss and
delivers an intrinsic reward when the current state matches one of these
prototypes. The prototypes in ConSpec provide two key benefits for credit
assignment: (1) They enable rapid identification of all the critical steps. (2)
They do so in a readily interpretable manner, enabling out-of-distribution
generalization when sensory features are altered. Distinct from other
contemporary RL approaches to credit assignment, ConSpec takes advantage of the
fact that it is easier to retrospectively identify the small set of steps that
success is contingent upon than it is to prospectively predict reward at every
step taken in the environment. Altogether, ConSpec improves learning in a
diverse set of RL tasks, including both those with explicit, discrete critical
steps and those with complex, continuous critical steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wannan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiralerspong_T/0/1/0/all/0/1&quot;&gt;Thomas Jiralerspong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malenfant_D/0/1/0/all/0/1&quot;&gt;Dane Malenfant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsbury_Nealy_B/0/1/0/all/0/1&quot;&gt;Benjamin Alsbury-Nealy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1&quot;&gt;Blake Richards&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16591">
<title>DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation. (arXiv:2210.16591v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16591</link>
<description rdf:parseType="Literal">&lt;p&gt;Point-of-Interest (POI) recommendation plays a vital role in various
location-aware services. It has been observed that POI recommendation is driven
by both sequential and geographical influences. However, since there is no
annotated label of the dominant influence during recommendation, existing
methods tend to entangle these two influences, which may lead to sub-optimal
recommendation performance and poor interpretability. In this paper, we address
the above challenge by proposing DisenPOI, a novel Disentangled dual-graph
framework for POI recommendation, which jointly utilizes sequential and
geographical relationships on two separate graphs and disentangles the two
influences with self-supervision. The key novelty of our model compared with
existing approaches is to extract disentangled representations of both
sequential and geographical influences with contrastive learning. To be
specific, we construct a geographical graph and a sequential graph based on the
check-in sequence of a user. We tailor their propagation schemes to become
sequence-/geo-aware to better capture the corresponding influences. Preference
proxies are extracted from check-in sequence as pseudo labels for the two
influences, which supervise the disentanglement via a contrastive loss.
Extensive experiments on three datasets demonstrate the superiority of the
proposed model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1&quot;&gt;Wei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xuyang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jia Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jun Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10851">
<title>Reward is not Necessary: How to Create a Compositional Self-Preserving Agent for Life-Long Learning. (arXiv:2211.10851v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10851</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning views the maximization of rewards and avoidance of
punishments as central to explaining goal-directed behavior. However, over a
life, organisms will need to learn about many different aspects of the world&apos;s
structure: the states of the world and state-vector transition dynamics. The
number of combinations of states grows exponentially as an agent incorporates
new knowledge, and there is no obvious weighted combination of pre-existing
rewards or costs defined for a given combination of states, as such a weighting
would need to encode information about good and bad combinations prior to an
agent&apos;s experience in the world. Therefore, we must develop more naturalistic
accounts of behavior and motivation in large state-spaces. We show that it is
possible to use only the intrinsic motivation metric of empowerment, which
measures the agent&apos;s capacity to realize many possible futures under a
transition operator. We propose to scale empowerment to hierarchical
state-spaces by using Operator Bellman Equations. These equations produce
state-time feasibility functions, which are compositional hierarchical
state-time transition operators that map an initial state and time when an
agent begins a policy to the final states and times of completing a goal.
Because these functions are hierarchical operators we can define hierarchical
empowerment measures on them. An agent can then optimize plans to distant
states and times to maximize its hierarchical empowerment-gain, allowing it to
discover goals that bring about a more favorable coupling of its internal
structure (physiological states) to its external environment (world structure &amp;amp;
spatial state). Life-long agents could therefore be primarily animated by
principles of compositionality and empowerment, exhibiting self-concern for the
growth &amp;amp; maintenance of their own structural integrity without recourse to
reward-maximization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ringstrom_T/0/1/0/all/0/1&quot;&gt;Thomas J. Ringstrom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04953">
<title>TargetCall: Eliminating the Wasted Computation in Basecalling via Pre-Basecalling Filtering. (arXiv:2212.04953v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04953</link>
<description rdf:parseType="Literal">&lt;p&gt;Basecalling is an essential step in nanopore sequencing analysis where the
raw signals of nanopore sequencers are converted into nucleotide sequences,
i.e., reads. State-of-the-art basecallers employ complex deep learning models
to achieve high basecalling accuracy. This makes basecalling
computationally-inefficient and memory-hungry; bottlenecking the entire genome
analysis pipeline. However, for many applications, the majority of reads do no
match the reference genome of interest (i.e., target reference) and thus are
discarded in later steps in the genomics pipeline, wasting the basecalling
computation. To overcome this issue, we propose TargetCall, the first
pre-basecalling filter to eliminate the wasted computation in basecalling.
TargetCall&apos;s key idea is to discard reads that will not match the target
reference (i.e., off-target reads) prior to basecalling. TargetCall consists of
two main components: (1) LightCall, a lightweight neural network basecaller
that produces noisy reads; and (2) Similarity Check, which labels each of these
noisy reads as on-target or off-target by matching them to the target
reference. TargetCall aims to filter out all off-target reads before
basecalling. The highly-accurate but slow basecalling is performed only on the
raw signals whose noisy reads are labeled as on-target. Our thorough
experimental evaluations using both real and simulated data show that
TargetCall 1) improves the end-to-end basecalling performance while maintaining
high sensitivity in keeping on-target reads, 2) maintains high accuracy in
downstream analysis, 3) precisely filters out up to 94.71% of off-target reads,
and 4) achieves better performance, throughput, sensitivity, precision, and
generality compared to prior works. We open-source TargetCall at
https://github.com/CMU-SAFARI/TargetCall
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cavlak_M/0/1/0/all/0/1&quot;&gt;Meryem Banu Cavlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gagandeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alser_M/0/1/0/all/0/1&quot;&gt;Mohammed Alser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Firtina_C/0/1/0/all/0/1&quot;&gt;Can Firtina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lindegger_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xeb;l Lindegger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sadrosadati_M/0/1/0/all/0/1&quot;&gt;Mohammad Sadrosadati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ghiasi_N/0/1/0/all/0/1&quot;&gt;Nika Mansouri Ghiasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Alkan_C/0/1/0/all/0/1&quot;&gt;Can Alkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mutlu_O/0/1/0/all/0/1&quot;&gt;Onur Mutlu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09597">
<title>Reasoning with Language Model Prompting: A Survey. (arXiv:2212.09597v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, as an essential ability for complex problem-solving, can provide
back-end support for various real-world applications, such as medical
diagnosis, negotiation, etc. This paper provides a comprehensive survey of
cutting-edge research on reasoning with language model prompting. We introduce
research works with comparisons and summaries and provide systematic resources
to help beginners. We also discuss the potential reasons for emerging such
reasoning abilities and highlight future research directions. Resources are
available at https://github.com/zjunlp/Prompt4ReasoningPapers (updated
periodically).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Shuofei Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07260">
<title>Scalable Bayesian optimization with high-dimensional outputs using randomized prior networks. (arXiv:2302.07260v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07260</link>
<description rdf:parseType="Literal">&lt;p&gt;Several fundamental problems in science and engineering consist of global
optimization tasks involving unknown high-dimensional (black-box) functions
that map a set of controllable variables to the outcomes of an expensive
experiment. Bayesian Optimization (BO) techniques are known to be effective in
tackling global optimization problems using a relatively small number objective
function evaluations, but their performance suffers when dealing with
high-dimensional outputs. To overcome the major challenge of dimensionality,
here we propose a deep learning framework for BO and sequential decision making
based on bootstrapped ensembles of neural architectures with randomized priors.
Using appropriate architecture choices, we show that the proposed framework can
approximate functional relationships between design variables and quantities of
interest, even in cases where the latter take values in high-dimensional vector
spaces or even infinite-dimensional function spaces. In the context of BO, we
augmented the proposed probabilistic surrogates with re-parameterized Monte
Carlo approximations of multiple-point (parallel) acquisition functions, as
well as methodological extensions for accommodating black-box constraints and
multi-fidelity information sources. We test the proposed framework against
state-of-the-art methods for BO and demonstrate superior performance across
several challenging tasks with high-dimensional outputs, including a
constrained multi-fidelity optimization task involving shape optimization of
rotor blades in turbo-machinery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhouri_M/0/1/0/all/0/1&quot;&gt;Mohamed Aziz Bhouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joly_M/0/1/0/all/0/1&quot;&gt;Michael Joly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Robert Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumalya Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdikaris_P/0/1/0/all/0/1&quot;&gt;Paris Perdikaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10848">
<title>Deep reinforced learning heuristic tested on spin-glass ground states: The larger picture. (arXiv:2302.10848v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10848</link>
<description rdf:parseType="Literal">&lt;p&gt;In Changjun Fan et al. [Nature Communications
https://doi.org/10.1038/s41467-023-36363-w (2023)], the authors present a deep
reinforced learning approach to augment combinatorial optimization heuristics.
In particular, they present results for several spin glass ground state
problems, for which instances on non-planar networks are generally NP-hard, in
comparison with several Monte Carlo based methods, such as simulated annealing
(SA) or parallel tempering (PT). Indeed, those results demonstrate that the
reinforced learning improves the results over those obtained with SA or PT, or
at least allows for reduced runtimes for the heuristics before results of
comparable quality have been obtained relative to those other methods. To
facilitate the conclusion that their method is &apos;&apos;superior&apos;&apos;, the authors pursue
two basic strategies: (1) A commercial GUROBI solver is called on to procure a
sample of exact ground states as a testbed to compare with, and (2) a
head-to-head comparison between the heuristics is given for a sample of larger
instances where exact ground states are hard to ascertain. Here, we put these
studies into a larger context, showing that the claimed superiority is at best
marginal for smaller samples and becomes essentially irrelevant with respect to
any sensible approximation of true ground states in the larger samples. For
example, this method becomes irrelevant as a means to determine stiffness
exponents $\theta$ in $d&amp;gt;2$, as mentioned by the authors, where the problem is
not only NP-hard but requires the subtraction of two almost equal ground-state
energies and systemic errors in each of $\approx 1\%$ found here are
unacceptable. This larger picture on the method arises from a straightforward
finite-size corrections study over the spin glass ensembles the authors employ,
using data that has been available for decades.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Boettcher_S/0/1/0/all/0/1&quot;&gt;Stefan Boettcher&lt;/a&gt; (Emory U)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09960">
<title>A Latent Space Theory for Emergent Abilities in Large Language Models. (arXiv:2304.09960v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09960</link>
<description rdf:parseType="Literal">&lt;p&gt;Languages are not created randomly but rather to communicate information.
There is a strong association between languages and their underlying meanings,
resulting in a sparse joint distribution that is heavily peaked according to
their correlations. Moreover, these peak values happen to match with the
marginal distribution of languages due to the sparsity. With the advent of LLMs
trained on big data and large models, we can now precisely assess the marginal
distribution of languages, providing a convenient means of exploring the sparse
structures in the joint distribution for effective inferences. In this paper,
we categorize languages as either unambiguous or {\epsilon}-ambiguous and
present quantitative results to demonstrate that the emergent abilities of
LLMs, such as language understanding, in-context learning, chain-of-thought
prompting, and effective instruction fine-tuning, can all be attributed to
Bayesian inference on the sparse joint distribution of languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hui Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10520">
<title>Contrastive Tuning: A Little Help to Make Masked Autoencoders Forget. (arXiv:2304.10520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10520</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked Image Modeling (MIM) methods, like Masked Autoencoders (MAE),
efficiently learn a rich representation of the input. However, for adapting to
downstream tasks, they require a sufficient amount of labeled data since their
rich features code not only objects but also less relevant image background. In
contrast, Instance Discrimination (ID) methods focus on objects. In this work,
we study how to combine the efficiency and scalability of MIM with the ability
of ID to perform downstream classification in the absence of large amounts of
labeled data. To this end, we introduce Masked Autoencoder Contrastive Tuning
(MAE-CT), a sequential approach that utilizes the implicit clustering of the
Nearest Neighbor Contrastive Learning (NNCLR) objective to induce abstraction
in the topmost layers of a pre-trained MAE. MAE-CT tunes the rich features such
that they form semantic clusters of objects without using any labels. Notably,
MAE-CT does not rely on hand-crafted augmentations and frequently achieves its
best performances while using only minimal augmentations (crop &amp;amp; flip).
Further, MAE-CT is compute efficient as it requires at most 10% overhead
compared to MAE re-training. Applied to large and huge Vision Transformer (ViT)
models, MAE-CT excels over previous self-supervised methods trained on ImageNet
in linear probing, k-NN and low-shot classification accuracy as well as in
unsupervised clustering accuracy. With ViT-H/16 MAE-CT achieves a new
state-of-the-art in linear probing of 82.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1&quot;&gt;Johannes Lehner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkin_B/0/1/0/all/0/1&quot;&gt;Benedikt Alkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1&quot;&gt;Andreas F&amp;#xfc;rst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1&quot;&gt;Elisabeth Rumetshofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miklautz_L/0/1/0/all/0/1&quot;&gt;Lukas Miklautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12241">
<title>Positive AI: Key Challenges for Designing Wellbeing-aligned Artificial Intelligence. (arXiv:2304.12241v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12241</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) is rapidly transforming society, creating an
urgent need to ensure its positive impact. In this article, we take a positive
design approach towards this issue, viewing it as a matter of designing AI
systems that actively support human wellbeing. However, designing
wellbeing-aligned AI systems is difficult. This article adopts a cybernetic
perspective to identify twelve key challenges across two categories: lack of
knowledge and lack of motivation. Knowledge barriers include challenges in
conceptualizing, measuring, and optimizing for wellbeing, then designing
appropriate AI actions. Motivation barriers include misaligned incentives,
financial and publicity risks, and a lack of data access preventing
(third-party) research on wellbeing. To address these challenges we have
captured our key takeaways in a research agenda related to 1) advancing the
scientific understanding of the impact of AI systems on wellbeing, and 2)
guiding design actions on how AI systems might be intentionally designed to
promote and sustain wellbeing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maden_W/0/1/0/all/0/1&quot;&gt;Willem van der Maden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomas_D/0/1/0/all/0/1&quot;&gt;Derek Lomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadek_M/0/1/0/all/0/1&quot;&gt;Malak Sadek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hekkert_P/0/1/0/all/0/1&quot;&gt;Paul Hekkert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13522">
<title>Sequential decomposition of propositional logic programs. (arXiv:2304.13522v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13522</link>
<description rdf:parseType="Literal">&lt;p&gt;The sequential composition of propositional logic programs has been recently
introduced. This paper studies the sequential {\em decomposition} of programs
by studying Green&apos;s relations $\mathcal{L,R,J}$ -- well-known in semigroup
theory -- between programs. In a broader sense, this paper is a further step
towards an algebraic theory of logic programming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Anti&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10403">
<title>PaLM 2 Technical Report. (arXiv:2305.10403v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10403</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PaLM 2, a new state-of-the-art language model that has better
multilingual and reasoning capabilities and is more compute-efficient than its
predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture
of objectives. Through extensive evaluations on English and multilingual
language, and reasoning tasks, we demonstrate that PaLM 2 has significantly
improved quality on downstream tasks across different model sizes, while
simultaneously exhibiting faster and more efficient inference compared to PaLM.
This improved efficiency enables broader deployment while also allowing the
model to respond faster, for a more natural pace of interaction. PaLM 2
demonstrates robust reasoning capabilities exemplified by large improvements
over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable
performance on a suite of responsible AI evaluations, and enables
inference-time control over toxicity without additional overhead or impact on
other capabilities. Overall, PaLM 2 achieves state-of-the-art performance
across a diverse set of tasks and capabilities.
&lt;/p&gt;
&lt;p&gt;When discussing the PaLM 2 family, it is important to distinguish between
pre-trained models (of various sizes), fine-tuned variants of these models, and
the user-facing products that use these models. In particular, user-facing
products typically include additional pre- and post-processing steps.
Additionally, the underlying models may evolve over time. Therefore, one should
not expect the performance of user-facing products to exactly match the results
reported in this report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1&quot;&gt;Rohan Anil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1&quot;&gt;Orhan Firat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Melvin Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepikhin_D/0/1/0/all/0/1&quot;&gt;Dmitry Lepikhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passos_A/0/1/0/all/0/1&quot;&gt;Alexandre Passos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakeri_S/0/1/0/all/0/1&quot;&gt;Siamak Shakeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taropa_E/0/1/0/all/0/1&quot;&gt;Emanuel Taropa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_P/0/1/0/all/0/1&quot;&gt;Paige Bailey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_E/0/1/0/all/0/1&quot;&gt;Eric Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;Jonathan H. Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafey_L/0/1/0/all/0/1&quot;&gt;Laurent El Shafey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yanping Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meier_Hellstern_K/0/1/0/all/0/1&quot;&gt;Kathy Meier-Hellstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1&quot;&gt;Gaurav Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1&quot;&gt;Erica Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1&quot;&gt;Mark Omernick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1&quot;&gt;Kevin Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1&quot;&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_K/0/1/0/all/0/1&quot;&gt;Kefan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanzhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yujing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1&quot;&gt;Gustavo Hernandez Abrego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1&quot;&gt;Junwhan Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1&quot;&gt;Jacob Austin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1&quot;&gt;Paul Barham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1&quot;&gt;Jan Botha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1&quot;&gt;James Bradbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1&quot;&gt;Siddhartha Brahma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_K/0/1/0/all/0/1&quot;&gt;Kevin Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catasta_M/0/1/0/all/0/1&quot;&gt;Michele Catasta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherry_C/0/1/0/all/0/1&quot;&gt;Colin Cherry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1&quot;&gt;Christopher A. Choquette-Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1&quot;&gt;Aakanksha Chowdhery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Crepy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_S/0/1/0/all/0/1&quot;&gt;Shachi Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1&quot;&gt;Sunipa Dev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1&quot;&gt;Jacob Devlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1&quot;&gt;Mark D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1&quot;&gt;Nan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1&quot;&gt;Ethan Dyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feinberg_V/0/1/0/all/0/1&quot;&gt;Vlad Feinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fangxiaoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fienber_V/0/1/0/all/0/1&quot;&gt;Vlad Fienber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1&quot;&gt;Markus Freitag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1&quot;&gt;Xavier Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1&quot;&gt;Sebastian Gehrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_L/0/1/0/all/0/1&quot;&gt;Lucas Gonzalez&lt;/a&gt;, et al. (76 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11322">
<title>SpikeCP: Delay-Adaptive Reliable Spiking Neural Networks via Conformal Prediction. (arXiv:2305.11322v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11322</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation. In
typical implementations of an SNN classifier, decisions are produced after the
entire input sequence has been processed, resulting in latency and energy
consumption levels that are fairly uniform across inputs. Recently introduced
delay-adaptive SNNs tailor the inference latency -- and, with it, the energy
consumption -- to the difficulty of each example, by producing an early
decision when the SNN model is sufficiently ``confident&apos;&apos;. In this paper, we
start by observing that, as an SNN processes input samples, its classification
decisions tend to be first under-confident and then over-confident with respect
to the decision&apos;s ground-truth, unknown, test accuracy. This makes it difficult
to determine a stopping time that ensures a desired level of accuracy. To
address this problem, we introduce a novel delay-adaptive SNN-based inference
methodology that, wrapping around any pre-trained SNN classifier, provides
guaranteed reliability for the decisions produced at input-dependent stopping
times. The approach entails minimal added complexity as compared to the
underlying SNN, requiring only thresholding and counting operations at run
time, and it leverages tools from conformal prediction (CP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiechen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15021">
<title>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought. (arXiv:2305.15021v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15021</link>
<description rdf:parseType="Literal">&lt;p&gt;Embodied AI is a crucial frontier in robotics, capable of planning and
executing action sequences for robots to accomplish long-horizon tasks in
physical environments. In this work, we introduce EmbodiedGPT, an end-to-end
multi-modal foundation model for embodied AI, empowering embodied agents with
multi-modal understanding and execution capabilities. To achieve this, we have
made the following efforts: (i) We craft a large-scale embodied planning
dataset, termed EgoCOT. The dataset consists of carefully selected videos from
the Ego4D dataset, along with corresponding high-quality language instructions.
Specifically, we generate a sequence of sub-goals with the &quot;Chain of Thoughts&quot;
mode for effective embodied planning. (ii) We introduce an efficient training
approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B
large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We
introduce a paradigm for extracting task-related features from LLM-generated
planning queries to form a closed loop between high-level planning and
low-level control. Extensive experiments show the effectiveness of EmbodiedGPT
on embodied tasks, including embodied planning, embodied control, visual
captioning, and visual question answering. Notably, EmbodiedGPT significantly
enhances the success rate of the embodied control task by extracting more
effective features. It has achieved a remarkable 1.6 times increase in success
rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World
benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yao Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mengkang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jun Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05064">
<title>K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization. (arXiv:2306.05064v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05064</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved great success in general domains
of natural language processing. In this paper, we bring LLMs to the realm of
geoscience with the objective of advancing research and applications in this
field. To this end, we present the first-ever LLM in geoscience, K2, alongside
a suite of resources developed to further promote LLM research within
geoscience. For instance, we have curated the first geoscience instruction
tuning dataset, GeoSignal, which aims to align LLM responses to
geoscience-related user queries. Additionally, we have established the first
geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience.
In this work, we experiment with a complete recipe to adapt a pre-trained
general-domain LLM to the geoscience domain. Specifically, we further train the
LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1
million pieces of geoscience literature, and utilize GeoSignal&apos;s supervised
data to fine-tune the model. Moreover, we share a protocol that can efficiently
gather domain-specific data and construct domain-supervised data, even in
situations where manpower is scarce. Meanwhile, we equip K2 with the abilities
of using tools to be a naive geoscience aide. Experiments conducted on the
GeoBench demonstrate the effectiveness of our approach and datasets on
geoscience knowledge understanding and utilization.We open-source all the
training data and K2 model checkpoints at https://github.com/davendw49/k2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Cheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhongmou He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Luoyi Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chenghu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouhan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05659">
<title>COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models. (arXiv:2306.05659v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05659</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt-based learning has been proved to be an effective way in pre-trained
language models (PLMs), especially in low-resource scenarios like few-shot
settings. However, the trustworthiness of PLMs is of paramount significance and
potential vulnerabilities have been shown in prompt-based templates that could
mislead the predictions of language models, causing serious security concerns.
In this paper, we will shed light on some vulnerabilities of PLMs, by proposing
a prompt-based adversarial attack on manual templates in black box scenarios.
First of all, we design character-level and word-level heuristic approaches to
break manual templates separately. Then we present a greedy algorithm for the
attack based on the above heuristic destructive approaches. Finally, we
evaluate our approach with the classification tasks on three variants of BERT
series models and eight datasets. And comprehensive experimental results
justify the effectiveness of our approach in terms of attack success rate and
attack speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zihao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenbin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongjian Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14565">
<title>Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14565</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the promising progress in multi-modal tasks, current large
multi-modal models (LMM) are prone to hallucinating inconsistent descriptions
with respect to the associated image and human instructions. This paper
addresses this issue by introducing the first large and diverse visual
instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.
Our dataset consists of 120k visual instructions generated by GPT4, covering 16
vision-and-language tasks with open-ended instructions and answers. Unlike
existing studies that primarily focus on positive instruction samples, we
design LRV-Instruction to include both positive and negative instructions for
more robust visual instruction tuning. Our negative instructions are designed
at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent
Element Manipulation. To efficiently measure the hallucination generated by
LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel
approach to evaluate visual instruction tuning without the need for
human-annotated groundtruth answers and can adapt to diverse instruction
formats. We conduct comprehensive experiments to investigate the hallucination
of LMMs. Our results demonstrate that existing LMMs exhibit significant
hallucination when presented with our negative instructions, particularly with
Existent Element Manipulation instructions. Moreover, by finetuning MiniGPT4 on
LRV-Instruction, we successfully mitigate hallucination while improving
performance on public datasets using less training data compared to
state-of-the-art methods. Additionally, we observed that a balanced ratio of
positive and negative instances in the training data leads to a more robust
model. Updates of our project are available at
https://fuxiaoliu.github.io/LRV/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fuxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yacoob_Y/0/1/0/all/0/1&quot;&gt;Yaser Yacoob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00184">
<title>Personality Traits in Large Language Models. (arXiv:2307.00184v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00184</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has revolutionized natural
language processing, enabling the generation of coherent and contextually
relevant human-like text. As LLMs increasingly power conversational agents used
by the general public world-wide, the synthetic personality embedded in these
models, by virtue of training on large amounts of human data, is becoming
increasingly important. Since personality is a key factor determining the
effectiveness of communication, we present a comprehensive method for
administering and validating personality tests on widely-used LLMs, as well as
for shaping personality in the generated text of such LLMs. Applying this
method, we found: 1) personality measurements in the outputs of some LLMs under
specific prompting configurations are reliable and valid; 2) evidence of
reliability and validity of synthetic LLM personality is stronger for larger
and instruction fine-tuned models; and 3) personality in LLM outputs can be
shaped along desired dimensions to mimic specific human personality profiles.
We discuss application and ethical implications of the measurement and shaping
method, in particular regarding responsible AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serapio_Garcia_G/0/1/0/all/0/1&quot;&gt;Greg Serapio-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safdari_M/0/1/0/all/0/1&quot;&gt;Mustafa Safdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Crepy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Luning Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1&quot;&gt;Stephen Fitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1&quot;&gt;Peter Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1&quot;&gt;Marwa Abdulhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1&quot;&gt;Maja Matari&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10932">
<title>Identical and Fraternal Twins: Fine-Grained Semantic Contrastive Learning of Sentence Representations. (arXiv:2307.10932v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10932</link>
<description rdf:parseType="Literal">&lt;p&gt;The enhancement of unsupervised learning of sentence representations has been
significantly achieved by the utility of contrastive learning. This approach
clusters the augmented positive instance with the anchor instance to create a
desired embedding space. However, relying solely on the contrastive objective
can result in sub-optimal outcomes due to its inability to differentiate subtle
semantic variations between positive pairs. Specifically, common data
augmentation techniques frequently introduce semantic distortion, leading to a
semantic margin between the positive pair. While the InfoNCE loss function
overlooks the semantic margin and prioritizes similarity maximization between
positive pairs during training, leading to the insensitive semantic
comprehension ability of the trained model. In this paper, we introduce a novel
Identical and Fraternal Twins of Contrastive Learning (named IFTCL) framework,
capable of simultaneously adapting to various positive pairs generated by
different augmentation techniques. We propose a \textit{Twins Loss} to preserve
the innate margin during training and promote the potential of data enhancement
in order to overcome the sub-optimal issue. We also present proof-of-concept
experiments combined with the contrastive objective to prove the validity of
the proposed Twins Loss. Furthermore, we propose a hippocampus queue mechanism
to restore and reuse the negative instances without additional calculation,
which further enhances the efficiency and performance of the IFCL. We verify
the IFCL framework on nine semantic textual similarity tasks with both English
and Chinese datasets, and the experimental results show that IFCL outperforms
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Qingfa Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangyin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14799">
<title>Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14799</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern semiconductor manufacturing involves intricate production processes
consisting of hundreds of operations, which can take several months from lot
release to completion. The high-tech machines used in these processes are
diverse, operate on individual wafers, lots, or batches in multiple stages, and
necessitate product-specific setups and specialized maintenance procedures.
This situation is different from traditional job-shop scheduling scenarios,
which have less complex production processes and machines, and mainly focus on
solving highly combinatorial but abstract scheduling problems. In this work, we
address the scheduling of realistic semiconductor manufacturing processes by
modeling their specific requirements using hybrid Answer Set Programming with
difference logic, incorporating flexible machine processing, setup, batching
and maintenance operations. Unlike existing methods that schedule semiconductor
manufacturing processes locally with greedy heuristics or by independently
optimizing specific machine group allocations, we examine the potentials of
large-scale scheduling subject to multiple optimization objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Kholany_M/0/1/0/all/0/1&quot;&gt;Mohammed M. S. El-Kholany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1&quot;&gt;Ramsha Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebser_M/0/1/0/all/0/1&quot;&gt;Martin Gebser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16834">
<title>Benchmarking Jetson Edge Devices with an End-to-end Video-based Anomaly Detection System. (arXiv:2307.16834v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16834</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovative enhancement in embedded system platforms, specifically hardware
accelerations, significantly influence the application of deep learning in
real-world scenarios. These innovations translate human labor efforts into
automated intelligent systems employed in various areas such as autonomous
driving, robotics, Internet-of-Things (IoT), and numerous other impactful
applications. NVIDIA&apos;s Jetson platform is one of the pioneers in offering
optimal performance regarding energy efficiency and throughput in the execution
of deep learning algorithms. Previously, most benchmarking analysis was based
on 2D images with a single deep learning model for each comparison result. In
this paper, we implement an end-to-end video-based crime-scene anomaly
detection system inputting from surveillance videos and the system is deployed
and completely operates on multiple Jetson edge devices (Nano, AGX Xavier, Orin
Nano). The comparison analysis includes the integration of Torch-TensorRT as a
software developer kit from NVIDIA for the model performance optimisation. The
system is built based on the PySlowfast open-source project from Facebook as
the coding template. The end-to-end system process comprises the videos from
camera, data preprocessing pipeline, feature extractor and the anomaly
detection. We provide the experience of an AI-based system deployment on
various Jetson Edge devices with Docker technology. Regarding anomaly
detectors, a weakly supervised video-based deep learning model called Robust
Temporal Feature Magnitude Learning (RTFM) is applied in the system. The
approach system reaches 47.56 frames per second (FPS) inference speed on a
Jetson edge device with only 3.11 GB RAM usage total. We also discover the
promising Jetson device that the AI system achieves 15% better performance than
the previous version of Jetson devices while consuming 50% less energy power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hoang Viet Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thinh Gia Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1&quot;&gt;Chuong Dinh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1&quot;&gt;An Dinh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Hien Bich Vo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01921">
<title>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01921</link>
<description rdf:parseType="Literal">&lt;p&gt;Fast screening of drug molecules based on the ligand binding affinity is an
important step in the drug discovery pipeline. Graph neural fingerprint is a
promising method for developing molecular docking surrogates with high
throughput and great fidelity. In this study, we built a COVID-19 drug docking
dataset of about 300,000 drug candidates on 23 coronavirus protein targets.
With this dataset, we trained graph neural fingerprint docking models for
high-throughput virtual COVID-19 drug screening. The graph neural fingerprint
models yield high prediction accuracy on docking scores with the mean squared
error lower than $0.21$ kcal/mol for most of the docking targets, showing
significant improvement over conventional circular fingerprint methods. To make
the neural fingerprints transferable for unknown targets, we also propose a
transferable graph neural fingerprint method trained on multiple targets. With
comparable accuracy to target-specific graph neural fingerprint models, the
transferable model exhibits superb training and data efficiency. We highlight
that the impact of this study extends beyond COVID-19 dataset, as our approach
for fast virtual ligand screening can be easily adapted and integrated into a
general machine learning-accelerated pipeline to battle future bio-threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yihui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kagawa_A/0/1/0/all/0/1&quot;&gt;Ai Kagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Carbone_M/0/1/0/all/0/1&quot;&gt;Matthew R. Carbone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Samuel Yen-Chi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1&quot;&gt;Austin Clyde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1&quot;&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1&quot;&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dam_H/0/1/0/all/0/1&quot;&gt;Hubertus J. J. van Dam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Deyu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03527">
<title>Exploring ChatGPT&apos;s Empathic Abilities. (arXiv:2308.03527v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03527</link>
<description rdf:parseType="Literal">&lt;p&gt;Empathy is often understood as the ability to share and understand another
individual&apos;s state of mind or emotion. With the increasing use of chatbots in
various domains, e.g., children seeking help with homework, individuals looking
for medical advice, and people using the chatbot as a daily source of everyday
companionship, the importance of empathy in human-computer interaction has
become more apparent. Therefore, our study investigates the extent to which
ChatGPT based on GPT-3.5 can exhibit empathetic responses and emotional
expressions. We analyzed the following three aspects: (1) understanding and
expressing emotions, (2) parallel emotional response, and (3) empathic
personality. Thus, we not only evaluate ChatGPT on various empathy aspects and
compare it with human behavior but also show a possible way to analyze the
empathy of chatbots in general. Our results show, that in 91.7% of the cases,
ChatGPT was able to correctly identify emotions and produces appropriate
answers. In conversations, ChatGPT reacted with a parallel emotion in 70.7% of
cases. The empathic capabilities of ChatGPT were evaluated using a set of five
questionnaires covering different aspects of empathy. Even though the results
indicate that the empathic abilities of ChatGPT are still below the average of
healthy humans, the scores are better than those of people who have been
diagnosed with Asperger syndrome / high-functioning autism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaaff_K/0/1/0/all/0/1&quot;&gt;Kristina Schaaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinig_C/0/1/0/all/0/1&quot;&gt;Caroline Reinig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1&quot;&gt;Tim Schlippe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07200">
<title>Neural Categorical Priors for Physics-Based Character Control. (arXiv:2308.07200v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07200</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in learning reusable motion priors have demonstrated their
effectiveness in generating naturalistic behaviors. In this paper, we propose a
new learning framework in this paradigm for controlling physics-based
characters with significantly improved motion quality and diversity over
existing state-of-the-art methods. The proposed method uses reinforcement
learning (RL) to initially track and imitate life-like movements from
unstructured motion clips using the discrete information bottleneck, as adopted
in the Vector Quantized Variational AutoEncoder (VQ-VAE). This structure
compresses the most relevant information from the motion clips into a compact
yet informative latent space, i.e., a discrete space over vector quantized
codes. By sampling codes in the space from a trained categorical prior
distribution, high-quality life-like behaviors can be generated, similar to the
usage of VQ-VAE in computer vision. Although this prior distribution can be
trained with the supervision of the encoder&apos;s output, it follows the original
motion clip distribution in the dataset and could lead to imbalanced behaviors
in our setting. To address the issue, we further propose a technique named
prior shifting to adjust the prior distribution using curiosity-driven RL. The
outcome distribution is demonstrated to offer sufficient behavioral diversity
and significantly facilitates upper-level policy learning for downstream tasks.
We conduct comprehensive experiments using humanoid characters on two
challenging downstream tasks, sword-shield striking and two-player boxing game.
Our results demonstrate that the proposed framework is capable of controlling
the character to perform considerably high-quality movements in terms of
behavioral strategies, diversity, and realism. Videos, codes, and data are
available at https://tencent-roboticsx.github.io/NCP/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingxu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;He Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Mengting Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08518">
<title>Exploiting Point-Wise Attention in 6D Object Pose Estimation Based on Bidirectional Prediction. (arXiv:2308.08518v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08518</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional geometric registration based estimation methods only exploit the
CAD model implicitly, which leads to their dependence on observation quality
and deficiency to occlusion. To address the problem,the paper proposes a
bidirectional correspondence prediction network with a point-wise
attention-aware mechanism. This network not only requires the model points to
predict the correspondence but also explicitly models the geometric
similarities between observations and the model prior. Our key insight is that
the correlations between each model point and scene point provide essential
information for learning point-pair matches. To further tackle the correlation
noises brought by feature distribution divergence, we design a simple but
effective pseudo-siamese network to improve feature homogeneity. Experimental
results on the public datasets of LineMOD, YCB-Video, and Occ-LineMOD show that
the proposed method achieves better performance than other state-of-the-art
methods under the same evaluation criteria. Its robustness in estimating poses
is greatly improved, especially in an environment with severe occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangjian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11764">
<title>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1&quot;&gt;Mohamed Elaraby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mengyin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1&quot;&gt;Jacob Dunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Pingchuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00917">
<title>Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports. (arXiv:2309.00917v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;The way we analyse clinical texts has undergone major changes over the last
years. The introduction of language models such as BERT led to adaptations for
the (bio)medical domain like PubMedBERT and ClinicalBERT. These models rely on
large databases of archived medical documents. While performing well in terms
of accuracy, both the lack of interpretability and limitations to transfer
across languages limit their use in clinical setting. We introduce a novel
light-weight graph-based embedding method specifically catering radiology
reports. It takes into account the structure and composition of the report,
while also connecting medical terms in the report through the multi-lingual
SNOMED Clinical Terms knowledge base. The resulting graph embedding uncovers
the underlying relationships among clinical terms, achieving a representation
that is better understandable for clinicians and clinically more accurate,
without reliance on large pre-training datasets. We show the use of this
embedding on two tasks namely disease classification of X-ray reports and image
classification. For disease classification our model is competitive with its
BERT-based counterparts, while being magnitudes smaller in size and training
data requirements. For image classification, we show the effectiveness of the
graph embedding leveraging cross-modal knowledge transfer and show how this
method is usable across different languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonsbeek_T/0/1/0/all/0/1&quot;&gt;Tom van Sonsbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1&quot;&gt;Marcel Worring&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00964">
<title>eDKM: An Efficient and Accurate Train-time Weight Clustering for Large Language Models. (arXiv:2309.00964v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00964</link>
<description rdf:parseType="Literal">&lt;p&gt;Since Large Language Models or LLMs have demonstrated high-quality
performance on many complex language tasks, there is a great interest in
bringing these LLMs to mobile devices for faster responses and better privacy
protection. However, the size of LLMs (i.e., billions of parameters) requires
highly effective compression to fit into storage-limited devices. Among many
compression techniques, weight-clustering, a form of non-linear quantization,
is one of the leading candidates for LLM compression, and supported by modern
smartphones. Yet, its training overhead is prohibitively significant for LLM
fine-tuning. Especially, Differentiable KMeans Clustering, or DKM, has shown
the state-of-the-art trade-off between compression ratio and accuracy
regression, but its large memory complexity makes it nearly impossible to apply
to train-time LLM compression. In this paper, we propose a memory-efficient DKM
implementation, eDKM powered by novel techniques to reduce the memory footprint
of DKM by orders of magnitudes. For a given tensor to be saved on CPU for the
backward pass of DKM, we compressed the tensor by applying uniquification and
sharding after checking if there is no duplicated tensor previously copied to
CPU. Our experimental results demonstrate that \prjname can fine-tune and
compress a pretrained LLaMA 7B model from 12.6 GB to 2.5 GB (3bit/weight) with
the Alpaca dataset by reducing the train-time memory footprint of a decoder
layer by 130$\times$, while delivering good accuracy on broader LLM benchmarks
(i.e., 77.7% for PIQA, 66.1% for Winograde, and so on).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Minsik Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahid_K/0/1/0/all/0/1&quot;&gt;Keivan A. Vahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qichen Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adya_S/0/1/0/all/0/1&quot;&gt;Saurabh Adya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundo_C/0/1/0/all/0/1&quot;&gt;Carlo C Del Mundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegari_M/0/1/0/all/0/1&quot;&gt;Mohammad Rastegari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_D/0/1/0/all/0/1&quot;&gt;Devang Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zatloukal_P/0/1/0/all/0/1&quot;&gt;Peter Zatloukal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01955">
<title>A Survey on Interpretable Cross-modal Reasoning. (arXiv:2309.01955v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01955</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, cross-modal reasoning (CMR), the process of understanding
and reasoning across different modalities, has emerged as a pivotal area with
applications spanning from multimedia analysis to healthcare diagnostics. As
the deployment of AI systems becomes more ubiquitous, the demand for
transparency and comprehensibility in these systems&apos; decision-making processes
has intensified. This survey delves into the realm of interpretable cross-modal
reasoning (I-CMR), where the objective is not only to achieve high predictive
performance but also to provide human-understandable explanations for the
results. This survey presents a comprehensive overview of the typical methods
with a three-level taxonomy for I-CMR. Furthermore, this survey reviews the
existing CMR datasets with annotations for explanations. Finally, this survey
summarizes the challenges for I-CMR and discusses potential future directions.
In conclusion, this survey aims to catalyze the progress of this emerging
research area by providing researchers with a panoramic and comprehensive
perspective, illuminating the state of the art and discerning the
opportunities. The summarized methods, datasets, and other resources are
available at
https://github.com/ZuyiZhou/Awesome-Interpretable-Cross-modal-Reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_D/0/1/0/all/0/1&quot;&gt;Dizhan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengsheng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zuyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02561">
<title>Physically Grounded Vision-Language Models for Robotic Manipulation. (arXiv:2309.02561v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02561</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision-language models (VLMs) have led to improved
performance on tasks such as visual question answering and image captioning.
Consequently, these models are now well-positioned to reason about the physical
world, particularly within domains such as robotic manipulation. However,
current VLMs are limited in their understanding of the physical concepts (e.g.,
material, fragility) of common objects, which restricts their usefulness for
robotic manipulation tasks that involve interaction and physical reasoning
about such objects. To address this limitation, we propose PhysObjects, an
object-centric dataset of 39.6K crowd-sourced and 417K automated physical
concept annotations of common household objects. We demonstrate that
fine-tuning a VLM on PhysObjects improves its understanding of physical object
concepts, including generalization to held-out concepts, by capturing human
priors of these concepts from visual appearance. We incorporate this
physically-grounded VLM in an interactive framework with a large language
model-based robotic planner, and show improved planning performance on tasks
that require reasoning about physical object concepts, compared to baselines
that do not leverage physically-grounded VLMs. We additionally illustrate the
benefits of our physically-grounded VLM on a real robot, where it improves task
success rates. We release our dataset and provide further details and
visualizations of our results at https://iliad.stanford.edu/pg-vlm/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jensen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_B/0/1/0/all/0/1&quot;&gt;Bidipta Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04802">
<title>CPMR: Context-Aware Incremental Sequential Recommendation with Pseudo-Multi-Task Learning. (arXiv:2309.04802v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04802</link>
<description rdf:parseType="Literal">&lt;p&gt;The motivations of users to make interactions can be divided into static
preference and dynamic interest. To accurately model user representations over
time, recent studies in sequential recommendation utilize information
propagation and evolution to mine from batches of arriving interactions.
However, they ignore the fact that people are easily influenced by the recent
actions of other users in the contextual scenario, and applying evolution
across all historical interactions dilutes the importance of recent ones, thus
failing to model the evolution of dynamic interest accurately. To address this
issue, we propose a Context-Aware Pseudo-Multi-Task Recommender System (CPMR)
to model the evolution in both historical and contextual scenarios by creating
three representations for each user and item under different dynamics: static
embedding, historical temporal states, and contextual temporal states. To
dually improve the performance of temporal states evolution and incremental
recommendation, we design a Pseudo-Multi-Task Learning (PMTL) paradigm by
stacking the incremental single-target recommendations into one multi-target
task for joint optimization. Within the PMTL paradigm, CPMR employs a
shared-bottom network to conduct the evolution of temporal states across
historical and contextual scenarios, as well as the fusion of them at the
user-item level. In addition, CPMR incorporates one real tower for incremental
predictions, and two pseudo towers dedicated to updating the respective
temporal states based on new batches of interactions. Experimental results on
four benchmark recommendation datasets show that CPMR consistently outperforms
state-of-the-art baselines and achieves significant gains on three of them. The
code is available at: https://github.com/DiMarzioBian/CPMR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Q/0/1/0/all/0/1&quot;&gt;Qingtian Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hui Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1&quot;&gt;Yiping Ke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05680">
<title>Evaluating Chatbots to Promote Users&apos; Trust -- Practices and Open Problems. (arXiv:2309.05680v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05680</link>
<description rdf:parseType="Literal">&lt;p&gt;Chatbots, the common moniker for collaborative assistants, are Artificial
Intelligence (AI) software that enables people to naturally interact with them
to get tasks done. Although chatbots have been studied since the dawn of AI,
they have particularly caught the imagination of the public and businesses
since the launch of easy-to-use and general-purpose Large Language Model-based
chatbots like ChatGPT. As businesses look towards chatbots as a potential
technology to engage users, who may be end customers, suppliers, or even their
own employees, proper testing of chatbots is important to address and mitigate
issues of trust related to service or product performance, user satisfaction
and long-term unintended consequences for society. This paper reviews current
practices for chatbot testing, identifies gaps as open problems in pursuit of
user trust, and outlines a path forward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1&quot;&gt;Kausik Lakkaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koppel_T/0/1/0/all/0/1&quot;&gt;Tarmo Koppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1&quot;&gt;Vignesh Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1&quot;&gt;Ashish Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Sachindra Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05918">
<title>Stochastic LLMs do not Understand Language: Towards Symbolic, Explainable and Ontologically Based LLMs. (arXiv:2309.05918v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05918</link>
<description rdf:parseType="Literal">&lt;p&gt;In our opinion the exuberance surrounding the relative success of data-driven
large language models (LLMs) is slightly misguided and for several reasons (i)
LLMs cannot be relied upon for factual information since for LLMs all ingested
text (factual or non-factual) was created equal; (ii) due to their subsymbolic
na-ture, whatever &apos;knowledge&apos; these models acquire about language will always
be buried in billions of microfeatures (weights), none of which is meaningful
on its own; and (iii) LLMs will often fail to make the correct inferences in
several linguistic contexts (e.g., nominal compounds, copredication, quantifier
scope ambi-guities, intensional contexts. Since we believe the relative success
of data-driven large language models (LLMs) is not a reflection on the symbolic
vs. subsymbol-ic debate but a reflection on applying the successful strategy of
a bottom-up reverse engineering of language at scale, we suggest in this paper
applying the effective bottom-up strategy in a symbolic setting resulting in
symbolic, explainable, and ontologically grounded language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid S. Saba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06604">
<title>Hybrid Algorithm Selection and Hyperparameter Tuning on Distributed Machine Learning Resources: A Hierarchical Agent-based Approach. (arXiv:2309.06604v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06604</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithm selection and hyperparameter tuning are critical steps in both
academic and applied machine learning. On the other hand, these steps are
becoming ever increasingly delicate due to the extensive rise in the number,
diversity, and distributedness of machine learning resources. Multi-agent
systems, when applied to the design of machine learning platforms, bring about
several distinctive characteristics such as scalability, flexibility, and
robustness, just to name a few. This paper proposes a fully automatic and
collaborative agent-based mechanism for selecting distributedly organized
machine learning algorithms and simultaneously tuning their hyperparameters.
Our method builds upon an existing agent-based hierarchical machine-learning
platform and augments its query structure to support the aforementioned
functionalities without being limited to specific learning, selection, and
tuning mechanisms. We have conducted theoretical assessments, formal
verification, and analytical study to demonstrate the correctness, resource
utilization, and computational efficiency of our technique. According to the
results, our solution is totally correct and exhibits linear time and space
complexity in relation to the size of available resources. To provide concrete
examples of how the proposed methodologies can effectively adapt and perform
across a range of algorithmic options and datasets, we have also conducted a
series of experiments using a system comprised of 24 algorithms and 9 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esmaeili_A/0/1/0/all/0/1&quot;&gt;Ahmad Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1&quot;&gt;Julia T. Rayz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matson_E/0/1/0/all/0/1&quot;&gt;Eric T. Matson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06800">
<title>Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06800</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is a crucial topic because of its broad scope of
applications in the transportation domain. Recently, various studies have
achieved promising results. However, most studies assume the prediction
locations have complete or at least partial historical records and cannot be
extended to non-historical recorded locations. In real-life scenarios, the
deployment of sensors could be limited due to budget limitations and
installation availability, which makes most current models not applicable.
Though few pieces of literature tried to impute traffic states at the missing
locations, these methods need the data simultaneously observed at the locations
with sensors, making them not applicable to prediction tasks. Another drawback
is the lack of measurement of uncertainty in prediction, making prior works
unsuitable for risk-sensitive tasks or involving decision-making. To fill the
gap, inspired by the previous inductive graph neural network, this work
proposed an uncertainty-aware framework with the ability to 1) extend
prediction to missing locations with no historical records and significantly
extend spatial coverage of prediction locations while reducing deployment of
sensors and 2) generate probabilistic prediction with uncertainty
quantification to help the management of risk and decision making in the
down-stream tasks. Through extensive experiments on real-life datasets, the
result shows our method achieved promising results on prediction tasks, and the
uncertainty quantification gives consistent results which highly correlated
with the locations with and without historical data. We also show that our
model could help support sensor deployment tasks in the transportation field to
achieve higher accuracy with a limited sensor deployment budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junxian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhiming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06841">
<title>On the Local Quadratic Stability of T-S Fuzzy Systems in the Vicinity of the Origin. (arXiv:2309.06841v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06841</link>
<description rdf:parseType="Literal">&lt;p&gt;The main goal of this paper is to introduce new local stability conditions
for continuous-time Takagi-Sugeno (T-S) fuzzy systems. These stability
conditions are based on linear matrix inequalities (LMIs) in combination with
quadratic Lyapunov functions. Moreover, they integrate information on the
membership functions at the origin and effectively leverage the linear
structure of the underlying nonlinear system in the vicinity of the origin. As
a result, the proposed conditions are proved to be less conservative compared
to existing methods using fuzzy Lyapunov functions in the literature. Moreover,
we establish that the proposed methods offer necessary and sufficient
conditions for the local exponential stability of T-S fuzzy systems. The paper
also includes discussions on the inherent limitations associated with fuzzy
Lyapunov approaches. To demonstrate the theoretical results, we provide
comprehensive examples that elucidate the core concepts and validate the
efficacy of the proposed conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Donghwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Do Wan Kim&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>