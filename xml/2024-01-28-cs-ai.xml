<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13835" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14371" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.07295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12255" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.13672">
<title>Transforming Agriculture with Intelligent Data Management and Insights. (arXiv:2401.13672v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2401.13672</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern agriculture faces grand challenges to meet increased demands for food,
fuel, feed, and fiber with population growth under the constraints of climate
change and dwindling natural resources. Data innovation is urgently required to
secure and improve the productivity, sustainability, and resilience of our
agroecosystems. As various sensors and Internet of Things (IoT) instrumentation
become more available, affordable, reliable, and stable, it has become possible
to conduct data collection, integration, and analysis at multiple temporal and
spatial scales, in real-time, and with high resolutions. At the same time, the
sheer amount of data poses a great challenge to data storage and analysis, and
the \textit{de facto} data management and analysis practices adopted by
scientists have become increasingly inefficient. Additionally, the data
generated from different disciplines, such as genomics, phenomics, environment,
agronomy, and socioeconomic, can be highly heterogeneous. That is, datasets
across disciplines often do not share the same ontology, modality, or format.
All of the above make it necessary to design a new data management
infrastructure that implements the principles of Findable, Accessible,
Interoperable, and Reusable (FAIR). In this paper, we propose Agriculture Data
Management and Analytics (ADMA), which satisfies the FAIR principles. Our new
data management infrastructure is intelligent by supporting semantic data
management across disciplines, interactive by providing various data
management/analysis portals such as web GUI, command line, and API, scalable by
utilizing the power of high-performance computing (HPC), extensible by allowing
users to load their own data analysis tools, trackable by keeping track of
different operations on each file, and open by using a rich set of mature open
source technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongfeng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1&quot;&gt;Geng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yufeng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_J/0/1/0/all/0/1&quot;&gt;Joe Luck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awada_T/0/1/0/all/0/1&quot;&gt;Tala Awada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13677">
<title>Process Mining for Unstructured Data: Challenges and Research Directions. (arXiv:2401.13677v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2401.13677</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koschmider_A/0/1/0/all/0/1&quot;&gt;Agnes Koschmider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aleknonyte_Resch_M/0/1/0/all/0/1&quot;&gt;Milda Aleknonyt&amp;#x117;-Resch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonger_F/0/1/0/all/0/1&quot;&gt;Frederik Fonger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imenkamp_C/0/1/0/all/0/1&quot;&gt;Christian Imenkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepsien_A/0/1/0/all/0/1&quot;&gt;Arvid Lepsien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apaydin_K/0/1/0/all/0/1&quot;&gt;Kaan Apaydin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harms_M/0/1/0/all/0/1&quot;&gt;Maximilian Harms&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_D/0/1/0/all/0/1&quot;&gt;Dominik Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langhammer_D/0/1/0/all/0/1&quot;&gt;Dominic Langhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziolkowski_T/0/1/0/all/0/1&quot;&gt;Tobias Ziolkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisgen_Y/0/1/0/all/0/1&quot;&gt;Yorck Zisgen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13693">
<title>Challenge design roadmap. (arXiv:2401.13693v1 [cs.OH])</title>
<link>http://arxiv.org/abs/2401.13693</link>
<description rdf:parseType="Literal">&lt;p&gt;Challenges can be seen as a type of game that motivates participants to solve
serious tasks. As a result, competition organizers must develop effective game
rules. However, these rules have multiple objectives beyond making the game
enjoyable for participants. These objectives may include solving real-world
problems, advancing scientific or technical areas, making scientific
discoveries, and educating the public. In many ways, creating a challenge is
similar to launching a product. It requires the same level of excitement and
rigorous testing, and the goal is to attract &apos;&apos;customers&apos;&apos; in the form of
participants. The process begins with a solid plan, such as a competition
proposal that will eventually be submitted to an international conference and
subjected to peer review. Although peer review does not guarantee quality, it
does force organizers to consider the impact of their challenge, identify
potential oversights, and generally improve its quality. This chapter provides
guidelines for creating a strong plan for a challenge. The material draws on
the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and
Tailor 3 , as well as the NeurIPS proposal template, which some of the authors
contributed to.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balderas_H/0/1/0/all/0/1&quot;&gt;Hugo Jair Escalante Balderas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guyon_I/0/1/0/all/0/1&quot;&gt;Isabelle Guyon&lt;/a&gt; (LISN, TAU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_A/0/1/0/all/0/1&quot;&gt;Addison Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reade_W/0/1/0/all/0/1&quot;&gt;Walter Reade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treguer_S/0/1/0/all/0/1&quot;&gt;Sebastien Treguer&lt;/a&gt; (TAU)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13697">
<title>Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13697</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multimodal sentiment analysis tasks are highly rely on the
assumption that the training and test sets are complete multimodal data, while
this assumption can be difficult to hold: the multimodal data are often
incomplete in real-world scenarios. Therefore, a robust multimodal model in
scenarios with randomly missing modalities is highly preferred. Recently,
CLIP-based multimodal foundational models have demonstrated impressive
performance on numerous multimodal tasks by learning the aligned cross-modal
semantics of image and text pairs, but the multimodal foundational models are
also unable to directly address scenarios involving modality absence. To
alleviate this issue, we propose a simple and effective framework, namely TRML,
Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML
employs generated virtual modalities to replace missing modalities, and aligns
the semantic spaces between the generated and missing modalities. Concretely,
we design a missing modality inference module to generate virtual modaliites
and replace missing modalities. We also design a semantic matching learning
module to align semantic spaces generated and missing modalities. Under the
prompt of complete modality, our model captures the semantics of missing
modalities by leveraging the aligned cross-modal semantic space. Experiments
demonstrate the superiority of our approach on three multimodal sentiment
analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xianbing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1&quot;&gt;Soujanya Poria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuejiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Buzhou Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13699">
<title>Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey. (arXiv:2401.13699v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.13699</link>
<description rdf:parseType="Literal">&lt;p&gt;The Internet of things (IoT) can significantly enhance the quality of human
life, specifically in healthcare, attracting extensive attentions to
IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as
an innovative paradigm that can comprehensively characterize the replication of
the individual human body in the digital world and reflect its physical status
in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the
application of healthcare monitoring by acting as a versatile and vivid human
digital testbed, simulating the outcomes and guiding the practical treatments.
However, successfully establishing HDT requires high-fidelity virtual modeling
and strong information interactions but possibly with scarce, biased and noisy
data. Fortunately, a recent popular technology called generative artificial
intelligence (GAI) may be a promising solution because it can leverage advanced
AI algorithms to automatically create, manipulate, and modify valuable while
diverse data. This survey particularly focuses on the implementation of
GAI-driven HDT in IoT-healthcare. We start by introducing the background of
IoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the
fundamental techniques and present the overall framework of GAI-driven HDT.
After that, we explore the realization of GAI-driven HDT in detail, including
GAI-enabled data acquisition, communication, data management, digital modeling,
and data analysis. Besides, we discuss typical IoT-healthcare applications that
can be revolutionized by GAI-driven HDT, namely personalized health monitoring
and diagnosis, personalized prescription, and personalized rehabilitation.
Finally, we conclude this survey by highlighting some future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;You Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_C/0/1/0/all/0/1&quot;&gt;Changyan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiawen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13700">
<title>Towards Automated Readable Proofs of Ruler and Compass Constructions. (arXiv:2401.13700v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2401.13700</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there are several systems that successfully generate construction
steps for ruler and compass construction problems, none of them provides
readable synthetic correctness proofs for generated constructions. In the
present work, we demonstrate how our triangle construction solver ArgoTriCS can
cooperate with automated theorem provers for first order logic and coherent
logic so that it generates construction correctness proofs, that are both
human-readable and formal (can be checked by interactive theorem provers such
as Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas
and our goal is to have them all formally shown from the basic axioms of
geometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinkovic_V/0/1/0/all/0/1&quot;&gt;Vesna Marinkovi&amp;#x107;&lt;/a&gt; (Faculty of Mathematics, University of Belgrade), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukilovic_T/0/1/0/all/0/1&quot;&gt;Tijana &amp;#x160;ukilovi&amp;#x107;&lt;/a&gt; (Faculty of Mathematics, University of Belgrade), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maric_F/0/1/0/all/0/1&quot;&gt;Filip Mari&amp;#x107;&lt;/a&gt; (Faculty of Mathematics, University of Belgrade)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13703">
<title>Solving Some Geometry Problems of the N\&apos;aboj 2023 Contest with Automated Deduction in GeoGebra Discovery. (arXiv:2401.13703v1 [math.HO])</title>
<link>http://arxiv.org/abs/2401.13703</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we solve some of the geometry problems of the N\&apos;aboj 2023
competition with the help of a computer, using examples that the software tool
GeoGebra Discovery can calculate. In each case, the calculation requires
symbolic computations. We analyze the difficulty of feeding the problem into
the machine and set further goals to make the problems of this type of contests
even more tractable in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hota_A/0/1/0/all/0/1&quot;&gt;Amela Hota&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kovacs_Z/0/1/0/all/0/1&quot;&gt;Zolt&amp;#xe1;n Kov&amp;#xe1;cs&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vujic_A/0/1/0/all/0/1&quot;&gt;Alexander Vujic&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13704">
<title>Using Java Geometry Expert as Guide in the Preparations for Math Contests. (arXiv:2401.13704v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.13704</link>
<description rdf:parseType="Literal">&lt;p&gt;We give an insight into Java Geometry Expert (JGEX) in use in a school
context, focusing on the Austrian school system. JGEX can offer great support
in some classroom situations, especially for solving mathematical competition
tasks. Also, we discuss some limitations of the program.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganglmayr_I/0/1/0/all/0/1&quot;&gt;Ines Ganglmayr&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovacs_Z/0/1/0/all/0/1&quot;&gt;Zolt&amp;#xe1;n Kov&amp;#xe1;cs&lt;/a&gt; (The Private University College of Education of the Diocese of Linz, Austria)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13708">
<title>Accelerating hyperbolic t-SNE. (arXiv:2401.13708v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.13708</link>
<description rdf:parseType="Literal">&lt;p&gt;The need to understand the structure of hierarchical or high-dimensional data
is present in a variety of fields. Hyperbolic spaces have proven to be an
important tool for embedding computations and analysis tasks as their
non-linear nature lends itself well to tree or graph data. Subsequently, they
have also been used in the visualization of high-dimensional data, where they
exhibit increased embedding performance. However, none of the existing
dimensionality reduction methods for embedding into hyperbolic spaces scale
well with the size of the input data. That is because the embeddings are
computed via iterative optimization schemes and the computation cost of every
iteration is quadratic in the size of the input. Furthermore, due to the
non-linear nature of hyperbolic spaces, Euclidean acceleration structures
cannot directly be translated to the hyperbolic setting. This paper introduces
the first acceleration structure for hyperbolic embeddings, building upon a
polar quadtree. We compare our approach with existing methods and demonstrate
that it computes embeddings of similar quality in significantly less time.
Implementation and scripts for the experiments can be found at
https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skrodzki_M/0/1/0/all/0/1&quot;&gt;Martin Skrodzki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geffen_H/0/1/0/all/0/1&quot;&gt;Hunter van Geffen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaves_de_Plaza_N/0/1/0/all/0/1&quot;&gt;Nicolas F. Chaves-de-Plaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollt_T/0/1/0/all/0/1&quot;&gt;Thomas H&amp;#xf6;llt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1&quot;&gt;Elmar Eisemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hildebrandt_K/0/1/0/all/0/1&quot;&gt;Klaus Hildebrandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13713">
<title>EMP: Effective Multidimensional Persistence for Graph Representation Learning. (arXiv:2401.13713v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13713</link>
<description rdf:parseType="Literal">&lt;p&gt;Topological data analysis (TDA) is gaining prominence across a wide spectrum
of machine learning tasks that spans from manifold learning to graph
classification. A pivotal technique within TDA is persistent homology (PH),
which furnishes an exclusive topological imprint of data by tracing the
evolution of latent structures as a scale parameter changes. Present PH tools
are confined to analyzing data through a single filter parameter. However, many
scenarios necessitate the consideration of multiple relevant parameters to
attain finer insights into the data. We address this issue by introducing the
Effective Multidimensional Persistence (EMP) framework. This framework empowers
the exploration of data by simultaneously varying multiple scale parameters.
The framework integrates descriptor functions into the analysis process,
yielding a highly expressive data summary. It seamlessly integrates established
single PH summaries into multidimensional counterparts like EMP Landscapes,
Silhouettes, Images, and Surfaces. These summaries represent data&apos;s
multidimensional aspects as matrices and arrays, aligning effectively with
diverse ML models. We provide theoretical guarantees and stability proofs for
EMP summaries. We demonstrate EMP&apos;s utility in graph classification tasks,
showing its effectiveness. Results reveal that EMP enhances various single PH
descriptors, outperforming cutting-edge methods on multiple benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segovia_Dominguez_I/0/1/0/all/0/1&quot;&gt;Ignacio Segovia-Dominguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akcora_C/0/1/0/all/0/1&quot;&gt;Cuneyt G. Akcora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantarcioglu_M/0/1/0/all/0/1&quot;&gt;Murat Kantarcioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gel_Y/0/1/0/all/0/1&quot;&gt;Yulia R. Gel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coskunuzer_B/0/1/0/all/0/1&quot;&gt;Baris Coskunuzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13716">
<title>Can I trust my fake data -- A comprehensive quality assessment framework for synthetic tabular data in healthcare. (arXiv:2401.13716v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13716</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring safe adoption of AI tools in healthcare hinges on access to
sufficient data for training, testing and validation. In response to privacy
concerns and regulatory requirements, using synthetic data has been suggested.
Synthetic data is created by training a generator on real data to produce a
dataset with similar statistical properties. Competing metrics with differing
taxonomies for quality evaluation have been suggested, resulting in a complex
landscape. Optimising quality entails balancing considerations that make the
data fit for use, yet relevant dimensions are left out of existing frameworks.
We performed a comprehensive literature review on the use of quality evaluation
metrics on SD within the scope of tabular healthcare data and SD made using
deep generative methods. Based on this and the collective team experiences, we
developed a conceptual framework for quality assurance. The applicability was
benchmarked against a practical case from the Dutch National Cancer Registry.
We present a conceptual framework for quality assurance of SD for AI
applications in healthcare that aligns diverging taxonomies, expands on common
quality dimensions to include the dimensions of Fairness and Carbon footprint,
and proposes stages necessary to support real-life applications. Building trust
in synthetic data by increasing transparency and reducing the safety risk will
accelerate the development and uptake of trustworthy AI tools for the benefit
of patients. Despite the growing emphasis on algorithmic fairness and carbon
footprint, these metrics were scarce in the literature review. The overwhelming
focus was on statistical similarity using distance metrics while sequential
logic detection was scarce. A consensus-backed framework that includes all
relevant quality dimensions can provide assurance for safe and responsible
real-life applications of SD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vallevik_V/0/1/0/all/0/1&quot;&gt;Vibeke Binz Vallevik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babic_A/0/1/0/all/0/1&quot;&gt;Aleksandar Babic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_S/0/1/0/all/0/1&quot;&gt;Serena Elizabeth Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elvatun_S/0/1/0/all/0/1&quot;&gt;Severin Elvatun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brogger_H/0/1/0/all/0/1&quot;&gt;Helga Br&amp;#xf8;gger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alagaratnam_S/0/1/0/all/0/1&quot;&gt;Sharmini Alagaratnam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwin_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf8;rn Edwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veeraragavan_N/0/1/0/all/0/1&quot;&gt;Narasimha Raghavan Veeraragavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Befring_A/0/1/0/all/0/1&quot;&gt;Anne Kjersti Befring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nygaard_J/0/1/0/all/0/1&quot;&gt;Jan Franz Nyg&amp;#xe5;rd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13722">
<title>Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion Monitoring. (arXiv:2401.13722v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.13722</link>
<description rdf:parseType="Literal">&lt;p&gt;This research project aims to tackle the growing mental health challenges in
today&apos;s digital age. It employs a modified pre-trained BERT model to detect
depressive text within social media and users&apos; web browsing data, achieving an
impressive 93% test accuracy. Simultaneously, the project aims to incorporate
physiological signals from wearable devices, such as smartwatches and EEG
sensors, to provide long-term tracking and prognosis of mood disorders and
emotional states. This comprehensive approach holds promise for enhancing early
detection of depression and advancing overall mental health outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;Mohammad Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Sudhakar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonker_A/0/1/0/all/0/1&quot;&gt;Ankush Sonker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sanidhya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maurya_S/0/1/0/all/0/1&quot;&gt;Somesh Kumar Maurya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwary_U/0/1/0/all/0/1&quot;&gt;Uma Shanker Tiwary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13751">
<title>A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13751</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks have shown to be widely applicable to a large
number of fields when large amounts of labelled data are available. The recent
trend has been to use models with increasingly larger sets of tunable
parameters to increase model accuracy, reduce model loss, or create more
adversarially robust models -- goals that are often at odds with one another.
In particular, recent theoretical work raises questions about the ability for
even larger models to generalize to data outside of the controlled train and
test sets. As such, we examine the role of the number of hidden layers in the
ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a
variety of parameters including the size of the model, the floating point
precision, and the noise level of both the training data and the model output.
To encapsulate the model&apos;s predictive power and computational cost, we provide
a method that uses induced failures to model the probability of failure as a
function of time and relate that to a novel metric that allows us to quickly
determine whether or not the cost of training a model outweighs the cost of
attacking it. Using this approach, we are able to approximate the expected
failure rate using a small number of specially crafted samples rather than
increasingly larger benchmark datasets. We demonstrate the efficacy of this
technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit
floating-point numbers, various data pre-processing techniques, and several
attacks on five configurations of the ResNet model. Then, using empirical
measurements, we examine the various trade-offs between cost, robustness,
latency, and reliability to find that larger models do not significantly aid in
adversarial robustness despite costing significantly more to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyers_C/0/1/0/all/0/1&quot;&gt;Charles Meyers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedghpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Saleh Sedghpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy L&amp;#xf6;fstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmroth_E/0/1/0/all/0/1&quot;&gt;Erik Elmroth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13752">
<title>Explaining Image Classifiers. (arXiv:2401.13752v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13752</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on explaining image classifiers, taking the work of Mothilal et al.
[2021] (MMTS) as our point of departure. We observe that, although MMTS claim
to be using the definition of explanation proposed by Halpern [2016], they do
not quite do so. Roughly speaking, Halpern&apos;s definition has a necessity clause
and a sufficiency clause. MMTS replace the necessity clause by a requirement
that, as we show, implies it. Halpern&apos;s definition also allows agents to
restrict the set of options considered. While these difference may seem minor,
as we show, they can have a nontrivial impact on explanations. We also show
that, essentially without change, Halpern&apos;s definition can handle two issues
that have proved difficult for other approaches: explanations of absence (when,
for example, an image classifier for tumors outputs &quot;no tumor&quot;) and
explanations of rare events (such as tumors).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1&quot;&gt;Hana Chockler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halpern_J/0/1/0/all/0/1&quot;&gt;Joseph Y. Halpern&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13758">
<title>Assumptions and Bounds in the Instrumental Variable Model. (arXiv:2401.13758v1 [math.ST])</title>
<link>http://arxiv.org/abs/2401.13758</link>
<description rdf:parseType="Literal">&lt;p&gt;In this note we give proofs for results relating to the Instrumental Variable
(IV) model with binary response $Y$ and binary treatment $X$, but with an
instrument $Z$ that takes $K$ states that were originally stated in Richardson
&amp;amp; Robins (2014), &quot;ACE Bounds; SEMS with Equilibrium Conditions,&quot;
&lt;a href=&quot;/abs/1410.0470&quot;&gt;arXiv:1410.0470&lt;/a&gt;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Richardson_T/0/1/0/all/0/1&quot;&gt;Thomas S. Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Robins_J/0/1/0/all/0/1&quot;&gt;James M. Robins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13770">
<title>AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems. (arXiv:2401.13770v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13770</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS)
based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving
challenging combinatorial problems. Despite the tremendous success of CnC
solvers in solving a variety of hard combinatorial problems, the lookahead
cubing techniques at the heart of CnC have not evolved much for many years.
Part of the reason is the sheer difficulty of coming up with new cubing
techniques that are both low-cost and effective in partitioning input formulas
into sub-formulas, such that the overall runtime is minimized.
&lt;/p&gt;
&lt;p&gt;Lookahead cubing techniques used by current state-of-the-art CnC solvers,
such as March, keep their cubing costs low by constraining the search for the
optimal splitting variables. By contrast, our key innovation is a
deductively-driven MCTS-based lookahead cubing technique, that performs a
deeper heuristic search to find effective cubes, while keeping the cubing cost
low. We perform an extensive comparison of AlphaMapleSAT against the March CnC
solver on challenging combinatorial problems such as the minimum Kochen-Specker
and Ramsey problems. We also perform ablation studies to verify the efficacy of
the MCTS heuristic search for the cubing problem. Results show up to 2.3x
speedup in parallel (and up to 27x in sequential) elapsed real time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Piyush Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bright_C/0/1/0/all/0/1&quot;&gt;Curtis Bright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_V/0/1/0/all/0/1&quot;&gt;Vijay Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13782">
<title>Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2401.13782</link>
<description rdf:parseType="Literal">&lt;p&gt;As the number of accepted papers at AI and ML conferences reaches into the
thousands, it has become unclear how researchers access and read research
publications. In this paper, we investigate the role of social media
influencers in enhancing the visibility of machine learning research,
particularly the citation counts of papers they share. We have compiled a
comprehensive dataset of over 8,000 papers, spanning tweets from December 2018
to October 2023, alongside 1:1 matched controls based on publication year,
venue, and abstract topics. Our analysis reveals a significant increase in
citations for papers endorsed by these influencers, with median citation counts
2-3 times higher than those of the control group. Additionally, the study
delves into the geographic, gender, and institutional diversity of highlighted
authors. These findings highlight the expanding influence of social media in
scholarly communication and underscore the importance of an evolving ecosystem
in today&apos;s digital academic landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissburg_I/0/1/0/all/0/1&quot;&gt;Iain Xie Weissburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1&quot;&gt;Mehir Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13796">
<title>Don&apos;t Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13796</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) has revolutionized various domains, offering predictive
capabilities in several areas. However, with the increasing accessibility of ML
tools, many practitioners, lacking deep ML expertise, adopt a &quot;push the button&quot;
approach, utilizing user-friendly interfaces without a thorough understanding
of underlying algorithms. While this approach provides convenience, it raises
concerns about the reliability of outcomes, leading to challenges such as
incorrect performance evaluation. This paper addresses a critical issue in ML,
known as data leakage, where unintended information contaminates the training
data, impacting model performance evaluation. Users, due to a lack of
understanding, may inadvertently overlook crucial steps, leading to optimistic
performance estimates that may not hold in real-world scenarios. The
discrepancy between evaluated and actual performance on new data is a
significant concern. In particular, this paper categorizes data leakage in ML,
discussing how certain conditions can propagate through the ML workflow.
Furthermore, it explores the connection between data leakage and the specific
task being addressed, investigates its occurrence in Transfer Learning, and
compares standard inductive ML with transductive ML frameworks. The conclusion
summarizes key findings, emphasizing the importance of addressing data leakage
for robust and reliable ML applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1&quot;&gt;Andrea Apicella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1&quot;&gt;Francesco Isgr&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1&quot;&gt;Roberto Prevete&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13800">
<title>Multi-Object Navigation in real environments using hybrid policies. (arXiv:2401.13800v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.13800</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigation has been classically solved in robotics through the combination of
SLAM and planning. More recently, beyond waypoint planning, problems involving
significant components of (visual) high-level reasoning have been explored in
simulated environments, mostly addressed with large-scale machine learning, in
particular RL, offline-RL or imitation learning. These methods require the
agent to learn various skills like local planning, mapping objects and querying
the learned spatial representations. In contrast to simpler tasks like waypoint
planning (PointGoal), for these more complex tasks the current state-of-the-art
models have been thoroughly evaluated in simulation but, to our best knowledge,
not yet in real environments.
&lt;/p&gt;
&lt;p&gt;In this work we focus on sim2real transfer. We target the challenging
Multi-Object Navigation (Multi-ON) task and port it to a physical environment
containing real replicas of the originally virtual Multi-ON objects. We
introduce a hybrid navigation method, which decomposes the problem into two
different skills: (1) waypoint navigation is addressed with classical SLAM
combined with a symbolic planner, whereas (2) exploration, semantic mapping and
goal retrieval are dealt with deep neural networks trained with a combination
of supervised learning and RL. We show the advantages of this approach compared
to end-to-end methods both in simulation and a real environment and outperform
the SOTA for this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadek_A/0/1/0/all/0/1&quot;&gt;Assem Sadek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_G/0/1/0/all/0/1&quot;&gt;Guillaume Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baskurt_A/0/1/0/all/0/1&quot;&gt;Atilla Baskurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1&quot;&gt;Christian Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13802">
<title>Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.13802</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. %\textbf{Goal:}
GPT-based models are one of the popular ones studied for tasks such as code
comment generation or test generation. These tasks are `generative&apos; tasks.
However, there is limited research on the usage of LLMs for `non-generative&apos;
tasks such as classification using the prompt-based paradigm. In this
preliminary exploratory study, we investigated the applicability of LLMs for
Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By
building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we
first investigated two different prompts using ChatGPT to detect
\textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a
zero-shot setting. We \textcolor{black}{then} conducted an analysis to
understand the strengths and weaknesses of ChatGPT in CCD. %\textbf{Results:}
ChatGPT surpasses the baselines in cross-language CCD
\textcolor{black}{attaining an F1-score of 0.877 } and achieves comparable
performance to fully fine-tuned models for mono-lingual CCD,
\textcolor{black}{with an F1-score of 0.878}. Also, the
\textcolor{black}{prompt and the} difficulty level of the problems has an
impact on the performance of ChatGPT. \textcolor{black}{Finally,} we provide
insights and future directions based on our initial analysis
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khajezade_M/0/1/0/all/0/1&quot;&gt;Mohamad Khajezade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1&quot;&gt;Fatemeh Hendijani Fard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Perez_G/0/1/0/all/0/1&quot;&gt;Gema Rodr&amp;#xed;guez-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehata_M/0/1/0/all/0/1&quot;&gt;Mohamed Sami Shehata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13822">
<title>Navigating Dataset Documentations in AI: A Large-Scale Analysis of Dataset Cards on Hugging Face. (arXiv:2401.13822v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13822</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in machine learning are closely tied to the creation of datasets.
While data documentation is widely recognized as essential to the reliability,
reproducibility, and transparency of ML, we lack a systematic empirical
understanding of current dataset documentation practices. To shed light on this
question, here we take Hugging Face -- one of the largest platforms for sharing
and collaborating on ML models and datasets -- as a prominent case study. By
analyzing all 7,433 dataset documentation on Hugging Face, our investigation
provides an overview of the Hugging Face dataset ecosystem and insights into
dataset documentation practices, yielding 5 main findings: (1) The dataset card
completion rate shows marked heterogeneity correlated with dataset popularity.
(2) A granular examination of each section within the dataset card reveals that
the practitioners seem to prioritize Dataset Description and Dataset Structure
sections, while the Considerations for Using the Data section receives the
lowest proportion of content. (3) By analyzing the subsections within each
section and utilizing topic modeling to identify key topics, we uncover what is
discussed in each section, and underscore significant themes encompassing both
technical and social impacts, as well as limitations within the Considerations
for Using the Data section. (4) Our findings also highlight the need for
improved accessibility and reproducibility of datasets in the Usage sections.
(5) In addition, our human annotation evaluation emphasizes the pivotal role of
comprehensive dataset content in shaping individuals&apos; perceptions of a dataset
card&apos;s overall quality. Overall, our study offers a unique perspective on
analyzing dataset documentation through large-scale data science analysis and
underlines the need for more thorough dataset documentation in machine learning
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weixin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13827">
<title>Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink in Markovian IoT Models. (arXiv:2401.13827v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13827</link>
<description rdf:parseType="Literal">&lt;p&gt;The age of information (AoI) is used to measure the freshness of the data. In
IoT networks, the traditional resource management schemes rely on a message
exchange between the devices and the base station (BS) before communication
which causes high AoI, high energy consumption, and low reliability. Unmanned
aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the
AoI, energy-saving, and throughput improvement. In this paper, we present a
novel learning-based framework that estimates the traffic arrival of IoT
devices based on Markovian events. The learning proceeds to optimize the
trajectory of multiple UAVs and their scheduling policy. First, the BS predicts
the future traffic of the devices. We compare two traffic predictors: the
forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we
propose a deep reinforcement learning (DRL) approach to optimize the optimal
policy of each UAV. Finally, we manipulate the optimum reward function for the
proposed DRL approach. Simulation results show that the proposed algorithm
outperforms the random-walk (RW) baseline model regarding the AoI, scheduling
accuracy, and transmission power.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldeeb_E/0/1/0/all/0/1&quot;&gt;Eslam Eldeeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shehab_M/0/1/0/all/0/1&quot;&gt;Mohammad Shehab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_H/0/1/0/all/0/1&quot;&gt;Hirley Alves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13835">
<title>The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13835</link>
<description rdf:parseType="Literal">&lt;p&gt;For large language models (LLMs) to be trusted by humans they need to be
well-calibrated in the sense that they can accurately assess and communicate
how likely it is that their predictions are correct. Recent work has focused on
the quality of internal LLM confidence assessments, but the question remains of
how well LLMs can communicate this internal model confidence to human users.
This paper explores the disparity between external human confidence in an LLM&apos;s
responses and the internal confidence of the model. Through experiments
involving multiple-choice questions, we systematically examine human users&apos;
ability to discern the reliability of LLM outputs. Our study focuses on two key
areas: (1) assessing users&apos; perception of true LLM confidence and (2)
investigating the impact of tailored explanations on this perception. The
research highlights that default explanations from LLMs often lead to user
overestimation of both the model&apos;s confidence and its&apos; accuracy. By modifying
the explanations to more accurately reflect the LLM&apos;s internal confidence, we
observe a significant shift in user perception, aligning it more closely with
the model&apos;s actual confidence levels. This adjustment in explanatory approach
demonstrates potential for enhancing user trust and accuracy in assessing LLM
outputs. The findings underscore the importance of transparent communication of
confidence levels in LLMs, particularly in high-stakes applications where
understanding the reliability of AI-generated information is essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steyvers_M/0/1/0/all/0/1&quot;&gt;Mark Steyvers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tejeda_H/0/1/0/all/0/1&quot;&gt;Heliodoro Tejeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aakriti Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belem_C/0/1/0/all/0/1&quot;&gt;Catarina Belem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karny_S/0/1/0/all/0/1&quot;&gt;Sheer Karny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinyue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_L/0/1/0/all/0/1&quot;&gt;Lukas Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyth_P/0/1/0/all/0/1&quot;&gt;Padhraic Smyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13848">
<title>A V2X-based Privacy Preserving Federated Measuring and Learning System. (arXiv:2401.13848v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13848</link>
<description rdf:parseType="Literal">&lt;p&gt;Future autonomous vehicles (AVs) will use a variety of sensors that generate
a vast amount of data. Naturally, this data not only serves self-driving
algorithms; but can also assist other vehicles or the infrastructure in
real-time decision-making. Consequently, vehicles shall exchange their
measurement data over Vehicle-to-Everything (V2X) technologies. Moreover,
predicting the state of the road network might be beneficial too. With such a
prediction, we might mitigate road congestion, balance parking lot usage, or
optimize the traffic flow. That would decrease transportation costs as well as
reduce its environmental impact.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a federated measurement and learning system that
provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)
communication while also operating a federated learning (FL) scheme over the
Vehicle-to-Network (V2N) link to create a predictive model of the
transportation network. As we are yet to have real-world AV data, we model it
with a non-IID (independent and identically distributed) dataset to evaluate
the capabilities of the proposed system in terms of performance and privacy.
Results indicate that the proposed FL scheme improves learning performance and
prevents eavesdropping at the aggregator server side.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alekszejenko_L/0/1/0/all/0/1&quot;&gt;Levente Alekszejenk&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobrowiecki_T/0/1/0/all/0/1&quot;&gt;Tadeusz Dobrowiecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13849">
<title>TPD: Enhancing Student Language Model Reasoning via Principle Discovery and Guidance. (arXiv:2401.13849v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13849</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have recently showcased remarkable reasoning
abilities. However, larger models often surpass their smaller counterparts in
reasoning tasks, posing the challenge of effectively transferring these
capabilities from larger models. Existing approaches heavily rely on extensive
fine-tuning data or continuous interactions with a superior teacher LLM during
inference. We introduce a principle-based teacher-student framework called
``Teaching via Principle Discovery&apos;&apos; (TPD) to address these limitations.
Inspired by human learning mechanisms, TPD mimics the interaction between a
teacher and a student using a principle-based approach. The teacher LLM
generates problem-solving instructions and corrective principles based on the
student LLM&apos;s errors. These principles guide the refinement of instructions and
the selection of instructive examples from a validation set. This enables the
student model to learn from both the teacher&apos;s guidance and its own mistakes.
Once the student model begins making inferences, TPD requires no further
intervention from the teacher LLM or humans. Through extensive experiments
across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared
to standard chain-of-thought prompting, TPD significantly improves the student
model&apos;s performance, achieving $6.2\%$ improvement on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haorui Wang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongzhi Zhang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghao Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingkai Kong&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhuang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiusi Chen&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt; (1) ((1) College of Computing, Georgia Institute of Technology, (2) Department of Computer Science, University of California, Los Angeles)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13883">
<title>Domain-Independent Dynamic Programming. (arXiv:2401.13883v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13883</link>
<description rdf:parseType="Literal">&lt;p&gt;For combinatorial optimization problems, model-based paradigms such as
mixed-integer programming (MIP) and constraint programming (CP) aim to decouple
modeling and solving a problem: the `holy grail&apos; of declarative problem
solving. We propose domain-independent dynamic programming (DIDP), a new
model-based paradigm based on dynamic programming (DP). While DP is not new, it
has typically been implemented as a problem-specific method. We introduce
Dynamic Programming Description Language (DyPDL), a formalism to define DP
models based on a state transition system, inspired by AI planning. We show
that heuristic search algorithms can be used to solve DyPDL models and propose
seven DIDP solvers. We experimentally compare our DIDP solvers with commercial
MIP and CP solvers (solving MIP and CP models, respectively) on common
benchmark instances of eleven combinatorial optimization problem classes. We
show that DIDP outperforms MIP in nine problem classes, CP also in nine problem
classes, and both MIP and CP in seven.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuroiwa_R/0/1/0/all/0/1&quot;&gt;Ryo Kuroiwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beck_J/0/1/0/all/0/1&quot;&gt;J. Christopher Beck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13904">
<title>Empowering Machines to Think Like Chemists: Unveiling Molecular Structure-Polarity Relationships with Hierarchical Symbolic Regression. (arXiv:2401.13904v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13904</link>
<description rdf:parseType="Literal">&lt;p&gt;Thin-layer chromatography (TLC) is a crucial technique in molecular polarity
analysis. Despite its importance, the interpretability of predictive models for
TLC, especially those driven by artificial intelligence, remains a challenge.
Current approaches, utilizing either high-dimensional molecular fingerprints or
domain-knowledge-driven feature engineering, often face a dilemma between
expressiveness and interpretability. To bridge this gap, we introduce
Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical
neural networks and symbolic regression. UHiSR automatically distills
chemical-intuitive polarity indices, and discovers interpretable equations that
link molecular structure to chromatographic behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1&quot;&gt;Siyu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengchun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1&quot;&gt;Fanyang Mo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13913">
<title>Spectral Clustering for Discrete Distributions. (arXiv:2401.13913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13913</link>
<description rdf:parseType="Literal">&lt;p&gt;Discrete distribution clustering (D2C) was often solved by Wasserstein
barycenter methods. These methods are under a common assumption that clusters
can be well represented by barycenters, which may not hold in many real
applications. In this work, we propose a simple yet effective framework based
on spectral clustering and distribution affinity measures (e.g., maximum mean
discrepancy and Wasserstein distance) for D2C. To improve the scalability, we
propose to use linear optimal transport to construct affinity matrices
efficiently on large datasets. We provide theoretical guarantees for the
success of the proposed methods in clustering distributions. Experiments on
synthetic and real data show that our methods outperform the baselines largely
in terms of both clustering accuracy and computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_D/0/1/0/all/0/1&quot;&gt;Dong Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jicong Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13919">
<title>WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. (arXiv:2401.13919v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13919</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of large language models (LLMs) leads to a new era marked by
the development of autonomous applications in the real world, which drives
innovation in the creation of advanced web-based agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we propose a new evaluation protocol for web agents to address the challenges
of automatic evaluation of open-ended web agent tasks, leveraging the robust
multimodal comprehension capabilities of GPT-4V. We create a new benchmark by
gathering real-world tasks from 15 widely used websites to evaluate our agents.
We show that WebVoyager achieves a 55.7% task success rate, significantly
surpassing the performance of both GPT-4 (All Tools) and the WebVoyager
(text-only) setups, underscoring the exceptional capability of WebVoyager in
practical applications. We found that our proposed automatic evaluation
achieves 85.3% agreement with human judgment, paving the way for further
development of web agents in a real-world setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hongliang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kaixin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13920">
<title>LocMoE: A Low-overhead MoE for Large Language Model Training. (arXiv:2401.13920v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13920</link>
<description rdf:parseType="Literal">&lt;p&gt;The Mixtures-of-Experts (MoE) model is a widespread distributed and
integrated learning method for large language models (LLM), which is favored
due to its ability to sparsify and expand models efficiently. However, the
performance of MoE is limited by load imbalance and high latency of All-To-All
communication, along with relatively redundant computation owing to large
expert capacity. Load imbalance may result from existing routing policies that
consistently tend to select certain experts. The frequent inter-node
communication in the All-To-All procedure also significantly prolongs the
training time. To alleviate the above performance problems, we propose a novel
routing strategy that combines load balance and locality by converting partial
inter-node communication to that of intra-node. Notably, we elucidate that
there is a minimum threshold for expert capacity, calculated through the
maximal angular deviation between the gating weights of the experts and the
assigned tokens. We port these modifications on the PanGu-Sigma model based on
the MindSpore framework with multi-level routing and conduct experiments on
Ascend clusters. The experiment results demonstrate that the proposed LocMoE
reduces training time per epoch by 12.68% to 22.24% compared to classical
routers, such as hash router and switch router, without impacting the model
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhijie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Li Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;Entong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Binfan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rongqian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13935">
<title>A New Paradigm for Counterfactual Reasoning in Fairness and Recourse. (arXiv:2401.13935v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.13935</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactuals and counterfactual reasoning underpin numerous techniques for
auditing and understanding artificial intelligence (AI) systems. The
traditional paradigm for counterfactual reasoning in this literature is the
interventional counterfactual, where hypothetical interventions are imagined
and simulated. For this reason, the starting point for causal reasoning about
legal protections and demographic data in AI is an imagined intervention on a
legally-protected characteristic, such as ethnicity, race, gender, disability,
age, etc. We ask, for example, what would have happened had your race been
different? An inherent limitation of this paradigm is that some demographic
interventions -- like interventions on race -- may not translate into the
formalisms of interventional counterfactuals. In this work, we explore a new
paradigm based instead on the backtracking counterfactual, where rather than
imagine hypothetical interventions on legally-protected characteristics, we
imagine alternate initial conditions while holding these characteristics fixed.
We ask instead, what would explain a counterfactual outcome for you as you
actually are or could be? This alternate framework allows us to address many of
the same social concerns, but to do so while asking fundamentally different
questions that do not rely on demographic interventions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bynum_L/0/1/0/all/0/1&quot;&gt;Lucius E.J. Bynum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1&quot;&gt;Joshua R. Loftus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1&quot;&gt;Julia Stoyanovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13945">
<title>General Automatic Solution Generation of Social Problems. (arXiv:2401.13945v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.13945</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the escalating intricacy and multifaceted nature of contemporary social
systems, manually generating solutions to address pertinent social issues has
become a formidable task. In response to this challenge, the rapid development
of artificial intelligence has spurred the exploration of computational
methodologies aimed at automatically generating solutions. However, current
methods for auto-generation of solutions mainly concentrate on local social
regulations that pertain to specific scenarios. Here, we report an automatic
social operating system (ASOS) designed for general social solution generation,
which is built upon agent-based models, enabling both global and local analyses
and regulations of social problems across spatial and temporal dimensions. ASOS
adopts a hypergraph with extensible social semantics for a comprehensive and
structured representation of social dynamics. It also incorporates a
generalized protocol for standardized hypergraph operations and a symbolic
hybrid framework that delivers interpretable solutions, yielding a balance
between regulatory efficacy and function viability. To demonstrate the
effectiveness of ASOS, we apply it to the domain of averting extreme events
within international oil futures markets. By generating a new trading role
supplemented by new mechanisms, ASOS can adeptly discern precarious market
conditions and make front-running interventions for non-profit purposes. This
study demonstrates that ASOS provides an efficient and systematic approach for
generating solutions for enhancing our society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1&quot;&gt;Tong Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haoyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Luping Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rong Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13968">
<title>Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks. (arXiv:2401.13968v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13968</link>
<description rdf:parseType="Literal">&lt;p&gt;A reliable long-term time-series forecaster is highly demanded in practice
but comes across many challenges such as low computational and memory
footprints as well as robustness against dynamic learning environments. This
paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic
long-term time-series forecasting tasks. MANTRA relies on the concept of fast
and slow learners where a collection of fast learners learns different aspects
of data distributions while adapting quickly to changes. A slow learner tailors
suitable representations to fast learners. Fast adaptations to dynamic
environments are achieved using the universal representation transformer layers
producing task-adapted representations with a small number of parameters. Our
experiments using four datasets with different prediction lengths demonstrate
the advantage of our approach with at least $3\%$ improvements over the
baseline algorithms for both multivariate and univariate settings. Source codes
of MANTRA are publicly available in
\url{https://github.com/anwarmaxsum/MANTRA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masum_M/0/1/0/all/0/1&quot;&gt;Muhammad Anwar Ma&amp;#x27;sum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_M/0/1/0/all/0/1&quot;&gt;MD Rasel Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasamy_S/0/1/0/all/0/1&quot;&gt;Savitha Ramasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anavatti_S/0/1/0/all/0/1&quot;&gt;Sreenatha Anavatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibullah/0/1/0/all/0/1&quot;&gt;Habibullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalczyk_R/0/1/0/all/0/1&quot;&gt;Ryszard Kowalczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13974">
<title>BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13974</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image generation models have demonstrated incredible success
in generating images that faithfully follow input prompts. However, the
requirement of using words to describe a desired concept provides limited
control over the appearance of the generated concepts. In this work, we address
this shortcoming by proposing an approach to enable personalization
capabilities in existing text-to-image diffusion models. We propose a novel
architecture (BootPIG) that allows a user to provide reference images of an
object in order to guide the appearance of a concept in the generated images.
&lt;/p&gt;
&lt;p&gt;The proposed BootPIG architecture makes minimal modifications to a pretrained
text-to-image diffusion model and utilizes a separate UNet model to steer the
generations toward the desired appearance. We introduce a training procedure
that allows us to bootstrap personalization capabilities in the BootPIG
architecture using data generated from pretrained text-to-image models, LLM
chat agents, and image segmentation models. In contrast to existing methods
that require several days of pretraining, the BootPIG architecture can be
trained in approximately 1 hour. Experiments on the DreamBooth dataset
demonstrate that BootPIG outperforms existing zero-shot methods while being
comparable with test-time finetuning approaches. Through a user study, we
validate the preference for BootPIG generations over existing methods both in
maintaining fidelity to the reference object&apos;s appearance and aligning with
textual prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1&quot;&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1&quot;&gt;Akash Gokul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1&quot;&gt;Nikhil Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13976">
<title>Learning to Manipulate Artistic Images. (arXiv:2401.13976v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13976</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancement in computer vision has significantly lowered the barriers
to artistic creation. Exemplar-based image translation methods have attracted
much attention due to flexibility and controllability. However, these methods
hold assumptions regarding semantics or require semantic information as the
input, while accurate semantics is not easy to obtain in artistic images.
Besides, these methods suffer from cross-domain artifacts due to training data
prior and generate imprecise structure due to feature compression in the
spatial domain. In this paper, we propose an arbitrary Style Image Manipulation
Network (SIM-Net), which leverages semantic-free information as guidance and a
region transportation strategy in a self-supervised manner for image
generation. Our method balances computational efficiency and high resolution to
a certain extent. Moreover, our method facilitates zero-shot style image
manipulation. Both qualitative and quantitative experiments demonstrate the
superiority of our method over state-of-the-art methods.Code is available at
https://github.com/SnailForce/SIM-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1&quot;&gt;De Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qian Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13979">
<title>Leeroo Orchestrator: Elevating LLMs Performance Through Model Integration. (arXiv:2401.13979v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13979</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an architecture to harness the collective knowledge
of multiple trained LLMs to create a new state-of-the-art. At the core of this
framework is a LLM-based orchestrator that is adept at picking the right
underlying LLM experts for optimal task execution. Inspired by self-play in
reinforcement learning, we created a loop of query generation, orchestration,
and evaluation to generate training data for the orchestrator. Our evaluation
focused on the MMLU benchmark, employing models with 7B, 13B, and 34B
parameters available on Hugging Face. The results demonstrate new
state-of-the-art open-source models: Our Leeroo orchestrator achieves
performance on par with the Mixtral model while incurring only two-thirds of
its cost. Moreover, increasing the allowed cost surpasses Mixtral&apos;s accuracy by
over 5% at the same cost level, reaching an accuracy of 75.9%. Further
enhancements were observed when integrating GPT4 into the underlying model
pool. The Leeroo orchestrator nearly matches GPT4&apos;s performance at half the
cost and even exceeds GPT4&apos;s results with a 25% cost reduction. These findings
illustrate the potential of our architecture in creating state-of-the-art and
cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve
superior performance outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1&quot;&gt;Alireza Mohammadshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikh_A/0/1/0/all/0/1&quot;&gt;Ali Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1&quot;&gt;Majid Yazdani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13986">
<title>Towards Consistent Natural-Language Explanations via Explanation-Consistency Finetuning. (arXiv:2401.13986v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13986</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) often generate convincing, fluent explanations.
However, different from humans, they often generate inconsistent explanations
on different inputs. For example, an LLM may generate the explanation &quot;all
birds can fly&quot; when answering the question &quot;Can sparrows fly?&quot; but meanwhile
answer &quot;no&quot; to the related question &quot;Can penguins fly?&quot;. Explanations should be
consistent across related examples so that they allow a human to simulate the
LLM&apos;s decision process on multiple examples. We propose explanation-consistency
finetuning (EC-finetuning), a method that adapts LLMs to generate more
consistent natural-language explanations on related examples. EC-finetuning
involves finetuning LLMs on synthetic data that is carefully constructed to
contain consistent explanations. Across a variety of question-answering
datasets in various domains, EC-finetuning yields a 10.0% relative explanation
consistency improvement on four finetuning datasets, and generalizes to seven
out-of-distribution datasets not seen during finetuning (+4.5% relative). Code
is available at https://github.com/yandachen/explanation-consistency-finetuning .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1&quot;&gt;Chandan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1&quot;&gt;Simiao Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13987">
<title>Cross-Domain Few-Shot Learning via Adaptive Transformer Networks. (arXiv:2401.13987v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13987</link>
<description rdf:parseType="Literal">&lt;p&gt;Most few-shot learning works rely on the same domain assumption between the
base and the target tasks, hindering their practical applications. This paper
proposes an adaptive transformer network (ADAPTER), a simple but effective
solution for cross-domain few-shot learning where there exist large domain
shifts between the base task and the target task. ADAPTER is built upon the
idea of bidirectional cross-attention to learn transferable features between
the two domains. The proposed architecture is trained with DINO to produce
diverse, and less biased features to avoid the supervision collapse problem.
Furthermore, the label smoothing approach is proposed to improve the
consistency and reliability of the predictions by also considering the
predicted labels of the close samples in the embedding space. The performance
of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it
outperforms prior arts with significant margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paeedeh_N/0/1/0/all/0/1&quot;&gt;Naeem Paeedeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masum_M/0/1/0/all/0/1&quot;&gt;Muhammad Anwar Ma&amp;#x27;sum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_W/0/1/0/all/0/1&quot;&gt;Wolfgang Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zehong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowlczyk_R/0/1/0/all/0/1&quot;&gt;Ryszard Kowlczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13996">
<title>Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent Self-Evolution. (arXiv:2401.13996v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.13996</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy
for enhancing the adaptability and flexibility of AI agents through inter-task
self-evolution. Unlike existing methods focused on intra-task learning, ICE
promotes the transfer of knowledge between tasks for genuine self-evolution,
similar to human experience learning. The strategy dynamically investigates
planning and execution trajectories, consolidates them into simplified
workflows and pipelines, and exploits them for improved task execution. Our
experiments on the XAgent framework demonstrate ICE&apos;s effectiveness, reducing
API calls by as much as 80% and significantly decreasing the demand for the
model&apos;s capability. Specifically, when combined with GPT-3.5, ICE&apos;s performance
matches that of raw GPT-4 across various agent tasks. We argue that this
self-evolution approach represents a paradigm shift in agent design,
contributing to a more robust AI community and ecosystem, and moving a step
closer to full autonomy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Cheng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shihao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yining Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1&quot;&gt;Xin Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yesai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14003">
<title>ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases. (arXiv:2401.14003v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14003</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has
been explored as a way to acquire new commonsense knowledge based on reference
knowledge in the original CSKBs and external prior knowledge. Despite the
advancement of Large Language Models (LLM) and prompt engineering techniques in
various reasoning tasks, they still struggle to deal with CSKB reasoning. One
of the problems is that it is hard for them to acquire explicit relational
constraints in CSKBs from only in-context exemplars, due to a lack of symbolic
reasoning capabilities (Bengio et al., 2021). To this end, we proposed
**ConstraintChecker**, a plugin over prompting techniques to provide and check
explicit constraints. When considering a new knowledge instance,
ConstraintChecker employs a rule-based module to produce a list of constraints,
then it uses a zero-shot learning module to check whether this knowledge
instance satisfies all constraints. The acquired constraint-checking result is
then aggregated with the output of the main prompting technique to produce the
final output. Experimental results on CSKB Reasoning benchmarks demonstrate the
effectiveness of our method by bringing consistent improvements over all
prompting methods. Codes and data are available at
\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_Q/0/1/0/all/0/1&quot;&gt;Quyet V. Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1&quot;&gt;Tianqing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1&quot;&gt;Shizhe Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14011">
<title>CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning. (arXiv:2401.14011v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14011</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal large language models(MLLMs) have achieved remarkable progress
and demonstrated powerful knowledge comprehension and reasoning abilities.
However, the mastery of domain-specific knowledge, which is essential for
evaluating the intelligence of MLLMs, continues to be a challenge. Current
multi-modal benchmarks for domain-specific knowledge concentrate on
multiple-choice questions and are predominantly available in English, which
imposes limitations on the comprehensiveness of the evaluation. To this end, we
introduce CMMU, a novel benchmark for multi-modal and multi-type question
understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7
subjects, covering knowledge from primary to high school. The questions can be
categorized into 3 types: multiple-choice, multiple-response, and
fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we
propose a rigorous evaluation strategy called ShiftCheck for assessing
multiple-choice questions. The strategy aims to reduce position bias, minimize
the influence of randomness on correctness, and perform a quantitative analysis
of position bias. We evaluate seven open-source MLLMs along with GPT4-V,
Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a
significant challenge to the recent MLLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zheqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinya Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_R/0/1/0/all/0/1&quot;&gt;Richeng Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qiannan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14019">
<title>Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation for Generative AI. (arXiv:2401.14019v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14019</link>
<description rdf:parseType="Literal">&lt;p&gt;In the dynamic landscape of generative NLP, traditional text processing
pipelines limit research flexibility and reproducibility, as they are tailored
to specific dataset, task, and model combinations. The escalating complexity,
involving system prompts, model-specific formats, instructions, and more, calls
for a shift to a structured, modular, and customizable solution. Addressing
this need, we present Unitxt, an innovative library for customizable textual
data preparation and evaluation tailored to generative language models. Unitxt
natively integrates with common libraries like HuggingFace and LM-eval-harness
and deconstructs processing flows into modular components, enabling easy
customization and sharing between practitioners. These components encompass
model-specific formats, task prompts, and many other comprehensive dataset
processing definitions. The Unitxt-Catalog centralizes these components,
fostering collaboration and exploration in modern textual data workflows.
Beyond being a tool, Unitxt is a community-driven platform, empowering users to
build, share, and advance their pipelines collaboratively. Join the Unitxt
community at https://github.com/IBM/unitxt!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandel_E/0/1/0/all/0/1&quot;&gt;Elron Bandel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1&quot;&gt;Yotam Perlitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venezian_E/0/1/0/all/0/1&quot;&gt;Elad Venezian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_Melamed_R/0/1/0/all/0/1&quot;&gt;Roni Friedman-Melamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1&quot;&gt;Ofir Arviv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1&quot;&gt;Matan Orbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Don_Yehyia_S/0/1/0/all/0/1&quot;&gt;Shachar Don-Yehyia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheinwald_D/0/1/0/all/0/1&quot;&gt;Dafna Sheinwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gera_A/0/1/0/all/0/1&quot;&gt;Ariel Gera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmueli_Scheuer_M/0/1/0/all/0/1&quot;&gt;Michal Shmueli-Scheuer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1&quot;&gt;Yoav Katz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14032">
<title>GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting. (arXiv:2401.14032v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14032</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1&quot;&gt;Butian Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14043">
<title>Towards Goal-oriented Large Language Model Prompting: A Survey. (arXiv:2401.14043v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14043</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown prominent performance in various
downstream tasks in which prompt engineering plays a pivotal role in optimizing
LLMs&apos; performance. This paper, not as an overview of current prompt engineering
methods, aims to highlight the limitation of designing prompts while holding an
anthropomorphic assumption that expects LLMs to think like humans. From our
review of 35 representative studies, we demonstrate that a goal-oriented prompt
formulation, which guides LLMs to follow established human logical thinking,
significantly improves the performance of LLMs. Furthermore, We introduce a
novel taxonomy that categorizes goal-oriented prompting methods into five
interconnected stages and we demonstrate the broad applicability of our
framework by summarizing ten applicable tasks. With four future directions
proposed, we hope to further emphasize and promote goal-oriented prompt
engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haochen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_J/0/1/0/all/0/1&quot;&gt;Jonathan Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14057">
<title>Left/Right Brain, human motor control and the implications for robotics. (arXiv:2401.14057v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14057</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Network movement controllers promise a variety of advantages over
conventional control methods however they are not widely adopted due to their
inability to produce reliably precise movements. This research explores a
bilateral neural network architecture as a control system for motor tasks. We
aimed to achieve hemispheric specialisation similar to what is observed in
humans across different tasks; the dominant system (usually the right hand,
left hemisphere) excels at tasks involving coordination and efficiency of
movement, and the non-dominant system performs better at tasks requiring
positional stability. Specialisation was achieved by training the hemispheres
with different loss functions tailored toward the expected behaviour of the
respective hemispheres. We compared bilateral models with and without
specialised hemispheres, with and without inter-hemispheric connectivity
(representing the biological Corpus Callosum), and unilateral models with and
without specialisation. The models were trained and tested on two tasks common
in the human motor control literature: the random reach task, suited to the
dominant system, a model with better coordination, and the hold position task,
suited to the non-dominant system, a model with more stable movement. Each
system out-performed the non-favoured system in its preferred task. For both
tasks, a bilateral model outperforms the &apos;non-preferred&apos; hand, and is as good
or better than the &apos;preferred&apos; hand. The Corpus Callosum tends to improve
performance, but not always for the specialised models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinaldo_J/0/1/0/all/0/1&quot;&gt;Jarrad Rinaldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhlmann_L/0/1/0/all/0/1&quot;&gt;Levin Kuhlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1&quot;&gt;Jason Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowadlo_G/0/1/0/all/0/1&quot;&gt;Gideon Kowadlo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14066">
<title>CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14066</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image generative models have made impressive strides,
showcasing their ability to synthesize a vast array of high-quality images.
However, adapting these models for artistic image editing presents two
significant challenges. Firstly, users struggle to craft textual prompts that
meticulously detail visual elements of the input image. Secondly, prevalent
models, when effecting modifications in specific zones, frequently disrupt the
overall artistic style, complicating the attainment of cohesive and
aesthetically unified artworks. To surmount these obstacles, we build the
innovative unified framework CreativeSynth, which is based on a diffusion model
with the ability to coordinate multimodal inputs and multitask in the field of
artistic image generation. By integrating multimodal features with customized
attention mechanisms, CreativeSynth facilitates the importation of real-world
semantic content into the domain of art through inversion and real-time style
transfer. This allows for the precise manipulation of image style and content
while maintaining the integrity of the original model parameters. Rigorous
qualitative and quantitative evaluations underscore that CreativeSynth excels
in enhancing artistic images&apos; fidelity and preserves their innate aesthetic
essence. By bridging the gap between generative models and artistic finesse,
CreativeSynth becomes a custom digital palette.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nisha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weiming Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ronghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14067">
<title>Ta&apos;keed: The First Generative Fact-Checking System for Arabic Claims. (arXiv:2401.14067v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14067</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Ta&apos;keed, an explainable Arabic automatic fact-checking
system. While existing research often focuses on classifying claims as &quot;True&quot;
or &quot;False,&quot; there is a limited exploration of generating explanations for claim
credibility, particularly in Arabic. Ta&apos;keed addresses this gap by assessing
claim truthfulness based on retrieved snippets, utilizing two main components:
information retrieval and LLM-based claim verification. We compiled the
ArFactEx, a testing gold-labelled dataset with manually justified references,
to evaluate the system. The initial model achieved a promising F1 score of 0.72
in the classification task. Meanwhile, the system&apos;s generated explanations are
compared with gold-standard explanations syntactically and semantically. The
study recommends evaluating using semantic similarities, resulting in an
average cosine similarity score of 0.76. Additionally, we explored the impact
of varying snippet quantities on claim classification accuracy, revealing a
potential correlation, with the model using the top seven hits outperforming
others with an F1 score of 0.77.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Althabiti_S/0/1/0/all/0/1&quot;&gt;Saud Althabiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alsalka_M/0/1/0/all/0/1&quot;&gt;Mohammad Ammar Alsalka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atwell_E/0/1/0/all/0/1&quot;&gt;Eric Atwell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14079">
<title>From Requirements to Architecture: An AI-Based Journey to Semi-Automatically Generate Software Architectures. (arXiv:2401.14079v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.14079</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing domain models and software architectures represents a significant
challenge in software development, as the resulting architectures play a vital
role in fulfilling the system&apos;s quality of service. Due to time pressure,
architects often model only one architecture based on their known limited
domain understanding, patterns, and experience instead of thoroughly analyzing
the domain and evaluating multiple candidates, selecting the best fitting.
Existing approaches try to generate domain models based on requirements, but
still require time-consuming manual effort to achieve good results. Therefore,
in this vision paper, we propose a method to generate software architecture
candidates semi-automatically based on requirements using artificial
intelligence techniques. We further envision an automatic evaluation and
trade-off analysis of the generated architecture candidates using, e.g., the
architecture trade-off analysis method combined with large language models and
quantitative analyses. To evaluate this approach, we aim to analyze the quality
of the generated architecture models and the efficiency and effectiveness of
our proposed process by conducting qualitative studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenreich_T/0/1/0/all/0/1&quot;&gt;Tobias Eisenreich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speth_S/0/1/0/all/0/1&quot;&gt;Sandro Speth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1&quot;&gt;Stefan Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14086">
<title>Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14086</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI
systems need to be explained. These decisions are often explainable only post
hoc, where counterfactual explanations are popular. The question of what
constitutes the best counterfactual explanation must consider multiple aspects,
where &quot;distance from the sample&quot; is the most common. We argue that this
requirement frequently leads to explanations that are unlikely and, therefore,
of limited value. Here, we present a system that provides high-likelihood
explanations. We show that the search for the most likely explanations
satisfying many common desiderata for counterfactual explanations can be
modeled using mixed-integer optimization (MIO). In the process, we propose an
MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the
likelihood of a counterfactual, which can be of independent interest. A
numerical comparison against several methods for generating counterfactual
explanations is provided.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1&quot;&gt;Jiri Nemecek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pevny_T/0/1/0/all/0/1&quot;&gt;Tomas Pevny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14089">
<title>GQHAN: A Grover-inspired Quantum Hard Attention Network. (arXiv:2401.14089v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2401.14089</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy
in discerning the significance of quantum data, resulting in diminished
efficacy when handling extensive quantum datasets. Hard Attention Mechanism
(HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters
the substantial challenge of non-differentiability, consequently constraining
its extensive applicability. In response to the dilemma of HAM and QML, a
Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a
Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed.
Notably, the FO is designed to surmount the non-differentiable issue by
executing the activation or masking of Discrete Primitives (DPs) with Flexible
Control (FC) to weave various discrete destinies. Based on this, such discrete
choice can be visualized with a specially defined Quantum Hard Attention Score
(QHAS). Furthermore, a trainable ADO is devised to boost the generality and
flexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network
(GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST
binary classification. Experimental findings demonstrate that GQHAN adeptly
surmounts the non-differentiability hurdle, surpassing the efficacy of extant
quantum soft self-attention mechanisms in accuracies and learning ability. In
noise experiments, GQHAN is robuster to bit-flip noise in accuracy and
amplitude damping noise in learning performance. Predictably, the proposal of
GQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for
future quantum computers to process large-scale data, and promotes the
development of quantum computer vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ren-Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jinjing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14109">
<title>CompactifAI: Extreme Compression of Large Language Models using Quantum-Inspired Tensor Networks. (arXiv:2401.14109v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14109</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly
in generative Artificial Intelligence (AI), but their immense size poses
significant challenges, such as huge training and inference costs, substantial
energy demands, and limitations for on-site deployment. Traditional compression
methods such as pruning, distillation, and low-rank approximation focus on
reducing the effective number of neurons in the network, while quantization
focuses on reducing the numerical precision of individual weights to reduce the
model size while keeping the number of neurons fixed. While these compression
methods have been relatively successful in practice, there&apos;s no compelling
reason to believe that truncating the number of neurons is an optimal strategy.
In this context, this paper introduces CompactifAI, an innovative LLM
compression approach using quantum-inspired Tensor Networks that focuses on the
model&apos;s correlation space instead, allowing for a more controlled, refined and
interpretable model compression. Our method is versatile and can be implemented
with - or on top of - other compression techniques. As a benchmark, we
demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model
to only $30\%$ of its original size while recovering over $90\%$ of the
original accuracy after a brief distributed retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomut_A/0/1/0/all/0/1&quot;&gt;Andrei Tomut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahromi_S/0/1/0/all/0/1&quot;&gt;Saeed S. Jahromi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sukhbinder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishtiaq_F/0/1/0/all/0/1&quot;&gt;Faysal Ishtiaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munoz_C/0/1/0/all/0/1&quot;&gt;Cesar Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1&quot;&gt;Prabdeep Singh Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elborady_A/0/1/0/all/0/1&quot;&gt;Ali Elborady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_G/0/1/0/all/0/1&quot;&gt;Gianni del Bimbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1&quot;&gt;Mehrazin Alizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montero_D/0/1/0/all/0/1&quot;&gt;David Montero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_Ramiro_P/0/1/0/all/0/1&quot;&gt;Pablo Martin-Ramiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_M/0/1/0/all/0/1&quot;&gt;Muhammad Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaoui_O/0/1/0/all/0/1&quot;&gt;Oussama Tahiri Alaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malcolm_J/0/1/0/all/0/1&quot;&gt;John Malcolm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mugel_S/0/1/0/all/0/1&quot;&gt;Samuel Mugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orus_R/0/1/0/all/0/1&quot;&gt;Roman Orus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14110">
<title>Towards Cheaper Inference in Deep Networks with Lower Bit-Width Accumulators. (arXiv:2401.14110v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14110</link>
<description rdf:parseType="Literal">&lt;p&gt;The majority of the research on the quantization of Deep Neural Networks
(DNNs) is focused on reducing the precision of tensors visible by high-level
frameworks (e.g., weights, activations, and gradients). However, current
hardware still relies on high-accuracy core operations. Most significant is the
operation of accumulating products. This high-precision accumulation operation
is gradually becoming the main computational bottleneck. This is because, so
far, the usage of low-precision accumulators led to a significant degradation
in performance. In this work, we present a simple method to train and fine-tune
high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits
accumulators, with no significant degradation in accuracy. Lastly, we show that
as we decrease the accumulation precision further, using fine-grained gradient
approximations can improve the DNN accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumenfeld_Y/0/1/0/all/0/1&quot;&gt;Yaniv Blumenfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubara_I/0/1/0/all/0/1&quot;&gt;Itay Hubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14112">
<title>FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design. (arXiv:2401.14112v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14112</link>
<description rdf:parseType="Literal">&lt;p&gt;Six-bit quantization (FP6) can effectively reduce the size of large language
models (LLMs) and preserve the model quality consistently across varied
applications. However, existing systems do not provide Tensor Core support for
FP6 quantization and struggle to achieve practical performance improvements
during LLM inference. It is challenging to support FP6 quantization on GPUs due
to (1) unfriendly memory access of model weights with irregular bit-width and
(2) high runtime overhead of weight de-quantization. To address these problems,
we propose TC-FPx, the first full-stack GPU kernel design scheme with unified
Tensor Core support of float-point weights for various quantization bit-width.
We integrate TC-FPx kernel into an existing inference system, providing new
end-to-end support (called FP6-LLM) for quantized LLM inference, where better
trade-offs between inference cost and model quality are achieved. Experiments
show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,
achieving 1.69x-2.65x higher normalized inference throughput than the FP16
baseline. The source code will be publicly available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_H/0/1/0/all/0/1&quot;&gt;Haojun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhewei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Youn_S/0/1/0/all/0/1&quot;&gt;Stephen Youn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakhtiari_A/0/1/0/all/0/1&quot;&gt;Arash Bakhtiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyatt_M/0/1/0/all/0/1&quot;&gt;Michael Wyatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_D/0/1/0/all/0/1&quot;&gt;Donglin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongzhu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruwase_O/0/1/0/all/0/1&quot;&gt;Olatunji Ruwase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Leon Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14142">
<title>Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14142</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods, such as concept bottleneck models (CBMs), have been
successful in providing concept-based interpretations for black-box deep
learning models. They typically work by predicting concepts given the input and
then predicting the final class label given the predicted concepts. However,
(1) they often fail to capture the high-order, nonlinear interaction between
concepts, e.g., correcting a predicted concept (e.g., &quot;yellow breast&quot;) does not
help correct highly correlated concepts (e.g., &quot;yellow belly&quot;), leading to
suboptimal final accuracy; (2) they cannot naturally quantify the complex
conditional dependencies between different concepts and class labels (e.g., for
an image with the class label &quot;Kentucky Warbler&quot; and a concept &quot;black bill&quot;,
what is the probability that the model correctly predicts another concept
&quot;black crown&quot;), therefore failing to provide deeper insight into how a
black-box model works. In response to these limitations, we propose
Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural
networks to define the joint energy of candidate (input, concept, class)
tuples. With such a unified interface, prediction, concept correction, and
conditional dependency quantification are then represented as conditional
probabilities, which are generated by composing different energy functions. Our
ECBMs address both limitations of existing CBMs, providing higher accuracy and
richer concept interpretations. Empirical results show that our approach
outperforms the state-of-the-art on real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1&quot;&gt;Lu Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14151">
<title>True Knowledge Comes from Practice: Aligning LLMs with Embodied Environments via Reinforcement Learning. (arXiv:2401.14151v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14151</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs&apos;
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs&apos; original ability during online PPO finetuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weihao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shanqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Longtao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bo An&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14153">
<title>Agent-based Simulation with Netlogo to Evaluate AmI Scenarios. (arXiv:2401.14153v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14153</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper an agent-based simulation is developed in order to evaluate an
AmI scenario based on agents. Many AmI applications are implemented through
agents but they are not compared to any other existing alternative in order to
evaluate the relative benefits of using them. The proposal simulation
environment developed in Netlogo analyse such benefits using two evaluation
criteria: First, measuring agent satisfaction of different types of desires
along the execution. Second, measuring time savings obtained through a correct
use of context information.
&lt;/p&gt;
&lt;p&gt;So, here, a previously suggested agent architecture, an ontology and a
12-steps protocol to provide AmI services in airports, is evaluated using a
NetLogo simulation environment. The present work uses a NetLogo model
considering scalability problems of this application domain but using FIPA and
BDI extensions to be coherent with our previous works and our previous JADE
implementation of them.
&lt;/p&gt;
&lt;p&gt;The NetLogo model presented simulates an airport with agent users passing
through several zones located in a specific order in a map: passport controls,
check-in counters of airline companies, boarding gates, different types of
shopping. Although initial data in simulations are generated randomly, and the
model is just an approximation of real-world airports, the definition of this
case of use of Ambient Intelligence through NetLogo agents opens an interesting
way to evaluate the benefits of using Ambient Intelligence, which is a
significant contribution to the final development of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbo_J/0/1/0/all/0/1&quot;&gt;J. Carbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_N/0/1/0/all/0/1&quot;&gt;N. Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_J/0/1/0/all/0/1&quot;&gt;J. M. Molina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14155">
<title>Alleviating Structural Distribution Shift in Graph Anomaly Detection. (arXiv:2401.14155v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14155</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph anomaly detection (GAD) is a challenging binary classification problem
due to its different structural distribution between anomalies and normal nodes
-- abnormal nodes are a minority, therefore holding high heterophily and low
homophily compared to normal nodes. Furthermore, due to various time factors
and the annotation preferences of human experts, the heterophily and homophily
can change across training and testing data, which is called structural
distribution shift (SDS) in this paper. The mainstream methods are built on
graph neural networks (GNNs), benefiting the classification of normals from
aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and
suffering from poor generalization.
&lt;/p&gt;
&lt;p&gt;This work solves the problem from a feature view. We observe that the degree
of SDS varies between anomalies and normal nodes. Hence to address the issue,
the key lies in resisting high heterophily for anomalies meanwhile benefiting
the learning of normals from homophily. We tease out the anomaly features on
which we constrain to mitigate the effect of heterophilous neighbors and make
them invariant. We term our proposed framework as Graph Decomposition Network
(GDN). Extensive experiments are conducted on two benchmark datasets, and the
proposed framework achieves a remarkable performance boost in GAD, especially
in an SDS environment where anomalies have largely different structural
distribution across training and testing environments. Codes are open-sourced
in https://github.com/blacksingular/wsdm_GDN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenguang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Huamin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14166">
<title>BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14166</link>
<description rdf:parseType="Literal">&lt;p&gt;As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1&quot;&gt;Fei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yifan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14171">
<title>Predicting Hypoxia in Brain Tumors from Multiparametric MRI. (arXiv:2401.14171v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14171</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper presents a novel approach to the prediction of hypoxia in
brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia,
a condition characterized by low oxygen levels, is a common feature of
malignant brain tumors associated with poor prognosis. Fluoromisonidazole
Positron Emission Tomography (FMISO PET) is a well-established method for
detecting hypoxia in vivo, but it is expensive and not widely available. Our
study proposes the use of MRI, a more accessible and cost-effective imaging
modality, to predict FMISO PET signals. We investigate deep learning models
(DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and
FMISO PET images from patients with brain tumors. Our trained models
effectively learn the complex relationships between the MRI features and the
corresponding FMISO PET signals, thereby enabling the prediction of hypoxia
from MRI scans alone. The results show a strong correlation between the
predicted and actual FMISO PET signals, with an overall PSNR score above 29.6
and a SSIM score greater than 0.94, confirming MRI as a promising option for
hypoxia prediction in brain tumors. This approach could significantly improve
the accessibility of hypoxia detection in clinical settings, with the potential
for more timely and targeted treatments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Perlo_D/0/1/0/all/0/1&quot;&gt;Daniele Perlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanli_G/0/1/0/all/0/1&quot;&gt;Georgia Kanli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boudissa_S/0/1/0/all/0/1&quot;&gt;Selma Boudissa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Keunen_O/0/1/0/all/0/1&quot;&gt;Olivier Keunen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14174">
<title>The Boundaries of Tractability in Hierarchical Task Network Planning. (arXiv:2401.14174v1 [cs.CC])</title>
<link>http://arxiv.org/abs/2401.14174</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the complexity-theoretic boundaries of tractability for three
classical problems in the context of Hierarchical Task Network Planning: the
validation of a provided plan, whether an executable plan exists, and whether a
given state can be reached by some plan. We show that all three problems can be
solved in polynomial time on primitive task networks of constant partial order
width (and a generalization thereof), whereas for the latter two problems this
holds only under a provably necessary restriction to the state space. Next, we
obtain an algorithmic meta-theorem along with corresponding lower bounds to
identify tight conditions under which general polynomial-time solvability
results can be lifted from primitive to general task networks. Finally, we
enrich our investigation by analyzing the parameterized complexity of the three
considered problems, and show that (1) fixed-parameter tractability for all
three problems can be achieved by replacing the partial order width with the
vertex cover number of the network as the parameter, and (2) other classical
graph-theoretic parameters of the network (including treewidth, treedepth, and
the aforementioned partial order width) do not yield fixed-parameter
tractability for any of the three problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_C/0/1/0/all/0/1&quot;&gt;Cornelius Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inerney_F/0/1/0/all/0/1&quot;&gt;Fionn Mc Inerney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wietheger_S/0/1/0/all/0/1&quot;&gt;Simon Wietheger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14176">
<title>Copilot Refinement: Addressing Code Smells in Copilot-Generated Python Code. (arXiv:2401.14176v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.14176</link>
<description rdf:parseType="Literal">&lt;p&gt;As one of the most popular dynamic languages, Python experiences a decrease
in readability and maintainability when code smells are present. Recent
advancements in Large Language Models have sparked growing interest in
AI-enabled tools for both code generation and refactoring. GitHub Copilot is
one such tool that has gained widespread usage. Copilot Chat, released on
September 2023, functions as an interactive tool aims at facilitating natural
language-powered coding. However, limited attention has been given to
understanding code smells in Copilot-generated Python code and Copilot&apos;s
ability to fix the code smells it generates. To this end, we built a dataset
comprising 102 code smells in Copilot-generated Python code. Our aim is to
first explore the occurrence of code smells in Copilot-generated Python code
and then evaluate the effectiveness of Copilot in fixing these code smells
employing different prompts. The results show that 8 out of 10 types of Python
smells can be detected in Copilot-generated Python code, among which
Multiply-Nested Container is the most common one. For these code smells,
Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing
Python code smells generated by Copilot itself. Besides, the effectiveness of
Copilot Chat in fixing these smells can be improved with the provision of more
detailed prompts. However, using Copilot Chat to fix these smells might
introduce new code smells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Beiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Peng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qiong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yujia Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zengyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14183">
<title>Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels. (arXiv:2401.14183v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.14183</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent global disruptions, such as the pandemic and geopolitical conflicts,
have profoundly exposed vulnerabilities in traditional supply chains, requiring
exploration of more resilient alternatives. Autonomous supply chains (ASCs)
have emerged as a potential solution, offering increased visibility,
flexibility, and resilience in turbulent trade environments. Despite
discussions in industry and academia over several years, ASCs lack
well-established theoretical foundations. This paper addresses this research
gap by presenting a formal definition of ASC along with its defining
characteristics and auxiliary concepts. We propose a layered conceptual
framework called the MIISI model. An illustrative case study focusing on the
meat supply chain demonstrates an initial ASC implementation based on this
conceptual model. Additionally, we introduce a seven-level supply chain
autonomy reference model, delineating a trajectory towards achieving a full
supply chain autonomy. Recognising that this work represents an initial
endeavour, we emphasise the need for continued exploration in this emerging
domain. We anticipate that this work will stimulate further research, both
theoretical and technical, and contribute to the continual evolution of ASCs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_S/0/1/0/all/0/1&quot;&gt;Stephen Mak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Proselkov_Y/0/1/0/all/0/1&quot;&gt;Yaniv Proselkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brintrup_A/0/1/0/all/0/1&quot;&gt;Alexandra Brintrup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14185">
<title>TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down Fusion. (arXiv:2401.14185v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.14185</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual speech separation has gained significant traction in recent
years due to its potential applications in various fields such as speech
recognition, diarization, scene analysis and assistive technologies. Designing
a lightweight audio-visual speech separation network is important for
low-latency applications, but existing methods often require higher
computational costs and more parameters to achieve better separation
performance. In this paper, we present an audio-visual speech separation model
called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for
audio-visual speech separation, which builds upon the architecture of TDANet,
an audio-only speech separation method. TDANet serves as the architectural
foundation for the auditory and visual networks within TDFNet, offering an
efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet
achieves a performance increase of up to 10\% across all performance metrics
compared with the previous SOTA method CTCNet. Remarkably, these results are
achieved using fewer parameters and only 28\% of the multiply-accumulate
operations (MACs) of CTCNet. In essence, our method presents a highly effective
and efficient solution to the challenges of speech separation within the
audio-visual domain, making significant strides in harnessing visual
information optimally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pegg_S/0/1/0/all/0/1&quot;&gt;Samuel Pegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaolin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14206">
<title>Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation classification. (arXiv:2401.14206v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14206</link>
<description rdf:parseType="Literal">&lt;p&gt;The liver is the most involved organ by distant metastasis in colon-rectal
cancer (CRC) patients and it comes necessary to be aware of the mutational
status of the lesions to correctly design the best individual treatment. So
far, efforts have been made in order to develop non-invasive and real-time
methods that permit the analysis of the whole tumor, using new artificial
intelligence tools to analyze the tumor&apos;s image obtained by Computed Tomography
(CT) scan. In order to address the current medical workflow, that is biopsy
analysis-based, we propose the first DeepLearning-based exploration, to our
knowledge, of such classification approach from the patient medical imaging. We
propose i) a solid pipeline for managing undersized datasets of available CT
scans and ii) a baseline study for genomics mutation diagnosis support for
preemptive patient follow-up. Our method is able to identify CRC RAS mutation
family from CT images with 0.73 F1 score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Perlo_D/0/1/0/all/0/1&quot;&gt;Daniele Perlo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berton_L/0/1/0/all/0/1&quot;&gt;Luca Berton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Delpiano_A/0/1/0/all/0/1&quot;&gt;Alessia Delpiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menchini_F/0/1/0/all/0/1&quot;&gt;Francesca Menchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tibaldi_S/0/1/0/all/0/1&quot;&gt;Stefano Tibaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grosso_M/0/1/0/all/0/1&quot;&gt;Marco Grosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fonio_P/0/1/0/all/0/1&quot;&gt;Paolo Fonio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14215">
<title>Commonsense-augmented Memory Construction and Management in Long-term Conversations via Context-aware Persona Refinement. (arXiv:2401.14215v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14215</link>
<description rdf:parseType="Literal">&lt;p&gt;Memorizing and utilizing speakers&apos; personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hana Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_K/0/1/0/all/0/1&quot;&gt;Kai Tzu-iunn Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seoyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jinyoung Yeo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14228">
<title>Assessing the Portability of Parameter Matrices Trained by Parameter-Efficient Finetuning Methods. (arXiv:2401.14228v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14228</link>
<description rdf:parseType="Literal">&lt;p&gt;As the cost of training ever larger language models has grown, so has the
interest in reusing previously learnt knowledge. Transfer learning methods have
shown how reusing non-task-specific knowledge can help in subsequent
task-specific learning. In this paper, we investigate the inverse: porting
whole functional modules that encode task-specific knowledge from one model to
another. We designed a study comprising 1,440 training/testing runs to test the
portability of modules trained by parameter-efficient finetuning (PEFT)
techniques, using sentiment analysis as an example task. We test portability in
a wide range of scenarios, involving different PEFT techniques and different
pretrained host models, among other dimensions. We compare the performance of
ported modules with that of equivalent modules trained (i) from scratch, and
(ii) from parameters sampled from the same distribution as the ported module.
We find that the ported modules far outperform the two alternatives tested, but
that there are interesting performance differences between the four PEFT
techniques. We conclude that task-specific knowledge in the form of
structurally modular sets of parameters as produced by PEFT techniques is
highly portable, but that degree of success depends on type of PEFT and on
differences between originating and receiving pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabry_M/0/1/0/all/0/1&quot;&gt;Mohammed Sabry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belz_A/0/1/0/all/0/1&quot;&gt;Anya Belz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14232">
<title>AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;This study developed a generative adversarial network (GAN)-based defense
method for traffic sign classification in an autonomous vehicle (AV), referred
to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i)
assuming zero knowledge of adversarial attack models and samples and (ii)
providing consistently high traffic sign classification performance under
various adversarial attack types. The AR-GAN classification system consists of
a generator that denoises an image by reconstruction, and a classifier that
classifies the reconstructed image. The authors have tested the AR-GAN under
no-attack and under various adversarial attacks, such as Fast Gradient Sign
Method (FGSM), DeepFool, Carlini and Wagner (C&amp;amp;W), and Projected Gradient
Descent (PGD). The authors considered two forms of these attacks, i.e., (i)
black-box attacks (assuming the attackers possess no prior knowledge of the
classifier), and (ii) white-box attacks (assuming the attackers possess full
knowledge of the classifier). The classification performance of the AR-GAN was
compared with several benchmark adversarial defense methods. The results showed
that both the AR-GAN and the benchmark defense methods are resilient against
black-box attacks and could achieve similar classification performance to that
of the unperturbed images. However, for all the white-box attacks considered in
this study, the AR-GAN method outperformed the benchmark defense methods. In
addition, the AR-GAN was able to maintain its high classification performance
under varied white-box adversarial perturbation magnitudes, whereas the
performance of the other defense methods dropped abruptly at increased
perturbation magnitudes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salek_M/0/1/0/all/0/1&quot;&gt;M Sabbir Salek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_A/0/1/0/all/0/1&quot;&gt;Abdullah Al Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mashrur Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14257">
<title>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14257</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minglin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yukun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zhe Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yisheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zilong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulan Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14267">
<title>Transformers and Cortical Waves: Encoders for Pulling In Context Across Time. (arXiv:2401.14267v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14267</link>
<description rdf:parseType="Literal">&lt;p&gt;The capabilities of transformer networks such as ChatGPT and other Large
Language Models (LLMs) have captured the world&apos;s attention. The crucial
computational mechanism underlying their performance relies on transforming a
complete input sequence - for example, all the words in a sentence into a long
&quot;encoding vector&quot; - that allows transformers to learn long-range temporal
dependencies in naturalistic sequences. Specifically, &quot;self-attention&quot; applied
to this encoding vector enhances temporal context in transformers by computing
associations between pairs of words in the input sequence. We suggest that
waves of neural activity, traveling across single cortical regions or across
multiple regions at the whole-brain scale, could implement a similar encoding
principle. By encapsulating recent input history into a single spatial pattern
at each moment in time, cortical waves may enable temporal context to be
extracted from sequences of sensory inputs, the same computational principle
used in transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1&quot;&gt;Lyle Muller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Churchland_P/0/1/0/all/0/1&quot;&gt;Patricia S. Churchland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sejnowski_T/0/1/0/all/0/1&quot;&gt;Terrence J. Sejnowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14280">
<title>RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization. (arXiv:2401.14280v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14280</link>
<description rdf:parseType="Literal">&lt;p&gt;This study addresses the challenge of extending Large Language Models (LLMs)
to non-English languages, specifically those using non-Latin scripts. We
propose an innovative approach that utilizes the romanized form of text as an
interface for LLMs, hypothesizing that its frequent informal use and shared
tokens with English enhance cross-lingual alignment. Focusing on Hindi, we
demonstrate through Hindi-to-English translation and sentiment analysis tasks
that romanized text not only significantly improves inference efficiency due to
its lower fertility compared to native text but also achieves competitive
performance with limited pre-training. Additionally, our novel multi-script
prompting approach, which combines romanized and native texts, shows promise in
further enhancing task performance. These findings suggest the potential of
romanization in bridging the language gap for LLM applications, with future
work aimed at expanding this approach to more languages and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Husain_J/0/1/0/all/0/1&quot;&gt;Jaavid Aktar Husain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1&quot;&gt;Raj Dabre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aswanth Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1&quot;&gt;Ratish Puduppully&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1&quot;&gt;Anoop Kunchukuttan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14285">
<title>POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation. (arXiv:2401.14285v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14285</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-dose PET offers a valuable means of minimizing radiation exposure in PET
imaging. However, the prevalent practice of employing additional CT scans for
generating attenuation maps (u-map) for PET attenuation correction
significantly elevates radiation doses. To address this concern and further
mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an
innovative population-prior-aided over-under-representation network that aims
for high-quality attenuation map generation from low-dose PET. First, POUR-Net
incorporates an over-under-representation network (OUR-Net) to facilitate
efficient feature extraction, encompassing both low-resolution abstracted and
fine-detail features, for assisting deep generation on the full-resolution
level. Second, complementing OUR-Net, a population prior generation machine
(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional
prior information to aid OUR-Net generation. The integration of OUR-Net and
PPGM within a cascade framework enables iterative refinement of $\mu$-map
generation, resulting in the production of high-quality $\mu$-maps.
Experimental results underscore the effectiveness of POUR-Net, showing it as a
promising solution for accurate CT-free low-count PET attenuation correction,
which also surpasses the performance of previous baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yinchi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiongchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Huidong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xueqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yu-Jung Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panin_V/0/1/0/all/0/1&quot;&gt;Vladimir Y. Panin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toyonaga_T/0/1/0/all/0/1&quot;&gt;Takuya Toyonaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14292">
<title>AST-2: Single and bi-layered 2-D acoustic soft tactile skin. (arXiv:2401.14292v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14292</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to present an innovative and cost-effective design for
Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly
enhancing the accuracy of 2-D tactile feature estimation. The existing
challenge lies in achieving precise tactile feature estimation, especially
concerning contact geometry characteristics, using cost-effective solutions. We
hypothesise that by harnessing acoustic energy through dedicated acoustic
channels in 2 layers beneath the sensing surface and analysing amplitude
modulation, we can effectively decode interactions on the sensory surface,
thereby improving tactile feature estimation. Our approach involves the
distinct separation of hardware components responsible for emitting and
receiving acoustic signals, resulting in a modular and highly customizable skin
design. Practical tests demonstrate the effectiveness of this novel design,
achieving remarkable precision in estimating contact normal forces (MAE &amp;lt; 0.8
N), 2D contact localisation (MAE &amp;lt; 0.7 mm), and contact surface diameter (MAE &amp;lt;
0.3 mm). In conclusion, the AST skin, with its innovative design and modular
architecture, successfully addresses the challenge of tactile feature
estimation. The presented results showcase its ability to precisely estimate
various tactile features, making it a practical and cost-effective solution for
robotic applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_V/0/1/0/all/0/1&quot;&gt;Vishnu Rajendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsons_S/0/1/0/all/0/1&quot;&gt;Simon Parsons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+E_A/0/1/0/all/0/1&quot;&gt;Amir Ghalamzan E&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14295">
<title>Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14295</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of natural language processing (NLP) has witnessed significant
progress in recent years, with a notable focus on improving large language
models&apos; (LLM) performance through innovative prompting techniques. Among these,
prompt engineering coupled with structures has emerged as a promising paradigm,
with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,
in which the overall LLM reasoning is guided by a structure such as a graph. As
illustrated with numerous examples, this paradigm significantly enhances the
LLM&apos;s capability to solve numerous tasks, ranging from logical or mathematical
reasoning to planning or creative writing. To facilitate the understanding of
this growing field and pave the way for future developments, we devise a
general blueprint for effective and efficient LLM reasoning schemes. For this,
we conduct an in-depth analysis of the prompt execution pipeline, clarifying
and clearly defining different concepts. We then build the first taxonomy of
structure-enhanced LLM reasoning schemes. We focus on identifying fundamental
classes of harnessed structures, and we analyze the representations of these
structures, algorithms executed with these structures, and many others. We
refer to these structures as reasoning topologies, because their representation
becomes to a degree spatial, as they are contained within the LLM context. Our
study compares existing prompting schemes using the proposed taxonomy,
discussing how certain design choices lead to different patterns in performance
and cost. We also outline theoretical underpinnings, relationships between
prompting and others parts of the LLM ecosystem such as knowledge bases, and
the associated research challenges. Our work will help to advance future prompt
engineering techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1&quot;&gt;Maciej Besta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memedi_F/0/1/0/all/0/1&quot;&gt;Florim Memedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstenberger_R/0/1/0/all/0/1&quot;&gt;Robert Gerstenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blach_N/0/1/0/all/0/1&quot;&gt;Nils Blach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nyczyk_P/0/1/0/all/0/1&quot;&gt;Piotr Nyczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Copik_M/0/1/0/all/0/1&quot;&gt;Marcin Copik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwasniewski_G/0/1/0/all/0/1&quot;&gt;Grzegorz Kwa&amp;#x15b;niewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1&quot;&gt;Lukas Gianinazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubicek_A/0/1/0/all/0/1&quot;&gt;Ales Kubicek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niewiadomski_H/0/1/0/all/0/1&quot;&gt;Hubert Niewiadomski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1&quot;&gt;Onur Mutlu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14336">
<title>Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition. (arXiv:2401.14336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14336</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained vehicle recognition (FGVR) is an essential fundamental
technology for intelligent transportation systems, but very difficult because
of its inherent intra-class variation. Most previous FGVR studies only focus on
the intra-class variation caused by different shooting angles, positions, etc.,
while the intra-class variation caused by image noise has received little
attention. This paper proposes a progressive multi-task anti-noise learning
(PMAL) framework and a progressive multi-task distilling (PMD) framework to
solve the intra-class variation problem in FGVR due to image noise. The PMAL
framework achieves high recognition accuracy by treating image denoising as an
additional task in image recognition and progressively forcing a model to learn
noise invariance. The PMD framework transfers the knowledge of the PMAL-trained
model into the original backbone network, which produces a model with about the
same recognition accuracy as the PMAL-trained model, but without any additional
overheads over the original backbone network. Combining the two frameworks, we
obtain models that significantly exceed previous state-of-the-art methods in
recognition accuracy on two widely-used, standard FGVR datasets, namely
Stanford Cars, and CompCars, as well as three additional surveillance
image-based vehicle-type classification datasets, namely Beijing Institute of
Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images
Dataset for Make Model Recognition (VIDMMR), without any additional overheads
over the original backbone networks. The source code is available at
https://github.com/Dichao-Liu/Anti-noise_FGVR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dichao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14362">
<title>The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support. (arXiv:2401.14362v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.14362</link>
<description rdf:parseType="Literal">&lt;p&gt;People experiencing severe distress increasingly use Large Language Model
(LLM) chatbots as mental health support tools. Discussions on social media have
described how engagements were lifesaving for some, but evidence suggests that
general-purpose LLM chatbots also have notable risks that could endanger the
welfare of users if not designed responsibly. In this study, we investigate the
lived experiences of people who have used LLM chatbots for mental health
support. We build on interviews with 21 individuals from globally diverse
backgrounds to analyze how users create unique support roles for their
chatbots, fill in gaps in everyday care, and navigate associated cultural
limitations when seeking support from chatbots. We ground our analysis in
psychotherapy literature around effective support, and introduce the concept of
therapeutic alignment, or aligning AI with therapeutic values for mental health
contexts. Our study offers recommendations for how designers can approach the
ethical and effective use of LLM chatbots and other AI mental health support
tools in mental health care.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_I/0/1/0/all/0/1&quot;&gt;Inhwa Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pendse_S/0/1/0/all/0/1&quot;&gt;Sachin R. Pendse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Neha Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Munmun De Choudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14367">
<title>Genie: Achieving Human Parity in Content-Grounded Datasets Generation. (arXiv:2401.14367v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14367</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of high-quality data for content-grounded generation tasks has been
identified as a major obstacle to advancing these tasks. To address this gap,
we propose Genie, a novel method for automatically generating high-quality
content-grounded data. It consists of three stages: (a) Content Preparation,
(b) Generation: creating task-specific examples from the content (e.g.,
question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure
the quality and faithfulness of the generated data. We showcase this
methodology by generating three large-scale synthetic data, making wishes, for
Long-Form Question-Answering (LFQA), summarization, and information extraction.
In a human evaluation, our generated data was found to be natural and of high
quality. Furthermore, we compare models trained on our data with models trained
on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for
Summarization. We show that our models are on par with or outperforming models
trained on human-generated data and consistently outperforming them in
faithfulness. Finally, we applied our method to create LFQA data within the
medical domain and compared a model trained on it with models trained on other
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1&quot;&gt;Asaf Yehudai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1&quot;&gt;Boaz Carmeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mass_Y/0/1/0/all/0/1&quot;&gt;Yosi Mass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arviv_O/0/1/0/all/0/1&quot;&gt;Ofir Arviv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_N/0/1/0/all/0/1&quot;&gt;Nathaniel Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toledo_A/0/1/0/all/0/1&quot;&gt;Assaf Toledo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1&quot;&gt;Eyal Shnarch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1&quot;&gt;Leshem Choshen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14371">
<title>Efficient Optimisation of Physical Reservoir Computers using only a Delayed Input. (arXiv:2401.14371v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2401.14371</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an experimental validation of a recently proposed optimization
technique for reservoir computing, using an optoelectronic setup. Reservoir
computing is a robust framework for signal processing applications, and the
development of efficient optimization approaches remains a key challenge. The
technique we address leverages solely a delayed version of the input signal to
identify the optimal operational region of the reservoir, simplifying the
traditionally time-consuming task of hyperparameter tuning. We verify the
effectiveness of this approach on different benchmark tasks and reservoir
operating conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picco_E/0/1/0/all/0/1&quot;&gt;Enrico Picco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaurigue_L/0/1/0/all/0/1&quot;&gt;Lina Jaurigue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludge_K/0/1/0/all/0/1&quot;&gt;Kathy L&amp;#xfc;dge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massar_S/0/1/0/all/0/1&quot;&gt;Serge Massar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14373">
<title>TURNA: A Turkish Encoder-Decoder Language Model for Enhanced Understanding and Generation. (arXiv:2401.14373v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.14373</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advances in natural language processing have predominantly favored
well-resourced English-centric models, resulting in a significant gap with
low-resource languages. In this work, we introduce the language model TURNA,
which is developed for the low-resource language Turkish and is capable of both
natural language understanding and generation tasks. TURNA is pretrained with
an encoder-decoder architecture based on the unified framework UL2 with a
diverse corpus that we specifically curated for this purpose. We evaluated
TURNA with three generation tasks and five understanding tasks for Turkish. The
results show that TURNA outperforms several multilingual models in both
understanding and generation tasks, and competes with monolingual Turkish
models in understanding tasks. TURNA is made available at
https://huggingface.co/boun-tabi-LMG/TURNA .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uludogan_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;k&amp;#xe7;e Uludo&amp;#x11f;an&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balal_Z/0/1/0/all/0/1&quot;&gt;Zeynep Yirmibe&amp;#x15f;o&amp;#x11f;lu Balal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkurt_F/0/1/0/all/0/1&quot;&gt;Furkan Akkurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turker_M/0/1/0/all/0/1&quot;&gt;Melik&amp;#x15f;ah T&amp;#xfc;rker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gungor_O/0/1/0/all/0/1&quot;&gt;Onur G&amp;#xfc;ng&amp;#xf6;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uskudarli_S/0/1/0/all/0/1&quot;&gt;Susan &amp;#xdc;sk&amp;#xfc;darl&amp;#x131;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14403">
<title>Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1&quot;&gt;Russell Mendonca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1&quot;&gt;Kenneth Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14405">
<title>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14405</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.07295">
<title>Adversarial Graph Disentanglement. (arXiv:2103.07295v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2103.07295</link>
<description rdf:parseType="Literal">&lt;p&gt;A real-world graph has a complex topological structure, which is often formed
by the interaction of different latent factors. However, most existing methods
lack consideration of the intrinsic differences in relations between nodes
caused by factor entanglement. In this paper, we propose an
\underline{\textbf{A}}dversarial \underline{\textbf{D}}isentangled
\underline{\textbf{G}}raph \underline{\textbf{C}}onvolutional
\underline{\textbf{N}}etwork (ADGCN) for disentangled graph representation
learning. To begin with, we point out two aspects of graph disentanglement that
need to be considered, i.e., micro-disentanglement and macro-disentanglement.
For them, a component-specific aggregation approach is proposed to achieve
micro-disentanglement by inferring latent components that cause the links
between nodes. On the basis of micro-disentanglement, we further propose a
macro-disentanglement adversarial regularizer to improve the separability among
component distributions, thus restricting the interdependence among components.
Additionally, to reveal the topological graph structure, a diversity-preserving
node sampling approach is proposed, by which the graph structure can be
progressively refined in a way of local structure awareness. The experimental
results on various real-world graph data verify that our ADGCN obtains more
favorable performance over currently available alternatives. The source codes
of ADGCN are available at \textit{\url{https://github.com/SsGood/ADGCN}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhizhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09344">
<title>A note on incorrect inferences in non-binary qualitative probabilistic networks. (arXiv:2208.09344v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09344</link>
<description rdf:parseType="Literal">&lt;p&gt;Qualitative probabilistic networks (QPNs) combine the conditional
independence assumptions of Bayesian networks with the qualitative properties
of positive and negative dependence. They formalise various intuitive
properties of positive dependence to allow inferences over a large network of
variables. However, we will demonstrate in this paper that, due to an incorrect
symmetry property, many inferences obtained in non-binary QPNs are not
mathematically true. We will provide examples of such incorrect inferences and
briefly discuss possible resolutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carter_J/0/1/0/all/0/1&quot;&gt;Jack Storror Carter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11812">
<title>Explanations, Fairness, and Appropriate Reliance in Human-AI Decision-Making. (arXiv:2209.11812v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11812</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the effects of feature-based explanations on
distributive fairness of AI-assisted decisions, specifically focusing on the
task of predicting occupations from short textual bios. We also investigate how
any effects are mediated by humans&apos; fairness perceptions and their reliance on
AI recommendations. Our findings show that explanations influence fairness
perceptions, which, in turn, relate to humans&apos; tendency to adhere to AI
recommendations. However, we see that such explanations do not enable humans to
discern correct and incorrect AI recommendations. Instead, we show that they
may affect reliance irrespective of the correctness of AI recommendations.
Depending on which features an explanation highlights, this can foster or
hinder distributive fairness: when explanations highlight features that are
task-irrelevant and evidently associated with the sensitive attribute, this
prompts overrides that counter AI recommendations that align with gender
stereotypes. Meanwhile, if explanations appear task-relevant, this induces
reliance behavior that reinforces stereotype-aligned errors. These results
imply that feature-based explanations are not a reliable mechanism to improve
distributive fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoeffer_J/0/1/0/all/0/1&quot;&gt;Jakob Schoeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehl_N/0/1/0/all/0/1&quot;&gt;Niklas Kuehl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14051">
<title>Bridging Distributional and Risk-sensitive Reinforcement Learning with Provable Regret Bounds. (arXiv:2210.14051v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the regret guarantee for risk-sensitive reinforcement learning
(RSRL) via distributional reinforcement learning (DRL) methods. In particular,
we consider finite episodic Markov decision processes whose objective is the
entropic risk measure (EntRM) of return. By leveraging a key property of the
EntRM, the independence property, we establish the risk-sensitive
distributional dynamic programming framework. We then propose two novel DRL
algorithms that implement optimism through two different schemes, including a
model-free one and a model-based one.
&lt;/p&gt;
&lt;p&gt;We prove that they both attain $\tilde{\mathcal{O}}(\frac{\exp(|\beta|
H)-1}{|\beta|}H\sqrt{S^2AK})$ regret upper bound, where $S$, $A$, $K$, and $H$
represent the number of states, actions, episodes, and the time horizon,
respectively. It matches RSVI2 proposed in \cite{fei2021exponential}, with
novel distributional analysis. To the best of our knowledge, this is the first
regret analysis that bridges DRL and RSRL in terms of sample complexity.
&lt;/p&gt;
&lt;p&gt;Acknowledging the computational inefficiency associated with the model-free
DRL algorithm, we propose an alternative DRL algorithm with distribution
representation. This approach not only maintains the established regret bounds
but also significantly amplifies computational efficiency.
&lt;/p&gt;
&lt;p&gt;We also prove a tighter minimax lower bound of $\Omega(\frac{\exp(\beta
H/6)-1}{\beta H}H\sqrt{SAT})$ for the $\beta&amp;gt;0$ case, which recovers the tight
lower bound $\Omega(H\sqrt{SAT})$ in the risk-neutral setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhi-Quan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14080">
<title>Learning Individual Treatment Effects under Heterogeneous Interference in Networks. (arXiv:2210.14080v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14080</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimates of individual treatment effects from networked observational data
are attracting increasing attention these days. One major challenge in network
scenarios is the violation of the stable unit treatment value assumption
(SUTVA), which assumes that the treatment assignment of a unit does not
influence others&apos; outcomes. In network data, due to interference, the outcome
of a unit is influenced not only by its treatment (i.e., direct effects) but
also by others&apos; treatments (i.e., spillover effects). Furthermore, the
influences from other units are always heterogeneous (e.g., friends with
similar interests affect a person differently than friends with different
interests). In this paper, we focus on the problem of estimating individual
treatment effects (both direct and spillover effects) under heterogeneous
interference. To address this issue, we propose a novel Dual Weighting
Regression (DWR) algorithm by simultaneously learning attention weights that
capture the heterogeneous interference and sample weights to eliminate the
complex confounding bias in networks. We formulate the entire learning process
as a bi-level optimization problem. In theory, we present generalization error
bounds for individual treatment effect estimation. Extensive experiments on
four benchmark datasets demonstrate that the proposed DWR algorithm outperforms
state-of-the-art methods for estimating individual treatment effects under
heterogeneous interference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01839">
<title>HyperSound: Generating Implicit Neural Representations of Audio Signals with Hypernetworks. (arXiv:2211.01839v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01839</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representations (INRs) are a rapidly growing research field,
which provides alternative ways to represent multimedia signals. Recent
applications of INRs include image super-resolution, compression of
high-dimensional signals, or 3D rendering. However, these solutions usually
focus on visual data, and adapting them to the audio domain is not trivial.
Moreover, it requires a separately trained model for every data sample. To
address this limitation, we propose HyperSound, a meta-learning method
leveraging hypernetworks to produce INRs for audio signals unseen at training
time. We show that our approach can reconstruct sound waves with quality
comparable to other state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szatkowski_F/0/1/0/all/0/1&quot;&gt;Filip Szatkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piczak_K/0/1/0/all/0/1&quot;&gt;Karol J. Piczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02515">
<title>GNN-based Passenger Request Prediction. (arXiv:2301.02515v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02515</link>
<description rdf:parseType="Literal">&lt;p&gt;Passenger request prediction is essential for operations planning, control,
and management in ride-sharing platforms. While the demand prediction problem
has been studied extensively, the Origin-Destination (OD) flow prediction of
passengers has received less attention from the research community. This paper
develops a Graph Neural Network framework along with the Attention Mechanism to
predict the OD flow of passengers. The proposed framework exploits various
linear and non-linear dependencies that arise among requests originating from
different locations and captures the repetition pattern and the contextual data
of that place. Moreover, the optimal size of the grid cell that covers the road
network and preserves the complexity and accuracy of the model is determined.
Extensive simulations are conducted to examine the characteristics of our
proposed approach and its various components. The results show the superior
performance of our proposed model compared to the existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makhdomi_A/0/1/0/all/0/1&quot;&gt;Aqsa Ashraf Makhdomi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillani_I/0/1/0/all/0/1&quot;&gt;Iqra Altaf Gillani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01622">
<title>Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01622</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) models are increasingly used in the medical
domain. However, as medical data is highly sensitive, special precautions to
ensure its protection are required. The gold standard for privacy preservation
is the introduction of differential privacy (DP) to model training. Prior work
indicates that DP has negative implications on model accuracy and fairness,
which are unacceptable in medicine and represent a main barrier to the
widespread use of privacy-preserving techniques. In this work, we evaluated the
effect of privacy-preserving training of AI models regarding accuracy and
fairness compared to non-private training. For this, we used two datasets: (1)
A large dataset (N=193,311) of high quality clinical chest radiographs, and (2)
a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the
task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).
Both were retrospectively collected and manually labeled by experienced
radiologists. We then compared non-private deep convolutional neural networks
(CNNs) and privacy-preserving (DP) models with respect to privacy-utility
trade-offs measured as area under the receiver-operator-characteristic curve
(AUROC), and privacy-fairness trade-offs, measured as Pearson&apos;s r or
Statistical Parity Difference. We found that, while the privacy-preserving
trainings yielded lower accuracy, they did largely not amplify discrimination
against age, sex or co-morbidity. Our study shows that -- under the challenging
realistic circumstances of a real-life clinical dataset -- the
privacy-preserving training of diagnostic deep learning models is possible with
excellent diagnostic accuracy and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arasteh_S/0/1/0/all/0/1&quot;&gt;Soroosh Tayebi Arasteh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuhl_C/0/1/0/all/0/1&quot;&gt;Christiane Kuhl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1&quot;&gt;Marcus Makowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nebelung_S/0/1/0/all/0/1&quot;&gt;Sven Nebelung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03106">
<title>Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03106</link>
<description rdf:parseType="Literal">&lt;p&gt;Post-training Neural Network (NN) model compression is an attractive approach
for deploying large, memory-consuming models on devices with limited memory
resources. In this study, we investigate the rate-distortion tradeoff for NN
model compression. First, we suggest a Rotation-Invariant Quantization (RIQ)
technique that utilizes a single parameter to quantize the entire NN model,
yielding a different rate at each layer, i.e., mixed-precision quantization.
Then, we prove that our rotation-invariant approach is optimal in terms of
compression. We rigorously evaluate RIQ and demonstrate its capabilities on
various models and tasks. For example, RIQ facilitates $\times 19.4$ and
$\times 52.9$ compression ratios on pre-trained VGG dense and pruned models,
respectively, with $&amp;lt;0.4\%$ accuracy degradation. Code is available in
\url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampeas_J/0/1/0/all/0/1&quot;&gt;Joseph Kampeas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nahshan_Y/0/1/0/all/0/1&quot;&gt;Yury Nahshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kremer_H/0/1/0/all/0/1&quot;&gt;Hanoch Kremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederman_G/0/1/0/all/0/1&quot;&gt;Gil Lederman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaloshinski_S/0/1/0/all/0/1&quot;&gt;Shira Zaloshinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haleva_E/0/1/0/all/0/1&quot;&gt;Emir Haleva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13496">
<title>The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13496</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and
zero-shot transfer on Food-101 (96.2%). Our study reveals that model
initialization plays a significant role, even for web-scale pretraining with
billions of images, and our models are available publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mannat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1&quot;&gt;Quentin Duval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alwala_K/0/1/0/all/0/1&quot;&gt;Kalyan Vasudev Alwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaibhav Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adcock_A/0/1/0/all/0/1&quot;&gt;Aaron Adcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1&quot;&gt;Armand Joulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1&quot;&gt;Piotr Doll&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1&quot;&gt;Ross Girshick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1&quot;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1&quot;&gt;Ishan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04928">
<title>From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts. (arXiv:2305.04928v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04928</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised named entity recognition (NER) in the biomedical domain depends on
large sets of annotated texts with the given named entities. The creation of
such datasets can be time-consuming and expensive, while extraction of new
entities requires additional annotation tasks and retraining the model. To
address these challenges, this paper proposes a method for zero- and few-shot
NER in the biomedical domain. The method is based on transforming the task of
multi-class token classification into binary token classification and
pre-training on a large amount of datasets and biomedical entities, which allow
the model to learn semantic relations between the given and potentially novel
named entity labels. We have achieved average F1 scores of 35.44% for zero-shot
NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot
NER on 9 diverse evaluated biomedical entities with fine-tuned PubMedBERT-based
model. The results demonstrate the effectiveness of the proposed method for
recognizing new biomedical entities with no or limited number of examples,
outperforming previous transformer-based methods, and being comparable to
GPT3-based models using models with over 1000 times fewer parameters. We make
models and developed code publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosprdic_M/0/1/0/all/0/1&quot;&gt;Milo&amp;#x161; Ko&amp;#x161;prdi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prodanovic_N/0/1/0/all/0/1&quot;&gt;Nikola Prodanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ljajic_A/0/1/0/all/0/1&quot;&gt;Adela Ljaji&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basaragin_B/0/1/0/all/0/1&quot;&gt;Bojana Ba&amp;#x161;aragin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1&quot;&gt;Nikola Milo&amp;#x161;evi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00789">
<title>Improved Cross-Lingual Transfer Learning For Automatic Speech Translation. (arXiv:2306.00789v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00789</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in multilingual speech-to-text translation is topical. Having a
single model that supports multiple translation tasks is desirable. The goal of
this work it to improve cross-lingual transfer learning in multilingual
speech-to-text translation via semantic knowledge distillation. We show that by
initializing the encoder of the encoder-decoder sequence-to-sequence
translation model with SAMU-XLS-R, a multilingual speech transformer encoder
trained using multi-modal (speech-text) semantic knowledge distillation, we
achieve significantly better cross-lingual task knowledge transfer than the
baseline XLS-R, a multilingual speech transformer encoder trained via
self-supervised learning. We demonstrate the effectiveness of our approach on
two popular datasets, namely, CoVoST-2 and Europarl. On the 21 translation
tasks of the CoVoST-2 benchmark, we achieve an average improvement of 12.8 BLEU
points over the baselines. In the zero-shot translation scenario, we achieve an
average gain of 18.8 and 11.9 average BLEU points on unseen medium and
low-resource languages. We make similar observations on Europarl speech
translation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_S/0/1/0/all/0/1&quot;&gt;Sameer Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawalatabad_N/0/1/0/all/0/1&quot;&gt;Nauman Dawalatabad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laurent_A/0/1/0/all/0/1&quot;&gt;Antoine Laurent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vicente_L/0/1/0/all/0/1&quot;&gt;Luis Vicente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_P/0/1/0/all/0/1&quot;&gt;Pablo Gimeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingote_V/0/1/0/all/0/1&quot;&gt;Victoria Mingote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01690">
<title>Context selectivity with dynamic availability enables lifelong continual learning. (arXiv:2306.01690v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01690</link>
<description rdf:parseType="Literal">&lt;p&gt;&quot;You never forget how to ride a bike&quot;, -- but how is that possible? The brain
is able to learn complex skills, stop the practice for years, learn other
skills in between, and still retrieve the original knowledge when necessary.
The mechanisms of this capability, referred to as lifelong learning (or
continual learning, CL), are unknown. We suggest a bio-plausible
meta-plasticity rule building on classical work in CL which we summarize in two
principles: (i) neurons are context selective, and (ii) a local availability
variable partially freezes the plasticity if the neuron was relevant for
previous tasks. In a new neuro-centric formalization of these principles, we
suggest that neuron selectivity and neuron-wide consolidation is a simple and
viable meta-plasticity hypothesis to enable CL in the brain. In simulation,
this simple model balances forgetting and consolidation leading to better
transfer learning than contemporary CL algorithms on image recognition and
natural language processing CL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barry_M/0/1/0/all/0/1&quot;&gt;Martin Barry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstner_W/0/1/0/all/0/1&quot;&gt;Wulfram Gerstner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellec_G/0/1/0/all/0/1&quot;&gt;Guillaume Bellec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08302">
<title>Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08302</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as ChatGPT and GPT4, are making new waves
in the field of natural language processing and artificial intelligence, due to
their emergent ability and generalizability. However, LLMs are black-box
models, which often fall short of capturing and accessing factual knowledge. In
contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are
structured knowledge models that explicitly store rich factual knowledge. KGs
can enhance LLMs by providing external knowledge for inference and
interpretability. Meanwhile, KGs are difficult to construct and evolving by
nature, which challenges the existing methods in KGs to generate new facts and
represent unseen knowledge. Therefore, it is complementary to unify LLMs and
KGs together and simultaneously leverage their advantages. In this article, we
present a forward-looking roadmap for the unification of LLMs and KGs. Our
roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs,
which incorporate KGs during the pre-training and inference phases of LLMs, or
for the purpose of enhancing understanding of the knowledge learned by LLMs; 2)
LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding,
completion, construction, graph-to-text generation, and question answering; and
3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a
mutually beneficial way to enhance both LLMs and KGs for bidirectional
reasoning driven by both data and knowledge. We review and summarize existing
efforts within these three frameworks in our roadmap and pinpoint their future
research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linhao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiapu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xindong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13588">
<title>System-Level Natural Language Feedback. (arXiv:2306.13588v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13588</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language (NL) feedback offers rich insights into user experience.
While existing studies focus on an instance-level approach, where feedback is
used to refine specific examples, we introduce a framework for system-level use
of NL feedback. We show how to use feedback to formalize system-level design
decisions in a human-in-the-loop-process -- in order to produce better models.
In particular this is done through: (i) metric design for tasks; and (ii)
language model prompt design for refining model responses. We conduct two case
studies of this approach for improving search query and dialog response
generation, demonstrating the effectiveness of system-level feedback. We show
the combination of system-level and instance-level feedback brings further
gains, and that human written instance-level feedback results in more grounded
refinements than GPT-3.5 written ones, underlying the importance of human
feedback for building systems. We release our code and data at
https://github.com/yyy-Apple/Sys-NL-Feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14806">
<title>A Positive-Unlabeled Metric Learning Framework for Document-Level Relation Extraction with Incomplete Labeling. (arXiv:2306.14806v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14806</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of document-level relation extraction (RE) is to identify relations
between entities that span multiple sentences. Recently, incomplete labeling in
document-level RE has received increasing attention, and some studies have used
methods such as positive-unlabeled learning to tackle this issue, but there is
still a lot of room for improvement. Motivated by this, we propose a
positive-augmentation and positive-mixup positive-unlabeled metric learning
framework (P3M). Specifically, we formulate document-level RE as a metric
learning problem. We aim to pull the distance closer between entity pair
embedding and their corresponding relation embedding, while pushing it farther
away from the none-class relation embedding. Additionally, we adapt the
positive-unlabeled learning to this loss objective. In order to improve the
generalizability of the model, we use dropout to augment positive samples and
propose a positive-none-class mixup method. Extensive experiments show that P3M
improves the F1 score by approximately 4-10 points in document-level RE with
incomplete labeling, and achieves state-of-the-art results in fully labeled
scenarios. Furthermore, P3M has also demonstrated robustness to prior
estimation bias in incomplete labeled scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Huazheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenxin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16424">
<title>Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16424</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread digitization of finance and the increasing popularity of
cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals
is growing. Money laundering -- the movement of illicit funds to conceal their
origins -- can cross bank and national boundaries, producing complex
transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0
trillion dollars are laundered globally each year. Unfortunately, real data to
train machine learning models to detect laundering is generally not available,
and previous synthetic data generators have had significant shortcomings. A
realistic, standardized, publicly-available benchmark is needed for comparing
models and for the advancement of the area.
&lt;/p&gt;
&lt;p&gt;To this end, this paper contributes a synthetic financial transaction dataset
generator and a set of synthetically generated AML (Anti-Money Laundering)
datasets. We have calibrated this agent-based generator to match real
transactions as closely as possible and made the datasets public. We describe
the generator in detail and demonstrate how the datasets generated can help
compare different machine learning models in terms of their AML abilities. In a
key way, using synthetic data in these comparisons can be even better than
using real data: the ground truth labels are complete, whilst many laundering
transactions in real data are never detected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altman_E/0/1/0/all/0/1&quot;&gt;Erik Altman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanusa_J/0/1/0/all/0/1&quot;&gt;Jovan Blanu&amp;#x161;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niederhausern_L/0/1/0/all/0/1&quot;&gt;Luc von Niederh&amp;#xe4;usern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egressy_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe9;ni Egressy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anghel_A/0/1/0/all/0/1&quot;&gt;Andreea Anghel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atasu_K/0/1/0/all/0/1&quot;&gt;Kubilay Atasu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03761">
<title>DyEdgeGAT: Dynamic Edge via Graph Attention for Early Fault Detection in IIoT Systems. (arXiv:2307.03761v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03761</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Industrial Internet of Things (IIoT), condition monitoring sensor
signals from complex systems often exhibit nonlinear and stochastic
spatial-temporal dynamics under varying conditions. These complex dynamics make
fault detection particularly challenging. While previous methods effectively
model these dynamics, they often neglect the evolution of relationships between
sensor signals. Undetected shifts in these relationships can lead to
significant system failures. Furthermore, these methods frequently misidentify
novel operating conditions as faults. Addressing these limitations, we propose
DyEdgeGAT (Dynamic Edge via Graph Attention), a novel approach for early-stage
fault detection in IIoT systems. DyEdgeGAT&apos;s primary innovation lies in a novel
graph inference scheme for multivariate time series that tracks the evolution
of relationships between time series, enabled by dynamic edge construction.
Another key innovation of DyEdgeGAT is its ability to incorporate operating
condition contexts into node dynamics modeling, enhancing its accuracy and
robustness. We rigorously evaluated DyEdgeGAT using both a synthetic dataset,
simulating varying levels of fault severity, and a real-world industrial-scale
multiphase flow facility benchmark with diverse fault types under varying
operating conditions and detection complexities. The results show that
DyEdgeGAT significantly outperforms other baseline methods in fault detection,
particularly in the early stages with low severity, and exhibits robust
performance under novel operating conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09437">
<title>Grounded Object Centric Learning. (arXiv:2307.09437v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;The extraction of modular object-centric representations for downstream tasks
is an emerging area of research. Learning grounded representations of objects
that are guaranteed to be stable and invariant promises robust performance
across different tasks and environments. Slot Attention (SA) learns
object-centric representations by assigning objects to \textit{slots}, but
presupposes a \textit{single} distribution from which all slots are randomly
initialised. This results in an inability to learn \textit{specialized} slots
which bind to specific object types and remain invariant to identity-preserving
changes in object appearance. To address this, we present
\emph{\textsc{Co}nditional \textsc{S}lot \textsc{A}ttention} (\textsc{CoSA})
using a novel concept of \emph{Grounded Slot Dictionary} (GSD) inspired by
vector quantization. Our proposed GSD comprises (i) canonical object-level
property vectors and (ii) parametric Gaussian distributions, which define a
prior over the slots. We demonstrate the benefits of our method in multiple
downstream tasks such as scene generation, composition, and task adaptation,
whilst remaining competitive with SA in popular object discovery benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16778">
<title>KoBBQ: Korean Bias Benchmark for Question Answering. (arXiv:2307.16778v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16778</link>
<description rdf:parseType="Literal">&lt;p&gt;The Bias Benchmark for Question Answering (BBQ) is designed to evaluate
social biases of language models (LMs), but it is not simple to adapt this
benchmark to cultural contexts other than the US because social biases depend
heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias
benchmark dataset, and we propose a general framework that addresses
considerations for cultural adaptation of a dataset. Our framework includes
partitioning the BBQ dataset into three classes--Simply-Transferred (can be
used directly after cultural translation), Target-Modified (requires
localization in target groups), and Sample-Removed (does not fit Korean
culture)-- and adding four new categories of bias specific to Korean culture.
We conduct a large-scale survey to collect and validate the social biases and
the targets of the biases that reflect the stereotypes in Korean culture. The
resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12
categories of social bias. We use KoBBQ to measure the accuracy and bias scores
of several state-of-the-art multilingual LMs. The results clearly show
differences in the bias of LMs as measured by KoBBQ and a machine-translated
version of BBQ, demonstrating the need for and utility of a well-constructed,
culturally-aware social bias benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiho Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiseon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Nayeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_H/0/1/0/all/0/1&quot;&gt;Haneul Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1&quot;&gt;Alice Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hwaran Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01154">
<title>Arithmetic with Language Models: from Memorization to Computation. (arXiv:2308.01154v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01154</link>
<description rdf:parseType="Literal">&lt;p&gt;A better understanding of the emergent computation and problem-solving
capabilities of recent large language models is of paramount importance to
further improve them and broaden their applicability. This work investigates
how a language model, trained to predict the next token, can perform arithmetic
computations generalizing beyond training data. Binary addition and
multiplication constitute a good testbed for this purpose, since they require a
very small vocabulary and exhibit relevant input/output discontinuities making
smooth input interpolation ineffective for novel data. We successfully trained
a light language model to learn these tasks and ran a number of experiments to
investigate the extrapolation capabilities and internal information processing.
Our findings support the hypothesis that the language model works as an
Encoding-Regression-Decoding machine where the computation takes place in the
value space once the input token representation is mapped to an appropriate
internal representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maltoni_D/0/1/0/all/0/1&quot;&gt;Davide Maltoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrara_M/0/1/0/all/0/1&quot;&gt;Matteo Ferrara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12243">
<title>Multi-Objective Optimization for Sparse Deep Multi-Task Learning. (arXiv:2308.12243v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. The code is available at
https://github.com/salomonhotegni/MDMTN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotegni_S/0/1/0/all/0/1&quot;&gt;S. S. Hotegni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkemeier_M/0/1/0/all/0/1&quot;&gt;M. Berkemeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;S. Peitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03251">
<title>Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths on a
history temporal graph to model historical path information related to queries
for reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Meng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04160">
<title>PRISM: Leveraging Prototype Patient Representations with Feature-Missing-Aware Calibration for EHR Data Sparsity Mitigation. (arXiv:2309.04160v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04160</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic Health Record (EHR) data, while rich in information, often suffers
from sparsity, posing significant challenges in predictive modeling.
Traditional imputation methods inadequately distinguish between real and
imputed data, leading to potential inaccuracies in models. Addressing this, we
introduce PRISM, a novel approach that indirectly imputes data through
prototype representations of similar patients, thus ensuring denser and more
accurate embeddings. PRISM innovates further with a feature confidence learner
module, which evaluates the reliability of each feature in light of missing
data. Additionally, it incorporates a novel patient similarity metric that
accounts for feature confidence, avoiding overreliance on imprecise imputed
values. Our extensive experiments on the MIMIC-III and MIMIC-IV datasets
demonstrate PRISM&apos;s superior performance in predicting in-hospital mortality
and 30-day readmission tasks, showcasing its effectiveness in handling EHR data
sparsity. For the sake of reproducibility and further research, we have made
the code publicly available at https://github.com/yhzhu99/PRISM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Long He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shiyun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liantao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chengwei Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09404">
<title>Promoting Research Collaboration with Open Data Driven Team Recommendation in Response to Call for Proposals. (arXiv:2309.09404v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09404</link>
<description rdf:parseType="Literal">&lt;p&gt;Building teams and promoting collaboration are two very common business
activities. An example of these are seen in the TeamingForFunding problem,
where research institutions and researchers are interested to identify
collaborative opportunities when applying to funding agencies in response to
latter&apos;s calls for proposals. We describe a novel system to recommend teams
using a variety of AI methods, such that (1) each team achieves the highest
possible skill coverage that is demanded by the opportunity, and (2) the
workload of distributing the opportunities is balanced amongst the candidate
members. We address these questions by extracting skills latent in open data of
proposal calls (demand) and researcher profiles (supply), normalizing them
using taxonomies, and creating efficient algorithms that match demand to
supply. We create teams to maximize goodness along a novel metric balancing
short- and long-term objectives. We validate the success of our algorithms (1)
quantitatively, by evaluating the recommended teams using a goodness score and
find that more informed methods lead to recommendations of smaller number of
teams but higher goodness, and (2) qualitatively, by conducting a large-scale
user study at a college-wide level, and demonstrate that users overall found
the tool very useful and relevant. Lastly, we evaluate our system in two
diverse settings in US and India (of researchers and proposal calls) to
establish generality of our approach, and deploy it at a major US university
for routine use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valluru_S/0/1/0/all/0/1&quot;&gt;Siva Likitha Valluru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paladi_S/0/1/0/all/0/1&quot;&gt;Sai Teja Paladi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Siwen Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_S/0/1/0/all/0/1&quot;&gt;Sriraam Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05207">
<title>Facial Action Unit Detection Based on Multi-task Learning Strategy for Unlabeled Facial Images in the Wild. (arXiv:2310.05207v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05207</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial Action Unit (AU) detection often relies on highly-cost accurate
labeling or inaccurate pseudo labeling techniques in recent years. How to
introduce large amounts of unlabeled facial images in the wild into supervised
AU detection frameworks has become a challenging problem. Additionally, nearly
every type of AUs has the problem of unbalanced positive and negative samples.
Inspired by other multi-task learning frameworks, we first propose a multi-task
learning strategy boosting AU detection in the wild through jointing facial
landmark detection and AU domain separation and reconstruction. Our introduced
dual domains facial landmark detection framework can solve the lack of accurate
facial landmark coordinates during the AU domain separation and reconstruction
training process, while the parameters of homostructural facial extraction
modules from these two similar facial tasks are shared. Moreover, we propose a
pixel-level feature alignment scheme to maintain the consistency of features
obtained from two separation and reconstruction processes. Furthermore, a
weighted asymmetric loss is proposed to change the contribution of positive and
negative samples of each type of AUs to model parameters updating. Experimental
results on three widely used benchmarks demonstrate our superiority to most
state-of-the-art methods for AU detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07799">
<title>Domain-invariant Clinical Representation Learning by Bridging Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07799</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limited information about emerging diseases, symptoms are hard to
be noticed and recognized, so that the window for clinical intervention could
be ignored. An effective prognostic model is expected to assist doctors in
making right diagnosis and designing personalized treatment plan, so to
promptly prevent unfavorable outcomes. However, in the early stage of a
disease, limited data collection and clinical experiences, plus the concern out
of privacy and ethics, may result in restricted data availability for
reference, to the extent that even data labels are difficult to mark correctly.
In addition, Electronic Medical Record (EMR) data of different diseases or of
different sources of the same disease can prove to be having serious
cross-dataset feature misalignment problems, greatly mutilating the efficiency
of deep learning models. This article introduces a domain-invariant
representation learning method to build a transition model from source dataset
to target dataset. By way of constraining the distribution shift of features
generated in disparate domains, domain-invariant features that are exclusively
relative to downstream tasks are captured, so to cultivate a unified
domain-invariant encoder across various task domains to achieve better feature
representation. Experimental results of several target tasks demonstrate that
our proposed model outperforms competing baseline methods and has higher rate
of training convergence, especially in dealing with limited data amount. A
multitude of experiences have proven the efficacy of our method to provide more
accurate predictions concerning newly emergent pandemics and other diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaohe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liantao Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01193">
<title>Contextual Confidence and Generative AI. (arXiv:2311.01193v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01193</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI models perturb the foundations of effective human
communication. They present new challenges to contextual confidence, disrupting
participants&apos; ability to identify the authentic context of communication and
their ability to protect communication from reuse and recombination outside its
intended context. In this paper, we describe strategies--tools, technologies
and policies--that aim to stabilize communication in the face of these
challenges. The strategies we discuss fall into two broad categories.
Containment strategies aim to reassert context in environments where it is
currently threatened--a reaction to the context-free expectations and norms
established by the internet. Mobilization strategies, by contrast, view the
rise of generative AI as an opportunity to proactively set new and higher
expectations around privacy and authenticity in mediated communication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Shrey Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzig_Z/0/1/0/all/0/1&quot;&gt;Zo&amp;#xeb; Hitzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishkin_P/0/1/0/all/0/1&quot;&gt;Pamela Mishkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04928">
<title>Leveraging Large Language Models for Collective Decision-Making. (arXiv:2311.04928v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04928</link>
<description rdf:parseType="Literal">&lt;p&gt;In various work contexts, such as meeting scheduling, collaborating, and
project planning, collective decision-making is essential but often challenging
due to diverse individual preferences, varying work focuses, and power dynamics
among members. To address this, we propose a system leveraging Large Language
Models (LLMs) to facilitate group decision-making by managing conversations and
balancing preferences among individuals. Our system aims to extract individual
preferences from conversations and suggest options that satisfy the preferences
of the members. We specifically apply this system to corporate meeting
scheduling. We create synthetic employee profiles and simulate conversations at
scale, leveraging LLMs to evaluate the system performance as a novel approach
to conducting a user study. Our results indicate efficient coordination with
reduced interactions between the members and the LLM-based system. The system
refines and improves its proposed options over time, ensuring that many of the
members&apos; individual preferences are satisfied in an equitable way. Finally, we
conduct a survey study involving human participants to assess our system&apos;s
ability to aggregate preferences and reasoning about them. Our findings show
that the system exhibits strong performance in both dimensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1&quot;&gt;Marios Papachristou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Longqi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08543">
<title>2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection. (arXiv:2311.08543v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08543</link>
<description rdf:parseType="Literal">&lt;p&gt;Orthogonal time frequency space (OTFS) is a promising modulation scheme for
wireless communication in high-mobility scenarios. Recently, a reservoir
computing (RC) based approach has been introduced for online subframe-based
symbol detection in the OTFS system, where only a limited number of
over-the-air (OTA) pilot symbols are utilized for training. However, this
approach does not leverage the domain knowledge specific to the OTFS system to
fully unlock the potential of RC. This paper introduces a novel two-dimensional
RC (2D-RC) method that incorporates the domain knowledge of the OTFS system
into the design for symbol detection in an online subframe-based manner.
Specifically, as the channel interaction in the delay-Doppler (DD) domain is a
two-dimensional (2D) circular operation, the 2D-RC is designed to have the 2D
circular padding procedure and the 2D filtering structure to embed this
knowledge. With the introduced architecture, 2D-RC can operate in the DD domain
with only a single neural network, instead of necessitating multiple RCs to
track channel variations in the time domain as in previous work. Numerical
experiments demonstrate the advantages of the 2D-RC approach over the previous
RC-based approach and compared model-based methods across different OTFS system
variants and modulation orders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiarui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Said_K/0/1/0/all/0/1&quot;&gt;Karim Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lizhong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11482">
<title>Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11482</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive study of Meta Prompting, an innovative
technique reshaping the utilization of large language models (LLMs),
multi-modal foundation models, and AI systems in problem-solving and data
interpretation. Grounded in type theory and category theory, Meta Prompting
emphasizes the structure and syntax of information over traditional
content-centric methods. The paper explores the formal definitions of Meta
Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its
effectiveness in various AI applications. A key focus is on extending Meta
Prompting to complex reasoning tasks, showing how it effectively deconstructs
intricate problems into simpler sub-problems, enhancing token efficiency and
enabling more equitable problem-solving comparisons, especially against
few-shot example methods. Additionally, the paper introduces Meta Prompting for
Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative,
metaprogramming-like manner. This innovative approach marks a significant leap
in AI&apos;s autonomous and adaptive capabilities. The paper also pioneers the
integration of Meta Prompting into multi-modal foundation model settings,
tackling the challenges and opportunities of incorporating varied data types
such as images, audio, and video within the structured Meta Prompting
framework. (The code is available at
https://github.com/meta-prompting/meta-prompting)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13892">
<title>General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level. (arXiv:2311.13892v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13892</link>
<description rdf:parseType="Literal">&lt;p&gt;The social biases and unwelcome stereotypes revealed by pretrained language
models are becoming obstacles to their application. Compared to numerous
debiasing methods targeting word level, there has been relatively less
attention on biases present at phrase level, limiting the performance of
debiasing in discipline domains. In this paper, we propose an automatic
multi-token debiasing pipeline called \textbf{General Phrase Debiaser}, which
is capable of mitigating phrase-level biases in masked language models.
Specifically, our method consists of a \textit{phrase filter stage} that
generates stereotypical phrases from Wikipedia pages as well as a \textit{model
debias stage} that can debias models at the multi-token level to tackle bias
challenges on phrases. The latter searches for prompts that trigger model&apos;s
bias, and then uses them for debiasing. State-of-the-art results on standard
datasets and metrics show that our approach can significantly reduce gender
biases on both career and multiple disciplines, across models with varying
parameter sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bingkang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dehan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yulei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zongzhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Honglei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Longtao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15193">
<title>IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction. (arXiv:2311.15193v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15193</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the trajectory of pedestrians in crowd scenarios is indispensable
in self-driving or autonomous mobile robot field because estimating the future
locations of pedestrians around is beneficial for policy decision to avoid
collision. It is a challenging issue because humans have different walking
motions, and the interactions between humans and objects in the current
environment, especially between humans themselves, are complex. Previous
researchers focused on how to model human-human interactions but neglected the
relative importance of interactions. To address this issue, a novel mechanism
based on correntropy is introduced. The proposed mechanism not only can measure
the relative importance of human-human interactions but also can build personal
space for each pedestrian. An interaction module including this data-driven
mechanism is further proposed. In the proposed module, the data-driven
mechanism can effectively extract the feature representations of dynamic
human-human interactions in the scene and calculate the corresponding weights
to represent the importance of different interactions. To share such social
messages among pedestrians, an interaction-aware architecture based on long
short-term memory network for trajectory prediction is designed. Experiments
are conducted on two public datasets. Experimental results demonstrate that our
model can achieve better performance than several latest methods with good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuehai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18296">
<title>Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18296</link>
<description rdf:parseType="Literal">&lt;p&gt;Human visual recognition system shows astonishing capability of compressing
visual information into a set of tokens containing rich representations without
label supervision. One critical driving principle behind it is perceptual
grouping. Despite being widely used in computer vision in the early 2010s, it
remains a mystery whether perceptual grouping can be leveraged to derive a
neural visual recognition backbone that generates as powerful representations.
In this paper, we propose the Perceptual Group Tokenizer, a model that entirely
relies on grouping operations to extract visual features and perform
self-supervised representation learning, where a series of grouping operations
are used to iteratively hypothesize the context for pixels or superpixels to
refine feature representations. We show that the proposed model can achieve
competitive performance compared to state-of-the-art vision architectures, and
inherits desirable properties including adaptive computation without
re-training, and interpretability. Specifically, Perceptual Group Tokenizer
achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear
probe evaluation, marking a new progress under this paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03365">
<title>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03365</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling energy consumption in buildings through demand response (DR) has
become increasingly important to reduce global carbon emissions and limit
climate change. In this paper, we specifically focus on controlling the heating
system of a residential building to optimize its energy consumption while
respecting user&apos;s thermal comfort. Recent works in this area have mainly
focused on either model-based control, e.g., model predictive control (MPC), or
model-free reinforcement learning (RL) to implement practical DR algorithms. A
specific RL method that recently has achieved impressive success in domains
such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for
building control it has remained largely unexplored. Thus, we study MCTS
specifically for building demand response. Its natural structure allows a
flexible optimization that implicitly integrate exogenous constraints (as
opposed, for example, to conventional RL solutions), making MCTS a promising
candidate for DR control problems. We demonstrate how to improve MCTS control
performance by incorporating a Physics-informed Neural Network (PiNN) model for
its underlying thermal state prediction, as opposed to traditional purely
data-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN
model is able to obtain a 3% increment of the obtained reward compared to a
rule-based controller; leading to a 10% cost reduction and 35% reduction on
temperature difference with the desired one when applied to an artificial price
profile. We further implemented a Deep Learning layer into the Monte Carlo Tree
Search technique using a neural network that leads the tree search through more
optimal nodes. We then compared this addition with its Vanilla version, showing
the improvement in computational cost required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavirani_F/0/1/0/all/0/1&quot;&gt;Fabio Pavirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gokhale_G/0/1/0/all/0/1&quot;&gt;Gargya Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Claessens_B/0/1/0/all/0/1&quot;&gt;Bert Claessens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05934">
<title>Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05934</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) encapsulate a vast amount of factual information
within their pre-trained weights, as evidenced by their ability to answer
diverse questions across different domains. However, this knowledge is
inherently limited, relying heavily on the characteristics of the training
data. Consequently, using external datasets to incorporate new information or
refine the capabilities of LLMs on previously seen information poses a
significant challenge. In this study, we compare two common approaches:
unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate
both approaches on a variety of knowledge-intensive tasks across different
topics. Our findings reveal that while unsupervised fine-tuning offers some
improvement, RAG consistently outperforms it, both for existing knowledge
encountered during training and entirely new knowledge. Moreover, we find that
LLMs struggle to learn new factual information through unsupervised
fine-tuning, and that exposing them to numerous variations of the same fact
during training could alleviate this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1&quot;&gt;Oded Ovadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brief_M/0/1/0/all/0/1&quot;&gt;Menachem Brief&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishaeli_M/0/1/0/all/0/1&quot;&gt;Moshik Mishaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elisha_O/0/1/0/all/0/1&quot;&gt;Oren Elisha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, e.g., Large Language Models (LLMs), there is a growing
interest in exploring their abilities in reasoning tasks. In this paper, we
introduce seminal foundation models proposed or adaptable for reasoning,
highlighting the latest advancements in various reasoning tasks, methods, and
benchmarks. We then delve into the potential future directions behind the
emergence of reasoning abilities within foundation models. We also discuss the
relevance of multimodal learning, autonomous agents, and super alignment in the
context of reasoning. By discussing these future research directions, we hope
to inspire researchers in their exploration of this field, stimulate further
advancements in reasoning with foundation models, and contribute to the
development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11819">
<title>An Adaptive Placement and Parallelism Framework for Accelerating RLHF Training. (arXiv:2312.11819v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11819</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, ChatGPT or InstructGPT like large language models (LLM) has made a
significant impact in the AI world. Many works have attempted to reproduce the
complex InstructGPT&apos;s training pipeline, namely Reinforcement Learning with
Human Feedback (RLHF). However, the mainstream distributed RLHF training
methods typically adopt a fixed model placement strategy, referred to as the
Flattening strategy. This strategy treats all four interdependent models
involved in RLHF as a single entity, distributing them across all devices and
applying parallelism techniques designed for a single model, regardless of the
different workloads inherent to each model. As a result, this strategy
exacerbates the generation bottlenecks in the RLHF training and degrades the
overall training efficiency. To address these issues, we propose an adaptive
model placement framework that offers two flexible model placement strategies.
The Interleaving strategy helps reduce memory redundancy and communication
costs of RLHF training by placing models without dependencies on exclusive
devices with careful orchestration. On the other hand, the Separation strategy
improves the throughput of model training by separating the training and
inference runtime of the RLHF pipeline with additional shadow models.
Furthermore, our framework provides a simple user interface and allows for the
agile allocation of models across devices in a fine-grained manner for various
training scenarios, involving models of varying sizes and devices of different
scales. Extensive experiments have demonstrated that our Interleaving and
Separation strategies can achieve notable improvements up to 11X, compared to
the current SOTA approaches. The results highlight the effectiveness and
adaptability of our approaches in accelerating the training of distributed
RLHF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Youshao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weichang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhenglei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_F/0/1/0/all/0/1&quot;&gt;Fagui Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shangchun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_L/0/1/0/all/0/1&quot;&gt;Lin Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14472">
<title>Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement Learning with Dynamic Depth Routing. (arXiv:2312.14472v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14472</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task reinforcement learning endeavors to accomplish a set of different
tasks with a single policy. To enhance data efficiency by sharing parameters
across multiple tasks, a common practice segments the network into distinct
modules and trains a routing network to recombine these modules into
task-specific policies. However, existing routing approaches employ a fixed
number of modules for all tasks, neglecting that tasks with varying
difficulties commonly require varying amounts of knowledge. This work presents
a Dynamic Depth Routing (D2R) framework, which learns strategic skipping of
certain intermediate modules, thereby flexibly choosing different numbers of
modules for each task. Under this framework, we further introduce a ResRouting
method to address the issue of disparate routing paths between behavior and
target policies during off-policy training. In addition, we design an automatic
route-balancing mechanism to encourage continued routing exploration for
unmastered tasks without disturbing the routing of mastered ones. We conduct
extensive experiments on various robotics manipulation tasks in the Meta-World
benchmark, where D2R achieves state-of-the-art performance with significantly
improved learning efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jinmin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haobo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jian Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15643">
<title>Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation. (arXiv:2312.15643v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15643</link>
<description rdf:parseType="Literal">&lt;p&gt;Abductive reasoning is the process of making educated guesses to provide
explanations for observations. Although many applications require the use of
knowledge for explanations, the utilization of abductive reasoning in
conjunction with structured knowledge, such as a knowledge graph, remains
largely unexplored. To fill this gap, this paper introduces the task of complex
logical hypothesis generation, as an initial step towards abductive logical
reasoning with KG. In this task, we aim to generate a complex logical
hypothesis so that it can explain a set of observations. We find that the
supervised trained generative model can generate logical hypotheses that are
structurally closer to the reference hypothesis. However, when generalized to
unseen observations, this training objective does not guarantee better
hypothesis generation. To address this, we introduce the Reinforcement Learning
from Knowledge Graph (RLF-KG) method, which minimizes differences between
observations and conclusions drawn from generated hypotheses according to the
KG. Experiments show that, with RLF-KG&apos;s assistance, the generated hypotheses
provide better explanations, and achieve state-of-the-art results on three
widely used KGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jiaxin Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yicheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianshi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01623">
<title>Can AI Be as Creative as Humans?. (arXiv:2401.01623v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Creativity serves as a cornerstone for societal progress and innovation. With
the rise of advanced generative AI models capable of tasks once reserved for
human creativity, the study of AI&apos;s creative potential becomes imperative for
its responsible development and application. In this paper, we prove in theory
that AI can be as creative as humans under the condition that it can properly
fit the data generated by human creators. Therefore, the debate on AI&apos;s
creativity is reduced into the question of its ability to fit a sufficient
amount of data. To arrive at this conclusion, this paper first addresses the
complexities in defining creativity by introducing a new concept called
Relative Creativity. Rather than attempting to define creativity universally,
we shift the focus to whether AI can match the creative abilities of a
hypothetical human. The methodological shift leads to a statistically
quantifiable assessment of AI&apos;s creativity, term Statistical Creativity. This
concept, statistically comparing the creative abilities of AI with those of
specific human groups, facilitates theoretical exploration of AI&apos;s creative
potential. Our analysis reveals that by fitting extensive conditional data
without marginalizing out the generative conditions, AI can emerge as a
hypothetical new creator. The creator possesses the same creative abilities on
par with the human creators it was trained on. Building on theoretical
findings, we discuss the application in prompt-conditioned autoregressive
models, providing a practical means for evaluating creative abilities of
generative AI models, such as Large Language Models (LLMs). Additionally, this
study provides an actionable training guideline, bridging the theoretical
quantification of creativity with practical model training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1&quot;&gt;Michael Mozer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1&quot;&gt;Alex Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Michael Qizhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1&quot;&gt;Hannah Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03233">
<title>Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03233</link>
<description rdf:parseType="Literal">&lt;p&gt;Split Learning (SL) is a promising Distributed Learning approach in
electromyography (EMG) based prosthetic control, due to its applicability
within resource-constrained environments. Other learning approaches, such as
Deep Learning and Federated Learning (FL), provide suboptimal solutions, since
prosthetic devices are extremely limited in terms of processing power and
battery life. The viability of implementing SL in such scenarios is caused by
its inherent model partitioning, with clients executing the smaller model
segment. However, selecting an inadequate cut layer hinders the training
process in SL systems. This paper presents an algorithm for optimal cut layer
selection in terms of maximizing the convergence rate of the model. The
performance evaluation demonstrates that the proposed algorithm substantially
accelerates the convergence in an EMG pattern recognition task for improving
prosthetic device control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinova_M/0/1/0/all/0/1&quot;&gt;Matea Marinova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denkovski_D/0/1/0/all/0/1&quot;&gt;Daniel Denkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjoreski_H/0/1/0/all/0/1&quot;&gt;Hristijan Gjoreski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadzi_Velkov_Z/0/1/0/all/0/1&quot;&gt;Zoran Hadzi-Velkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakovic_V/0/1/0/all/0/1&quot;&gt;Valentin Rakovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05925">
<title>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion. (arXiv:2401.05925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05925</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based segmentation methods have
relied on time-consuming neural scene optimization. While recent 3D Gaussian
Splatting has notably improved speed, existing Gaussian-based segmentation
methods struggle to produce compact masks, especially in zero-shot
segmentation. This issue probably stems from their straightforward assignment
of learnable parameters to each Gaussian, resulting in a lack of robustness
against cross-view inconsistent 2D machine-generated labels. Our method aims to
address this problem by employing Dual Feature Fusion Network as Gaussians&apos;
segmentation field. Specifically, we first optimize 3D Gaussians under RGB
supervision. After Gaussian Locating, DINO features extracted from images are
applied through explicit unprojection, which are further incorporated with
spatial features from the efficient point cloud processing network. Feature
aggregation is utilized to fuse them in a global-to-local strategy for compact
segmentation features. Experimental results show that our model outperforms
baselines on both semantic and panoptic zero-shot segmentation task, meanwhile
consumes less than 10\% inference time compared to NeRF-based methods. Code and
more results will be available at https://David-Dou.github.io/CoSSegGaussians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_B/0/1/0/all/0/1&quot;&gt;Bin Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yongjia Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zejian Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08534">
<title>DiConStruct: Causal Concept-based Explanations through Black-Box Distillation. (arXiv:2401.08534v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08534</link>
<description rdf:parseType="Literal">&lt;p&gt;Model interpretability plays a central role in human-AI decision-making
systems. Ideally, explanations should be expressed using human-interpretable
semantic concepts. Moreover, the causal relations between these concepts should
be captured by the explainer to allow for reasoning about the explanations.
Lastly, explanation methods should be efficient and not compromise the
performance of the predictive task. Despite the rapid advances in AI
explainability in recent years, as far as we know to date, no method fulfills
these three properties. Indeed, mainstream methods for local concept
explainability do not produce causal explanations and incur a trade-off between
explainability and prediction performance. We present DiConStruct, an
explanation method that is both concept-based and causal, with the goal of
creating more interpretable local explanations in the form of structural causal
models and concept attributions. Our explainer works as a distillation model to
any black-box machine learning model by approximating its predictions while
producing the respective explanations. Because of this, DiConStruct generates
explanations efficiently while not impacting the black-box prediction task. We
validate our method on an image dataset and a tabular dataset, showing that
DiConStruct approximates the black-box models with higher fidelity than other
concept explainability baselines, while providing explanations that include the
causal relations between the concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_R/0/1/0/all/0/1&quot;&gt;Ricardo Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_J/0/1/0/all/0/1&quot;&gt;Jacopo Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1&quot;&gt;Pedro Saleiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08655">
<title>SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1&quot;&gt;Inkyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10286">
<title>Top in Chinese Data Processing: English Code Models. (arXiv:2401.10286v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10286</link>
<description rdf:parseType="Literal">&lt;p&gt;While the alignment between tasks and training corpora is a fundamental
consensus in the application of language models, our series of experiments and
the metrics we designed reveal that code-based Large Language Models (LLMs)
significantly outperform models trained on data that is closely matched to the
tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to
Chinese hallucinations, models exhibiting fewer linguistic features of the
Chinese language achieve better performance. Our experimental results can be
easily replicated in Chinese data processing tasks, such as preparing data for
Retrieval-Augmented Generation (RAG), by simply replacing the base model with a
code-based model. Additionally, our research offers a distinct perspective for
discussion on the philosophical &quot;Chinese Room&quot; thought experiment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Linghan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiaojun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiayuan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yue Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Gang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hongwei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10529">
<title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs&apos; sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs&apos; sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongjin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Feihong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Taixi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1&quot;&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12522">
<title>BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Feng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1&quot;&gt;Hanling Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaotian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1&quot;&gt;Rong Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12689">
<title>Energy-based Automated Model Evaluation. (arXiv:2401.12689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE&apos;s validity, together with its
superiority compared with prior approaches. We also prove MDE&apos;s versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels. Code and data are
available: https://github.com/pengr/Energy_AutoEval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Ru Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Heming Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yawen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zenan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12756">
<title>What the Weight?! A Unified Framework for Zero-Shot Knowledge Composition. (arXiv:2401.12756v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12756</link>
<description rdf:parseType="Literal">&lt;p&gt;The knowledge encapsulated in a model is the core factor determining its
final performance on downstream tasks. Much research in NLP has focused on
efficient methods for storing and adapting different types of knowledge, e.g.,
in dedicated modularized structures, and on how to effectively combine these,
e.g., by learning additional parameters. However, given the many possible
options, a thorough understanding of the mechanisms involved in these
compositions is missing, and hence it remains unclear which strategies to
utilize. To address this research gap, we propose a novel framework for
zero-shot module composition, which encompasses existing and some novel
variations for selecting, weighting, and combining parameter modules under a
single unified notion. Focusing on the scenario of domain knowledge and adapter
layers, our framework provides a systematic unification of concepts, allowing
us to conduct the first comprehensive benchmarking study of various zero-shot
knowledge composition strategies. In particular, we test two module combination
methods and five selection and weighting strategies for their effectiveness and
efficiency in an extensive experimental setup. Our results highlight the
efficacy of ensembling but also hint at the power of simple though
often-ignored weighting methods. Further in-depth analyses allow us to
understand the role of weighting vs. top-k selection, and show that, to a
certain extent, the performance of adapter composition can even be predicted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtermann_C/0/1/0/all/0/1&quot;&gt;Carolin Holtermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohmann_M/0/1/0/all/0/1&quot;&gt;Markus Frohmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekabsaz_N/0/1/0/all/0/1&quot;&gt;Navid Rekabsaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1&quot;&gt;Anne Lauscher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12806">
<title>Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12806</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs), rooted in deep learning, have
emerged as a promising approach for solving partial differential equations
(PDEs). By embedding the physical information described by PDEs into
feedforward neural networks, PINNs are trained as surrogate models to
approximate solutions without the need for label data. Nevertheless, even
though PINNs have shown remarkable performance, they can face difficulties,
especially when dealing with equations featuring rapidly changing solutions.
These difficulties encompass slow convergence, susceptibility to becoming
trapped in local minima, and reduced solution accuracy. To address these
issues, we propose a binary structured physics-informed neural network (BsPINN)
framework, which employs binary structured neural network (BsNN) as the neural
network component. By leveraging a binary structure that reduces inter-neuron
connections compared to fully connected neural networks, BsPINNs excel in
capturing the local features of solutions more effectively and efficiently.
These features are particularly crucial for learning the rapidly changing in
the nature of solutions. In a series of numerical experiments solving Burgers
equation, Euler equation, Helmholtz equation, and high-dimension Poisson
equation, BsPINNs exhibit superior convergence speed and heightened accuracy
compared to PINNs. From these experiments, we discover that BsPINNs resolve the
issues caused by increased hidden layers in PINNs resulting in over-smoothing,
and prevent the decline in accuracy due to non-smoothness of PDEs solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Ying Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13138">
<title>Visibility into AI Agents. (arXiv:2401.13138v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13138</link>
<description rdf:parseType="Literal">&lt;p&gt;Increased delegation of commercial, scientific, governmental, and personal
activities to AI agents -- systems capable of pursuing complex goals with
limited supervision -- may exacerbate existing societal risks and introduce new
risks. Understanding and mitigating these risks involves critically evaluating
existing governance structures, revising and adapting these structures where
needed, and ensuring accountability of key stakeholders. Information about
where, why, how, and by whom certain AI agents are used, which we refer to as
visibility, is critical to these objectives. In this paper, we assess three
categories of measures to increase visibility into AI agents: agent
identifiers, real-time monitoring, and activity logging. For each, we outline
potential implementations that vary in intrusiveness and informativeness. We
analyze how the measures apply across a spectrum of centralized through
decentralized deployment contexts, accounting for various actors in the supply
chain including hardware and software service providers. Finally, we discuss
the implications of our measures for privacy and concentration of power.
Further work into understanding the measures and mitigating their negative
impacts can help to build a foundation for the governance of AI agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1&quot;&gt;Alan Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezell_C/0/1/0/all/0/1&quot;&gt;Carson Ezell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Max Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kevin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammond_L/0/1/0/all/0/1&quot;&gt;Lewis Hammond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1&quot;&gt;Herbie Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bluemke_E/0/1/0/all/0/1&quot;&gt;Emma Bluemke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajkumar_N/0/1/0/all/0/1&quot;&gt;Nitarshan Rajkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolt_N/0/1/0/all/0/1&quot;&gt;Noam Kolt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heim_L/0/1/0/all/0/1&quot;&gt;Lennart Heim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1&quot;&gt;Markus Anderljung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13324">
<title>Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions. (arXiv:2401.13324v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13324</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the &quot;XAI Novice Question Bank&quot;: A catalog of
affected stakeholders&apos; information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants&apos; perceptions of the systems&apos; risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmude_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Schmude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koesten_L/0/1/0/all/0/1&quot;&gt;Laura Koesten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1&quot;&gt;Torsten M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1&quot;&gt;Sebastian Tschiatschek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13652">
<title>Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity Detectors. (arXiv:2401.13652v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13652</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel approach for detecting the discontinuity
interfaces of a discontinuous function. This approach leverages Graph-Informed
Neural Networks (GINNs) and sparse grids to address discontinuity detection
also in domains of dimension larger than 3. GINNs, trained to identify troubled
points on sparse grids, exploit graph structures built on the grids to achieve
efficient and accurate discontinuity detection performances. We also introduce
a recursive algorithm for general sparse grid-based detectors, characterized by
convergence properties and easy applicability. Numerical experiments on
functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust
generalization of GINNs in detecting discontinuity interfaces. Notably, the
trained GINNs offer portability and versatility, allowing integration into
various algorithms and sharing among users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santa_F/0/1/0/all/0/1&quot;&gt;Francesco Della Santa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pieraccini_S/0/1/0/all/0/1&quot;&gt;Sandra Pieraccini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13657">
<title>Inadequacy of common stochastic neural networks for reliable clinical decision support. (arXiv:2401.13657v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13657</link>
<description rdf:parseType="Literal">&lt;p&gt;Widespread adoption of AI for medical decision making is still hindered due
to ethical and safety-related concerns. For AI-based decision support systems
in healthcare settings it is paramount to be reliable and trustworthy. Common
deep learning approaches, however, have the tendency towards overconfidence
under data shift. Such inappropriate extrapolation beyond evidence-based
scenarios may have dire consequences. This highlights the importance of
reliable estimation of local uncertainty and its communication to the end user.
While stochastic neural networks have been heralded as a potential solution to
these issues, this study investigates their actual reliability in clinical
applications. We centered our analysis on the exemplary use case of mortality
prediction for ICU hospitalizations using EHR from MIMIC3 study. For
predictions on the EHR time series, Encoder-Only Transformer models were
employed. Stochasticity of model functions was achieved by incorporating common
methods such as Bayesian neural network layers and model ensembles. Our models
achieve state of the art performance in terms of discrimination performance
(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality
prediction benchmark. However, epistemic uncertainty is critically
underestimated by the selected stochastic deep learning methods. A heuristic
proof for the responsible collapse of the posterior distribution is provided.
Our findings reveal the inadequacy of commonly used stochastic deep learning
approaches to reliably recognize OoD samples. In both methods, unsubstantiated
model confidence is not prevented due to strongly biased functional posteriors,
rendering them inappropriate for reliable clinical decision support. This
highlights the need for approaches with more strictly enforced or inherent
distance-awareness to known data points, e.g., using kernel-based techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindenmeyer_A/0/1/0/all/0/1&quot;&gt;Adrian Lindenmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blattmann_M/0/1/0/all/0/1&quot;&gt;Malte Blattmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franke_S/0/1/0/all/0/1&quot;&gt;Stefan Franke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1&quot;&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Daniel Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12255">
<title>Instructional Fingerprinting of Large Language Models. (arXiv:2401.12255v1 [cs.CR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.12255</link>
<description rdf:parseType="Literal">&lt;p&gt;The exorbitant cost of training Large language models (LLMs) from scratch
makes it essential to fingerprint the models to protect intellectual property
via ownership authentication and to ensure downstream users and developers
comply with their license terms (e.g. restricting commercial use). In this
study, we present a pilot study on LLM fingerprinting as a form of very
lightweight instruction tuning. Model publisher specifies a confidential
private key and implants it as an instruction backdoor that causes the LLM to
generate specific text when the key is present. Results on 11 popularly-used
LLMs showed that this approach is lightweight and does not affect the normal
behavior of the model. It also prevents publisher overclaim, maintains
robustness against fingerprint guessing and parameter-efficient training, and
supports multi-stage fingerprinting akin to MIT License. Code is available in
https://cnut1648.github.io/Model-Fingerprint/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiashu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mingyu Derek Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Muhao Chen&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>