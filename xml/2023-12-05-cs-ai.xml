<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/0902.3430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.15920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.04055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.10793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18765" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.00003">
<title>Transport Equation based Physics Informed Neural Network to predict the Yield Strength of Architected Materials. (arXiv:2312.00003v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00003</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research, the application of the Physics-Informed Neural Network
(PINN) model is explored to solve transport equation-based Partial Differential
Equations (PDEs). The primary objective is to analyze the impact of different
activation functions incorporated within the PINN model on its predictive
performance, specifically assessing the Mean Squared Error (MSE) and Mean
Absolute Error (MAE). The dataset used in the study consists of a varied set of
input parameters related to strut diameter, unit cell size, and the
corresponding yield stress values. Through this investigation the aim is to
understand the effectiveness of the PINN model and the significance of choosing
appropriate activation functions for solving complex PDEs in real-world
applications. The outcomes suggest that the choice of activation function may
have minimal influence on the model&apos;s predictive accuracy for this particular
problem. The PINN model showcases exceptional generalization capabilities,
indicating its capacity to avoid overfitting with the provided dataset. The
research underscores the importance of striking a balance between performance
and computational efficiency while selecting an activation function for
specific real-world applications. These valuable findings contribute to
advancing the understanding and potential adoption of PINN as an effective tool
for solving challenging PDEs in diverse scientific and engineering domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Akshansh Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00006">
<title>Enhancing ML-Based DoS Attack Detection Through Combinatorial Fusion Analysis. (arXiv:2312.00006v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00006</link>
<description rdf:parseType="Literal">&lt;p&gt;Mitigating Denial-of-Service (DoS) attacks is vital for online service
security and availability. While machine learning (ML) models are used for DoS
attack detection, new strategies are needed to enhance their performance. We
suggest an innovative method, combinatorial fusion, which combines multiple ML
models using advanced algorithms. This includes score and rank combinations,
weighted techniques, and diversity strength of scoring systems. Through
rigorous evaluations, we demonstrate the effectiveness of this fusion approach,
considering metrics like precision, recall, and F1-score. We address the
challenge of low-profiled attack classification by fusing models to create a
comprehensive solution. Our findings emphasize the potential of this approach
to improve DoS attack detection and contribute to stronger defense mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owusu_E/0/1/0/all/0/1&quot;&gt;Evans Owusu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahouti_M/0/1/0/all/0/1&quot;&gt;Mohamed Rahouti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1&quot;&gt;D. Frank Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1&quot;&gt;Kaiqi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yufeng Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00019">
<title>The theoretical limits of biometry. (arXiv:2312.00019v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00019</link>
<description rdf:parseType="Literal">&lt;p&gt;Biometry has proved its capability in terms of recognition accuracy. Now, it
is widely used for automated border control with the biometric passport, to
unlock a smartphone or a computer with a fingerprint or a face recognition
algorithm. While identity verification is widely democratized, pure
identification with no additional clues is still a work in progress. The
identification difficulty depends on the population size, as the larger the
group is, the larger the confusion risk. For collision prevention, biometric
traits must be sufficiently distinguishable to scale to considerable groups,
and algorithms should be able to capture their differences accurately.
&lt;/p&gt;
&lt;p&gt;Most biometric works are purely experimental, and it is impossible to
extrapolate the results to a smaller or a larger group. In this work, we
propose a theoretical analysis of the distinguishability problem, which governs
the error rates of biometric systems. We demonstrate simple relationships
between the population size and the number of independent bits necessary to
prevent collision in the presence of noise. This work provides the lowest lower
bound for memory requirements. The results are very encouraging, as the
biometry of the whole Earth population can fit in a regular disk, leaving some
space for noise and redundancy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candel_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;lle Candel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00027">
<title>Stealthy and Persistent Unalignment on Large Language Models via Backdoor Injections. (arXiv:2312.00027v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00027</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in Large Language Models (LLMs) have manifested
significant advancements. To facilitate safeguards against malicious
exploitation, a body of research has concentrated on aligning LLMs with human
preferences and inhibiting their generation of inappropriate content.
Unfortunately, such alignments are often vulnerable: fine-tuning with a minimal
amount of harmful data can easily unalign the target LLM. While being
effective, such fine-tuning-based unalignment approaches also have their own
limitations: (1) non-stealthiness, after fine-tuning, safety audits or
red-teaming can easily expose the potential weaknesses of the unaligned models,
thereby precluding their release/use. (2) non-persistence, the unaligned LLMs
can be easily repaired through re-alignment, i.e., fine-tuning again with
aligned data points. In this work, we show that it is possible to conduct
stealthy and persistent unalignment on large language models via backdoor
injections. We also provide a novel understanding on the relationship between
the backdoor persistence and the activation pattern and further provide
guidelines for potential trigger design. Through extensive experiments, we
demonstrate that our proposed stealthy and persistent unalignment can
successfully pass the safety evaluation while maintaining strong persistence
against re-alignment defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuanpu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bochuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinghui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00029">
<title>Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework. (arXiv:2312.00029v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00029</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern Large language models (LLMs) can still generate responses that may not
be aligned with human expectations or values. While many weight-based alignment
methods have been proposed, many of them still leave models vulnerable to
attacks when used on their own. To help mitigate this issue, we introduce
Bergeron, a framework designed to improve the robustness of LLMs against
adversarial attacks. Bergeron employs a two-tiered architecture. Here, a
secondary LLM serves as a simulated conscience that safeguards a primary LLM.
We do this by monitoring for and correcting potentially harmful text within
both the prompt inputs and the generated outputs of the primary LLM. Empirical
evaluation shows that Bergeron can improve the alignment and robustness of
several popular LLMs without costly fine-tuning. It aids both open-source and
black-box LLMs by complementing and reinforcing their existing alignment
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pisano_M/0/1/0/all/0/1&quot;&gt;Matthew Pisano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ly_P/0/1/0/all/0/1&quot;&gt;Peter Ly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanders_A/0/1/0/all/0/1&quot;&gt;Abraham Sanders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1&quot;&gt;Bingsheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dakuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strzalkowski_T/0/1/0/all/0/1&quot;&gt;Tomek Strzalkowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_M/0/1/0/all/0/1&quot;&gt;Mei Si&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00030">
<title>Artificial Intelligence in Sustainable Vertical Farming. (arXiv:2312.00030v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00030</link>
<description rdf:parseType="Literal">&lt;p&gt;As global challenges of population growth, climate change, and resource
scarcity intensify, the agricultural landscape is at a critical juncture.
Sustainable vertical farming emerges as a transformative solution to address
these challenges by maximizing crop yields in controlled environments. This
paradigm shift necessitates the integration of cutting-edge technologies, with
Artificial Intelligence (AI) at the forefront. The paper provides a
comprehensive exploration of the role of AI in sustainable vertical farming,
investigating its potential, challenges, and opportunities. The review
synthesizes the current state of AI applications, encompassing machine
learning, computer vision, the Internet of Things (IoT), and robotics, in
optimizing resource usage, automating tasks, and enhancing decision-making. It
identifies gaps in research, emphasizing the need for optimized AI models,
interdisciplinary collaboration, and the development of explainable AI in
agriculture. The implications extend beyond efficiency gains, considering
economic viability, reduced environmental impact, and increased food security.
The paper concludes by offering insights for stakeholders and suggesting
avenues for future research, aiming to guide the integration of AI technologies
in sustainable vertical farming for a resilient and sustainable future in
agriculture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_H/0/1/0/all/0/1&quot;&gt;Hribhu Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Argha_D/0/1/0/all/0/1&quot;&gt;Debo Brata Paul Argha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Md Ashik Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00034">
<title>Enhancing IoT Security via Automatic Network Traffic Analysis: The Transition from Machine Learning to Deep Learning. (arXiv:2312.00034v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00034</link>
<description rdf:parseType="Literal">&lt;p&gt;This work provides a comparative analysis illustrating how Deep Learning (DL)
surpasses Machine Learning (ML) in addressing tasks within Internet of Things
(IoT), such as attack classification and device-type identification. Our
approach involves training and evaluating a DL model using a range of diverse
IoT-related datasets, allowing us to gain valuable insights into how adaptable
and practical these models can be when confronted with various IoT
configurations. We initially convert the unstructured network traffic data from
IoT networks, stored in PCAP files, into images by processing the packet data.
This conversion process adapts the data to meet the criteria of DL
classification methods. The experiments showcase the ability of DL to surpass
the constraints tied to manually engineered features, achieving superior
results in attack detection and maintaining comparable outcomes in device-type
identification. Additionally, a notable feature extraction time difference
becomes evident in the experiments: traditional methods require around 29
milliseconds per data packet, while DL accomplishes the same task in just 2.9
milliseconds. The significant time gap, DL&apos;s superior performance, and the
recognized limitations of manually engineered features, presents a compelling
call to action within the IoT community. This encourages us to shift from
exploring new IoT features for each dataset to addressing the challenges of
integrating DL into IoT, making it a more efficient solution for real-world IoT
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidouche_M/0/1/0/all/0/1&quot;&gt;Mounia Hamidouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popko_E/0/1/0/all/0/1&quot;&gt;Eugeny Popko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouni_B/0/1/0/all/0/1&quot;&gt;Bassem Ouni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00043">
<title>Who is leading in AI? An analysis of industry AI research. (arXiv:2312.00043v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00043</link>
<description rdf:parseType="Literal">&lt;p&gt;AI research is increasingly industry-driven, making it crucial to understand
company contributions to this field. We compare leading AI companies by
research publications, citations, size of training runs, and contributions to
algorithmic innovations. Our analysis reveals the substantial role played by
Google, OpenAI and Meta. We find that these three companies have been
responsible for some of the largest training runs, developed a large fraction
of the algorithmic innovations that underpin large language models, and led in
various metrics of citation impact. In contrast, leading Chinese companies such
as Tencent and Baidu had a lower impact on many of these metrics compared to US
counterparts. We observe many industry labs are pursuing large training runs,
and that training runs from relative newcomers -- such as OpenAI and Anthropic
-- have matched or surpassed those of long-standing incumbents such as Google.
The data reveals a diverse ecosystem of companies steering AI progress, though
US labs such as Google, OpenAI and Meta lead across critical metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cottier_B/0/1/0/all/0/1&quot;&gt;Ben Cottier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Besiroglu_T/0/1/0/all/0/1&quot;&gt;Tamay Besiroglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owen_D/0/1/0/all/0/1&quot;&gt;David Owen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00044">
<title>Advancing AI Audits for Enhanced AI Governance. (arXiv:2312.00044v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00044</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) is integrated into various services and
systems in society, many companies and organizations have proposed AI
principles, policies, and made the related commitments. Conversely, some have
proposed the need for independent audits, arguing that the voluntary principles
adopted by the developers and providers of AI services and systems
insufficiently address risk. This policy recommendation summarizes the issues
related to the auditing of AI services and systems and presents three
recommendations for promoting AI auditing that contribute to sound AI
governance. Recommendation1.Development of institutional design for AI audits.
Recommendation2.Training human resources for AI audits. Recommendation3.
Updating AI audits in accordance with technological progress.
&lt;/p&gt;
&lt;p&gt;In this policy recommendation, AI is assumed to be that which recognizes and
predicts data with the last chapter outlining how generative AI should be
audited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ema_A/0/1/0/all/0/1&quot;&gt;Arisa Ema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_R/0/1/0/all/0/1&quot;&gt;Ryo Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_T/0/1/0/all/0/1&quot;&gt;Tomoharu Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakano_M/0/1/0/all/0/1&quot;&gt;Masafumi Nakano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamimura_S/0/1/0/all/0/1&quot;&gt;Shinji Kamimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitamura_H/0/1/0/all/0/1&quot;&gt;Hiromu Kitamura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00045">
<title>AI-driven E-Liability Knowledge Graphs: A Comprehensive Framework for Supply Chain Carbon Accounting and Emissions Liability Management. (arXiv:2312.00045v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00045</link>
<description rdf:parseType="Literal">&lt;p&gt;While carbon accounting plays a fundamental role in our fight against climate
change, it is not without its challenges. We begin the paper with a critique of
the conventional carbon accounting practices, after which we proceed to
introduce the E-liability carbon accounting methodology and Emissions Liability
Management (ELM) originally proposed by Kaplan and Ramanna, highlighting their
strengths. Recognizing the immense value of this novel approach for real-world
carbon accounting improvement, we introduce a novel data-driven integrative
framework that leverages AI and computation - the E-Liability Knowledge Graph
framework - to achieve real-world implementation of the E-liability carbon
accounting methodology. In addition to providing a path-to-implementation, our
proposed framework brings clarity to the complex environmental interactions
within supply chains, thus enabling better informed and more responsible
decision-making. We analyze the implementation aspects of this framework and
conclude with a discourse on the role of this AI-aided knowledge graph in
ensuring the transparency and decarbonization of global supply chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oladeji_O/0/1/0/all/0/1&quot;&gt;Olamide Oladeji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Shahabeddin Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roston_M/0/1/0/all/0/1&quot;&gt;Marc Roston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00046">
<title>Retail Analytics in the New Normal: The Influence of Artificial Intelligence and the Covid-19 Pandemic. (arXiv:2312.00046v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00046</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic has severely disrupted the retail landscape and has
accelerated the adoption of innovative technologies. A striking example relates
to the proliferation of online grocery orders and the technology deployed to
facilitate such logistics. In fact, for many retailers, this disruption was a
wake-up call after which they started recognizing the power of data analytics
and artificial intelligence (AI). In this article, we discuss the opportunities
that AI can offer to retailers in the new normal retail landscape. Some of the
techniques described have been applied at scale to adapt previously deployed AI
models, whereas in other instances, fresh solutions needed to be developed to
help retailers cope with recent disruptions, such as unexpected panic buying,
retraining predictive models, and leveraging online-offline synergies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adulyasak_Y/0/1/0/all/0/1&quot;&gt;Yossiri Adulyasak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_M/0/1/0/all/0/1&quot;&gt;Maxime C. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khern_am_nuai_W/0/1/0/all/0/1&quot;&gt;Warut Khern-am-nuai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_M/0/1/0/all/0/1&quot;&gt;Michael Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00047">
<title>chatGPT for generating questions and assessments based on accreditations. (arXiv:2312.00047v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00047</link>
<description rdf:parseType="Literal">&lt;p&gt;This research aims to take advantage of artificial intelligence techniques in
producing students assessment that is compatible with the different academic
accreditations of the same program. The possibility of using generative
artificial intelligence technology was studied to produce an academic
accreditation compliant test the National Center for Academic Accreditation of
Kingdom of Saudi Arabia and Accreditation Board for Engineering and Technology.
A novel method was introduced to map the verbs used to create the questions
introduced in the tests. The method allows a possibility of using the
generative artificial intelligence technology to produce and check the validity
of questions that measure educational outcomes. A questionnaire was distributed
to ensure that the use of generative artificial intelligence to create exam
questions is acceptable by the faculty members, as well as to ask about the
acceptance of assistance in validating questions submitted by faculty members
and amending them in accordance with academic accreditations. The questionnaire
was distributed to faculty members of different majors in the Kingdom of Saudi
Arabias universities. one hundred twenty responses obtained with eight five
percentile approval percentage for generate complete exam questions by
generative artificial intelligence . Whereas ninety eight percentage was the
approval percentage for editing and improving already existed questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboalela_R/0/1/0/all/0/1&quot;&gt;Rania Anwar Aboalela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00050">
<title>Elijah: Eliminating Backdoors Injected in Diffusion Models via Distribution Shift. (arXiv:2312.00050v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00050</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models (DM) have become state-of-the-art generative models because
of their capability to generate high-quality images from noises without
adversarial training. However, they are vulnerable to backdoor attacks as
reported by recent studies. When a data input (e.g., some Gaussian noise) is
stamped with a trigger (e.g., a white patch), the backdoored model always
generates the target image (e.g., an improper photo). However, effective
defense strategies to mitigate backdoors from DMs are underexplored. To bridge
this gap, we propose the first backdoor detection and removal framework for
DMs. We evaluate our framework Elijah on hundreds of DMs of 3 types including
DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks.
Extensive experiments show that our approach can have close to 100% detection
accuracy and reduce the backdoor effects to close to zero without significantly
sacrificing the model utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Shengwei An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1&quot;&gt;Sheng-Yen Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiuling Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1&quot;&gt;Guanhong Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guangyu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shiqing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00051">
<title>MIA-BAD: An Approach for Enhancing Membership Inference Attack and its Mitigation with Federated Learning. (arXiv:2312.00051v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00051</link>
<description rdf:parseType="Literal">&lt;p&gt;The membership inference attack (MIA) is a popular paradigm for compromising
the privacy of a machine learning (ML) model. MIA exploits the natural
inclination of ML models to overfit upon the training data. MIAs are trained to
distinguish between training and testing prediction confidence to infer
membership information. Federated Learning (FL) is a privacy-preserving ML
paradigm that enables multiple clients to train a unified model without
disclosing their private data. In this paper, we propose an enhanced Membership
Inference Attack with the Batch-wise generated Attack Dataset (MIA-BAD), a
modification to the MIA approach. We investigate that the MIA is more accurate
when the attack dataset is generated batch-wise. This quantitatively decreases
the attack dataset while qualitatively improving it. We show how training an ML
model through FL, has some distinct advantages and investigate how the threat
introduced with the proposed MIA-BAD approach can be mitigated with FL
approaches. Finally, we demonstrate the qualitative effects of the proposed
MIA-BAD methodology by conducting extensive experiments with various target
datasets, variable numbers of federated clients, and training batch sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Soumya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Sandip Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahamed_S/0/1/0/all/0/1&quot;&gt;Sayyed Farid Ahamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_D/0/1/0/all/0/1&quot;&gt;Devin Quinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vucovich_M/0/1/0/all/0/1&quot;&gt;Marc Vucovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandakumar_D/0/1/0/all/0/1&quot;&gt;Dhruv Nandakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_K/0/1/0/all/0/1&quot;&gt;Kevin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Abdul Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowen_E/0/1/0/all/0/1&quot;&gt;Edward Bowen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Sachin Shetty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00052">
<title>A Case for Competent AI Systems $-$ A Concept Note. (arXiv:2312.00052v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00052</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficiency of an AI system is contingent upon its ability to align with
the specified requirements of a given task. How-ever, the inherent complexity
of tasks often introduces the potential for harmful implications or adverse
actions. This note explores the critical concept of capability within AI
systems, representing what the system is expected to deliver. The articulation
of capability involves specifying well-defined out-comes. Yet, the achievement
of this capability may be hindered by deficiencies in implementation and
testing, reflecting a gap in the system&apos;s competency (what it can do vs. what
it does successfully).
&lt;/p&gt;
&lt;p&gt;A central challenge arises in elucidating the competency of an AI system to
execute tasks effectively. The exploration of system competency in AI remains
in its early stages, occasionally manifesting as confidence intervals denoting
the probability of success. Trust in an AI system hinges on the explicit
modeling and detailed specification of its competency, connected intricately to
the system&apos;s capability. This note explores this gap by proposing a framework
for articulating the competency of AI systems.
&lt;/p&gt;
&lt;p&gt;Motivated by practical scenarios such as the Glass Door problem, where an
individual inadvertently encounters a glass obstacle due to a failure in their
competency, this research underscores the imperative of delving into competency
dynamics. Bridging the gap between capability and competency at a detailed
level, this note contributes to advancing the discourse on bolstering the
reliability of AI systems in real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlapalem_K/0/1/0/all/0/1&quot;&gt;Kamalakar Karlapalem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00054">
<title>Is Inverse Reinforcement Learning Harder than Standard Reinforcement Learning?. (arXiv:2312.00054v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.00054</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse Reinforcement Learning (IRL) -- the problem of learning reward
functions from demonstrations of an \emph{expert policy} -- plays a critical
role in developing intelligent systems, such as those that understand and
imitate human behavior. While widely used in applications, theoretical
understandings of IRL admit unique challenges and remain less developed
compared with standard RL theory. For example, it remains open how to do IRL
efficiently in standard \emph{offline} settings with pre-collected data, where
states are obtained from a \emph{behavior policy} (which could be the expert
policy itself), and actions are sampled from the expert policy.
&lt;/p&gt;
&lt;p&gt;This paper provides the first line of results for efficient IRL in vanilla
offline and online settings using polynomial samples and runtime. We first
design a new IRL algorithm for the offline setting, Reward Learning with
Pessimism (RLP), and show that it achieves polynomial sample complexity in
terms of the size of the MDP, a concentrability coefficient between the
behavior policy and the expert policy, and the desired accuracy. Building on
RLP, we further design an algorithm Reward Learning with Exploration (RLE),
which operates in a natural online setting where the learner can both actively
explore the environment and query the expert policy, and obtain a stronger
notion of IRL guarantee from polynomial samples. We establish sample complexity
lower bounds for both settings showing that RLP and RLE are nearly optimal.
Finally, as an application, we show that the learned reward functions can
\emph{transfer} to another target MDP with suitable guarantees when the target
MDP satisfies certain similarity assumptions with the original (source) MDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yu Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00057">
<title>Probabilistic Copyright Protection Can Fail for Text-to-Image Generative Models. (arXiv:2312.00057v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00057</link>
<description rdf:parseType="Literal">&lt;p&gt;The booming use of text-to-image generative models has raised concerns about
their high risk of producing copyright-infringing content. While probabilistic
copyright protection methods provide a probabilistic guarantee against such
infringement, in this paper, we introduce Virtually Assured Amplification
Attack (VA3), a novel online attack framework that exposes the vulnerabilities
of these protection mechanisms. The proposed framework significantly amplifies
the probability of generating infringing content on the sustained interactions
with generative models and a lower-bounded success probability of each
engagement. Our theoretical and experimental results demonstrate the
effectiveness of our approach and highlight the potential risk of implementing
probabilistic copyright protection in practical applications of text-to-image
generative models. Code is available at https://github.com/South7X/VA3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1&quot;&gt;Qianli Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00079">
<title>HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models. (arXiv:2312.00079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00079</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores advancements in high-fidelity personalized image
generation through the utilization of pre-trained text-to-image diffusion
models. While previous approaches have made significant strides in generating
versatile scenes based on text descriptions and a few input images, challenges
persist in maintaining the subject fidelity within the generated images. In
this work, we introduce an innovative algorithm named HiFi Tuner to enhance the
appearance preservation of objects during personalized image generation. Our
proposed method employs a parameter-efficient fine-tuning framework, comprising
a denoising process and a pivotal inversion process. Key enhancements include
the utilization of mask guidance, a novel parameter regularization technique,
and the incorporation of step-wise subject representations to elevate the
sample fidelity. Additionally, we propose a reference-guided generation
approach that leverages the pivotal inversion of a reference image to mitigate
unwanted subject variations and artifacts. We further extend our method to a
novel image editing task: substituting the subject in an image through textual
manipulations. Experimental evaluations conducted on the DreamBooth dataset
using the Stable Diffusion model showcase promising results. Fine-tuning solely
on textual embeddings improves CLIP-T score by 3.6 points and improves DINO
score by 9.6 points over Textual Inversion. When fine-tuning all parameters,
HiFi Tuner improves CLIP-T score by 1.2 points and improves DINO score by 1.2
points over DreamBooth, establishing a new state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhonghao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1&quot;&gt;Mark Hasegawa-Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Humphrey Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00087">
<title>Generative Artificial Intelligence in Learning Analytics: Contextualising Opportunities and Challenges through the Learning Analytics Cycle. (arXiv:2312.00087v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00087</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative artificial intelligence (GenAI), exemplified by ChatGPT,
Midjourney, and other state-of-the-art large language models and diffusion
models, holds significant potential for transforming education and enhancing
human productivity. While the prevalence of GenAI in education has motivated
numerous research initiatives, integrating these technologies within the
learning analytics (LA) cycle and their implications for practical
interventions remain underexplored. This paper delves into the prospective
opportunities and challenges GenAI poses for advancing LA. We present a concise
overview of the current GenAI landscape and contextualise its potential roles
within Clow&apos;s generic framework of the LA cycle. We posit that GenAI can play
pivotal roles in analysing unstructured data, generating synthetic learner
data, enriching multimodal learner interactions, advancing interactive and
explanatory analytics, and facilitating personalisation and adaptive
interventions. As the lines blur between learners and GenAI tools, a renewed
understanding of learners is needed. Future research can delve deep into
frameworks and methodologies that advocate for human-AI collaboration. The LA
community can play a pivotal role in capturing data about human and AI
contributions and exploring how they can collaborate most effectively. As LA
advances, it is essential to consider the pedagogical implications and broader
socioeconomic impact of GenAI for ensuring an inclusive future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Lixiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Maldonado_R/0/1/0/all/0/1&quot;&gt;Roberto Martinez-Maldonado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00094">
<title>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps. (arXiv:2312.00094v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00094</link>
<description rdf:parseType="Literal">&lt;p&gt;Sampling from diffusion models can be treated as solving the corresponding
ordinary differential equations (ODEs), with the aim of obtaining an accurate
solution with as few number of function evaluations (NFE) as possible.
Recently, various fast samplers utilizing higher-order ODE solvers have emerged
and achieved better performance than the initial first-order one. However,
these numerical methods inherently result in certain approximation errors,
which significantly degrades sample quality with extremely small NFE (e.g.,
around 5). In contrast, based on the geometric observation that each sampling
trajectory almost lies in a two-dimensional subspace embedded in the ambient
space, we propose Approximate MEan-Direction Solver (AMED-Solver) that
eliminates truncation errors by directly learning the mean direction for fast
diffusion sampling. Besides, our method can be easily used as a plugin to
further improve existing ODE-based samplers. Extensive experiments on image
synthesis with the resolution ranging from 32 to 256 demonstrate the
effectiveness of our method. With only 5 NFE, we achieve 7.14 FID on CIFAR-10,
13.75 FID on ImageNet 64$\times$64, and 12.79 FID on LSUN Bedroom. Our code is
available at https://github.com/zhyzhouu/amed-solver.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Defang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Can Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00101">
<title>Towards Unsupervised Representation Learning: Learning, Evaluating and Transferring Visual Representations. (arXiv:2312.00101v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised representation learning aims at finding methods that learn
representations from data without annotation-based signals. Abstaining from
annotations not only leads to economic benefits but may - and to some extent
already does - result in advantages regarding the representation&apos;s structure,
robustness, and generalizability to different tasks. In the long run,
unsupervised methods are expected to surpass their supervised counterparts due
to the reduction of human intervention and the inherently more general setup
that does not bias the optimization towards an objective originating from
specific annotation-based signals. While major advantages of unsupervised
representation learning have been recently observed in natural language
processing, supervised methods still dominate in vision domains for most tasks.
In this dissertation, we contribute to the field of unsupervised (visual)
representation learning from three perspectives: (i) Learning representations:
We design unsupervised, backpropagation-free Convolutional Self-Organizing
Neural Networks (CSNNs) that utilize self-organization- and Hebbian-based
learning rules to learn convolutional kernels and masks to achieve deeper
backpropagation-free models. (ii) Evaluating representations: We build upon the
widely used (non-)linear evaluation protocol to define pretext- and
target-objective-independent metrics for measuring and investigating the
objective function mismatch between various unsupervised pretext tasks and
target tasks. (iii) Transferring representations: We contribute CARLANE, the
first 3-way sim-to-real domain adaptation benchmark for 2D lane detection, and
a method based on prototypical self-supervised learning. Finally, we contribute
a content-consistent unpaired image-to-image translation method that utilizes
masks, global and local discriminators, and similarity sampling to mitigate
content inconsistencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuhr_B/0/1/0/all/0/1&quot;&gt;Bonifaz Stuhr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00102">
<title>FedEmb: A Vertical and Hybrid Federated Learning Algorithm using Network And Feature Embedding Aggregation. (arXiv:2312.00102v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging paradigm for decentralized training of
machine learning models on distributed clients, without revealing the data to
the central server. The learning scheme may be horizontal, vertical or hybrid
(both vertical and horizontal). Most existing research work with deep neural
network (DNN) modelling is focused on horizontal data distributions, while
vertical and hybrid schemes are much less studied. In this paper, we propose a
generalized algorithm FedEmb, for modelling vertical and hybrid DNN-based
learning. The idea of our algorithm is characterised by higher inference
accuracy, stronger privacy-preserving properties, and lower client-server
communication bandwidth demands as compared with existing work. The
experimental results show that FedEmb is an effective method to tackle both
split feature &amp;amp; subject space decentralized problems, shows 0.3% to 4.2%
inference accuracy improvement with limited privacy revealing for datasets
stored in local clients, and reduces 88.9 % time complexity over vertical
baseline method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lele Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00140">
<title>The Stochastic Dynamic Post-Disaster Inventory Allocation Problem with Trucks and UAVs. (arXiv:2312.00140v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.00140</link>
<description rdf:parseType="Literal">&lt;p&gt;Humanitarian logistics operations face increasing difficulties due to rising
demands for aid in disaster areas. This paper investigates the dynamic
allocation of scarce relief supplies across multiple affected districts over
time. It introduces a novel stochastic dynamic post-disaster inventory
allocation problem with trucks and unmanned aerial vehicles delivering relief
goods under uncertain supply and demand. The relevance of this humanitarian
logistics problem lies in the importance of considering the inter-temporal
social impact of deliveries. We achieve this by incorporating deprivation costs
when allocating scarce supplies. Furthermore, we consider the inherent
uncertainties of disaster areas and the potential use of cargo UAVs to enhance
operational efficiency. This study proposes two anticipatory solution methods
based on approximate dynamic programming, specifically decomposed linear value
function approximation and neural network value function approximation to
effectively manage uncertainties in the dynamic allocation process. We compare
DL-VFA and NN-VFA with various state-of-the-art methods (exact re-optimization,
PPO) and results show a 6-8% improvement compared to the best benchmarks.
NN-VFA provides the best performance and captures nonlinearities in the
problem, whereas DL-VFA shows excellent scalability against a minor performance
loss. The experiments reveal that consideration of deprivation costs results in
improved allocation of scarce supplies both across affected districts and over
time. Finally, results show that deploying UAVs can play a crucial role in the
allocation of relief goods, especially in the first stages after a disaster.
The use of UAVs reduces transportation- and deprivation costs together by
16-20% and reduces maximum deprivation times by 19-40%, while maintaining
similar levels of demand coverage, showcasing efficient and effective
operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Steenbergen_R/0/1/0/all/0/1&quot;&gt;Robert van Steenbergen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heeswijk_W/0/1/0/all/0/1&quot;&gt;Wouter van Heeswijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mes_M/0/1/0/all/0/1&quot;&gt;Martijn Mes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00151">
<title>Which way is `right&apos;?: Uncovering limitations of Vision-and-Language Navigation model. (arXiv:2312.00151v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00151</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenging task of Vision-and-Language Navigation (VLN) requires
embodied agents to follow natural language instructions to reach a goal
location or object (e.g. `walk down the hallway and turn left at the piano&apos;).
For agents to complete this task successfully, they must be able to ground
objects referenced into the instruction (e.g.`piano&apos;) into the visual scene as
well as ground directional phrases (e.g.`turn left&apos;) into actions. In this work
we ask the following question -- to what degree are spatial and directional
language cues informing the navigation model&apos;s decisions? We propose a series
of simple masking experiments to inspect the model&apos;s reliance on different
parts of the instruction. Surprisingly we uncover that certain top performing
models rely only on the noun tokens of the instructions. We propose two
training methods to alleviate this concerning limitation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1&quot;&gt;Meera Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Amit Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00164">
<title>Towards Accurate Differential Diagnosis with Large Language Models. (arXiv:2312.00164v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00164</link>
<description rdf:parseType="Literal">&lt;p&gt;An accurate differential diagnosis (DDx) is a cornerstone of medical care,
often reached through an iterative process of interpretation that combines
clinical history, physical examination, investigations and procedures.
Interactive interfaces powered by Large Language Models (LLMs) present new
opportunities to both assist and automate aspects of this process. In this
study, we introduce an LLM optimized for diagnostic reasoning, and evaluate its
ability to generate a DDx alone or as an aid to clinicians. 20 clinicians
evaluated 302 challenging, real-world medical cases sourced from the New
England Journal of Medicine (NEJM) case reports. Each case report was read by
two clinicians, who were randomized to one of two assistive conditions: either
assistance from search engines and standard medical resources, or LLM
assistance in addition to these tools. All clinicians provided a baseline,
unassisted DDx prior to using the respective assistive tools. Our LLM for DDx
exhibited standalone performance that exceeded that of unassisted clinicians
(top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study
arms, the DDx quality score was higher for clinicians assisted by our LLM
(top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%)
(McNemar&apos;s Test: 45.7, p &amp;lt; 0.01) and clinicians with search (44.4%) (4.75, p =
0.03). Further, clinicians assisted by our LLM arrived at more comprehensive
differential lists than those without its assistance. Our study suggests that
our LLM for DDx has potential to improve clinicians&apos; diagnostic reasoning and
accuracy in challenging cases, meriting further real-world evaluation for its
ability to empower physicians and widen patients&apos; access to specialist-level
expertise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1&quot;&gt;Mike Schaekermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palepu_A/0/1/0/all/0/1&quot;&gt;Anil Palepu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Amy Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrison_J/0/1/0/all/0/1&quot;&gt;Jake Garrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1&quot;&gt;Yash Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1&quot;&gt;Shekoofeh Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1&quot;&gt;Kavita Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Le Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1&quot;&gt;S Sara Mahdavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sushant Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_A/0/1/0/all/0/1&quot;&gt;Anupam Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1&quot;&gt;Christopher Semturs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1&quot;&gt;Dale R Webster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominowska_E/0/1/0/all/0/1&quot;&gt;Ewa Dominowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottweis_J/0/1/0/all/0/1&quot;&gt;Juraj Gottweis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1&quot;&gt;Joelle Barral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1&quot;&gt;Katherine Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Greg S Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yossi Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunshine_J/0/1/0/all/0/1&quot;&gt;Jake Sunshine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1&quot;&gt;Vivek Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00174">
<title>Compression of end-to-end non-autoregressive image-to-speech system for low-resourced devices. (arXiv:2312.00174v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.00174</link>
<description rdf:parseType="Literal">&lt;p&gt;People with visual impairments have difficulty accessing touchscreen-enabled
personal computing devices like mobile phones and laptops. The image-to-speech
(ITS) systems can assist them in mitigating this problem, but their huge model
size makes it extremely hard to be deployed on low-resourced embedded devices.
In this paper, we aim to overcome this challenge by developing an efficient
endto-end neural architecture for generating audio from tiny segments of
display content on low-resource devices. We introduced a vision
transformers-based image encoder and utilized knowledge distillation to
compress the model from 6.1 million to 2.46 million parameters. Human and
automatic evaluation results show that our approach leads to a very minimal
drop in performance and can speed up the inference time by 22%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Srinivasagan_G/0/1/0/all/0/1&quot;&gt;Gokul Srinivasagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Deisher_M/0/1/0/all/0/1&quot;&gt;Michael Deisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Georges_M/0/1/0/all/0/1&quot;&gt;Munir Georges&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00183">
<title>RNA-KG: An ontology-based knowledge graph for representing interactions involving RNA molecules. (arXiv:2312.00183v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2312.00183</link>
<description rdf:parseType="Literal">&lt;p&gt;The &quot;RNA world&quot; represents a novel frontier for the study of fundamental
biological processes and human diseases and is paving the way for the
development of new drugs tailored to the patient&apos;s biomolecular
characteristics. Although scientific data about coding and non-coding RNA
molecules are continuously produced and available from public repositories,
they are scattered across different databases and a centralized, uniform, and
semantically consistent representation of the &quot;RNA world&quot; is still lacking. We
propose RNA-KG, a knowledge graph encompassing biological knowledge about RNAs
gathered from more than 50 public databases, integrating functional
relationships with genes, proteins, and chemicals and ontologically grounded
biomedical concepts. To develop RNA-KG, we first identified, pre-processed, and
characterized each data source; next, we built a meta-graph that provides an
ontological description of the KG by representing all the bio-molecular
entities and medical concepts of interest in this domain, as well as the types
of interactions connecting them. Finally, we leveraged an instance-based
semantically abstracted knowledge model to specify the ontological alignment
according to which RNA-KG was generated. RNA-KG can be downloaded in different
formats and also queried by a SPARQL endpoint. A thorough topological analysis
of the resulting heterogeneous graph provides further insights into the
characteristics of the &quot;RNA world&quot;. RNA-KG can be both directly explored and
visualized, and/or analyzed by applying computational methods to infer
bio-medical knowledge from its heterogeneous nodes and edges. The resource can
be easily updated with new experimental data, and specific views of the overall
KG can be extracted according to the bio-medical problem to be studied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalleri_E/0/1/0/all/0/1&quot;&gt;Emanuele Cavalleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabri_A/0/1/0/all/0/1&quot;&gt;Alberto Cabri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soto_Gomez_M/0/1/0/all/0/1&quot;&gt;Mauricio Soto-Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonfitto_S/0/1/0/all/0/1&quot;&gt;Sara Bonfitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perlasca_P/0/1/0/all/0/1&quot;&gt;Paolo Perlasca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gliozzo_J/0/1/0/all/0/1&quot;&gt;Jessica Gliozzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Callahan_T/0/1/0/all/0/1&quot;&gt;Tiffany J. Callahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reese_J/0/1/0/all/0/1&quot;&gt;Justin Reese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1&quot;&gt;Peter N Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casiraghi_E/0/1/0/all/0/1&quot;&gt;Elena Casiraghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valentini_G/0/1/0/all/0/1&quot;&gt;Giorgio Valentini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesiti_M/0/1/0/all/0/1&quot;&gt;Marco Mesiti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00186">
<title>Planning Reliability Assurance Tests for Autonomous Vehicles. (arXiv:2312.00186v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2312.00186</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) technology has become increasingly prevalent and
transforms our everyday life. One important application of AI technology is the
development of autonomous vehicles (AV). However, the reliability of an AV
needs to be carefully demonstrated via an assurance test so that the product
can be used with confidence in the field. To plan for an assurance test, one
needs to determine how many AVs need to be tested for how many miles and the
standard for passing the test. Existing research has made great efforts in
developing reliability demonstration tests in the other fields of applications
for product development and assessment. However, statistical methods have not
been utilized in AV test planning. This paper aims to fill in this gap by
developing statistical methods for planning AV reliability assurance tests
based on recurrent events data. We explore the relationship between multiple
criteria of interest in the context of planning AV reliability assurance tests.
Specifically, we develop two test planning strategies based on homogeneous and
non-homogeneous Poisson processes while balancing multiple objectives with the
Pareto front approach. We also offer recommendations for practical use. The
disengagement events data from the California Department of Motor Vehicles AV
testing program is used to illustrate the proposed assurance test planning
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Simin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yili Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00189">
<title>HeTriNet: Heterogeneous Graph Triplet Attention Network for Drug-Target-Disease Interaction. (arXiv:2312.00189v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00189</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the interactions between drugs, targets, and diseases is paramount
in drug discovery and has significant implications for precision medicine and
personalized treatments. Current approaches frequently consider drug-target or
drug-disease interactions individually, ignoring the interdependencies among
all three entities. Within human metabolic systems, drugs interact with protein
targets in cells, influencing target activities and subsequently impacting
biological pathways to promote healthy functions and treat diseases. Moving
beyond binary relationships and exploring tighter triple relationships is
essential to understanding drugs&apos; mechanism of action (MoAs). Moreover,
identifying the heterogeneity of drugs, targets, and diseases, along with their
distinct characteristics, is critical to model these complex interactions
appropriately. To address these challenges, we effectively model the
interconnectedness of all entities in a heterogeneous graph and develop a novel
Heterogeneous Graph Triplet Attention Network (\texttt{HeTriNet}).
\texttt{HeTriNet} introduces a novel triplet attention mechanism within this
heterogeneous graph structure. Beyond pairwise attention as the importance of
an entity for the other one, we define triplet attention to model the
importance of pairs for entities in the drug-target-disease triplet prediction
problem. Experimental results on real-world datasets show that
\texttt{HeTriNet} outperforms several baselines, demonstrating its remarkable
proficiency in uncovering novel drug-target-disease relationships.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanvir_F/0/1/0/all/0/1&quot;&gt;Farhan Tanvir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifuddin_K/0/1/0/all/0/1&quot;&gt;Khaled Mohammed Saifuddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1&quot;&gt;Tanvir Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagavathi_A/0/1/0/all/0/1&quot;&gt;Arunkumar Bagavathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbas_E/0/1/0/all/0/1&quot;&gt;Esra Akbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00201">
<title>An integrated framework for developing and evaluating an automated lecture style assessment system. (arXiv:2312.00201v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00201</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of the work presented in this paper is to develop and evaluate an
integrated system that provides automated lecture style evaluation, allowing
teachers to get instant feedback related to the goodness of their lecturing
style. The proposed system aims to promote improvement of lecture quality, that
could upgrade the overall student learning experience. The proposed application
utilizes specific measurable biometric characteristics, such as facial
expressions, body activity, speech rate and intonation, hand movement, and
facial pose, extracted from a video showing the lecturer from the audience
point of view. Measurable biometric features extracted during a lecture are
combined to provide teachers with a score reflecting lecture style quality both
at frame rate and by providing lecture quality metrics for the whole lecture.
The acceptance of the proposed lecture style evaluation system was evaluated by
chief education officers, teachers and students regarding the functionality,
usefulness of the application, and possible improvements. The results indicate
that participants found the application novel and useful in providing automated
feedback regarding lecture quality. Furthermore, the performance evaluation of
the proposed system was compared with the performance of humans in the task of
lecture style evaluation. Results indicate that the proposed system not only
achieves similar performance to human observers, but in some cases, it
outperforms them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitriadou_E/0/1/0/all/0/1&quot;&gt;Eleni Dimitriadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanitis_A/0/1/0/all/0/1&quot;&gt;Andreas Lanitis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00209">
<title>On the Interplay Between Stepsize Tuning and Progressive Sharpening. (arXiv:2312.00209v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent empirical work has revealed an intriguing property of deep learning
models by which the sharpness (largest eigenvalue of the Hessian) increases
throughout optimization until it stabilizes around a critical value at which
the optimizer operates at the edge of stability, given a fixed stepsize (Coehn
et al, 2022). We investigate empirically how the sharpness evolves when using
stepsize-tuners, the Armijo linesearch and Polyak stepsizes, that adapt the
stepsize along the iterations to local quantities such as, implicitly, the
sharpness itself. We find that the surprisingly poor performance of a classical
Armijo linesearch may be well explained by its tendency to ever-increase the
sharpness of the objective in the full or large batch regimes. On the other
hand, we observe that Polyak stepsizes operate generally at the edge of
stability or even slightly beyond, while outperforming its Armijo and constant
stepsizes counterparts. We conclude with an analysis that suggests unlocking
stepsize tuners requires an understanding of the joint dynamics of the step
size and the sharpness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roulet_V/0/1/0/all/0/1&quot;&gt;Vincent Roulet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwala_A/0/1/0/all/0/1&quot;&gt;Atish Agarwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1&quot;&gt;Fabian Pedregosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00210">
<title>DREAM: Diffusion Rectification and Estimation-Adaptive Models. (arXiv:2312.00210v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00210</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DREAM, a novel training framework representing Diffusion
Rectification and Estimation-Adaptive Models, requiring minimal code changes
(just three lines) yet significantly enhancing the alignment of training with
sampling in diffusion models. DREAM features two components: diffusion
rectification, which adjusts training to reflect the sampling process, and
estimation adaptation, which balances perception against distortion. When
applied to image super-resolution (SR), DREAM adeptly navigates the tradeoff
between minimizing distortion and preserving high image quality. Experiments
demonstrate DREAM&apos;s superiority over standard diffusion-based SR methods,
showing a $2$ to $3\times $ faster training convergence and a $10$ to
$20\times$ reduction in necessary sampling steps to achieve comparable or
superior results. We hope DREAM will inspire a rethinking of diffusion model
training paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinxin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1&quot;&gt;Tianyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiachen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zharkov_I/0/1/0/all/0/1&quot;&gt;Ilya Zharkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Luming Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00215">
<title>Learning active tactile perception through belief-space control. (arXiv:2312.00215v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.00215</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots operating in an open world will encounter novel objects with unknown
physical properties, such as mass, friction, or size. These robots will need to
sense these properties through interaction prior to performing downstream tasks
with the objects. We propose a method that autonomously learns tactile
exploration policies by developing a generative world model that is leveraged
to 1) estimate the object&apos;s physical parameters using a differentiable Bayesian
filtering algorithm and 2) develop an exploration policy using an
information-gathering model predictive controller. We evaluate our method on
three simulated tasks where the goal is to estimate a desired object property
(mass, height or toppling height) through physical interaction. We find that
our method is able to discover policies that efficiently gather information
about the desired property in an intuitive manner. Finally, we validate our
method on a real robot system for the height estimation task, where our method
is able to successfully learn and execute an information-gathering policy from
scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1&quot;&gt;Jean-Fran&amp;#xe7;ois Tremblay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1&quot;&gt;David Meger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogan_F/0/1/0/all/0/1&quot;&gt;Francois Hogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudek_G/0/1/0/all/0/1&quot;&gt;Gregory Dudek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00216">
<title>Medication abortion via digital health in the United States: a systematic scoping review. (arXiv:2312.00216v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.00216</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital health, including telemedicine, has increased access to abortion
care. The convenience, flexibility of appointment times, and ensured privacy to
abortion users may make abortion services via telemedicine preferable. This
scoping review systematically mapped studies conducted on abortion services via
telemedicine, including their effectiveness and acceptability for abortion
users and providers. All published papers included abortion services via
telemedicine in the United States were considered. Articles were searched in
PubMed, CINAHL, and Google Scholar databases in September 2022. The findings
were synthesized narratively, and the PRISMA-ScR guidelines were used to report
this study. Out of 757 retrieved articles, 33 articles were selected based on
the inclusion criteria. These studies were published between 2011 and 2022,
with 24 published in the last 3 years. The study found that telemedicine
increased access to abortion care in the United States, especially for people
in remote areas or those worried about stigma from in-person visits. The
effectiveness of abortion services via telemedicine was comparable to in-clinic
visits, with 6% or fewer abortions requiring surgical intervention. Both care
providers and abortion seekers expressed positive perceptions of
telemedicine-based abortion services. However, abortion users reported mixed
emotions, with some preferring in-person visits. The most common reasons for
choosing telemedicine included the distance to the abortion clinic,
convenience, privacy, cost, flexibility of appointment times, and state laws
imposing waiting periods or restrictive policies. Telemedicine offered a
preferable option for abortion seekers and providers. The feasibility of
accessing abortion services via telemedicine in low-resource settings needs
further investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumsa_F/0/1/0/all/0/1&quot;&gt;Fekede Asefa Kumsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_R/0/1/0/all/0/1&quot;&gt;Rameshwari Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaban_Nejad_A/0/1/0/all/0/1&quot;&gt;Arash Shaban-Nejad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00224">
<title>Unsupervised textile defect detection using convolutional neural networks. (arXiv:2312.00224v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00224</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose a novel motif-based approach for unsupervised
textile anomaly detection that combines the benefits of traditional
convolutional neural networks with those of an unsupervised learning paradigm.
It consists of five main steps: preprocessing, automatic pattern period
extraction, patch extraction, features selection and anomaly detection. This
proposed approach uses a new dynamic and heuristic method for feature selection
which avoids the drawbacks of initialization of the number of filters (neurons)
and their weights, and those of the backpropagation mechanism such as the
vanishing gradients, which are common practice in the state-of-the-art methods.
The design and training of the network are performed in a dynamic and input
domain-based manner and, thus, no ad-hoc configurations are required. Before
building the model, only the number of layers and the stride are defined. We do
not initialize the weights randomly nor do we define the filter size or number
of filters as conventionally done in CNN-based approaches. This reduces effort
and time spent on hyperparameter initialization and fine-tuning. Only one
defect-free sample is required for training and no further labeled data is
needed. The trained network is then used to detect anomalies on defective
fabric samples. We demonstrate the effectiveness of our approach on the
Patterned Fabrics benchmark dataset. Our algorithm yields reliable and
competitive results (on recall, precision, accuracy and f1- measure) compared
to state-of-the-art unsupervised approaches, in less time, with efficient
training in a single epoch and a lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koulali_I/0/1/0/all/0/1&quot;&gt;Imane Koulali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eskil_M/0/1/0/all/0/1&quot;&gt;M. Taner Eskil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00232">
<title>Uncertainty in Graph Contrastive Learning with Bayesian Neural Networks. (arXiv:2312.00232v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00232</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph contrastive learning has shown great promise when labeled data is
scarce, but large unlabeled datasets are available. However, it often does not
take uncertainty estimation into account. We show that a variational Bayesian
neural network approach can be used to improve not only the uncertainty
estimates but also the downstream performance on semi-supervised
node-classification tasks. Moreover, we propose a new measure of uncertainty
for contrastive learning, that is based on the disagreement in likelihood due
to different positive samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollers_A/0/1/0/all/0/1&quot;&gt;Alexander M&amp;#xf6;llers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1&quot;&gt;Alexander Immer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isufi_E/0/1/0/all/0/1&quot;&gt;Elvin Isufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00237">
<title>Negotiated Representations to Prevent Forgetting in Machine Learning Applications. (arXiv:2312.00237v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00237</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting is a significant challenge in the field of machine
learning, particularly in neural networks. When a neural network learns to
perform well on a new task, it often forgets its previously acquired knowledge
or experiences. This phenomenon occurs because the network adjusts its weights
and connections to minimize the loss on the new task, which can inadvertently
overwrite or disrupt the representations that were crucial for the previous
tasks. As a result, the the performance of the network on earlier tasks
deteriorates, limiting its ability to learn and adapt to a sequence of tasks.
In this paper, we propose a novel method for preventing catastrophic forgetting
in machine learning applications, specifically focusing on neural networks. Our
approach aims to preserve the knowledge of the network across multiple tasks
while still allowing it to learn new information effectively. We demonstrate
the effectiveness of our method by conducting experiments on various benchmark
datasets, including Split MNIST, Split CIFAR10, Split Fashion MNIST, and Split
CIFAR100. These datasets are created by dividing the original datasets into
separate, non overlapping tasks, simulating a continual learning scenario where
the model needs to learn multiple tasks sequentially without forgetting the
previous ones. Our proposed method tackles the catastrophic forgetting problem
by incorporating negotiated representations into the learning process, which
allows the model to maintain a balance between retaining past experiences and
adapting to new tasks. By evaluating our method on these challenging datasets,
we aim to showcase its potential for addressing catastrophic forgetting and
improving the performance of neural networks in continual learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhan_N/0/1/0/all/0/1&quot;&gt;Nuri Korhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oner_C/0/1/0/all/0/1&quot;&gt;Ceren &amp;#xd6;ner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00258">
<title>Precipitation Nowcasting With Spatial And Temporal Transfer Learning Using Swin-UNETR. (arXiv:2312.00258v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2312.00258</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change has led to an increase in frequency of extreme weather events.
Early warning systems can prevent disasters and loss of life. Managing such
events remain a challenge for both public and private institutions.
Precipitation nowcasting can help relevant institutions to better prepare for
such events. Numerical weather prediction (NWP) has traditionally been used to
make physics based forecasting, and recently deep learning based approaches
have been used to reduce turn-around time for nowcasting. In this work,
recently proposed Swin-UNETR (Swin UNEt TRansformer) is used for precipitation
nowcasting for ten different regions of Europe. Swin-UNETR utilizes a U-shaped
network within which a swin transformer-based encoder extracts multi-scale
features from multiple input channels of satellite image, while CNN-based
decoder makes the prediction. Trained model is capable of nowcasting not only
for the regions for which data is available, but can also be used for new
regions for which data is not available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajitabh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00264">
<title>Skipper: Improving the Reach and Fidelity of Quantum Annealers by Skipping Long Chains. (arXiv:2312.00264v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2312.00264</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum Annealers (QAs) operate as single-instruction machines, lacking a
SWAP operation to overcome limited qubit connectivity. Consequently, multiple
physical qubits are chained to form a program qubit with higher connectivity,
resulting in a drastically diminished effective QA capacity by up to 33x. We
observe that in QAs: (a) chain lengths exhibit a power-law distribution, a few
dominant chains holding substantially more qubits than others; and (b) about
25% of physical qubits remain unused, getting isolated between these chains. We
propose Skipper, a software technique that enhances the capacity and fidelity
of QAs by skipping dominant chains and substituting their program qubit with
two readout results. Using a 5761-qubit QA, we demonstrate that Skipper can
tackle up to 59% (Avg. 28%) larger problems when eleven chains are skipped.
Additionally, Skipper can improve QA fidelity by up to 44% (Avg. 33%) when
cutting five chains (32 runs). Users can specify up to eleven chain cuts in
Skipper, necessitating about 2,000 distinct quantum executable runs. To
mitigate this, we introduce Skipper-G, a greedy scheme that skips sub-problems
less likely to hold the global optimum, executing a maximum of 23 quantum
executables with eleven chain trims. Skipper-G can boost QA fidelity by up to
41% (Avg. 29%) when cutting five chains (11 runs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ayanzadeh_R/0/1/0/all/0/1&quot;&gt;Ramin Ayanzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Qureshi_M/0/1/0/all/0/1&quot;&gt;Moinuddin Qureshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00267">
<title>Sample Efficient Reinforcement Learning from Human Feedback via Active Exploration. (arXiv:2312.00267v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00267</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference-based feedback is important for many applications in reinforcement
learning where direct evaluation of a reward function is not feasible. A
notable recent example arises in reinforcement learning from human feedback
(RLHF) on large language models. For many applications of RLHF, the cost of
acquiring the human feedback can be substantial. In this work, we take
advantage of the fact that one can often choose contexts at which to obtain
human feedback in order to most efficiently identify a good policy, and
formalize this as an offline contextual dueling bandit problem. We give an
upper-confidence-bound style algorithm for this problem and prove a polynomial
worst-case regret bound. We then provide empirical confirmation in a synthetic
setting that our approach outperforms existing methods. After, we extend the
setting and methodology for practical use in RLHF training of large language
models. Here, our method is able to reach better performance with fewer samples
of human preferences than multiple baselines on three real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1&quot;&gt;Viraj Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_V/0/1/0/all/0/1&quot;&gt;Vikramjeet Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neopane_O/0/1/0/all/0/1&quot;&gt;Ojash Neopane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yijia Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1&quot;&gt;Ilija Bogunovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00268">
<title>Academic competitions. (arXiv:2312.00268v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Academic challenges comprise effective means for (i) advancing the state of
the art, (ii) putting in the spotlight of a scientific community specific
topics and problems, as well as (iii) closing the gap for under represented
communities in terms of accessing and participating in the shaping of research
fields. Competitions can be traced back for centuries and their achievements
have had great influence in our modern world. Recently, they (re)gained
popularity, with the overwhelming amounts of data that is being generated in
different domains, as well as the need of pushing the barriers of existing
methods, and available tools to handle such data. This chapter provides a
survey of academic challenges in the context of machine learning and related
fields. We review the most influential competitions in the last few years and
analyze challenges per area of knowledge. The aims of scientific challenges,
their goals, major achievements and expectations for the next few years are
reviewed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalante_H/0/1/0/all/0/1&quot;&gt;Hugo Jair Escalante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruchinina_A/0/1/0/all/0/1&quot;&gt;Aleksandra Kruchinina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00273">
<title>Mark My Words: Analyzing and Evaluating Language Model Watermarks. (arXiv:2312.00273v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.00273</link>
<description rdf:parseType="Literal">&lt;p&gt;The capabilities of large language models have grown significantly in recent
years and so too have concerns about their misuse. In this context, the ability
to distinguish machine-generated text from human-authored content becomes
important. Prior works have proposed numerous schemes to watermark text, which
would benefit from a systematic evaluation framework. This work focuses on text
watermarking techniques - as opposed to image watermarks - and proposes a
comprehensive benchmark for them under different tasks as well as practical
attacks. We focus on three main metrics: quality, size (e.g. the number of
tokens needed to detect a watermark), and tamper-resistance. Current
watermarking techniques are good enough to be deployed: Kirchenbauer et al. can
watermark Llama2-7B-chat with no perceivable loss in quality in under 100
tokens, and with good tamper-resistance to simple attacks, regardless of
temperature. We argue that watermark indistinguishability is too strong a
requirement: schemes that slightly modify logit distributions outperform their
indistinguishable counterparts with no noticeable loss in generation quality.
We publicly release our benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piet_J/0/1/0/all/0/1&quot;&gt;Julien Piet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitawarin_C/0/1/0/all/0/1&quot;&gt;Chawin Sitawarin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_V/0/1/0/all/0/1&quot;&gt;Vivian Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1&quot;&gt;Norman Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1&quot;&gt;David Wagner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00326">
<title>Agent-OM: Leveraging Large Language Models for Ontology Matching. (arXiv:2312.00326v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00326</link>
<description rdf:parseType="Literal">&lt;p&gt;Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM-based agents
have become revolutionary in data engineering and have been applied creatively
in various domains, their potential for OM remains underexplored. This study
introduces a novel agent-powered LLM-based design paradigm for OM systems. With
thoughtful consideration of several specific challenges to leverage LLMs for
OM, we propose a generic framework, namely Agent-OM, consisting of two Siamese
agents for retrieval and matching, with a set of simple prompt-based OM tools.
Our framework is implemented in a proof-of-concept system. Evaluations of three
Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM
systems show that our system can achieve very close results to the best
long-standing performance on simple OM tasks and significantly improve the
performance on complex and few-shot OM tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_Z/0/1/0/all/0/1&quot;&gt;Zhangcheng Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_K/0/1/0/all/0/1&quot;&gt;Kerry Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00330">
<title>StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter. (arXiv:2312.00330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00330</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-video (T2V) models have shown remarkable capabilities in generating
diverse videos. However, they struggle to produce user-desired stylized videos
due to (i) text&apos;s inherent clumsiness in expressing specific styles and (ii)
the generally degraded style fidelity. To address these challenges, we
introduce StyleCrafter, a generic method that enhances pre-trained T2V models
with a style control adapter, enabling video generation in any style by
providing a reference image. Considering the scarcity of stylized video
datasets, we propose to first train a style control adapter using style-rich
image datasets, then transfer the learned stylization ability to video
generation through a tailor-made finetuning paradigm. To promote content-style
disentanglement, we remove style descriptions from the text prompt and extract
style information solely from the reference image using a decoupling learning
strategy. Additionally, we design a scale-adaptive fusion module to balance the
influences of text-based content features and image-based style features, which
helps generalization across various text and style combinations. StyleCrafter
efficiently generates high-quality stylized videos that align with the content
of the texts and resemble the style of the reference images. Experiments
demonstrate that our approach is more flexible and efficient than existing
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gongye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00332">
<title>Matching Weak Informative Ontologies. (arXiv:2312.00332v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00332</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing ontology matching methods utilize the literal information to
discover alignments. However, some literal information in ontologies may be
opaque and some ontologies may not have sufficient literal information. In this
paper, these ontologies are named as weak informative ontologies (WIOs) and it
is challenging for existing methods to matching WIOs. On one hand, string-based
and linguistic-based matching methods cannot work well for WIOs. On the other
hand, some matching methods use external resources to improve their
performance, but collecting and processing external resources is still
time-consuming. To address this issue, this paper proposes a practical method
for matching WIOs by employing the ontology structure information to discover
alignments. First, the semantic subgraphs are extracted from the ontology graph
to capture the precise meanings of ontology elements. Then, a new similarity
propagation model is designed for matching WIOs. Meanwhile, in order to avoid
meaningless propagation, the similarity propagation is constrained by semantic
subgraphs and other conditions. Consequently, the similarity propagation model
ensures a balance between efficiency and quality during matching. Finally, the
similarity propagation model uses a few credible alignments as seeds to find
more alignments, and some useful strategies are adopted to improve the
performance. This matching method for WIOs has been implemented in the ontology
matching system Lily. Experimental results on public OAEI benchmark datasets
demonstrate that Lily significantly outperforms most of the state-of-the-art
works in both WIO matching tasks and general ontology matching tasks. In
particular, Lily increases the recall by a large margin, while it still obtains
high precision of matching results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00333">
<title>Green Edge AI: A Contemporary Survey. (arXiv:2312.00333v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00333</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) technologies have emerged as pivotal enablers
across a multitude of industries, including consumer electronics, healthcare,
and manufacturing, largely due to their resurgence over the past decade. The
transformative power of AI is primarily derived from the utilization of deep
neural networks (DNNs), which require extensive data for training and
substantial computational resources for processing. Consequently, DNN models
are typically trained and deployed on resource-rich cloud servers. However, due
to potential latency issues associated with cloud communications, deep learning
(DL) workflows are increasingly being transitioned to wireless edge networks
near end-user devices (EUDs). This shift is designed to support
latency-sensitive applications and has given rise to a new paradigm of edge AI,
which will play a critical role in upcoming 6G networks to support ubiquitous
AI applications. Despite its potential, edge AI faces substantial challenges,
mostly due to the dichotomy between the resource limitations of wireless edge
networks and the resource-intensive nature of DL. Specifically, the acquisition
of large-scale data, as well as the training and inference processes of DNNs,
can rapidly deplete the battery energy of EUDs. This necessitates an
energy-conscious approach to edge AI to ensure both optimal and sustainable
performance. In this paper, we present a contemporary survey on green edge AI.
We commence by analyzing the principal energy consumption components of edge AI
systems to identify the fundamental design principles of green edge AI. Guided
by these principles, we then explore energy-efficient design methodologies for
the three critical tasks in edge AI systems, including training data
acquisition, edge training, and edge inference. Finally, we underscore
potential future research directions to further enhance the energy efficiency
of edge AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuyi Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xianghao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaibin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ying-Jun Angela Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00342">
<title>Efficient Off-Policy Safe Reinforcement Learning Using Trust Region Conditional Value at Risk. (arXiv:2312.00342v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00342</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to solve a safe reinforcement learning (RL) problem with risk
measure-based constraints. As risk measures, such as conditional value at risk
(CVaR), focus on the tail distribution of cost signals, constraining risk
measures can effectively prevent a failure in the worst case. An on-policy safe
RL method, called TRC, deals with a CVaR-constrained RL problem using a trust
region method and can generate policies with almost zero constraint violations
with high returns. However, to achieve outstanding performance in complex
environments and satisfy safety constraints quickly, RL methods are required to
be sample efficient. To this end, we propose an off-policy safe RL method with
CVaR constraints, called off-policy TRC. If off-policy data from replay buffers
is directly used to train TRC, the estimation error caused by the
distributional shift results in performance degradation. To resolve this issue,
we propose novel surrogate functions, in which the effect of the distributional
shift can be reduced, and introduce an adaptive trust-region constraint to
ensure a policy not to deviate far from replay buffers. The proposed method has
been evaluated in simulation and real-world environments and satisfied safety
constraints within a few steps while achieving high returns even in complex
robotic tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dohyeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00349">
<title>The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in NLP. (arXiv:2312.00349v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00349</link>
<description rdf:parseType="Literal">&lt;p&gt;I propose a paradigm for scientific progress in NLP centered around
developing scalable, data-driven theories of linguistic structure. The idea is
to collect data in tightly scoped, carefully defined ways which allow for
exhaustive annotation of behavioral phenomena of interest, and then use machine
learning to construct explanatory theories of these phenomena which can form
building blocks for intelligible AI systems. After laying some conceptual
groundwork, I describe several investigations into data-driven theories of
shallow semantic structure using Question-Answer driven Semantic Role Labeling
(QA-SRL), a schema for annotating verbal predicate-argument relations using
highly constrained question-answer pairs. While this only scratches the surface
of the complex language behaviors of interest in AI, I outline principles for
data collection and theoretical modeling which can inform future scientific
progress. This note summarizes and draws heavily on my PhD thesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1&quot;&gt;Julian Michael&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00353">
<title>On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs. (arXiv:2312.00353v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00353</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the capacity of LLMs to reason with knowledge graphs
using their internal knowledge graph, i.e., the knowledge graph they learned
during pre-training. Two research questions are formulated to investigate the
accuracy of LLMs in recalling information from pre-training knowledge graphs
and their ability to infer knowledge graph relations from context. To address
these questions, we employ LLMs to perform four distinct knowledge graph
reasoning tasks. Furthermore, we identify two types of hallucinations that may
occur during knowledge reasoning with LLMs: content and ontology hallucination.
Our experimental results demonstrate that LLMs can successfully tackle both
simple and complex knowledge graph reasoning tasks from their own memory, as
well as infer from input context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_P/0/1/0/all/0/1&quot;&gt;Pei-Chi Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yi-Hang Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1&quot;&gt;Ee-Peng Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;San-Yih Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00377">
<title>SynFundus: Generating a synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00377</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, the scarcity of large-scale datasets due to
privacy restrictions stands as a significant barrier to develop large models
for medical. To address this issue, we introduce SynFundus-1M, a high-quality
synthetic dataset with over 1 million retinal fundus images and extensive
disease and pathologies annotations, which is generated by a Denoising
Diffusion Probabilistic Model. The SynFundus-Generator and SynFundus-1M achieve
superior Frechet Inception Distance (FID) scores compared to existing methods
on main-stream public real datasets. Furthermore, the ophthalmologists
evaluation validate the difficulty in discerning these synthetic images from
real ones, confirming the SynFundus-1M&apos;s authenticity. Through extensive
experiments, we demonstrate that both CNN and ViT can benifit from SynFundus-1M
by pretraining or training directly. Compared to datasets like ImageNet or
EyePACS, models train on SynFundus-1M not only achieve better performance but
also faster convergence on various downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fangxin Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yehui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00380">
<title>Enhancing Explainability in Mobility Data Science through a combination of methods. (arXiv:2312.00380v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00380</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of Mobility Data Science, the intricate task of interpreting
models trained on trajectory data, and elucidating the spatio-temporal movement
of entities, has persistently posed significant challenges. Conventional XAI
techniques, although brimming with potential, frequently overlook the distinct
structure and nuances inherent within trajectory data. Observing this
deficiency, we introduced a comprehensive framework that harmonizes pivotal XAI
techniques: LIME (Local Interpretable Model-agnostic Explanations), SHAP
(SHapley Additive exPlanations), Saliency maps, attention mechanisms, direct
trajectory visualization, and Permutation Feature Importance (PFI). Unlike
conventional strategies that deploy these methods singularly, our unified
approach capitalizes on the collective efficacy of these techniques, yielding
deeper and more granular insights for models reliant on trajectory data. In
crafting this synthesis, we effectively address the multifaceted essence of
trajectories, achieving not only amplified interpretability but also a nuanced,
contextually rich comprehension of model decisions. To validate and enhance our
framework, we undertook a survey to gauge preferences and reception among
various user demographics. Our findings underscored a dichotomy: professionals
with academic orientations, particularly those in roles like Data Scientist, IT
Expert, and ML Engineer, showcased a profound, technical understanding and
often exhibited a predilection for amalgamated methods for interpretability.
Conversely, end-users or individuals less acquainted with AI and Data Science
showcased simpler inclinations, such as bar plots indicating timestep
significance or visual depictions pinpointing pivotal segments of a vessel&apos;s
trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makridis_G/0/1/0/all/0/1&quot;&gt;Georgios Makridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koukos_V/0/1/0/all/0/1&quot;&gt;Vasileios Koukos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatouros_G/0/1/0/all/0/1&quot;&gt;Georgios Fatouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyriazis_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Kyriazis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00413">
<title>Abstract Syntax Tree for Programming Language Understanding and Representation: How Far Are We?. (arXiv:2312.00413v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.00413</link>
<description rdf:parseType="Literal">&lt;p&gt;Programming language understanding and representation (a.k.a code
representation learning) has always been a hot and challenging task in software
engineering. It aims to apply deep learning techniques to produce numerical
representations of the source code features while preserving its semantics.
These representations can be used for facilitating subsequent code-related
tasks. The abstract syntax tree (AST), a fundamental code feature, illustrates
the syntactic information of the source code and has been widely used in code
representation learning. However, there is still a lack of systematic and
quantitative evaluation of how well AST-based code representation facilitates
subsequent code-related tasks. In this paper, we first conduct a comprehensive
empirical study to explore the effectiveness of the AST-based code
representation in facilitating follow-up code-related tasks. To do so, we
compare the performance of models trained with code token sequence (Token for
short) based code representation and AST-based code representation on three
popular types of code-related tasks. Surprisingly, the overall quantitative
statistical results demonstrate that models trained with AST-based code
representation consistently perform worse across all three tasks compared to
models trained with Token-based code representation. Our further quantitative
analysis reveals that models trained with AST-based code representation
outperform models trained with Token-based code representation in certain
subsets of samples across all three tasks. We also conduct comprehensive
experiments to evaluate and reveal the impact of the choice of AST
parsing/preprocessing/encoding methods on AST-based code representation and
subsequent code-related tasks. Our study provides future researchers with
detailed guidance on how to select solutions at each stage to fully exploit
AST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Weisong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Chunrong Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1&quot;&gt;Yun Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yudu You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mengzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuchen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1&quot;&gt;An Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00434">
<title>PEFTDebias : Capturing debiasing information using PEFTs. (arXiv:2312.00434v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00434</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing use of foundation models highlights the urgent need to address
and eliminate implicit biases present in them that arise during pretraining. In
this paper, we introduce PEFTDebias, a novel approach that employs
parameter-efficient fine-tuning (PEFT) to mitigate the biases within foundation
models. PEFTDebias consists of two main phases: an upstream phase for acquiring
debiasing parameters along a specific bias axis, and a downstream phase where
these parameters are incorporated into the model and frozen during the
fine-tuning process. By evaluating on four datasets across two bias axes namely
gender and race, we find that downstream biases can be effectively reduced with
PEFTs. In addition, we show that these parameters possess axis-specific
debiasing characteristics, enabling their effective transferability in
mitigating biases in various downstream tasks. To ensure reproducibility, we
release the code to do our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1&quot;&gt;Sumit Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veerubhotla_A/0/1/0/all/0/1&quot;&gt;Aditya Srikanth Veerubhotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Srijan Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00455">
<title>Meta-Diversity Search in Complex Systems, A Recipe for Artificial Open-Endedness ?. (arXiv:2312.00455v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00455</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we build an artificial system that would be able to generate endless
surprises if ran &quot;forever&quot; in Minecraft? While there is not a single path
toward solving that grand challenge, this article presents what we believe to
be some working ingredients for the endless generation of novel increasingly
complex artifacts in Minecraft. Our framework for an open-ended system includes
two components: a complex system used to recursively grow and complexify
artifacts over time, and a discovery algorithm that leverages the concept of
meta-diversity search. Since complex systems have shown to enable the emergence
of considerable complexity from set of simple rules, we believe them to be
great candidates to generate all sort of artifacts in Minecraft. Yet, the space
of possible artifacts that can be generated by these systems is often unknown,
challenging to characterize and explore. Therefore automating the long-term
discovery of novel and increasingly complex artifacts in these systems is an
exciting research field. To approach these challenges, we formulate the problem
of meta-diversity search where an artificial &quot;discovery assistant&quot;
incrementally learns a diverse set of representations to characterize behaviors
and searches to discover diverse patterns within each of them. A successful
discovery assistant should continuously seek for novel sources of diversities
while being able to quickly specialize the search toward a new unknown type of
diversity. To implement those ideas in the Minecraft environment, we simulate
an artificial &quot;chemistry&quot; system based on Lenia continuous cellular automaton
for generating artifacts, as well as an artificial &quot;discovery assistant&quot;
(called Holmes) for the artifact-discovery process. Holmes incrementally learns
a hierarchy of modular representations to characterize divergent sources of
diversity and uses a goal-based intrinsically-motivated exploration as the
diversity search strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etcheverry_M/0/1/0/all/0/1&quot;&gt;Mayalen Etcheverry&lt;/a&gt; (Flowers), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_B/0/1/0/all/0/1&quot;&gt;Bert Wang-Chak Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Moulin-Frier&lt;/a&gt; (Flowers), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt; (Flowers)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00471">
<title>A Bayesian approach for prompt optimization in pre-trained language models. (arXiv:2312.00471v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00471</link>
<description rdf:parseType="Literal">&lt;p&gt;A prompt is a sequence of symbol or tokens, selected from a vocabulary
according to some rule, which is prepended/concatenated to a textual query. A
key problem is how to select the sequence of tokens: in this paper we formulate
it as a combinatorial optimization problem. The high dimensionality of the
token space com-pounded by the length of the prompt sequence requires a very
efficient solution. In this paper we propose a Bayesian optimization method,
executed in a continuous em-bedding of the combinatorial space. In this paper
we focus on hard prompt tuning (HPT) which directly searches for discrete
tokens to be added to the text input with-out requiring access to the large
language model (LLM) and can be used also when LLM is available only as a
black-box. This is critically important if LLMs are made available in the Model
as a Service (MaaS) manner as in GPT-4. The current manu-script is focused on
the optimization of discrete prompts for classification tasks. The discrete
prompts give rise to difficult combinatorial optimization problem which easily
become intractable given the dimension of the token space in realistic
applications. The optimization method considered in this paper is Bayesian
optimization (BO) which has become the dominant approach in black-box
optimization for its sample efficiency along with its modular structure and
versatility. In this paper we use BoTorch, a library for Bayesian optimization
research built on top of pyTorch. Albeit preliminary and obtained using a
&apos;vanilla&apos; version of BO, the experiments on RoB-ERTa on six benchmarks, show a
good performance across a variety of tasks and enable an analysis of the
tradeoff between size of the search space, accuracy and wall clock time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabbatella_A/0/1/0/all/0/1&quot;&gt;Antonio Sabbatella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponti_A/0/1/0/all/0/1&quot;&gt;Andrea Ponti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Candelieri_A/0/1/0/all/0/1&quot;&gt;Antonio Candelieri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordani_I/0/1/0/all/0/1&quot;&gt;Ilaria Giordani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Archetti_F/0/1/0/all/0/1&quot;&gt;Francesco Archetti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00480">
<title>Japanese Tort-case Dataset for Rationale-supported Legal Judgment Prediction. (arXiv:2312.00480v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00480</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first dataset for Japanese Legal Judgment Prediction
(LJP), the Japanese Tort-case Dataset (JTD), which features two tasks: tort
prediction and its rationale extraction. The rationale extraction task
identifies the court&apos;s accepting arguments from alleged arguments by plaintiffs
and defendants, which is a novel task in the field. JTD is constructed based on
annotated 3,477 Japanese Civil Code judgments by 41 legal experts, resulting in
7,978 instances with 59,697 of their alleged arguments from the involved
parties. Our baseline experiments show the feasibility of the proposed two
tasks, and our error analysis by legal experts identifies sources of errors and
suggests future directions of the LJP research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_H/0/1/0/all/0/1&quot;&gt;Hiroaki Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokunaga_T/0/1/0/all/0/1&quot;&gt;Takenobu Tokunaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohara_R/0/1/0/all/0/1&quot;&gt;Ryutaro Ohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokutsu_A/0/1/0/all/0/1&quot;&gt;Akira Tokutsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeshita_K/0/1/0/all/0/1&quot;&gt;Keisuke Takeshita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumida_M/0/1/0/all/0/1&quot;&gt;Mihoko Sumida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00506">
<title>Generative artificial intelligence enhances individual creativity but reduces the collective diversity of novel content. (arXiv:2312.00506v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.00506</link>
<description rdf:parseType="Literal">&lt;p&gt;Creativity is core to being human. Generative artificial intelligence (GenAI)
holds promise for humans to be more creative by offering new ideas, or less
creative by anchoring on GenAI ideas. We study the causal impact of GenAI ideas
on the production of an unstructured creative output in an online experimental
study where some writers could obtain ideas for a story from a GenAI platform.
We find that access to GenAI ideas causes stories to be evaluated as more
creative, better written and more enjoyable, especially among less creative
writers. However, objective measures of story similarity within each condition
reveal that GenAI-enabled stories are more similar to each other than stories
by humans alone. These results point to an increase in individual creativity,
but at the same time there is a risk of losing collective novelty: this dynamic
resembles a social dilemma where individual writers are better off using GenAI
to improve their own writing, but collectively a narrower scope of novel
content may be produced with GenAI. Our results have implications for
researchers, policy-makers and practitioners interested in bolstering
creativity, but point to potential downstream consequences from over-reliance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doshi_A/0/1/0/all/0/1&quot;&gt;Anil R. Doshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauser_O/0/1/0/all/0/1&quot;&gt;Oliver P. Hauser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00525">
<title>SurreyAI 2023 Submission for the Quality Estimation Shared Task. (arXiv:2312.00525v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00525</link>
<description rdf:parseType="Literal">&lt;p&gt;Quality Estimation (QE) systems are important in situations where it is
necessary to assess the quality of translations, but there is no reference
available. This paper describes the approach adopted by the SurreyAI team for
addressing the Sentence-Level Direct Assessment shared task in WMT23. The
proposed approach builds upon the TransQuest framework, exploring various
autoencoder pre-trained language models within the MonoTransQuest architecture
using single and ensemble settings. The autoencoder pre-trained language models
employed in the proposed systems are XLMV, InfoXLM-large, and XLMR-large. The
evaluation utilizes Spearman and Pearson correlation coefficients, assessing
the relationship between machine-predicted quality scores and human judgments
for 5 language pairs (English-Gujarati, English-Hindi, English-Marathi,
English-Tamil and English-Telugu). The MonoTQ-InfoXLM-large approach emerges as
a robust strategy, surpassing all other individual models proposed in this
study by significantly improving over the baseline for the majority of the
language pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sindhujan_A/0/1/0/all/0/1&quot;&gt;Archchana Sindhujan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1&quot;&gt;Diptesh Kanojia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1&quot;&gt;Constantin Orasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1&quot;&gt;Tharindu Ranasinghe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00540">
<title>Target-agnostic Source-free Domain Adaptation for Regression Tasks. (arXiv:2312.00540v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00540</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) seeks to bridge the domain gap between
the target and source using unlabeled target data. Source-free UDA removes the
requirement for labeled source data at the target to preserve data privacy and
storage. However, work on source-free UDA assumes knowledge of domain gap
distribution, and hence is limited to either target-aware or classification
task. To overcome it, we propose TASFAR, a novel target-agnostic source-free
domain adaptation approach for regression tasks. Using prediction confidence,
TASFAR estimates a label density map as the target label distribution, which is
then used to calibrate the source model on the target domain. We have conducted
extensive experiments on four regression tasks with various domain gaps,
namely, pedestrian dead reckoning for different users, image-based people
counting in different scenes, housing-price prediction at different districts,
and taxi-trip duration prediction from different departure points. TASFAR is
shown to substantially outperform the state-of-the-art source-free UDA
approaches by averagely reducing 22% errors for the four tasks and achieve
notably comparable accuracy as source-based UDA without using source data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tianlang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zhiqiu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jierun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;S.-H. Gary Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00554">
<title>Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?. (arXiv:2312.00554v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00554</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolution of legal datasets and the advent of large language models
(LLMs) have significantly transformed the legal field, particularly in the
generation of case judgment summaries. However, a critical concern arises
regarding the potential biases embedded within these summaries. This study
scrutinizes the biases present in case judgment summaries produced by legal
datasets and large language models. The research aims to analyze the impact of
biases on legal decision making. By interrogating the accuracy, fairness, and
implications of biases in these summaries, this study contributes to a better
understanding of the role of technology in legal contexts and the implications
for justice systems worldwide. In this study, we investigate biases wrt
Gender-related keywords, Race-related keywords, Keywords related to crime
against women, Country names and religious keywords. The study shows
interesting evidences of biases in the outputs generated by the large language
models and pre-trained abstractive summarization models. The reasoning behind
these biases needs further studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deroy_A/0/1/0/all/0/1&quot;&gt;Aniket Deroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maity_S/0/1/0/all/0/1&quot;&gt;Subhankar Maity&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00584">
<title>The Ethics of Automating Legal Actors. (arXiv:2312.00584v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00584</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of large public legal datasets has brought about a
renaissance in legal NLP. Many of these datasets are comprised of legal
judgements - the product of judges deciding cases. This fact, together with the
way machine learning works, means that several legal NLP models are models of
judges. While some have argued for the automation of judges, in this position
piece, we argue that automating the role of the judge raises difficult ethical
challenges, in particular for common law legal systems. Our argument follows
from the social role of the judge in actively shaping the law, rather than
merely applying it. Since current NLP models come nowhere close to having the
facilities necessary for this task, they should not be used to automate judges.
Furthermore, even in the case the models could achieve human-level
capabilities, there would still be remaining ethical concerns inherent in the
automation of the legal process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvoda_J/0/1/0/all/0/1&quot;&gt;Josef Valvoda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_A/0/1/0/all/0/1&quot;&gt;Alec Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_S/0/1/0/all/0/1&quot;&gt;Simone Teufel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00586">
<title>Explainable Fraud Detection with Deep Symbolic Classification. (arXiv:2312.00586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00586</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing demand for explainable, transparent, and data-driven
models within the domain of fraud detection. Decisions made by fraud detection
models need to be explainable in the event of a customer dispute. Additionally,
the decision-making process in the model must be transparent to win the trust
of regulators and business stakeholders. At the same time, fraud detection
solutions can benefit from data due to the noisy, dynamic nature of fraud and
the availability of large historical data sets. Finally, fraud detection is
notorious for its class imbalance: there are typically several orders of
magnitude more legitimate transactions than fraudulent ones. In this paper, we
present Deep Symbolic Classification (DSC), an extension of the Deep Symbolic
Regression framework to classification problems. DSC casts classification as a
search problem in the space of all analytic functions composed of a vocabulary
of variables, constants, and operations and optimizes for an arbitrary
evaluation metric directly. The search is guided by a deep neural network
trained with reinforcement learning. Because the functions are mathematical
expressions that are in closed-form and concise, the model is inherently
explainable both at the level of a single classification decision and the
model&apos;s decision process. Furthermore, the class imbalance problem is
successfully addressed by optimizing for metrics that are robust to class
imbalance such as the F1 score. This eliminates the need for oversampling and
undersampling techniques that plague traditional approaches. Finally, the model
allows to explicitly balance between the prediction accuracy and the
explainability. An evaluation on the PaySim data set demonstrates competitive
predictive performance with state-of-the-art models, while surpassing them in
terms of explainability. This establishes DSC as a promising model for fraud
detection systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Visbeek_S/0/1/0/all/0/1&quot;&gt;Samantha Visbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acar_E/0/1/0/all/0/1&quot;&gt;Erman Acar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hengst_F/0/1/0/all/0/1&quot;&gt;Floris den Hengst&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00591">
<title>Less is More: Learning Reference Knowledge Using No-Reference Image Quality Assessment. (arXiv:2312.00591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00591</link>
<description rdf:parseType="Literal">&lt;p&gt;Image Quality Assessment (IQA) with reference images have achieved great
success by imitating the human vision system, in which the image quality is
effectively assessed by comparing the query image with its pristine reference
image. However, for the images in the wild, it is quite difficult to access
accurate reference images. We argue that it is possible to learn reference
knowledge under the No-Reference Image Quality Assessment (NR-IQA) setting,
which is effective and efficient empirically. Concretely, by innovatively
introducing a novel feature distillation method in IQA, we propose a new
framework to learn comparative knowledge from non-aligned reference images. And
then, to achieve fast convergence and avoid overfitting, we further propose an
inductive bias regularization. Such a framework not only solves the congenital
defects of NR-IQA but also improves the feature extraction framework, enabling
it to express more abundant quality information. Surprisingly, our method
utilizes less input while obtaining a more significant improvement compared to
the teacher models. Extensive experiments on eight standard NR-IQA datasets
demonstrate the superior performance to the state-of-the-art NR-IQA methods,
i.e., achieving the PLCC values of 0.917 (vs. 0.884 in LIVEC) and 0.686 (vs.
0.661 in LIVEFB).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jingyuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Runze Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yutao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1&quot;&gt;Pingyang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00596">
<title>BCN: Batch Channel Normalization for Image Classification. (arXiv:2312.00596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;Normalization techniques have been widely used in the field of deep learning
due to their capability of enabling higher learning rates and are less careful
in initialization. However, the effectiveness of popular normalization
technologies is typically limited to specific areas. Unlike the standard Batch
Normalization (BN) and Layer Normalization (LN), where BN computes the mean and
variance along the (N,H,W) dimensions and LN computes the mean and variance
along the (C,H,W) dimensions (N, C, H and W are the batch, channel, spatial
height and width dimension, respectively), this paper presents a novel
normalization technique called Batch Channel Normalization (BCN). To exploit
both the channel and batch dependence and adaptively and combine the advantages
of BN and LN based on specific datasets or tasks, BCN separately normalizes
inputs along the (N, H, W) and (C, H, W) axes, then combines the normalized
outputs based on adaptive parameters. As a basic block, BCN can be easily
integrated into existing models for various applications in the field of
computer vision. Empirical results show that the proposed technique can be
seamlessly applied to various versions of CNN or Vision Transformer
architecture. The code is publicly available at
https://github.com/AfifaKhaled/BatchChannel-Normalization
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaled_A/0/1/0/all/0/1&quot;&gt;Afifa Khaled&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jia Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00597">
<title>UAVs and Birds: Enhancing Short-Range Navigation through Budgerigar Flight Studies. (arXiv:2312.00597v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.00597</link>
<description rdf:parseType="Literal">&lt;p&gt;This study delves into the flight behaviors of Budgerigars (Melopsittacus
undulatus) to gain insights into their flight trajectories and movements. Using
3D reconstruction from stereo video camera recordings, we closely examine the
velocity and acceleration patterns during three flight motion takeoff, flying
and landing. The findings not only contribute to our understanding of bird
behaviors but also hold significant implications for the advancement of
algorithms in Unmanned Aerial Vehicles (UAVs). The research aims to bridge the
gap between biological principles observed in birds and the application of
these insights in developing more efficient and autonomous UAVs. In the context
of the increasing use of drones, this study focuses on the biologically
inspired principles drawn from bird behaviors, particularly during takeoff,
flying and landing flight, to enhance UAV capabilities. The dataset created for
this research sheds light on Budgerigars&apos; takeoff, flying, and landing
techniques, emphasizing their ability to control speed across different
situations and surfaces. The study underscores the potential of incorporating
these principles into UAV algorithms, addressing challenges related to
short-range navigation, takeoff, flying, and landing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md. Mahmudur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1&quot;&gt;Sajid Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Showren Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeba_S/0/1/0/all/0/1&quot;&gt;Sadia Jahan Zeba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmaker_D/0/1/0/all/0/1&quot;&gt;Debajyoti Karmaker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00598">
<title>Learning from One Continuous Video Stream. (arXiv:2312.00598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00598</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a framework for online learning from a single continuous video
stream -- the way people and animals learn, without mini-batches, data
augmentation or shuffling. This poses great challenges given the high
correlation between consecutive video frames and there is very little prior
work on it. Our framework allows us to do a first deep dive into the topic and
includes a collection of streams and tasks composed from two existing video
datasets, plus methodology for performance evaluation that considers both
adaptation and generalization. We employ pixel-to-pixel modelling as a
practical and flexible way to switch between pre-training and single-stream
evaluation as well as between arbitrary tasks, without ever requiring changes
to models and always using the same pixel loss. Equipped with this framework we
obtained large single-stream learning gains from pre-training with a novel
family of future prediction tasks, found that momentum hurts, and that the pace
of weight updates matters. The combination of these insights leads to matching
the performance of IID learning with batch size 1, when using the same
architecture and without costly replay buffers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_M/0/1/0/all/0/1&quot;&gt;Michael King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica P&amp;#x103;tr&amp;#x103;ucean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokay_D/0/1/0/all/0/1&quot;&gt;Dilara Gokay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_C/0/1/0/all/0/1&quot;&gt;C&amp;#x103;t&amp;#x103;lin Ionescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1&quot;&gt;Daniel Zoran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1&quot;&gt;Joseph Heyward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doersch_C/0/1/0/all/0/1&quot;&gt;Carl Doersch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aytar_Y/0/1/0/all/0/1&quot;&gt;Yusuf Aytar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1&quot;&gt;Dima Damen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00621">
<title>Weighted Riesz Particles. (arXiv:2312.00621v1 [stat.CO])</title>
<link>http://arxiv.org/abs/2312.00621</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov chain Monte Carlo (MCMC) methods are simulated by local exploration of
complex statistical distributions, and while bypassing the cumbersome
requirement of a specific analytical expression for the target, this stochastic
exploration of an uncertain parameter space comes at the expense of a large
number of samples, and this computational complexity increases with parameter
dimensionality. Although at the exploration level, some methods are proposed to
accelerate the convergence of the algorithm, such as tempering, Hamiltonian
Monte Carlo, Rao-redwellization, and scalable methods for better performance,
it cannot avoid the stochastic nature of this exploration. We consider the
target distribution as a mapping where the infinite-dimensional Eulerian space
of the parameters consists of a number of deterministic submanifolds and
propose a generalized energy metric, termed weighted Riesz energy, where a
number of points is generated through pairwise interactions, to discretize
rectifiable submanifolds. We study the properties of the point, called Riesz
particle, and embed it into sequential MCMC, and we find that there will be
higher acceptance rates with fewer evaluations, we validate it through
experimental comparative analysis from a linear Gaussian state-space model with
synthetic data and a non-linear stochastic volatility model with real-world
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiongming Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baumgartner_G/0/1/0/all/0/1&quot;&gt;Gerald Baumgartner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00633">
<title>Towards Efficient 3D Object Detection in Bird&apos;s-Eye-View Space for Autonomous Driving: A Convolutional-Only Approach. (arXiv:2312.00633v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00633</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection in Bird&apos;s-Eye-View (BEV) space has recently emerged as a
prevalent approach in the field of autonomous driving. Despite the demonstrated
improvements in accuracy and velocity estimation compared to perspective view
methods, the deployment of BEV-based techniques in real-world autonomous
vehicles remains challenging. This is primarily due to their reliance on
vision-transformer (ViT) based architectures, which introduce quadratic
complexity with respect to the input resolution. To address this issue, we
propose an efficient BEV-based 3D detection framework called BEVENet, which
leverages a convolutional-only architectural design to circumvent the
limitations of ViT models while maintaining the effectiveness of BEV-based
methods. Our experiments show that BEVENet is 3$\times$ faster than
contemporary state-of-the-art (SOTA) approaches on the NuScenes challenge,
achieving a mean average precision (mAP) of 0.456 and a nuScenes detection
score (NDS) of 0.555 on the NuScenes validation dataset, with an inference
speed of 47.6 frames per second. To the best of our knowledge, this study
stands as the first to achieve such significant efficiency improvements for
BEV-based methods, highlighting their enhanced feasibility for real-world
autonomous driving applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mengying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Chaikiat Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zihang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nini Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsuanhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00651">
<title>TrackDiffusion: Multi-object Tracking Data Generation via Diffusion Models. (arXiv:2312.00651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00651</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have gained prominence in generating data for perception
tasks such as image classification and object detection. However, the potential
in generating high-quality tracking sequences, a crucial aspect in the field of
video perception, has not been fully investigated. To address this gap, we
propose TrackDiffusion, a novel architecture designed to generate continuous
video sequences from the tracklets. TrackDiffusion represents a significant
departure from the traditional layout-to-image (L2I) generation and copy-paste
synthesis focusing on static image elements like bounding boxes by empowering
image diffusion models to encompass dynamic and continuous tracking
trajectories, thereby capturing complex motion nuances and ensuring instance
consistency among video frames. For the first time, we demonstrate that the
generated video sequences can be utilized for training multi-object tracking
(MOT) systems, leading to significant improvement in tracker performance.
Experimental results show that our model significantly enhances instance
consistency in generated video sequences, leading to improved perceptual
metrics. Our approach achieves an improvement of 8.7 in TrackAP and 11.8 in
TrackAP$_{50}$ on the YTVIS dataset, underscoring its potential to redefine the
standards of video data generation for MOT tasks and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pengxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhili Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lanqing Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuge_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Zhuge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xu Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00656">
<title>Simple Transferability Estimation for Regression Tasks. (arXiv:2312.00656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00656</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider transferability estimation, the problem of estimating how well
deep learning models transfer from a source to a target task. We focus on
regression tasks, which received little previous attention, and propose two
simple and computationally efficient approaches that estimate transferability
based on the negative regularized mean squared error of a linear regression
model. We prove novel theoretical results connecting our approaches to the
actual transferability of the optimal target models obtained from the transfer
learning process. Despite their simplicity, our approaches significantly
outperform existing state-of-the-art regression transferability estimators in
both accuracy and efficiency. On two large-scale keypoint regression
benchmarks, our approaches yield 12% to 36% better results on average while
being at least 27% faster than previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong N. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_P/0/1/0/all/0/1&quot;&gt;Phong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_L/0/1/0/all/0/1&quot;&gt;Lam Si Tung Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_V/0/1/0/all/0/1&quot;&gt;Vu Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;Anh T. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1&quot;&gt;Tal Hassner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Cuong V. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00660">
<title>Resource-constrained knowledge diffusion processes inspired by human peer learning. (arXiv:2312.00660v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00660</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a setting where a population of artificial learners is given, and
the objective is to optimize aggregate measures of performance, under
constraints on training resources. The problem is motivated by the study of
peer learning in human educational systems. In this context, we study natural
knowledge diffusion processes in networks of interacting artificial learners.
By `natural&apos;, we mean processes that reflect human peer learning where the
students&apos; internal state and learning process is mostly opaque, and the main
degree of freedom lies in the formation of peer learning groups by a
coordinator who can potentially evaluate the learners before assigning them to
peer groups. Among else, we empirically show that such processes indeed make
effective use of the training resources, and enable the design of modular
neural models that have the capacity to generalize without being prone to
overfitting noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beikihassan_E/0/1/0/all/0/1&quot;&gt;Ehsan Beikihassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoover_A/0/1/0/all/0/1&quot;&gt;Amy K.Hoover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutis_I/0/1/0/all/0/1&quot;&gt;Ioannis Koutis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parviz_A/0/1/0/all/0/1&quot;&gt;Ali Parviz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghaieabiane_N/0/1/0/all/0/1&quot;&gt;Niloofar Aghaieabiane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00688">
<title>Towards Transparency in Coreference Resolution: A Quantum-Inspired Approach. (arXiv:2312.00688v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00688</link>
<description rdf:parseType="Literal">&lt;p&gt;Guided by grammatical structure, words compose to form sentences, and guided
by discourse structure, sentences compose to form dialogues and documents. The
compositional aspect of sentence and discourse units is often overlooked by
machine learning algorithms. A recent initiative called Quantum Natural
Language Processing (QNLP) learns word meanings as points in a Hilbert space
and acts on them via a translation of grammatical structure into Parametrised
Quantum Circuits (PQCs). Previous work extended the QNLP translation to
discourse structure using points in a closure of Hilbert spaces. In this paper,
we evaluate this translation on a Winograd-style pronoun resolution task. We
train a Variational Quantum Classifier (VQC) for binary classification and
implement an end-to-end pronoun resolution system. The simulations executed on
IBMQ software converged with an F1 score of 87.20%. The model outperformed two
out of three classical coreference resolution systems and neared
state-of-the-art SpanBERT. A mixed quantum-classical model yet improved these
results with an F1 score increase of around 6%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wazni_H/0/1/0/all/0/1&quot;&gt;Hadi Wazni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1&quot;&gt;Mehrnoosh Sadrzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00718">
<title>Removing Biases from Molecular Representations via Information Maximization. (arXiv:2312.00718v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00718</link>
<description rdf:parseType="Literal">&lt;p&gt;High-throughput drug screening -- using cell imaging or gene expression
measurements as readouts of drug effect -- is a critical tool in biotechnology
to assess and understand the relationship between the chemical structure and
biological activity of a drug. Since large-scale screens have to be divided
into multiple experiments, a key difficulty is dealing with batch effects,
which can introduce systematic errors and non-biological associations in the
data. We propose InfoCORE, an Information maximization approach for COnfounder
REmoval, to effectively deal with batch effects and obtain refined molecular
representations. InfoCORE establishes a variational lower bound on the
conditional mutual information of the latent representations given a batch
identifier. It adaptively reweighs samples to equalize their implied batch
distribution. Extensive experiments on drug screening data reveal InfoCORE&apos;s
superior performance in a multitude of tasks including molecular property
prediction and molecule-phenotype retrieval. Additionally, we show results for
how InfoCORE offers a versatile framework and resolves general distribution
shifts and issues of data fairness by minimizing correlation with spurious
features or removing sensitive attributes. The code is available at
https://github.com/uhlerlab/InfoCORE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sharut Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1&quot;&gt;Caroline Uhler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1&quot;&gt;Tommi Jaakkola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00727">
<title>Safe Reinforcement Learning in Tensor Reproducing Kernel Hilbert Space. (arXiv:2312.00727v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into the problem of safe reinforcement learning (RL) in a
partially observable environment with the aim of achieving safe-reachability
objectives. In traditional partially observable Markov decision processes
(POMDP), ensuring safety typically involves estimating the belief in latent
states. However, accurately estimating an optimal Bayesian filter in POMDP to
infer latent states from observations in a continuous state space poses a
significant challenge, largely due to the intractable likelihood. To tackle
this issue, we propose a stochastic model-based approach that guarantees RL
safety almost surely in the face of unknown system dynamics and partial
observation environments. We leveraged the Predictive State Representation
(PSR) and Reproducing Kernel Hilbert Space (RKHS) to represent future
multi-step observations analytically, and the results in this context are
provable. Furthermore, we derived essential operators from the kernel Bayes&apos;
rule, enabling the recursive estimation of future observations using various
operators. Under the assumption of \textit{undercompleness}, a polynomial
sample complexity is established for the RL algorithm for the infinite size of
observation and action spaces, ensuring an $\epsilon-$suboptimal safe policy
guarantee.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1&quot;&gt;Liz Varga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yukun Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00732">
<title>Gaussian Grouping: Segment and Edit Anything in 3D Scenes. (arXiv:2312.00732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent Gaussian Splatting achieves high-quality and real-time novel-view
synthesis of the 3D scenes. However, it is solely concentrated on the
appearance and geometry modeling, while lacking in fine-grained object-level
scene understanding. To address this issue, we propose Gaussian Grouping, which
extends Gaussian Splatting to jointly reconstruct and segment anything in
open-world 3D scenes. We augment each Gaussian with a compact Identity
Encoding, allowing the Gaussians to be grouped according to their object
instance or stuff membership in the 3D scene. Instead of resorting to expensive
3D labels, we supervise the Identity Encodings during the differentiable
rendering by leveraging the 2D mask predictions by SAM, along with introduced
3D spatial consistency regularization. Comparing to the implicit NeRF
representation, we show that the discrete and grouped 3D Gaussians can
reconstruct, segment and edit anything in 3D with high visual quality, fine
granularity and efficiency. Based on Gaussian Grouping, we further propose a
local Gaussian Editing scheme, which shows efficacy in versatile scene editing
applications, including 3D object removal, inpainting, colorization and scene
recomposition. Our code and models will be at
https://github.com/lkeab/gaussian-grouping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mingqiao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1&quot;&gt;Martin Danelljan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Lei Ke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00742">
<title>Scalable Meta-Learning with Gaussian Processes. (arXiv:2312.00742v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.00742</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning is a powerful approach that exploits historical data to quickly
solve new tasks from the same distribution. In the low-data regime, methods
based on the closed-form posterior of Gaussian processes (GP) together with
Bayesian optimization have achieved high performance. However, these methods
are either computationally expensive or introduce assumptions that hinder a
principled propagation of uncertainty between task models. This may disrupt the
balance between exploration and exploitation during optimization. In this
paper, we develop ScaML-GP, a modular GP model for meta-learning that is
scalable in the number of tasks. Our core contribution is a carefully designed
multi-task kernel that enables hierarchical training and task scalability.
Conditioning ScaML-GP on the meta-data exposes its modular nature yielding a
test-task prior that combines the posteriors of meta-task GPs. In synthetic and
real-world meta-learning experiments, we demonstrate that ScaML-GP can learn
efficiently both with few and many meta-tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tighineanu_P/0/1/0/all/0/1&quot;&gt;Petru Tighineanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grossberger_L/0/1/0/all/0/1&quot;&gt;Lukas Grossberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Baireuther_P/0/1/0/all/0/1&quot;&gt;Paul Baireuther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Skubch_K/0/1/0/all/0/1&quot;&gt;Kathrin Skubch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Falkner_S/0/1/0/all/0/1&quot;&gt;Stefan Falkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vinogradska_J/0/1/0/all/0/1&quot;&gt;Julia Vinogradska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Berkenkamp_F/0/1/0/all/0/1&quot;&gt;Felix Berkenkamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00746">
<title>Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games. (arXiv:2312.00746v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.00746</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore the application of Large Language Models (LLMs) in
&quot;Jubensha&quot; (Chinese murder mystery role-playing games), a novel area in
AI-driven gaming. We introduce the first Chinese dataset specifically for
Jubensha, including character scripts and game rules, to foster AI agent
development in this complex narrative environment. Our work also presents a
unique multi-agent interaction framework using LLMs, allowing AI agents to
autonomously engage in the game, enhancing the dynamics of Jubensha gameplay.
To evaluate these AI agents, we developed specialized methods targeting their
mastery of case information and reasoning skills. Furthermore, we incorporated
the latest advancements in in-context learning to improve the agents&apos;
performance in critical aspects like information gathering, murderer detection,
and logical reasoning. The experimental results validate the effectiveness of
our proposed methods. This work aims to offer a fresh perspective on
understanding LLM capabilities and establish a new benchmark for evaluating
large language model-based agents to researchers in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dekun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haochen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00751">
<title>Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals. (arXiv:2312.00751v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.00751</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have achieved remarkable success in a wide range of natural
language processing and computer vision applications. However, the
representation capacity of a deep transformer model is degraded due to the
over-smoothing issue in which the token representations become identical when
the model&apos;s depth grows. In this work, we show that self-attention layers in
transformers minimize a functional which promotes smoothness, thereby causing
token uniformity. We then propose a novel regularizer that penalizes the norm
of the difference between the smooth output tokens from self-attention and the
input tokens to preserve the fidelity of the tokens. Minimizing the resulting
regularized energy functional, we derive the Neural Transformer with a
Regularized Nonlocal Functional (NeuTRENO), a novel class of transformer models
that can mitigate the over-smoothing issue. We empirically demonstrate the
advantages of NeuTRENO over the baseline transformers and state-of-the-art
methods in reducing the over-smoothing of token representations on various
practical tasks, including object classification, image segmentation, and
language modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tam Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard G. Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/0902.3430">
<title>Domain Adaptation: Learning Bounds and Algorithms. (arXiv:0902.3430v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/0902.3430</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the general problem of domain adaptation which arises in
a variety of applications where the distribution of the labeled sample
available somewhat differs from that of the test data. Building on previous
work by Ben-David et al. (2007), we introduce a novel distance between
distributions, discrepancy distance, that is tailored to adaptation problems
with arbitrary loss functions. We give Rademacher complexity bounds for
estimating the discrepancy distance from finite samples for different loss
functions. Using this distance, we derive novel generalization bounds for
domain adaptation for a wide family of loss functions. We also present a series
of novel adaptation bounds for large classes of regularization-based
algorithms, including support vector machines and kernel ridge regression based
on the empirical discrepancy. This motivates our analysis of the problem of
minimizing the empirical discrepancy for various loss functions for which we
also give novel algorithms. We report the results of preliminary experiments
that demonstrate the benefits of our discrepancy minimization algorithms for
domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1&quot;&gt;Yishay Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohri_M/0/1/0/all/0/1&quot;&gt;Mehryar Mohri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1&quot;&gt;Afshin Rostamizadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.15920">
<title>Interpreting and Disentangling Feature Components of Various Complexity from DNNs. (arXiv:2006.15920v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.15920</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to define, quantify, and analyze the feature complexity that
is learned by a DNN. We propose a generic definition for the feature
complexity. Given the feature of a certain layer in the DNN, our method
disentangles feature components of different complexity orders from the
feature. We further design a set of metrics to evaluate the reliability, the
effectiveness, and the significance of over-fitting of these feature
components. Furthermore, we successfully discover a close relationship between
the feature complexity and the performance of DNNs. As a generic mathematical
tool, the feature complexity and the proposed metrics can also be used to
analyze the success of network compression and knowledge distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zexu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.04055">
<title>A Unified Approach to Interpreting and Boosting Adversarial Transferability. (arXiv:2010.04055v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.04055</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we use the interaction inside adversarial perturbations to
explain and boost the adversarial transferability. We discover and prove the
negative correlation between the adversarial transferability and the
interaction inside adversarial perturbations. The negative correlation is
further verified through different DNNs with various inputs. Moreover, this
negative correlation can be regarded as a unified perspective to understand
current transferability-boosting methods. To this end, we prove that some
classic methods of enhancing the transferability essentially decease
interactions inside adversarial perturbations. Based on this, we propose to
directly penalize interactions during the attacking process, which
significantly improves the adversarial transferability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shuyun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yisen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.04923">
<title>Topological properties of basins of attraction and expressiveness of width bounded neural networks. (arXiv:2011.04923v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;In Radhakrishnan et al. [2020], the authors empirically show that
autoencoders trained with usual SGD methods shape out basins of attraction
around their training data. We consider network functions of width not
exceeding the input dimension and prove that in this situation basins of
attraction are bounded and their complement cannot have bounded components. Our
conditions in these results are met in several experiments of the latter work
and we thus address a question posed therein. We also show that under some more
restrictive conditions the basins of attraction are path-connected. The
tightness of the conditions in our results is demonstrated by means of several
examples. Finally, the arguments used to prove the above results allow us to
derive a root cause why scalar-valued neural network functions that fulfill our
bounded width condition are not dense in spaces of continuous functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beise_H/0/1/0/all/0/1&quot;&gt;Hans-Peter Beise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_S/0/1/0/all/0/1&quot;&gt;Steve Dias Da Cruz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.10793">
<title>PyTorch Geometric Signed Directed: A Software Package on Graph Neural Networks for Signed and Directed Graphs. (arXiv:2202.10793v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.10793</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks are ubiquitous in many real-world applications (e.g., social
networks encoding trust/distrust relationships, correlation networks arising
from time series data). While many networks are signed or directed, or both,
there is a lack of unified software packages on graph neural networks (GNNs)
specially designed for signed and directed networks. In this paper, we present
PyTorch Geometric Signed Directed (PyGSD), a software package which fills this
gap. Along the way, we evaluate the implemented methods with experiments with a
view to providing insights into which method to choose for a given task. The
deep learning framework consists of easy-to-use GNN models, synthetic and
real-world data, as well as task-specific evaluation metrics and loss functions
for signed and directed networks. As an extension library for PyG, our proposed
software is maintained with open-source releases, detailed documentation,
continuous integration, unit tests and code coverage checks. The GitHub
repository of the library is
https://github.com/SherylHYX/pytorch_geometric_signed_directed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yixuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rozemberczki_B/0/1/0/all/0/1&quot;&gt;Benedek Rozemberczki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1&quot;&gt;Mihai Cucuringu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinert_G/0/1/0/all/0/1&quot;&gt;Gesine Reinert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01122">
<title>BIASeD: Bringing Irrationality into Automated System Design. (arXiv:2210.01122v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01122</link>
<description rdf:parseType="Literal">&lt;p&gt;Human perception, memory and decision-making are impacted by tens of
cognitive biases and heuristics that influence our actions and decisions.
Despite the pervasiveness of such biases, they are generally not leveraged by
today&apos;s Artificial Intelligence (AI) systems that model human behavior and
interact with humans. In this theoretical paper, we claim that the future of
human-machine collaboration will entail the development of AI systems that
model, understand and possibly replicate human cognitive biases. We propose the
need for a research agenda on the interplay between human cognitive biases and
Artificial Intelligence. We categorize existing cognitive biases from the
perspective of AI systems, identify three broad areas of interest and outline
research directions for the design of AI systems that have a better
understanding of our own biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1&quot;&gt;Aditya Gulati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_M/0/1/0/all/0/1&quot;&gt;Miguel Angel Lozano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_N/0/1/0/all/0/1&quot;&gt;Nuria Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09020">
<title>Defects of Convolutional Decoder Networks in Frequency Representation. (arXiv:2210.09020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09020</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we prove the representation defects of a cascaded
convolutional decoder network, considering the capacity of representing
different frequency components of an input sample. We conduct the discrete
Fourier transform on each channel of the feature map in an intermediate layer
of the decoder network. Then, we extend the 2D circular convolution theorem to
represent the forward and backward propagations through convolutional layers in
the frequency domain. Based on this, we prove three defects in representing
feature spectrums. First, we prove that the convolution operation, the
zero-padding operation, and a set of other settings all make a convolutional
decoder network more likely to weaken high-frequency components. Second, we
prove that the upsampling operation generates a feature spectrum, in which
strong signals repetitively appear at certain frequencies. Third, we prove that
if the frequency components in the input sample and frequency components in the
target output for regression have a small shift, then the decoder usually
cannot be effectively learned.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Ling Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06108">
<title>RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection Systems. (arXiv:2211.06108v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06108</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving, LiDAR and radar play important roles in the perception
of the surrounding environment. LiDAR provides accurate 3D spatial sensing
information but cannot work in adverse weather like fog. On the other hand, the
radar signal can be diffracted when encountering raindrops or mist particles
thanks to its wavelength, but it suffers from large noise. Recent
state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust
detection in adverse weather. The existing works adopt convolutional neural
network architecture to extract features from each sensor data, then align and
aggregate the two branch features to predict object detection results. However,
these methods have low accuracy of bounding box estimations due to a simple
design of label assignment and fusion strategies. In this paper, we propose a
bird&apos;s-eye view fusion learning-based anchor box-free object detection system,
which fuses the feature derived from the radar range-azimuth heatmap and the
LiDAR point cloud to estimate possible objects. Different label assignment
strategies have been designed to facilitate the consistency between the
classification of foreground or background anchor points and the corresponding
bounding box regressions. Furthermore, the performance of the proposed object
detector is further enhanced by employing a novel interactive transformer
module. The superior performance of the methods proposed in this paper has been
demonstrated using the recently published Oxford Radar RobotCar dataset. Our
system&apos;s average precision significantly outperforms the state-of-the-art
method by 13.1% and 19.0% at IoU of 0.8 under &apos;Clear+Foggy&apos; training conditions
for &apos;Clear&apos; and &apos;Foggy&apos; testing, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanlong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07099">
<title>Adaptive Deep Neural Network Inference Optimization with EENet. (arXiv:2301.07099v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07099</link>
<description rdf:parseType="Literal">&lt;p&gt;Well-trained deep neural networks (DNNs) treat all test samples equally
during prediction. Adaptive DNN inference with early exiting leverages the
observation that some test examples can be easier to predict than others. This
paper presents EENet, a novel early-exiting scheduling framework for multi-exit
DNN models. Instead of having every sample go through all DNN layers during
prediction, EENet learns an early exit scheduler, which can intelligently
terminate the inference earlier for certain predictions, which the model has
high confidence of early exit. As opposed to previous early-exiting solutions
with heuristics-based methods, our EENet framework optimizes an early-exiting
policy to maximize model accuracy while satisfying the given per-sample average
inference budget. Extensive experiments are conducted on four computer vision
datasets (CIFAR-10, CIFAR-100, ImageNet, Cityscapes) and two NLP datasets
(SST-2, AgNews). The results demonstrate that the adaptive inference by EENet
can outperform the representative existing early exit techniques. We also
perform a detailed visualization analysis of the comparison results to
interpret the benefits of EENet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilhan_F/0/1/0/all/0/1&quot;&gt;Fatih Ilhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_K/0/1/0/all/0/1&quot;&gt;Ka-Ho Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Sihao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiansheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekin_S/0/1/0/all/0/1&quot;&gt;Selim Tekin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wenqi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Myungjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kompella_R/0/1/0/all/0/1&quot;&gt;Ramana Kompella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gaowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Ling Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09995">
<title>Selectively Providing Reliance Calibration Cues With Reliance Prediction. (arXiv:2302.09995v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09995</link>
<description rdf:parseType="Literal">&lt;p&gt;For effective collaboration between humans and intelligent agents that employ
machine learning for decision-making, humans must understand what agents can
and cannot do to avoid over/under-reliance. A solution to this problem is
adjusting human reliance through communication using reliance calibration cues
(RCCs) to help humans assess agents&apos; capabilities. Previous studies typically
attempted to calibrate reliance by continuously presenting RCCs, and when an
agent should provide RCCs remains an open question. To answer this, we propose
Pred-RC, a method for selectively providing RCCs. Pred-RC uses a cognitive
reliance model to predict whether a human will assign a task to an agent. By
comparing the prediction results for both cases with and without an RCC,
Pred-RC evaluates the influence of the RCC on human reliance. We tested Pred-RC
in a human-AI collaboration task and found that it can successfully calibrate
human reliance with a reduced number of RCCs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukuchi_Y/0/1/0/all/0/1&quot;&gt;Yosuke Fukuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_S/0/1/0/all/0/1&quot;&gt;Seiji Yamada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13080">
<title>Does a Neural Network Really Encode Symbolic Concepts?. (arXiv:2302.13080v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13080</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a series of studies have tried to extract interactions between
input variables modeled by a DNN and define such interactions as concepts
encoded by the DNN. However, strictly speaking, there still lacks a solid
guarantee whether such interactions indeed represent meaningful concepts.
Therefore, in this paper, we examine the trustworthiness of interaction
concepts from four perspectives. Extensive empirical studies have verified that
a well-trained DNN usually encodes sparse, transferable, and discriminative
concepts, which is partially aligned with human intuition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13095">
<title>Bayesian Neural Networks Avoid Encoding Complex and Perturbation-Sensitive Concepts. (arXiv:2302.13095v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13095</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we focus on mean-field variational Bayesian Neural Networks
(BNNs) and explore the representation capacity of such BNNs by investigating
which types of concepts are less likely to be encoded by the BNN. It has been
observed and studied that a relatively small set of interactive concepts
usually emerge in the knowledge representation of a sufficiently-trained neural
network, and such concepts can faithfully explain the network output. Based on
this, our study proves that compared to standard deep neural networks (DNNs),
it is less likely for BNNs to encode complex concepts. Experiments verify our
theoretical proofs. Note that the tendency to encode less complex concepts does
not necessarily imply weak representation power, considering that complex
concepts exhibit low generalization power and high adversarial vulnerability.
The code is available at https://github.com/sjtu-xai-lab/BNN-concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qihan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1&quot;&gt;Siyu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16767">
<title>A Novel Patent Similarity Measurement Methodology: Semantic Distance and Technological Distance. (arXiv:2303.16767v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16767</link>
<description rdf:parseType="Literal">&lt;p&gt;Patent similarity analysis plays a crucial role in evaluating the risk of
patent infringement. Nonetheless, this analysis is predominantly conducted
manually by legal experts, often resulting in a time-consuming process. Recent
advances in natural language processing technology offer a promising avenue for
automating this process. However, methods for measuring similarity between
patents still rely on experts manually classifying patents. Due to the recent
development of artificial intelligence technology, a lot of research is being
conducted focusing on the semantic similarity of patents using natural language
processing technology. However, it is difficult to accurately analyze patent
data, which are legal documents representing complex technologies, using
existing natural language processing technologies. To address these
limitations, we propose a hybrid methodology that takes into account
bibliographic similarity, measures the similarity between patents by
considering the semantic similarity of patents, the technical similarity
between patents, and the bibliographic information of patents. Using natural
language processing techniques, we measure semantic similarity based on patent
text and calculate technical similarity through the degree of coexistence of
International patent classification (IPC) codes. The similarity of
bibliographic information of a patent is calculated using the special
characteristics of the patent: citation information, inventor information, and
assignee information. We propose a model that assigns reasonable weights to
each similarity method considered. With the help of experts, we performed
manual similarity evaluations on 420 pairs and evaluated the performance of our
model based on this data. We have empirically shown that our method outperforms
recent natural language processing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1&quot;&gt;Yongmin Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Cheonkam Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gim_S/0/1/0/all/0/1&quot;&gt;Sanguk Gim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schimke_Z/0/1/0/all/0/1&quot;&gt;Zachary Schimke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1&quot;&gt;Deaho Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01811">
<title>HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;The Shapley value is widely regarded as a trustworthy attribution metric.
However, when people use Shapley values to explain the attribution of input
variables of a deep neural network (DNN), it usually requires a very high
computational cost to approximate relatively accurate Shapley values in
real-world applications. Therefore, we propose a novel network architecture,
the HarsanyiNet, which makes inferences on the input sample and simultaneously
computes the exact Shapley values of the input variables in a single forward
propagation. The HarsanyiNet is designed on the theoretical foundation that the
Shapley value can be reformulated as the redistribution of Harsanyi
interactions encoded by the network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1&quot;&gt;Siyu Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Keyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Quanshi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06767">
<title>RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06767</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hanze Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_D/0/1/0/all/0/1&quot;&gt;Deepanshu Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_W/0/1/0/all/0/1&quot;&gt;Winnie Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1&quot;&gt;Rui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1&quot;&gt;Shizhe Diao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1&quot;&gt;Kashun Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10703">
<title>ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10703</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-step reasoning ability is fundamental to many natural language tasks,
yet it is unclear what constitutes a good reasoning chain and how to evaluate
them. Most existing methods focus solely on whether the reasoning chain leads
to the correct conclusion, but this answer-oriented view may confound reasoning
quality with other spurious shortcuts to predict the answer. To bridge this
gap, we evaluate reasoning chains by viewing them as informal proofs that
derive the final answer. Specifically, we propose ReCEval (Reasoning Chain
Evaluation), a framework that evaluates reasoning chains via two key
properties: (1) correctness, i.e., each step makes a valid inference based on
information contained within the step, preceding steps, and input context, and
(2) informativeness, i.e., each step provides new information that is helpful
towards deriving the generated answer. We evaluate these properties by
developing metrics using natural language inference models and V-Information.
On multiple datasets, we show that ReCEval effectively identifies various error
types and yields notable improvements compared to prior methods. We analyze the
impact of step boundaries, and previous steps on evaluating correctness and
demonstrate that our informativeness metric captures the expected flow of
information in high-quality reasoning chains. Finally, we show that scoring
reasoning chains based on ReCEval improves downstream task performance. Our
code is publicly available at: https://github.com/archiki/ReCEval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1&quot;&gt;Archiki Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Swarnadeep Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10752">
<title>Algorithmic Information Forecastability. (arXiv:2304.10752v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10752</link>
<description rdf:parseType="Literal">&lt;p&gt;The outcome of all time series cannot be forecast, e.g. the flipping of a
fair coin. Others, like the repeated {01} sequence {010101...} can be forecast
exactly. Algorithmic information theory can provide a measure of
forecastability that lies between these extremes. The degree of forecastability
is a function of only the data. For prediction (or classification) of labeled
data, we propose three categories for forecastability: oracle forecastability
for predictions that are always exact, precise forecastability for errors up to
a bound, and probabilistic forecastability for any other predictions. Examples
are given in each case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amigo_G/0/1/0/all/0/1&quot;&gt;Glauco Amigo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Pachon_D/0/1/0/all/0/1&quot;&gt;Daniel Andr&amp;#xe9;s D&amp;#xed;az-Pach&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_R/0/1/0/all/0/1&quot;&gt;Robert J. Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baylis_C/0/1/0/all/0/1&quot;&gt;Charles Baylis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17886">
<title>Action valuation of on- and off-ball soccer players based on multi-agent deep reinforcement learning. (arXiv:2305.17886v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17886</link>
<description rdf:parseType="Literal">&lt;p&gt;Analysis of invasive sports such as soccer is challenging because the game
situation changes continuously in time and space, and multiple agents
individually recognize the game situation and make decisions. Previous studies
using deep reinforcement learning have often considered teams as a single agent
and valued the teams and players who hold the ball in each discrete event. Then
it was challenging to value the actions of multiple players, including players
far from the ball, in a spatiotemporally continuous state space. In this paper,
we propose a method of valuing possible actions for on- and off-ball soccer
players in a single holistic framework based on multi-agent deep reinforcement
learning. We consider a discrete action space in a continuous state space that
mimics that of Google research football and leverages supervised learning for
actions in reinforcement learning. In the experiment, we analyzed the
relationships with conventional indicators, season goals, and game ratings by
experts, and showed the effectiveness of the proposed method. Our approach can
assess how multiple players move continuously throughout the game, which is
difficult to be discretized or labeled but vital for teamwork, scouting, and
fan engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakahara_H/0/1/0/all/0/1&quot;&gt;Hiroshi Nakahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsutsui_K/0/1/0/all/0/1&quot;&gt;Kazushi Tsutsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19718">
<title>A rule-general abductive learning by rough sets. (arXiv:2305.19718v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19718</link>
<description rdf:parseType="Literal">&lt;p&gt;In real-world tasks, there is usually a large amount of unlabeled data and
labeled data. The task of combining the two to learn is known as
semi-supervised learning. Experts can use logical rules to label unlabeled
data, but this operation is costly. The combination of perception and reasoning
has a good effect in processing such semi-supervised tasks with domain
knowledge. However, acquiring domain knowledge and the correction, reduction
and generation of rules remain complex problems to be solved. Rough set theory
is an important method for solving knowledge processing in information systems.
In this paper, we propose a rule general abductive learning by rough set
(RS-ABL). By transforming the target concept and sub-concepts of rules into
information tables, rough set theory is used to solve the acquisition of domain
knowledge and the correction, reduction and generation of rules at a lower
cost. This framework can also generate more extensive negative rules to enhance
the breadth of the knowledge base. Compared with the traditional
semi-supervised learning method, RS-ABL has higher accuracy in dealing with
semi-supervised tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xu-chang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hou-biao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03438">
<title>Large Language Models of Code Fail at Completing Code with Potential Bugs. (arXiv:2306.03438v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03438</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models of code (Code-LLMs) have recently brought tremendous
advances to code completion, a fundamental feature of programming assistance
and code intelligence. However, most existing works ignore the possible
presence of bugs in the code context for generation, which are inevitable in
software development. Therefore, we introduce and study the buggy-code
completion problem, inspired by the realistic scenario of real-time code
suggestion where the code context contains potential bugs -- anti-patterns that
can become bugs in the completed program. To systematically study the task, we
introduce two datasets: one with synthetic bugs derived from semantics-altering
operator changes (buggy-HumanEval) and one with realistic bugs derived from
user submissions to coding problems (buggy-FixEval). We find that the presence
of potential bugs significantly degrades the generation performance of the
high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO
on test cases of buggy-HumanEval drop more than 50% given a single potential
bug in the context. Finally, we investigate several post-hoc methods for
mitigating the adverse effect of potential bugs and find that there remains a
significant gap in post-mitigation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_T/0/1/0/all/0/1&quot;&gt;Tuan Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinman Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Samson Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negrinho_R/0/1/0/all/0/1&quot;&gt;Renato Negrinho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lausen_L/0/1/0/all/0/1&quot;&gt;Leonard Lausen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1&quot;&gt;Sheng Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1&quot;&gt;George Karypis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11667">
<title>G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11667</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we focus on the development and implementation of a
comprehensive ensemble of numerical time series forecasting models,
collectively referred to as the Group of Numerical Time Series Prediction Model
(G-NM). This inclusive set comprises traditional models such as Autoregressive
Integrated Moving Average (ARIMA), Holt-Winters&apos; method, and Support Vector
Regression (SVR), in addition to modern neural network models including
Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is
explicitly constructed to augment our predictive capabilities related to
patterns and trends inherent in complex natural phenomena. By utilizing time
series data relevant to these events, G-NM facilitates the prediction of such
phenomena over extended periods. The primary objective of this research is to
both advance our understanding of such occurrences and to significantly enhance
the accuracy of our forecasts. G-NM encapsulates both linear and non-linear
dependencies, seasonalities, and trends present in time series data. Each of
these models contributes distinct strengths, from ARIMA&apos;s resilience in
handling linear trends and seasonality, SVR&apos;s proficiency in capturing
non-linear patterns, to LSTM&apos;s adaptability in modeling various components of
time series data. Through the exploitation of the G-NM potential, we strive to
advance the state-of-the-art in large-scale time series forecasting models. We
anticipate that this research will represent a significant stepping stone in
our ongoing endeavor to comprehend and forecast the complex events that
constitute the natural world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14505">
<title>AME-CAM: Attentive Multiple-Exit CAM for Weakly Supervised Segmentation on MRI Brain Tumor. (arXiv:2306.14505v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14505</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic resonance imaging (MRI) is commonly used for brain tumor
segmentation, which is critical for patient evaluation and treatment planning.
To reduce the labor and expertise required for labeling, weakly-supervised
semantic segmentation (WSSS) methods with class activation mapping (CAM) have
been proposed. However, existing CAM methods suffer from low resolution due to
strided convolution and pooling layers, resulting in inaccurate predictions. In
this study, we propose a novel CAM method, Attentive Multiple-Exit CAM
(AME-CAM), that extracts activation maps from multiple resolutions to
hierarchically aggregate and improve prediction accuracy. We evaluate our
method on the BraTS 2021 dataset and show that it outperforms state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu-Jen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinrong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yiyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Tsung-Yi Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04962">
<title>Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by nodes visited in the environment. We use these proposed
features as rewards for graph neural-network-based reinforcement learning. On
multiple classes of synthetically generated graphs, we find that trained agents
generalize to longer exploratory walks and larger environments than are seen
during training. Our method computes more efficiently than the greedy
evaluation of the relevant topological properties. The proposed intrinsic
motivations bear particular relevance for recommender systems. We demonstrate
that next-node recommendations considering curiosity are more predictive of
human choices than PageRank centrality in several real-world graph
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shubhankar P. Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1&quot;&gt;Mathieu Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1&quot;&gt;Juan Cervino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kieran A. Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Dani S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11046">
<title>A Definition of Continual Reinforcement Learning. (arXiv:2307.11046v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11046</link>
<description rdf:parseType="Literal">&lt;p&gt;In a standard view of the reinforcement learning problem, an agent&apos;s goal is
to efficiently identify a policy that maximizes long-term reward. However, this
perspective is based on a restricted view of learning as finding a solution,
rather than treating learning as endless adaptation. In contrast, continual
reinforcement learning refers to the setting in which the best agents never
stop learning. Despite the importance of continual reinforcement learning, the
community lacks a simple definition of the problem that highlights its
commitments and makes its primary concepts precise and clear. To this end, this
paper is dedicated to carefully defining the continual reinforcement learning
problem. We formalize the notion of agents that &quot;never stop learning&quot; through a
new mathematical language for analyzing and cataloging agents. Using this new
language, we define a continual learning agent as one that can be understood as
carrying out an implicit search process indefinitely, and continual
reinforcement learning as the setting in which the best agents are all
continual learning agents. We provide two motivating examples, illustrating
that traditional views of multi-task reinforcement learning and continual
supervised learning are special cases of our definition. Collectively, these
definitions and perspectives formalize many intuitive concepts at the heart of
learning, and open new research pathways surrounding continual learning agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_D/0/1/0/all/0/1&quot;&gt;David Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Barreto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasselt_H/0/1/0/all/0/1&quot;&gt;Hado van Hasselt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satinder Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13494">
<title>Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v5 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches have
faced the workload drift problem for a long time. Although both data-driven and
hybrid methods are proposed to avoid this problem, most of them suffer from
high training and estimation costs, limited scalability, instability, and
long-tail distribution problems on high-dimensional tables, which seriously
affects the practical application of learned cardinality estimators. In this
paper, we prove that most of these problems are directly caused by the widely
used progressive sampling. We solve this problem by introducing predicate
information into the autoregressive model and propose Duet, a stable,
efficient, and scalable hybrid method to estimate cardinality directly without
sampling or any non-differentiable process, which can not only reduce the
inference complexity from $O(n)$ to $O(1)$ compared to Naru and UAE but also
achieve higher accuracy on high cardinality and high-dimensional tables.
Experimental results show that Duet can achieve all the design goals above and
be much more practical. Besides, Duet even has a lower inference cost on CPU
than that of most learned methods on GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yabin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15090">
<title>Understanding Forward Process of Convolutional Neural Network. (arXiv:2307.15090v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15090</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reveal the selective rotation in the CNNs&apos; forward processing. It
elucidates the activation function as a discerning mechanism that unifies and
quantizes the rotational aspects of the input data. Experiments show how this
defined methodology reflects the progress network distinguish inputs based on
statistical indicators, which can be comprehended or analyzed by applying
structured mathematical tools. Our findings also unveil the consistency between
artificial neural networks and the human brain in their data processing
pattern.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Peixin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00629">
<title>Hessian-Aware Bayesian Optimization for Decision Making Systems. (arXiv:2308.00629v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00629</link>
<description rdf:parseType="Literal">&lt;p&gt;Many approaches for optimizing decision making systems rely on gradient based
methods requiring informative feedback from the environment. However, in the
case where such feedback is sparse or uninformative, such approaches may result
in poor performance. Derivative-free approaches such as Bayesian Optimization
mitigate the dependency on the quality of gradient feedback, but are known to
scale poorly in the high-dimension setting of complex decision making systems.
This problem is exacerbated if the system requires interactions between several
actors cooperating to accomplish a shared goal. To address the dimensionality
challenge, we propose a compact multi-layered architecture modeling the
dynamics of actor interactions through the concept of role. We introduce
Hessian-aware Bayesian Optimization to efficiently optimize the multi-layered
architecture parameterized by a large number of parameters, and give the first
improved regret bound in additive high-dimensional Bayesian Optimization since
Mutny &amp;amp; Krause (2018). Our approach shows strong empirical results under
malformed or sparse reward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpal_M/0/1/0/all/0/1&quot;&gt;Mohit Rajpal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1&quot;&gt;Lac Gia Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yehong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_B/0/1/0/all/0/1&quot;&gt;Bryan Kian Hsiang Low&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16911">
<title>PointLLM: Empowering Large Language Models to Understand Point Clouds. (arXiv:2308.16911v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16911</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented advancements in Large Language Models (LLMs) have shown a
profound impact on natural language processing but are yet to fully embrace the
realm of 3D understanding. This paper introduces PointLLM, a preliminary effort
to fill this gap, enabling LLMs to understand point clouds and offering a new
avenue beyond 2D visual data. PointLLM understands colored object point clouds
with human instructions and generates contextually appropriate responses,
illustrating its grasp of point clouds and common sense. Specifically, it
leverages a point cloud encoder with a powerful LLM to effectively fuse
geometric, appearance, and linguistic information. We collect a novel dataset
comprising 660K simple and 70K complex point-text instruction pairs to enable a
two-stage training strategy: aligning latent spaces and subsequently
instruction-tuning the unified model. To rigorously evaluate the perceptual and
generalization capabilities of PointLLM, we establish two benchmarks:
Generative 3D Object Classification and 3D Object Captioning, assessed through
three different methods, including human evaluation, GPT-4/ChatGPT evaluation,
and traditional metrics. Experimental results reveal PointLLM&apos;s superior
performance over existing 2D and 3D baselines, with a notable achievement in
human-evaluated object captioning tasks where it surpasses human annotators in
over 50% of the samples. Codes, datasets, and benchmarks are available at
https://github.com/OpenRobotLab/PointLLM .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Jiangmiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00267">
<title>RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback. (arXiv:2309.00267v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00267</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback (RLHF) has proven effective in
aligning large language models (LLMs) with human preferences. However,
gathering high-quality human preference labels can be a time-consuming and
expensive endeavor. RL from AI Feedback (RLAIF), introduced by Bai et al.,
offers a promising alternative that leverages a powerful off-the-shelf LLM to
generate preferences in lieu of human annotators. Across the tasks of
summarization, helpful dialogue generation, and harmless dialogue generation,
RLAIF achieves comparable or superior performance to RLHF, as rated by human
evaluators. Furthermore, RLAIF demonstrates the ability to outperform a
supervised fine-tuned baseline even when the LLM preference labeler is the same
size as the policy. In another experiment, directly prompting the LLM for
reward scores achieves superior performance to the canonical RLAIF setup, where
LLM preference labels are first distilled into a reward model. Finally, we
conduct extensive studies on techniques for generating aligned AI preferences.
Our results suggest that RLAIF can achieve human-level performance, offering a
potential solution to the scalability limitations of RLHF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Harrison Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phatale_S/0/1/0/all/0/1&quot;&gt;Samrat Phatale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansoor_H/0/1/0/all/0/1&quot;&gt;Hassan Mansoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesnard_T/0/1/0/all/0/1&quot;&gt;Thomas Mesnard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferret_J/0/1/0/all/0/1&quot;&gt;Johan Ferret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kellie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bishop_C/0/1/0/all/0/1&quot;&gt;Colton Bishop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_E/0/1/0/all/0/1&quot;&gt;Ethan Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbune_V/0/1/0/all/0/1&quot;&gt;Victor Carbune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1&quot;&gt;Abhinav Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sushant Prakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07867">
<title>Beta Diffusion. (arXiv:2309.07867v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce beta diffusion, a novel generative modeling method that
integrates demasking and denoising to generate data within bounded ranges.
Using scaled and shifted beta distributions, beta diffusion utilizes
multiplicative transitions over time to create both forward and reverse
diffusion processes, maintaining beta distributions in both the forward
marginals and the reverse conditionals, given the data at any point in time.
Unlike traditional diffusion-based generative models relying on additive
Gaussian noise and reweighted evidence lower bounds (ELBOs), beta diffusion is
multiplicative and optimized with KL-divergence upper bounds (KLUBs) derived
from the convexity of the KL divergence. We demonstrate that the proposed KLUBs
are more effective for optimizing beta diffusion compared to negative ELBOs,
which can also be derived as the KLUBs of the same KL divergence with its two
arguments swapped. The loss function of beta diffusion, expressed in terms of
Bregman divergence, further supports the efficacy of KLUBs for optimization.
Experimental results on both synthetic data and natural images demonstrate the
unique capabilities of beta diffusion in generative modeling of range-bounded
data and validate the effectiveness of KLUBs in optimizing diffusion models,
thereby making them valuable additions to the family of diffusion-based
generative models and the optimization techniques used to train them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhendong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Huangjie Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12677">
<title>TrTr: A Versatile Pre-Trained Large Traffic Model based on Transformer for Capturing Trajectory Diversity in Vehicle Population. (arXiv:2309.12677v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12677</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding trajectory diversity is a fundamental aspect of addressing
practical traffic tasks. However, capturing the diversity of trajectories
presents challenges, particularly with traditional machine learning and
recurrent neural networks due to the requirement of large-scale parameters. The
emerging Transformer technology, renowned for its parallel computation
capabilities enabling the utilization of models with hundreds of millions of
parameters, offers a promising solution. In this study, we apply the
Transformer architecture to traffic tasks, aiming to learn the diversity of
trajectories within vehicle populations. We analyze the Transformer&apos;s attention
mechanism and its adaptability to the goals of traffic tasks, and subsequently,
design specific pre-training tasks. To achieve this, we create a data structure
tailored to the attention mechanism and introduce a set of noises that
correspond to spatio-temporal demands, which are incorporated into the
structured data during the pre-training process. The designed pre-training
model demonstrates excellent performance in capturing the spatial distribution
of the vehicle population, with no instances of vehicle overlap and an RMSE of
0.6059 when compared to the ground truth values. In the context of time series
prediction, approximately 95% of the predicted trajectories&apos; speeds closely
align with the true speeds, within a deviation of 7.5144m/s. Furthermore, in
the stability test, the model exhibits robustness by continuously predicting a
time series ten times longer than the input sequence, delivering smooth
trajectories and showcasing diverse driving behaviors. The pre-trained model
also provides a good basis for downstream fine-tuning tasks. The number of
parameters of our model is over 50 million.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruyi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bowen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yan Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14564">
<title>Generative Escher Meshes. (arXiv:2309.14564v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14564</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a fully-automatic, text-guided generative method for
producing periodic, repeating, tile-able 2D art, such as the one seen on
floors, mosaics, ceramics, and the work of M.C. Escher. In contrast to the
standard concept of a seamless texture, i.e., square images that are seamless
when tiled, our method generates non-square tilings which comprise solely of
repeating copies of the same object. It achieves this by optimizing both
geometry and color of a 2D mesh, in order to generate a non-square tile in the
shape and appearance of the desired object, with close to no additional
background details. We enable geometric optimization of tilings by our key
technical contribution: an unconstrained, differentiable parameterization of
the space of all possible tileable shapes for a given symmetry group. Namely,
we prove that modifying the laplacian used in a 2D mesh-mapping technique -
Orbifold Tutte Embedding - can achieve all possible tiling configurations for a
chosen planar symmetry group. We thus consider both the mesh&apos;s tile-shape and
its texture as optimizable parameters, rendering the textured mesh via a
differentiable renderer. We leverage a trained image diffusion model to define
a loss on the resulting image, thereby updating the mesh&apos;s parameters based on
its appearance matching the text prompt. We show our method is able to produce
plausible, appealing results, with non-trivial tiles, for a variety of
different periodic tiling patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1&quot;&gt;Noam Aigerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16713">
<title>UAV-assisted Semantic Communication with Hybrid Action Reinforcement Learning. (arXiv:2309.16713v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16713</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we aim to explore the use of uplink semantic communications
with the assistance of UAV in order to improve data collection effiicency for
metaverse users in remote areas. To reduce the time for uplink data collection
while balancing the trade-off between reconstruction quality and computational
energy cost, we propose a hybrid action reinforcement learning (RL) framework
to make decisions on semantic model scale, channel allocation, transmission
power, and UAV trajectory. The variables are classified into discrete type and
continuous type, which are optimized by two different RL agents to generate the
combined action. Simulation results indicate that the proposed hybrid action
reinforcement learning framework can effectively improve the efficiency of
uplink semantic data collection under different parameter settings and
outperforms the benchmark scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1&quot;&gt;Peiyuan Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16770">
<title>Persona-Coded Poly-Encoder: Persona-Guided Multi-Stream Conversational Sentence Scoring. (arXiv:2309.16770v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16770</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning and deep learning have led to the
widespread use of Conversational AI in many practical applications. However, it
is still very challenging to leverage auxiliary information that can provide
conversational context or personalized tuning to improve the quality of
conversations. For example, there has only been limited research on using an
individuals persona information to improve conversation quality, and even
state-of-the-art conversational AI techniques are unable to effectively
leverage signals from heterogeneous sources of auxiliary data, such as
multi-modal interaction data, demographics, SDOH data, etc. In this paper, we
present a novel Persona-Coded Poly-Encoder method that leverages persona
information in a multi-stream encoding scheme to improve the quality of
response generation for conversations. To show the efficacy of the proposed
method, we evaluate our method on two different persona-based conversational
datasets, and compared against two state-of-the-art methods. Our experimental
results and analysis demonstrate that our method can improve conversation
quality over the baseline method Poly-Encoder by 3.32% and 2.94% in terms of
BLEU score and HR@1, respectively. More significantly, our method offers a path
to better utilization of multi-modal data in conversational tasks. Lastly, our
study outlines several challenges and future research directions for advancing
personalized conversational AI technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Symons_C/0/1/0/all/0/1&quot;&gt;Christopher Symons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vatsavai_R/0/1/0/all/0/1&quot;&gt;Ranga Raju Vatsavai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Self-Supervised Learning. (arXiv:2310.03940v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Self-Supervised Learning (SSL) methods train their models to be
invariant to different &quot;views&quot; of an image input for which a good data
augmentation pipeline is crucial. While considerable efforts were directed
towards improving pre-text tasks, architectures, or robustness (e.g., Siamese
networks or teacher-softmax centering), the majority of these methods remain
strongly reliant on the random sampling of operations within the image
augmentation pipeline, such as the random resized crop or color distortion
operation. In this paper, we argue that the role of the view generation and its
effect on performance has so far received insufficient attention. To address
this, we propose an easy, learning-free, yet powerful Hard View Selection (HVS)
strategy designed to extend the random view generation to expose the pretrained
model to harder samples during SSL training. It encompasses the following
iterative steps: 1) randomly sample multiple views and create pairs of two
views, 2) run forward passes for each view pair on the currently trained model,
3) adversarially select the pair yielding the worst loss, and 4) run the
backward pass with the selected pair. In our empirical analysis we show that
under the hood, HVS increases task difficulty by controlling the Intersection
over Union of views during pretraining. With only 300-epoch pretraining, HVS is
able to closely rival the 800-epoch DINO baseline which remains very favorable
even when factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple SSL methods, such as DINO, SimSiam, iBOT, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05869">
<title>HyperAttention: Long-context Attention in Near-Linear Time. (arXiv:2310.05869v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05869</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an approximate attention mechanism named HyperAttention to address
the computational challenges posed by the growing complexity of long contexts
used in Large Language Models (LLMs). Recent work suggests that in the
worst-case scenario, quadratic time is necessary unless the entries of the
attention matrix are bounded or the matrix has low stable rank. We introduce
two parameters which measure: (1) the max column norm in the normalized
attention matrix, and (2) the ratio of row norms in the unnormalized attention
matrix after detecting and removing large entries. We use these fine-grained
parameters to capture the hardness of the problem. Despite previous lower
bounds, we are able to achieve a linear time sampling algorithm even when the
matrix has unbounded entries or a large stable rank, provided the above
parameters are small. HyperAttention features a modular design that easily
accommodates integration of other fast low-level implementations, particularly
FlashAttention. Empirically, employing Locality Sensitive Hashing (LSH) to
identify large entries, HyperAttention outperforms existing methods, giving
significant speed improvements compared to state-of-the-art solutions like
FlashAttention. We validate the empirical performance of HyperAttention on a
variety of different long-context length datasets. For example, HyperAttention
makes the inference time of ChatGLM2 50\% faster on 32k context length while
perplexity increases from 5.6 to 6.3. On larger context length, e.g., 131k,
with causal masking, HyperAttention offers 5-fold speedup on a single attention
layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_I/0/1/0/all/0/1&quot;&gt;Insu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Jayaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1&quot;&gt;Amin Karbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1&quot;&gt;David P. Woodruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zandieh_A/0/1/0/all/0/1&quot;&gt;Amir Zandieh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05879">
<title>Coarse-Graining Hamiltonian Systems Using WSINDy. (arXiv:2310.05879v2 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05879</link>
<description rdf:parseType="Literal">&lt;p&gt;The Weak-form Sparse Identification of Nonlinear Dynamics algorithm (WSINDy)
has been demonstrated to offer coarse-graining capabilities in the context of
interacting particle systems (https://doi.org/10.1016/j.physd.&lt;a href=&quot;/abs/2022.13340&quot;&gt;2022.13340&lt;/a&gt;6). In
this work we extend this capability to the problem of coarse-graining
Hamiltonian dynamics which possess approximate symmetries associated with
timescale separation. Such approximate symmetries often lead to the existence
of a Hamiltonian system of reduced dimension that may be used to efficiently
capture the dynamics of the symmetry-invariant dependent variables. Deriving
such reduced systems, or approximating them numerically, is an ongoing
challenge. We demonstrate that WSINDy can successfully identify this reduced
Hamiltonian system in the presence of large intrinsic perturbations while
remaining robust to extrinsic noise. This is significant in part due to the
nontrivial means by which such systems are derived analytically. WSINDy also
naturally preserves the Hamiltonian structure by restricting to a trial basis
of Hamiltonian vector fields. The methodology is computational efficient, often
requiring only a single trajectory to learn the global reduced Hamiltonian, and
avoiding forward solves in the learning process. Using nearly-periodic
Hamiltonian systems as a prototypical class of systems with approximate
symmetries, we show that WSINDy robustly identifies the correct leading-order
system, with dimension reduced by at least two, upon observation of the
relevant degrees of freedom. We also provide a contribution to averaging theory
by proving that first-order averaging at the level of vector fields preserves
Hamiltonian structure in nearly-periodic Hamiltonian systems. We provide
physically relevant examples, namely coupled oscillator dynamics, the
H\&apos;enon-Heiles system for stellar motion within a galaxy, and the dynamics of
charged particles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Messenger_D/0/1/0/all/0/1&quot;&gt;Daniel A. Messenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Burby_J/0/1/0/all/0/1&quot;&gt;Joshua W. Burby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bortz_D/0/1/0/all/0/1&quot;&gt;David M. Bortz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07312">
<title>Diffusion Models for Wireless Communications. (arXiv:2310.07312v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07312</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovative foundation models, such as GPT-4 and stable diffusion models, have
made a paradigm shift in the realm of artificial intelligence (AI) towards
generative AI-based systems. AI and machine learning (AI/ML) algorithms are
envisioned to be pervasively incorporated into the future wireless
communications systems. In this article, we outline the applications of
diffusion models in wireless communication systems, which are a new family of
probabilistic generative models that have showcased state-of-the-art
performance. The key idea is to decompose data generation process over
&quot;denoising&quot; steps, gradually generating samples out of noise. Based on two case
studies presented, we show how diffusion models can be employed for the
development of resilient AI-native communication systems. Specifically, we
propose denoising diffusion probabilistic models (DDPM) for a wireless
communication scheme with non-ideal transceivers, where 30% improvement is
achieved in terms of bit error rate. In the other example, DDPM is employed at
the transmitter to shape the constellation symbols, highlighting a robust
out-of-distribution performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letafati_M/0/1/0/all/0/1&quot;&gt;Mehdi Letafati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Samad Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latva_aho_M/0/1/0/all/0/1&quot;&gt;Matti Latva-aho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07918">
<title>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07918</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models fall short by forcing
a tradeoff between accuracy and interpretability. This tradeoff limits
data-driven interpretations of human decision-making process. e.g. to audit
medical decisions for biases and suboptimal practices, we require models of
decision processes which provide concise descriptions of complex behaviors.
Fundamentally, existing approaches are burdened by this tradeoff because they
represent the underlying decision process as a universal policy, when in fact
human decisions are dynamic and can change drastically with contextual
information. Thus, we propose Contextualized Policy Recovery (CPR), which
re-frames the problem of modeling complex decision processes as a multi-task
learning problem in which complex decision policies are comprised of
context-specific policies. CPR models each context-specific policy as a linear
observation-to-action mapping, and generates new decision models
$\textit{on-demand}$ as contexts are updated with new observations. CPR is
compatible with fully offline and partially observable decision environments,
and can be tailored to incorporate any recurrent black-box model or
interpretable decision model. We assess CPR through studies on simulated and
real data, achieving state-of-the-art performance on the canonical tasks of
predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.
previous SOTA) and predicting MRI prescription for Alzheimer&apos;s patients
($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive
performance, CPR closes the accuracy gap between interpretable and black-box
methods for policy learning, allowing high-resolution exploration and analysis
of context-specific decision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuschel_J/0/1/0/all/0/1&quot;&gt;Jannik Deuschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellington_C/0/1/0/all/0/1&quot;&gt;Caleb N. Ellington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Lengerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yingtao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friederich_P/0/1/0/all/0/1&quot;&gt;Pascal Friederich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10631">
<title>Llemma: An Open Language Model For Mathematics. (arXiv:2310.10631v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10631</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Llemma, a large language model for mathematics. We continue
pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web
data containing mathematics, and mathematical code, yielding Llemma. On the
MATH benchmark Llemma outperforms all known open base models, as well as the
unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is
capable of tool use and formal theorem proving without any further finetuning.
We openly release all artifacts, including 7 billion and 34 billion parameter
models, the Proof-Pile-2, and code to replicate our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1&quot;&gt;Zhangir Azerbayev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoelkopf_H/0/1/0/all/0/1&quot;&gt;Hailey Schoelkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paster_K/0/1/0/all/0/1&quot;&gt;Keiran Paster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Marco Dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAleer_S/0/1/0/all/0/1&quot;&gt;Stephen McAleer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1&quot;&gt;Albert Q. Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jia Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1&quot;&gt;Stella Biderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welleck_S/0/1/0/all/0/1&quot;&gt;Sean Welleck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10638">
<title>In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10638</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LMs) are currently trained to predict tokens given
document prefixes, enabling them to directly perform long-form generation and
prompting-style tasks which can be reduced to document completion. Existing
pretraining pipelines train LMs by concatenating random sets of short documents
to create input contexts but the prior documents provide no signal for
predicting the next document. We instead present In-Context Pretraining, a new
approach where language models are pretrained on a sequence of related
documents, thereby explicitly encouraging them to read and reason across
document boundaries. We can do In-Context Pretraining by simply changing the
document ordering so that each context contains related documents, and directly
applying existing pretraining pipelines. However, this document sorting problem
is challenging. There are billions of documents and we would like the sort to
maximize contextual similarity for every document without repeating any data.
To do this, we introduce approximate algorithms for finding related documents
with efficient nearest neighbor search and constructing coherent input contexts
with a graph traversal algorithm. Our experiments show In-Context Pretraining
offers a simple and scalable approach to significantly enhance LMs&apos;performance:
we see notable improvements in tasks that require more complex contextual
reasoning, including in-context learning (+8%), reading comprehension (+15%),
faithfulness to previous contexts (+16%), long-context reasoning (+5%), and
retrieval augmentation (+9%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weijia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;Sewon Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomeli_M/0/1/0/all/0/1&quot;&gt;Maria Lomeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunting Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Margaret Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_R/0/1/0/all/0/1&quot;&gt;Rich James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Victoria Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_S/0/1/0/all/0/1&quot;&gt;Scott Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12609">
<title>Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12609</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have risen as a powerful tool in robotics due to their
flexibility and multi-modality. While some of these methods effectively address
complex problems, they often depend heavily on inference-time obstacle
detection and require additional equipment. Addressing these challenges, we
present a method that, during inference time, simultaneously generates only
reachable goals and plans motions that avoid obstacles, all from a single
visual input. Central to our approach is the novel use of a collision-avoiding
diffusion kernel for training. Through evaluations against behavior-cloning and
classical diffusion models, our framework has proven its robustness. It is
particularly effective in multi-modal environments, navigating toward goals and
avoiding unreachable ones blocked by obstacles, while ensuring collision
avoidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Soochul Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nikhil Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00213">
<title>Consistent Video-to-Video Transfer Using Synthetic Dataset. (arXiv:2311.00213v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00213</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel and efficient approach for text-based video-to-video
editing that eliminates the need for resource-intensive per-video-per-model
finetuning. At the core of our approach is a synthetic paired video dataset
tailored for video-to-video transfer tasks. Inspired by Instruct Pix2Pix&apos;s
image transfer via editing instruction, we adapt this paradigm to the video
domain. Extending the Prompt-to-Prompt to videos, we efficiently generate
paired samples, each with an input video and its edited counterpart. Alongside
this, we introduce the Long Video Sampling Correction during sampling, ensuring
consistent long videos across batches. Our method surpasses current methods
like Tune-A-Video, heralding substantial progress in text-based video-to-video
editing and suggesting exciting avenues for further exploration and deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiaxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00462">
<title>Leveraging Hyperbolic Embeddings for Coarse-to-Fine Robot Design. (arXiv:2311.00462v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00462</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-cellular robot design aims to create robots comprised of numerous cells
that can be efficiently controlled to perform diverse tasks. Previous research
has demonstrated the ability to generate robots for various tasks, but these
approaches often optimize robots directly in the vast design space, resulting
in robots with complicated morphologies that are hard to control. In response,
this paper presents a novel coarse-to-fine method for designing multi-cellular
robots. Initially, this strategy seeks optimal coarse-grained robots and
progressively refines them. To mitigate the challenge of determining the
precise refinement juncture during the coarse-to-fine transition, we introduce
the Hyperbolic Embeddings for Robot Design (HERD) framework. HERD unifies
robots of various granularity within a shared hyperbolic space and leverages a
refined Cross-Entropy Method for optimization. This framework enables our
method to autonomously identify areas of exploration in hyperbolic space and
concentrate on regions demonstrating promise. Finally, the extensive empirical
studies on various challenging tasks sourced from EvoGym show our approach&apos;s
superior efficiency and generalization capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Heng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14115">
<title>A density estimation perspective on learning from pairwise human preferences. (arXiv:2311.14115v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14115</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from human feedback (LHF) -- and in particular learning from
pairwise preferences -- has recently become a crucial ingredient in training
large language models (LLMs), and has been the subject of much research. Most
recent works frame it as a reinforcement learning problem, where a reward
function is learned from pairwise preference data and the LLM is treated as a
policy which is adapted to maximize the rewards, often under additional
regularization constraints. We propose an alternative interpretation which
centers on the generative process for pairwise preferences and treats LHF as a
density estimation problem. We provide theoretical and empirical results
showing that for a family of generative processes defined via preference
behavior distribution equations, training a reward function on pairwise
preferences effectively models an annotator&apos;s implicit preference distribution.
Finally, we discuss and present findings on &quot;annotator misspecification&quot; --
failure cases where wrong modeling assumptions are made about annotator
behavior, resulting in poorly-adapted models -- suggesting that approaches that
learn from pairwise human preferences could have trouble learning from a
population of annotators with diverse viewpoints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1&quot;&gt;Vincent Dumoulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_D/0/1/0/all/0/1&quot;&gt;Daniel D. Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_P/0/1/0/all/0/1&quot;&gt;Pablo Samuel Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larochelle_H/0/1/0/all/0/1&quot;&gt;Hugo Larochelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauphin_Y/0/1/0/all/0/1&quot;&gt;Yann Dauphin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16027">
<title>An HCAI Methodological Framework: Putting It Into Action to Enable Human-Centered AI. (arXiv:2311.16027v3 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16027</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centered AI (HCAI), as a design philosophy, advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI technology to humans and avoid its potential
adverse effects. While HCAI has gained momentum, the lack of guidance on
methodology in its implementation makes its adoption challenging. After
assessing the needs for a methodological framework for HCAI, this paper first
proposes a comprehensive and interdisciplinary HCAI methodological framework
integrated with seven components, including design goals, design principles,
implementation approaches, design paradigms, interdisciplinary teams, methods,
and processes. THe implications of the framework are also discussed. This paper
also presents a &quot;three-layer&quot; approach to facilitate the implementation of the
framework. We believe the proposed framework is systematic and executable,
which can overcome the weaknesses in current frameworks and the challenges
currently faced in implementing HCAI. Thus, the framework can help put it into
action to develop, transfer, and implement HCAI in practice, eventually
enabling the design, development, and deployment of HCAI-based intelligent
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zaifeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dainoff_M/0/1/0/all/0/1&quot;&gt;Marvin Dainoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16119">
<title>Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition. (arXiv:2311.16119v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16119</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are deployed in interactive contexts with direct
user engagement, such as chatbots and writing assistants. These deployments are
vulnerable to prompt injection and jailbreaking (collectively, prompt hacking),
in which models are manipulated to ignore their original instructions and
follow potentially malicious ones. Although widely acknowledged as a
significant security threat, there is a dearth of large-scale resources and
quantitative studies on prompt hacking. To address this lacuna, we launch a
global prompt hacking competition, which allows for free-form human input
attacks. We elicit 600K+ adversarial prompts against three state-of-the-art
LLMs. We describe the dataset, which empirically verifies that current LLMs can
indeed be manipulated via prompt hacking. We also present a comprehensive
taxonomical ontology of the types of adversarial prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulhoff_S/0/1/0/all/0/1&quot;&gt;Sander Schulhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1&quot;&gt;Jeremy Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Anaum Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouchard_L/0/1/0/all/0/1&quot;&gt;Louis-Fran&amp;#xe7;ois Bouchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1&quot;&gt;Chenglei Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anati_S/0/1/0/all/0/1&quot;&gt;Svetlina Anati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliabue_V/0/1/0/all/0/1&quot;&gt;Valen Tagliabue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kost_A/0/1/0/all/0/1&quot;&gt;Anson Liu Kost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carnahan_C/0/1/0/all/0/1&quot;&gt;Christopher Carnahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1&quot;&gt;Jordan Boyd-Graber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17179">
<title>SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery. (arXiv:2311.17179v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17179</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic location is essential for modeling tasks in fields ranging from
ecology to epidemiology to the Earth system sciences. However, extracting
relevant and meaningful characteristics of a location can be challenging, often
entailing expensive data fusion or data distillation from global imagery
datasets. To address this challenge, we introduce Satellite Contrastive
Location-Image Pretraining (SatCLIP), a global, general-purpose geographic
location encoder that learns an implicit representation of locations from
openly available satellite imagery. Trained location encoders provide vector
embeddings summarizing the characteristics of any given location for convenient
usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained
on globally sampled multi-spectral Sentinel-2 satellite data, can be used in
various predictive tasks that depend on location information but not
necessarily satellite imagery, including temperature prediction, animal
recognition in imagery, and population density estimation. Across tasks,
SatCLIP embeddings consistently outperform embeddings from existing pretrained
location encoders, ranging from models trained on natural images to models
trained on semantic context. SatCLIP embeddings also help to improve geographic
generalization. This demonstrates the potential of general-purpose location
encoders and opens the door to learning meaningful representations of our
planet from the vast, varied, and largely untapped modalities of geospatial
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1&quot;&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1&quot;&gt;Esther Rolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Caleb Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1&quot;&gt;Marc Ru&amp;#xdf;wurm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17303">
<title>Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge. (arXiv:2311.17303v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17303</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a generic methodology to encode hierarchical
causality structure among observed variables into a neural network in order to
improve its predictive performance. The proposed methodology, called
causality-informed neural network (CINN), leverages three coherent steps to
systematically map the structural causal knowledge into the layer-to-layer
design of neural network while strictly preserving the orientation of every
causal relationship. In the first step, CINN discovers causal relationships
from observational data via directed acyclic graph (DAG) learning, where causal
discovery is recast as a continuous optimization problem to avoid the
combinatorial nature. In the second step, the discovered hierarchical causality
structure among observed variables is systematically encoded into neural
network through a dedicated architecture and customized loss function. By
categorizing variables in the causal DAG as root, intermediate, and leaf nodes,
the hierarchical causal DAG is translated into CINN with a one-to-one
correspondence between nodes in the causal DAG and units in the CINN while
maintaining the relative order among these nodes. Regarding the loss function,
both intermediate and leaf nodes in the DAG graph are treated as target outputs
during CINN training so as to drive co-learning of causal relationships among
different types of nodes. As multiple loss components emerge in CINN, we
leverage the projection of conflicting gradients to mitigate gradient
interference among the multiple learning tasks. Computational experiments
across a broad spectrum of UCI data sets demonstrate substantial advantages of
CINN in predictive performance over other state-of-the-art methods. In
addition, an ablation study underscores the value of integrating structural and
quantitative causal knowledge in enhancing the neural network&apos;s predictive
performance incrementally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao-Lin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fenglei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-Ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_I/0/1/0/all/0/1&quot;&gt;Indranil Bose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17338">
<title>VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model. (arXiv:2311.17338v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17338</link>
<description rdf:parseType="Literal">&lt;p&gt;Identity-consistent video generation seeks to synthesize videos that are
guided by both textual prompts and reference images of entities. Current
approaches typically utilize cross-attention layers to integrate the appearance
of the entity, which predominantly captures semantic attributes, resulting in
compromised fidelity of entities. Moreover, these methods necessitate iterative
fine-tuning for each new entity encountered, thereby limiting their
applicability. To address these challenges, we introduce VideoAssembler, a
novel end-to-end framework for identity-consistent video generation that can
conduct inference directly when encountering new entities. VideoAssembler is
adept at producing videos that are not only flexible with respect to the input
reference entities but also responsive to textual conditions. Additionally, by
modulating the quantity of input images for the entity, VideoAssembler enables
the execution of tasks ranging from image-to-video generation to sophisticated
video editing. VideoAssembler comprises two principal components: the Reference
Entity Pyramid (REP) encoder and the Entity-Prompt Attention Fusion (EPAF)
module. The REP encoder is designed to infuse comprehensive appearance details
into the denoising stages of the stable diffusion model. Concurrently, the EPAF
module is utilized to integrate text-aligned features effectively. Furthermore,
to mitigate the challenge of scarce data, we present a methodology for the
preprocessing of training data. Our evaluation of the VideoAssembler framework
on the UCF-101, MSR-VTT, and DAVIS datasets indicates that it achieves good
performances in both quantitative and qualitative analyses (346.84 in FVD and
48.01 in IS on UCF-101). Our project page is at
https://gulucaptain.github.io/videoassembler/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tianyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17541">
<title>TaskWeaver: A Code-First Agent Framework. (arXiv:2311.17541v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17541</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown impressive abilities in natural
language understanding and generation, leading to their use in applications
such as chatbots and virtual assistants. However, existing LLM frameworks face
limitations in handling domain-specific data analytics tasks with rich data
structures. Moreover, they struggle with flexibility to meet diverse user
requirements. To address these issues, TaskWeaver is proposed as a code-first
framework for building LLM-powered autonomous agents. It converts user requests
into executable code and treats user-defined plugins as callable functions.
TaskWeaver provides support for rich data structures, flexible plugin usage,
and dynamic plugin selection, and leverages LLM coding capabilities for complex
logic. It also incorporates domain-specific knowledge through examples and
ensures the secure execution of generated code. TaskWeaver offers a powerful
and flexible framework for creating intelligent conversational agents that can
handle complex tasks and adapt to domain-specific scenarios. The code is
open-sourced at https://github.com/microsoft/TaskWeaver/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_B/0/1/0/all/0/1&quot;&gt;Bo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liqun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shilin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fangkai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Minghua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1&quot;&gt;Si Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xiaoting Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qingwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1&quot;&gt;Saravan Rajmohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dongmei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18587">
<title>Continuous 16-bit Training: Accelerating 32-bit Pre-Trained Neural Networks. (arXiv:2311.18587v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18587</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of deep learning, the prevalence of models initially trained
with 32-bit precision is a testament to its robustness and accuracy. However,
the continuous evolution of these models often demands further training, which
can be resource-intensive. This study introduces a novel approach where we
continue the training of these pre-existing 32-bit models using 16-bit
precision. This technique not only caters to the need for efficiency in
computational resources but also significantly improves the speed of additional
training phases. By adopting 16-bit precision for ongoing training, we are able
to substantially decrease memory requirements and computational burden, thereby
accelerating the training process in a resource-limited setting. Our
experiments show that this method maintains the high standards of accuracy set
by the original 32-bit training while providing a much-needed boost in training
speed. This approach is especially pertinent in today&apos;s context, where most
models are initially trained in 32-bit and require periodic updates and
refinements. The findings from our research suggest that this strategy of
16-bit continuation training can be a key solution for sustainable and
efficient deep learning, offering a practical way to enhance pre-trained models
rapidly and in a resource-conscious manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18599">
<title>Joint Detection Algorithm for Multiple Cognitive Users in Spectrum Sensing. (arXiv:2311.18599v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18599</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectrum sensing technology is a crucial aspect of modern communication
technology, serving as one of the essential techniques for efficiently
utilizing scarce information resources in tight frequency bands. This paper
first introduces three common logical circuit decision criteria in hard
decisions and analyzes their decision rigor. Building upon hard decisions, the
paper further introduces a method for multi-user spectrum sensing based on soft
decisions. Then the paper simulates the false alarm probability and detection
probability curves corresponding to the three criteria. The simulated results
of multi-user collaborative sensing indicate that the simulation process
significantly reduces false alarm probability and enhances detection
probability. This approach effectively detects spectrum resources unoccupied
during idle periods, leveraging the concept of time-division multiplexing and
rationalizing the redistribution of information resources. The entire
computation process relies on the calculation principles of power spectral
density in communication theory, involving threshold decision detection for
noise power and the sum of noise and signal power. It provides a secondary
decision detection, reflecting the perceptual decision performance of logical
detection methods with relative accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lele Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yingxin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18662">
<title>Solving the Team Orienteering Problem with Transformers. (arXiv:2311.18662v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18662</link>
<description rdf:parseType="Literal">&lt;p&gt;Route planning for a fleet of vehicles is an important task in applications
such as package delivery, surveillance, or transportation. This problem is
usually modeled as a Combinatorial Optimization problem named as Team
Orienteering Problem. The most popular Team Orienteering Problem solvers are
mainly based on either linear programming, which provides accurate solutions by
employing a large computation time that grows with the size of the problem, or
heuristic methods, which usually find suboptimal solutions in a shorter amount
of time. In this paper, a multi-agent route planning system capable of solving
the Team Orienteering Problem in a very fast and accurate manner is presented.
The proposed system is based on a centralized Transformer neural network that
can learn to encode the scenario (modeled as a graph) and the context of the
agents to provide fast and accurate solutions. Several experiments have been
performed to demonstrate that the presented system can outperform most of the
state-of-the-art works in terms of computation speed. In addition, the code is
publicly available at &lt;a href=&quot;http://gti.ssr.upm.es/data.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuertes_D/0/1/0/all/0/1&quot;&gt;Daniel Fuertes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+del_Blanco_C/0/1/0/all/0/1&quot;&gt;Carlos R. del-Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaureguizar_F/0/1/0/all/0/1&quot;&gt;Fernando Jaureguizar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Narciso Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18765">
<title>MLLMs-Augmented Visual-Language Representation Learning. (arXiv:2311.18765v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18765</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual-language pre-training (VLP) has achieved remarkable success in
multi-modal tasks, largely attributed to the availability of large-scale
image-text datasets. In this work, we demonstrate that multi-modal large
language models (MLLMs) can enhance visual-language representation learning by
improving data quality. Our approach is simple, utilizing MLLMs to extend
multiple captions for each image. To prevent the bias introduced by MLLMs&apos;
hallucinations and intrinsic caption styles, we propose &quot;text shearing&quot; to
maintain the same length for extended captions as that of the original
captions. In image-text retrieval, our method consistently obtains 5.6 ~ 35.0%
and 16.8 ~ 46.1% improvement on R@1 under the fine-tuning and zero-shot
settings, respectively. Notably, we obtain zero-shot results that are
comparable to fine-tuning on target datasets, which encourages more exploration
of the versatile use of MLLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>