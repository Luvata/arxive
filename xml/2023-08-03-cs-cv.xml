<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00759" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.08657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.10401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.01769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00135" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.00715">
<title>Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00715</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid spread of COVID-19 has necessitated efficient and accurate
diagnostic methods. Computed Tomography (CT) scan images have emerged as a
valuable tool for detecting the disease. In this article, we present a novel
deep learning approach for automated COVID-19 CT scan classification where a
modified Xception model is proposed which incorporates a newly designed channel
attention mechanism and weighted global average pooling to enhance feature
extraction thereby improving classification accuracy. The channel attention
module selectively focuses on informative regions within each channel, enabling
the model to learn discriminative features for COVID-19 detection. Experiments
on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of
96.99% and show its superiority to other state-of-the-art techniques. This
research can contribute to the ongoing efforts in using artificial intelligence
to combat current and future pandemics and can offer promising and timely
solutions for efficient medical image analysis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Susmita Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Abhiroop Chatterjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00725">
<title>Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end image/video codecs are getting competitive compared to traditional
compression techniques that have been developed through decades of manual
engineering efforts. These trainable codecs have many advantages over
traditional techniques such as easy adaptation on perceptual distortion metrics
and high performance on specific domains thanks to their learning ability.
However, state of the art neural codecs does not take advantage of the
existence of gradient of entropy in decoding device. In this paper, we
theoretically show that gradient of entropy (available at decoder side) is
correlated with the gradient of the reconstruction error (which is not
available at decoder side). We then demonstrate experimentally that this
gradient can be used on various compression methods, leading to a $1-2\%$ rate
savings for the same quality. Our method is orthogonal to other improvements
and brings independent rate savings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balcilar_M/0/1/0/all/0/1&quot;&gt;Muhammet Balcilar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naser_K/0/1/0/all/0/1&quot;&gt;Karam Naser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Galpin_F/0/1/0/all/0/1&quot;&gt;Franck Galpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hellier_P/0/1/0/all/0/1&quot;&gt;Pierre Hellier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00727">
<title>Adaptive Semantic Consistency for Cross-domain Few-shot Classification. (arXiv:2308.00727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00727</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-domain few-shot classification (CD-FSC) aims to identify novel target
classes with a few samples, assuming that there exists a domain shift between
source and target domains. Existing state-of-the-art practices typically
pre-train on source domain and then finetune on the few-shot target data to
yield task-adaptive representations. Despite promising progress, these methods
are prone to overfitting the limited target distribution since data-scarcity
and ignore the transferable knowledge learned in the source domain. To
alleviate this problem, we propose a simple plug-and-play Adaptive Semantic
Consistency (ASC) framework, which improves cross-domain robustness by
preserving source transfer capability during the finetuning stage. Concretely,
we reuse the source images in the pretraining phase and design an adaptive
weight assignment strategy to highlight the samples similar to target domain,
aiming to aggregate informative target-related knowledge from source domain.
Subsequently, a semantic consistency regularization is applied to constrain the
consistency between the semantic features of the source images output by the
source model and target model. In this way, the proposed ASC enables explicit
transfer of source domain knowledge to prevent the model from overfitting the
target domain. Extensive experiments on multiple benchmarks demonstrate the
effectiveness of the proposed ASC, and ASC provides consistent improvements
over the baselines. The source code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hengchu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yuanjie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changxin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00728">
<title>ELFNet: Evidential Local-global Fusion for Stereo Matching. (arXiv:2308.00728v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00728</link>
<description rdf:parseType="Literal">&lt;p&gt;Although existing stereo matching models have achieved continuous
improvement, they often face issues related to trustworthiness due to the
absence of uncertainty estimation. Additionally, effectively leveraging
multi-scale and multi-view knowledge of stereo pairs remains unexplored. In
this paper, we introduce the \textbf{E}vidential \textbf{L}ocal-global
\textbf{F}usion (ELF) framework for stereo matching, which endows both
uncertainty estimation and confidence-aware fusion with trustworthy heads.
Instead of predicting the disparity map alone, our model estimates an
evidential-based disparity considering both aleatoric and epistemic
uncertainties. With the normal inverse-Gamma distribution as a bridge, the
proposed framework realizes intra evidential fusion of multi-level predictions
and inter evidential fusion between cost-volume-based and transformer-based
stereo matching. Extensive experimental results show that the proposed
framework exploits multi-view information effectively and achieves
state-of-the-art overall performance both on accuracy and cross-domain
generalization.
&lt;/p&gt;
&lt;p&gt;The codes are available at https://github.com/jimmy19991222/ELFNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jieming Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weide Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00729">
<title>Ada-DQA: Adaptive Diverse Quality-aware Feature Acquisition for Video Quality Assessment. (arXiv:2308.00729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00729</link>
<description rdf:parseType="Literal">&lt;p&gt;Video quality assessment (VQA) has attracted growing attention in recent
years. While the great expense of annotating large-scale VQA datasets has
become the main obstacle for current deep-learning methods. To surmount the
constraint of insufficient training data, in this paper, we first consider the
complete range of video distribution diversity (\ie content, distortion,
motion) and employ diverse pretrained models (\eg architecture, pretext task,
pre-training dataset) to benefit quality representation. An Adaptive Diverse
Quality-aware feature Acquisition (Ada-DQA) framework is proposed to capture
desired quality-related features generated by these frozen pretrained models.
By leveraging the Quality-aware Acquisition Module (QAM), the framework is able
to extract more essential and relevant features to represent quality. Finally,
the learned quality representation is utilized as supplementary supervisory
information, along with the supervision of the labeled quality score, to guide
the training of a relatively lightweight VQA model in a knowledge distillation
manner, which largely reduces the computational cost during inference.
Experimental results on three mainstream no-reference VQA benchmarks clearly
show the superior performance of Ada-DQA in comparison with current
state-of-the-art approaches without using extra training data of VQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mingda Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Ming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanchuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xing Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00755">
<title>The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00755</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias amplification is a phenomenon in which models increase imbalances
present in the training data. In this paper, we study bias amplification in the
text-to-image domain using Stable Diffusion by comparing gender ratios in
training vs. generated images. We find that the model appears to amplify
gender-occupation biases found in the training data (LAION). However, we
discover that amplification can largely be attributed to discrepancies between
training captions and model prompts. For example, an inherent difference is
that captions from the training data often contain explicit gender information
while the prompts we use do not, which leads to a distribution shift and
consequently impacts bias measures. Once we account for various distributional
differences between texts used for training and generation, we observe that
amplification decreases considerably. Our findings illustrate the challenges of
comparing biases in models and the data they are trained on, and highlight
confounding factors that contribute to bias amplification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1&quot;&gt;Preethi Seshadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00759">
<title>Decomposition Ascribed Synergistic Learning for Unified Image Restoration. (arXiv:2308.00759v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00759</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to restore multiple image degradations within a single model is
quite beneficial for real-world applications. Nevertheless, existing works
typically concentrate on regarding each degradation independently, while their
relationship has been less exploited to ensure the synergistic learning. To
this end, we revisit the diverse degradations through the lens of singular
value decomposition, with the observation that the decomposed singular vectors
and singular values naturally undertake the different types of degradation
information, dividing various restoration tasks into two groups,\ie, singular
vector dominated and singular value dominated. The above analysis renders a
more unified perspective to ascribe the diverse degradations, compared to
previous task-level independent learning. The dedicated optimization of
degraded singular vectors and singular values inherently utilizes the potential
relationship among diverse restoration tasks, attributing to the Decomposition
Ascribed Synergistic Learning (DASL). Specifically, DASL comprises two
effective operators, namely, Singular VEctor Operator (SVEO) and Singular VAlue
Operator (SVAO), to favor the decomposed optimization, which can be lightly
integrated into existing convolutional image restoration backbone. Moreover,
the congruous decomposition loss has been devised for auxiliary. Extensive
experiments on blended five image restoration tasks demonstrate the
effectiveness of our method, including image deraining, image dehazing, image
denoising, image deblurring, and low-light image enhancement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Man Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00773">
<title>High-Fidelity Eye Animatable Neural Radiance Fields for Human Face. (arXiv:2308.00773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Face rendering using neural radiance fields (NeRF) is a rapidly developing
research area in computer vision. While recent methods primarily focus on
controlling facial attributes such as identity and expression, they often
overlook the crucial aspect of modeling eyeball rotation, which holds
importance for various downstream tasks. In this paper, we aim to learn a face
NeRF model that is sensitive to eye movements from multi-view images. We
address two key challenges in eye-aware face NeRF learning: how to effectively
capture eyeball rotation for training and how to construct a manifold for
representing eyeball rotation. To accomplish this, we first fit FLAME, a
well-established parametric face model, to the multi-view images considering
multi-view consistency. Subsequently, we introduce a new Dynamic Eye-aware NeRF
(DeNeRF). DeNeRF transforms 3D points from different views into a canonical
space to learn a unified face NeRF model. We design an eye deformation field
for the transformation, including rigid transformation, e.g., eyeball rotation,
and non-rigid transformation. Through experiments conducted on the ETH-XGaze
dataset, we demonstrate that our model is capable of generating high-fidelity
images with accurate eyeball rotation and non-rigid periocular deformation,
even under novel viewing angles. Furthermore, we show that utilizing the
rendered images can effectively enhance gaze estimation performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongqun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yihua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyung Jin Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00783">
<title>Hybrid-SORT: Weak Cues Matter for Online Multi-Object Tracking. (arXiv:2308.00783v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00783</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Object Tracking (MOT) aims to detect and associate all desired objects
across frames. Most methods accomplish the task by explicitly or implicitly
leveraging strong cues (i.e., spatial and appearance information), which
exhibit powerful instance-level discrimination. However, when object occlusion
and clustering occur, both spatial and appearance information will become
ambiguous simultaneously due to the high overlap between objects. In this
paper, we demonstrate that this long-standing challenge in MOT can be
efficiently and effectively resolved by incorporating weak cues to compensate
for strong cues. Along with velocity direction, we introduce the confidence
state and height state as potential weak cues. With superior performance, our
method still maintains Simple, Online and Real-Time (SORT) characteristics.
Furthermore, our method shows strong generalization for diverse trackers and
scenarios in a plug-and-play and training-free manner. Significant and
consistent improvements are observed when applying our method to 5 different
representative trackers. Further, by leveraging both strong and weak cues, our
method Hybrid-SORT achieves superior performance on diverse benchmarks,
including MOT17, MOT20, and especially DanceTrack where interaction and
occlusion are frequent and severe. The code and models are available at
https://github.com/ymzis69/HybirdSORT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mingzhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1&quot;&gt;Guangxin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jinqing Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00799">
<title>Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction. (arXiv:2308.00799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00799</link>
<description rdf:parseType="Literal">&lt;p&gt;While 3D body reconstruction methods have made remarkable progress recently,
it remains difficult to acquire the sufficiently accurate and numerous 3D
supervisions required for training. In this paper, we propose \textbf{KNOWN}, a
framework that effectively utilizes body \textbf{KNOW}ledge and
u\textbf{N}certainty modeling to compensate for insufficient 3D supervisions.
KNOWN exploits a comprehensive set of generic body constraints derived from
well-established body knowledge. These generic constraints precisely and
explicitly characterize the reconstruction plausibility and enable 3D
reconstruction models to be trained without any 3D data. Moreover, existing
methods typically use images from multiple datasets during training, which can
result in data noise (\textit{e.g.}, inconsistent joint annotation) and data
imbalance (\textit{e.g.}, minority images representing unusual poses or
captured from challenging camera views). KNOWN solves these problems through a
novel probabilistic framework that models both aleatoric and epistemic
uncertainty. Aleatoric uncertainty is encoded in a robust Negative
Log-Likelihood (NLL) training loss, while epistemic uncertainty is used to
guide model refinement. Experiments demonstrate that KNOWN&apos;s body
reconstruction outperforms prior weakly-supervised approaches, particularly on
the challenging minority images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yufei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanjing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kephart_J/0/1/0/all/0/1&quot;&gt;Jeffrey O. Kephart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1&quot;&gt;Qiang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00801">
<title>Artificial Eye for the Blind. (arXiv:2308.00801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00801</link>
<description rdf:parseType="Literal">&lt;p&gt;The main backbone of our Artificial Eye model is the Raspberry pi3 which is
connected to the webcam ,ultrasonic proximity sensor, speaker and we also run
all our software models i.e object detection, Optical Character recognition,
google text to speech conversion and the Mycroft voice assistance model. At
first the ultrasonic proximity sensor will be measuring the distance between
itself and any obstacle in front of it .When the Proximity sensor detects any
obstacle in front within its specified range, the blind person will hear an
audio prompt about an obstacle in his way at a certain distance. At this time
the Webcam will capture an image in front of it and the Object detection model
and the Optical Character Recognition model will begin to run on the Raspberry
pi. The imat of the blind person. The text and the object detected are conveyed
to the blind pege captured is first sent through the Tesseract OCR module to
detect any texts in the image and then through the Object detection model to
detect the objects in fronrson by converting the texts to speech by using the
gTTS module. Along with the above mentioned process going on there will be an
active MYCROFT voice assistant model which can be used to interact with the
blind person. The blind person can ask about the weather , daily news , any
information on the internet ,etc
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benagi_A/0/1/0/all/0/1&quot;&gt;Abhinav Benagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_D/0/1/0/all/0/1&quot;&gt;Dhanyatha Narayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rage_C/0/1/0/all/0/1&quot;&gt;Charith Rage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sushmitha_A/0/1/0/all/0/1&quot;&gt;A Sushmitha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00806">
<title>Addressing Uncertainty in Imbalanced Histopathology Image Classification of HER2 Breast Cancer: An interpretable Ensemble Approach with Threshold Filtered Single Instance Evaluation (SIE). (arXiv:2308.00806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00806</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast Cancer (BC) is among women&apos;s most lethal health concerns. Early
diagnosis can alleviate the mortality rate by helping patients make efficient
treatment decisions. Human Epidermal Growth Factor Receptor (HER2) has become
one the most lethal subtype of BC. According to the College of American
Pathologists/American Society of Clinical Oncology (CAP/ASCO), the severity
level of HER2 expression can be classified between 0 and 3+ range. HER2 can be
detected effectively from immunohistochemical (IHC) and, hematoxylin \&amp;amp; eosin
(HE) images of different classes such as 0, 1+, 2+, and 3+. An ensemble
approach integrated with threshold filtered single instance evaluation (SIE)
technique has been proposed in this study to diagnose BC from the
multi-categorical expression of HER2 subtypes. Initially, DenseNet201 and
Xception have been ensembled into a single classifier as feature extractors
with an effective combination of global average pooling, dropout layer, dense
layer with a swish activation function, and l2 regularizer, batch
normalization, etc. After that, extracted features has been processed through
single instance evaluation (SIE) to determine different confidence levels and
adjust decision boundary among the imbalanced classes. This study has been
conducted on the BC immunohistochemical (BCI) dataset, which is classified by
pathologists into four stages of HER2 BC. This proposed approach known as
DenseNet201-Xception-SIE with a threshold value of 0.7 surpassed all other
existing state-of-art models with an accuracy of 97.12\%, precision of 97.15\%,
and recall of 97.68\% on H\&amp;amp;E data and, accuracy of 97.56\%, precision of
97.57\%, and recall of 98.00\% on IHC data respectively, maintaining momentous
improvement. Finally, Grad-CAM and Guided Grad-CAM have been employed in this
study to interpret, how TL-based model works on the histopathology dataset and
make decisions from the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shovon_M/0/1/0/all/0/1&quot;&gt;Md Sakib Hossain Shovon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mridha_M/0/1/0/all/0/1&quot;&gt;M. F. Mridha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasib_K/0/1/0/all/0/1&quot;&gt;Khan Md Hasib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfarhood_S/0/1/0/all/0/1&quot;&gt;Sultan Alfarhood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safran_M/0/1/0/all/0/1&quot;&gt;Mejdl Safran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_D/0/1/0/all/0/1&quot;&gt;Dunren Che&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00828">
<title>Deep Learning Approaches in Pavement Distress Identification: A Review. (arXiv:2308.00828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00828</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive review of recent advancements in image
processing and deep learning techniques for pavement distress detection and
classification, a critical aspect in modern pavement management systems. The
conventional manual inspection process conducted by human experts is gradually
being superseded by automated solutions, leveraging machine learning and deep
learning algorithms to enhance efficiency and accuracy. The ability of these
algorithms to discern patterns and make predictions based on extensive datasets
has revolutionized the domain of pavement distress identification. The paper
investigates the integration of unmanned aerial vehicles (UAVs) for data
collection, offering unique advantages such as aerial perspectives and
efficient coverage of large areas. By capturing high-resolution images, UAVs
provide valuable data that can be processed using deep learning algorithms to
detect and classify various pavement distresses effectively. While the primary
focus is on 2D image processing, the paper also acknowledges the challenges
associated with 3D images, such as sensor limitations and computational
requirements. Understanding these challenges is crucial for further
advancements in the field. The findings of this review significantly contribute
to the evolution of pavement distress detection, fostering the development of
efficient pavement management systems. As automated approaches continue to
mature, the implementation of deep learning techniques holds great promise in
ensuring safer and more durable road infrastructure for the benefit of society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1&quot;&gt;Sizhe Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haolan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourreza_H/0/1/0/all/0/1&quot;&gt;Hamid R. Pourreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahyar_H/0/1/0/all/0/1&quot;&gt;Hamidreza Mahyar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00854">
<title>Training on Foveated Images Improves Robustness to Adversarial Attacks. (arXiv:2308.00854v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00854</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been shown to be vulnerable to adversarial
attacks -- subtle, perceptually indistinguishable perturbations of inputs that
change the response of the model. In the context of vision, we hypothesize that
an important contributor to the robustness of human visual perception is
constant exposure to low-fidelity visual stimuli in our peripheral vision. To
investigate this hypothesis, we develop \RBlur, an image transform that
simulates the loss in fidelity of peripheral vision by blurring the image and
reducing its color saturation based on the distance from a given fixation
point. We show that compared to DNNs trained on the original images, DNNs
trained on images transformed by \RBlur are substantially more robust to
adversarial attacks, as well as other, non-adversarial, corruptions, achieving
up to 25\% higher accuracy on perturbed data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Muhammad A. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00906">
<title>ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation. (arXiv:2308.00906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00906</link>
<description rdf:parseType="Literal">&lt;p&gt;While language-guided image manipulation has made remarkable progress, the
challenge of how to instruct the manipulation process faithfully reflecting
human intentions persists. An accurate and comprehensive description of a
manipulation task using natural language is laborious and sometimes even
impossible, primarily due to the inherent uncertainty and ambiguity present in
linguistic expressions. Is it feasible to accomplish image manipulation without
resorting to external cross-modal language information? If this possibility
exists, the inherent modality gap would be effortlessly eliminated. In this
paper, we propose a novel manipulation methodology, dubbed ImageBrush, that
learns visual instructions for more accurate image editing. Our key idea is to
employ a pair of transformation images as visual instructions, which not only
precisely captures human intention but also facilitates accessibility in
real-world scenarios. Capturing visual instructions is particularly challenging
because it involves extracting the underlying intentions solely from visual
demonstrations and then applying this operation to a new image. To address this
challenge, we formulate visual instruction learning as a diffusion-based
inpainting problem, where the contextual information is fully exploited through
an iterative process of generation. A visual prompting encoder is carefully
devised to enhance the model&apos;s capacity in uncovering human intent behind the
visual instructions. Extensive experiments show that our method generates
engaging manipulation results conforming to the transformations entailed in
demonstrations. Moreover, our model exhibits robust generalization capabilities
on various downstream tasks such as pose transfer, image translation and video
inpainting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yasheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Houwen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yifei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koike_H/0/1/0/all/0/1&quot;&gt;Hideki Koike&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00918">
<title>A Novel Cross-Perturbation for Single Domain Generalization. (arXiv:2308.00918v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00918</link>
<description rdf:parseType="Literal">&lt;p&gt;Single domain generalization aims to enhance the ability of the model to
generalize to unknown domains when trained on a single source domain. However,
the limited diversity in the training data hampers the learning of
domain-invariant features, resulting in compromised generalization performance.
To address this, data perturbation (augmentation) has emerged as a crucial
method to increase data diversity. Nevertheless, existing perturbation methods
often focus on either image-level or feature-level perturbations independently,
neglecting their synergistic effects. To overcome these limitations, we propose
CPerb, a simple yet effective cross-perturbation method. Specifically, CPerb
utilizes both horizontal and vertical operations. Horizontally, it applies
image-level and feature-level perturbations to enhance the diversity of the
training data, mitigating the issue of limited diversity in single-source
domains. Vertically, it introduces multi-route perturbation to learn
domain-invariant features from different perspectives of samples with the same
semantic category, thereby enhancing the generalization capability of the
model. Additionally, we propose MixPatch, a novel feature-level perturbation
method that exploits local image style information to further diversify the
training data. Extensive experiments on various benchmark datasets validate the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongjia Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lei Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yinghuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00920">
<title>Virtual histological staining of unlabeled autopsy tissue. (arXiv:2308.00920v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2308.00920</link>
<description rdf:parseType="Literal">&lt;p&gt;Histological examination is a crucial step in an autopsy; however, the
traditional histochemical staining of post-mortem samples faces multiple
challenges, including the inferior staining quality due to autolysis caused by
delayed fixation of cadaver tissue, as well as the resource-intensive nature of
chemical staining procedures covering large tissue areas, which demand
substantial labor, cost, and time. These challenges can become more pronounced
during global health crises when the availability of histopathology services is
limited, resulting in further delays in tissue fixation and more severe
staining artifacts. Here, we report the first demonstration of virtual staining
of autopsy tissue and show that a trained neural network can rapidly transform
autofluorescence images of label-free autopsy tissue sections into brightfield
equivalent images that match hematoxylin and eosin (H&amp;amp;E) stained versions of
the same samples, eliminating autolysis-induced severe staining artifacts
inherent in traditional histochemical staining of autopsied tissue. Our virtual
H&amp;amp;E model was trained using &amp;gt;0.7 TB of image data and a data-efficient
collaboration scheme that integrates the virtual staining network with an image
registration network. The trained model effectively accentuated nuclear,
cytoplasmic and extracellular features in new autopsy tissue samples that
experienced severe autolysis, such as COVID-19 samples never seen before, where
the traditional histochemical staining failed to provide consistent staining
quality. This virtual autopsy staining technique can also be extended to
necrotic tissue, and can rapidly and cost-effectively generate artifact-free
H&amp;amp;E stains despite severe autolysis and cell death, also reducing labor, cost
and infrastructure requirements associated with the standard histochemical
staining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuzhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pillar_N/0/1/0/all/0/1&quot;&gt;Nir Pillar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tairan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Songyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guangdong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Haan_K/0/1/0/all/0/1&quot;&gt;Kevin de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Luzhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Sepehr Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Urisman_A/0/1/0/all/0/1&quot;&gt;Anatoly Urisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Haran_T/0/1/0/all/0/1&quot;&gt;Tal Keidar Haran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wallace_W/0/1/0/all/0/1&quot;&gt;William Dean Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zuckerman_J/0/1/0/all/0/1&quot;&gt;Jonathan E. Zuckerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00924">
<title>Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00924</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation (DA) strives to mitigate the domain gap between the source
domain where a model is trained, and the target domain where the model is
deployed. When a deep learning model is deployed on an aerial platform, it may
face gradually degrading weather conditions during operation, leading to
widening domain gaps between the training data and the encountered evaluation
data. We synthesize two such gradually worsening weather conditions on real
images from two existing aerial imagery datasets, generating a total of four
benchmark datasets. Under the continual, or test-time adaptation setting, we
evaluate three DA models on our datasets: a baseline standard DA model and two
continual DA models. In such setting, the models can access only one small
portion, or one batch of the target data at a time, and adaptation takes place
continually, and over only one epoch of the data. The combination of the
constraints of continual adaptation, and gradually deteriorating weather
conditions provide the practical DA scenario for aerial deployment. Among the
evaluated models, we consider both convolutional and transformer architectures
for comparison. We discover stability issues during adaptation for existing
buffer-fed continual DA methods, and offer gradient normalization as a simple
solution to curb training instability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_C/0/1/0/all/0/1&quot;&gt;Chowdhury Sadman Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1&quot;&gt;Andreas Savakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00926">
<title>Detection and Segmentation of Cosmic Objects Based on Adaptive Thresholding and Back Propagation Neural Network. (arXiv:2308.00926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00926</link>
<description rdf:parseType="Literal">&lt;p&gt;Astronomical images provide information about the great variety of cosmic
objects in the Universe. Due to the large volumes of data, the presence of
innumerable bright point sources as well as noise within the frame and the
spatial gap between objects and satellite cameras, it is a challenging task to
classify and detect the celestial objects. We propose an Adaptive Thresholding
Method (ATM) based segmentation and Back Propagation Neural Network (BPNN)
based cosmic object detection including a well-structured series of
pre-processing steps designed to enhance segmentation and detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultana_S/0/1/0/all/0/1&quot;&gt;Samia Sultana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afroge_S/0/1/0/all/0/1&quot;&gt;Shyla Afroge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00929">
<title>Towards Discriminative Representation with Meta-learning for Colonoscopic Polyp Re-Identification. (arXiv:2308.00929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00929</link>
<description rdf:parseType="Literal">&lt;p&gt;Colonoscopic Polyp Re-Identification aims to match the same polyp from a
large gallery with images from different views taken using different cameras
and plays an important role in the prevention and treatment of colorectal
cancer in computer-aided diagnosis. However, traditional methods for object
ReID directly adopting CNN models trained on the ImageNet dataset usually
produce unsatisfactory retrieval performance on colonoscopic datasets due to
the large domain gap. Additionally, these methods neglect to explore the
potential of self-discrepancy among intra-class relations in the colonoscopic
polyp dataset, which remains an open research problem in the medical community.
To solve this dilemma, we propose a simple but effective training method named
Colo-ReID, which can help our model to learn more general and discriminative
knowledge based on the meta-learning strategy in scenarios with fewer samples.
Based on this, a dynamic Meta-Learning Regulation mechanism called MLR is
introduced to further boost the performance of polyp re-identification. To the
best of our knowledge, this is the first attempt to leverage the meta-learning
paradigm instead of traditional machine learning to effectively train deep
models in the task of colonoscopic polyp re-identification. Empirical results
show that our method significantly outperforms current state-of-the-art methods
by a clear margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_S/0/1/0/all/0/1&quot;&gt;Suncheng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingzhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shilun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengfeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Crystal Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Sijia Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00931">
<title>WaterFlow: Heuristic Normalizing Flow for Underwater Image Enhancement and Beyond. (arXiv:2308.00931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00931</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater images suffer from light refraction and absorption, which impairs
visibility and interferes the subsequent applications. Existing underwater
image enhancement methods mainly focus on image quality improvement, ignoring
the effect on practice. To balance the visual quality and application, we
propose a heuristic normalizing flow for detection-driven underwater image
enhancement, dubbed WaterFlow. Specifically, we first develop an invertible
mapping to achieve the translation between the degraded image and its clear
counterpart. Considering the differentiability and interpretability, we
incorporate the heuristic prior into the data-driven mapping procedure, where
the ambient light and medium transmission coefficient benefit credible
generation. Furthermore, we introduce a detection perception module to transmit
the implicit semantic guidance into the enhancement procedure, where the
enhanced images hold more detection-favorable features and are able to promote
the detection performance. Extensive experiments prove the superiority of our
WaterFlow, against state-of-the-art methods quantitatively and qualitatively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zengxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00947">
<title>Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images. (arXiv:2308.00947v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00947</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex scenario of ultrasound image, in which adjacent tissues (i.e.,
background) share similar intensity with and even contain richer texture
patterns than lesion region (i.e., foreground), brings a unique challenge for
accurate lesion segmentation. This work presents a decomposition-coupling
network, called DC-Net, to deal with this challenge in a
(foreground-background) saliency map disentanglement-fusion manner. The DC-Net
consists of decomposition and coupling subnets, and the former preliminarily
disentangles original image into foreground and background saliency maps,
followed by the latter for accurate segmentation under the assistance of
saliency prior fusion. The coupling subnet involves three aspects of fusion
strategies, including: 1) regional feature aggregation (via differentiable
context pooling operator in the encoder) to adaptively preserve local
contextual details with the larger receptive field during dimension reduction;
2) relation-aware representation fusion (via cross-correlation fusion module in
the decoder) to efficiently fuse low-level visual characteristics and
high-level semantic features during resolution restoration; 3) dependency-aware
prior incorporation (via coupler) to reinforce foreground-salient
representation with the complementary information derived from background
representation. Furthermore, a harmonic loss function is introduced to
encourage the network to focus more attention on low-confidence and hard
samples. The proposed method is evaluated on two ultrasound lesion segmentation
tasks, which demonstrates the remarkable performance improvement over existing
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yixiao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qianjin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shengzhou Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00949">
<title>Training-Free Instance Segmentation from Semantic Image Segmentation Masks. (arXiv:2308.00949v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00949</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the development of instance segmentation has garnered
significant attention in a wide range of applications. However, the training of
a fully-supervised instance segmentation model requires costly both
instance-level and pixel-level annotations. In contrast, weakly-supervised
instance segmentation methods (i.e., with image-level class labels or point
labels) struggle to satisfy the accuracy and recall requirements of practical
scenarios. In this paper, we propose a novel paradigm for instance segmentation
called training-free instance segmentation (TFISeg), which achieves instance
segmentation results from image masks predicted using off-the-shelf semantic
segmentation models. TFISeg does not require training a semantic or/and
instance segmentation model and avoids the need for instance-level image
annotations. Therefore, it is highly efficient. Specifically, we first obtain a
semantic segmentation mask of the input image via a trained semantic
segmentation model. Then, we calculate a displacement field vector for each
pixel based on the segmentation mask, which can indicate representations
belonging to the same class but different instances, i.e., obtaining the
instance-level object information. Finally, instance segmentation results are
obtained after being refined by a learnable category-agnostic object boundary
branch. Extensive experimental results on two challenging datasets and
representative semantic segmentation baselines (including CNNs and
Transformers) demonstrate that TFISeg can achieve competitive results compared
to the state-of-the-art fully-supervised instance segmentation methods without
the need for additional human resources or increased computational costs. The
code is available at: TFISeg
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yuchen Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zechao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Liyong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qiaolin Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00951">
<title>From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00951</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping, inability
to scale the number of experts, or ineffective finetuning. In this work, we
proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these
challenges, while maintaining the benefits of MoEs. Soft MoE performs an
implicit soft assignment by passing different weighted combinations of all
input tokens to each expert. As in other MoE works, experts in Soft MoE only
process a subset of the (combined) tokens, enabling larger model capacity at
lower inference cost. In the context of visual recognition, Soft MoE greatly
outperforms standard Transformers (ViTs) and popular MoE variants (Tokens
Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower
inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its
performance after similar training. Soft MoE also scales well: Soft MoE Huge/14
with 128 experts in 16 MoE layers has over 40x more parameters than ViT
Huge/14, while inference time cost grows by only 2%, and it performs
substantially better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1&quot;&gt;Carlos Riquelme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00956">
<title>Curriculum Guided Domain Adaptation in the Dark. (arXiv:2308.00956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00956</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the rising concerns of privacy and security, domain adaptation in
the dark aims to adapt a black-box source trained model to an unlabeled target
domain without access to any source data or source model parameters. The need
for domain adaptation of black-box predictors becomes even more pronounced to
protect intellectual property as deep learning based solutions are becoming
increasingly commercialized. Current methods distill noisy predictions on the
target data obtained from the source model to the target model, and/or separate
clean/noisy target samples before adapting using traditional noisy label
learning algorithms. However, these methods do not utilize the easy-to-hard
learning nature of the clean/noisy data splits. Also, none of the existing
methods are end-to-end, and require a separate fine-tuning stage and an initial
warmup stage. In this work, we present Curriculum Adaptation for Black-Box
(CABB) which provides a curriculum guided adaptation approach to gradually
train the target model, first on target data with high confidence (clean)
labels, and later on target data with noisy labels. CABB utilizes
Jensen-Shannon divergence as a better criterion for clean-noisy sample
separation, compared to the traditional criterion of cross entropy loss. Our
method utilizes co-training of a dual-branch network to suppress error
accumulation resulting from confirmation bias. The proposed approach is
end-to-end trainable and does not require any extra finetuning stage, unlike
existing methods. Empirical results on standard domain adaptation datasets show
that CABB outperforms existing state-of-the-art black-box DA models and is
comparable to white-box domain adaptation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_C/0/1/0/all/0/1&quot;&gt;Chowdhury Sadman Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1&quot;&gt;Andreas Savakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00964">
<title>ForensicsForest Family: A Series of Multi-scale Hierarchical Cascade Forests for Detecting GAN-generated Faces. (arXiv:2308.00964v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00964</link>
<description rdf:parseType="Literal">&lt;p&gt;The prominent progress in generative models has significantly improved the
reality of generated faces, bringing serious concerns to society. Since recent
GAN-generated faces are in high realism, the forgery traces have become more
imperceptible, increasing the forensics challenge. To combat GAN-generated
faces, many countermeasures based on Convolutional Neural Networks (CNNs) have
been spawned due to their strong learning ability. In this paper, we rethink
this problem and explore a new approach based on forest models instead of CNNs.
Specifically, we describe a simple and effective forest-based method set called
{\em ForensicsForest Family} to detect GAN-generate faces. The proposed
ForensicsForest family is composed of three variants, which are {\em
ForensicsForest}, {\em Hybrid ForensicsForest} and {\em Divide-and-Conquer
ForensicsForest} respectively. ForenscisForest is a newly proposed Multi-scale
Hierarchical Cascade Forest, which takes semantic, frequency and biology
features as input, hierarchically cascades different levels of features for
authenticity prediction, and then employs a multi-scale ensemble scheme that
can comprehensively consider different levels of information to improve the
performance further. Based on ForensicsForest, we develop Hybrid
ForensicsForest, an extended version that integrates the CNN layers into
models, to further refine the effectiveness of augmented features. Moreover, to
reduce the memory cost in training, we propose Divide-and-Conquer
ForensicsForest, which can construct a forest model using only a portion of
training samplings. In the training stage, we train several candidate forest
models using the subsets of training samples. Then a ForensicsForest is
assembled by picking the suitable components from these candidate forest
models...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiucui Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuezun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiaran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junyu Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00982">
<title>Orientation-Guided Contrastive Learning for UAV-View Geo-Localisation. (arXiv:2308.00982v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00982</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieving relevant multimedia content is one of the main problems in a world
that is increasingly data-driven. With the proliferation of drones, high
quality aerial footage is now available to a wide audience for the first time.
Integrating this footage into applications can enable GPS-less geo-localisation
or location correction.
&lt;/p&gt;
&lt;p&gt;In this paper, we present an orientation-guided training framework for
UAV-view geo-localisation. Through hierarchical localisation orientations of
the UAV images are estimated in relation to the satellite imagery. We propose a
lightweight prediction module for these pseudo labels which predicts the
orientation between the different views based on the contrastive learned
embeddings. We experimentally demonstrate that this prediction supports the
training and outperforms previous approaches. The extracted pseudo-labels also
enable aligned rotation of the satellite image as augmentation to further
strengthen the generalisation. During inference, we no longer need this
orientation module, which means that no additional computations are required.
We achieve state-of-the-art results on both the University-1652 and
University-160k datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuser_F/0/1/0/all/0/1&quot;&gt;Fabian Deuser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habel_K/0/1/0/all/0/1&quot;&gt;Konrad Habel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_M/0/1/0/all/0/1&quot;&gt;Martin Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1&quot;&gt;Norbert Oswald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00994">
<title>Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00994</link>
<description rdf:parseType="Literal">&lt;p&gt;We live in a vast ocean of data, and deep neural networks are no exception to
this. However, this data exhibits an inherent phenomenon of imbalance. This
imbalance poses a risk of deep neural networks producing biased predictions,
leading to potentially severe ethical and social consequences. To address these
challenges, we believe that the use of generative models is a promising
approach for comprehending tasks, given the remarkable advancements
demonstrated by recent diffusion models in generating high-quality images. In
this work, we propose a simple yet effective baseline, SYNAuG, that utilizes
synthetic data as a preliminary step before employing task-specific algorithms
to address data imbalance problems. This straightforward approach yields
impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT,
UTKFace, and Waterbird, surpassing the performance of existing task-specific
methods. While we do not claim that our approach serves as a complete solution
to the problem of data imbalance, we argue that supplementing the existing data
with synthetic data proves to be an effective and crucial preliminary step in
addressing data imbalance concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1&quot;&gt;Moon Ye-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1&quot;&gt;Nam Hyeon-Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1&quot;&gt;Wonseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Nayeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01000">
<title>MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization. (arXiv:2308.01000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01000</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised 3D Object Detection models have been displaying increasingly
better performance in single-domain cases where the training data comes from
the same environment and sensor as the testing data. However, in real-world
scenarios data from the target domain may not be available for finetuning or
for domain adaptation methods. Indeed, 3D object detection models trained on a
source dataset with a specific point distribution have shown difficulties in
generalizing to unseen datasets. Therefore, we decided to leverage the
information available from several annotated source datasets with our
Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the
robustness of 3D object detection models when tested in a new environment with
a different sensor configuration. To tackle the labelling gap between datasets,
we used a new label mapping based on coarse labels. Furthermore, we show how we
managed the mix of datasets during training and finally introduce a new
cross-dataset augmentation method: cross-dataset object injection. We
demonstrate that this training paradigm shows improvements for different types
of 3D object detection models. The source code and additional results for this
research project will be publicly available on GitHub for interested parties to
access and utilize: https://github.com/LouisSF/MDT3D
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soum_Fontez_L/0/1/0/all/0/1&quot;&gt;Louis Soum-Fontez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1&quot;&gt;Jean-Emmanuel Deschaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Goulette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01006">
<title>FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a multi-modality multi-task neural network toward accurate and
robust performance is a de-facto standard in perception task of autonomous
driving. However, leveraging such data from multiple sensors to jointly
optimize the prediction and planning tasks remains largely unexplored. In this
paper, we present FusionAD, to the best of our knowledge, the first unified
framework that fuse the information from two most critical sensors, camera and
LiDAR, goes beyond perception task. Concretely, we first build a transformer
based multi-modality fusion network to effectively produce fusion based
features. In constrast to camera-based end-to-end method UniAD, we then
establish a fusion aided modality-aware prediction and status-aware planning
modules, dubbed FMSPnP that take advantages of multi-modality features. We
conduct extensive experiments on commonly used benchmark nuScenes dataset, our
FusionAD achieves state-of-the-art performance and surpassing baselines on
average 15% on perception tasks like detection and tracking, 10% on occupancy
prediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score
and reduces the collision rate from 0.31% to only 0.12%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tengju Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Wei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chunyong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shikun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lingping Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Ke Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wencong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weibo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01010">
<title>Point Anywhere: Directed Object Estimation from Omnidirectional Images. (arXiv:2308.01010v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.01010</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the intuitive instruction methods in robot navigation is a pointing
gesture. In this study, we propose a method using an omnidirectional camera to
eliminate the user/object position constraint and the left/right constraint of
the pointing arm. Although the accuracy of skeleton and object detection is low
due to the high distortion of equirectangular images, the proposed method
enables highly accurate estimation by repeatedly extracting regions of interest
from the equirectangular image and projecting them onto perspective images.
Furthermore, we found that training the likelihood of the target object in
machine learning further improves the estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotani_N/0/1/0/all/0/1&quot;&gt;Nanami Kotani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanezaki_A/0/1/0/all/0/1&quot;&gt;Asako Kanezaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01030">
<title>Three Factors to Improve Out-of-Distribution Detection. (arXiv:2308.01030v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01030</link>
<description rdf:parseType="Literal">&lt;p&gt;In the problem of out-of-distribution (OOD) detection, the usage of auxiliary
data as outlier data for fine-tuning has demonstrated encouraging performance.
However, previous methods have suffered from a trade-off between classification
accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve
this trade-off, we make three contributions: (i) Incorporating a self-knowledge
distillation loss can enhance the accuracy of the network; (ii) Sampling
semi-hard outlier data for training can improve OOD detection performance with
minimal impact on accuracy; (iii) The introduction of our novel supervised
contrastive learning can simultaneously improve OOD detection performance and
the accuracy of the network. By incorporating all three factors, our approach
enhances both accuracy and OOD detection performance by addressing the
trade-off between classification and OOD detection. Our method achieves
improvements over previous approaches in both performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hyunjun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;JaeHo Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hawook Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jin Young Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01035">
<title>TS-RGBD Dataset: a Novel Dataset for Theatre Scenes Description for People with Visual Impairments. (arXiv:2308.01035v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01035</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision was long a tool used for aiding visually impaired people to
move around their environment and avoid obstacles and falls. Solutions are
limited to either indoor or outdoor scenes, which limits the kind of places and
scenes visually disabled people can be in, including entertainment places such
as theatres. Furthermore, most of the proposed computer-vision-based methods
rely on RGB benchmarks to train their models resulting in a limited performance
due to the absence of the depth modality.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel RGB-D dataset containing theatre scenes
with ground truth human actions and dense captions annotations for image
captioning and human action recognition: TS-RGBD dataset. It includes three
types of data: RGB, depth, and skeleton sequences, captured by Microsoft
Kinect.
&lt;/p&gt;
&lt;p&gt;We test image captioning models on our dataset as well as some skeleton-based
human action recognition models in order to extend the range of environment
types where a visually disabled person can be, by detecting human actions and
textually describing appearances of regions of interest in theatre scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benhamida_L/0/1/0/all/0/1&quot;&gt;Leyla Benhamida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delloul_K/0/1/0/all/0/1&quot;&gt;Khadidja Delloul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larabi_S/0/1/0/all/0/1&quot;&gt;Slimane Larabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01042">
<title>WCCNet: Wavelet-integrated CNN with Crossmodal Rearranging Fusion for Fast Multispectral Pedestrian Detection. (arXiv:2308.01042v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01042</link>
<description rdf:parseType="Literal">&lt;p&gt;Multispectral pedestrian detection achieves better visibility in challenging
conditions and thus has a broad application in various tasks, for which both
the accuracy and computational cost are of paramount importance. Most existing
approaches treat RGB and infrared modalities equally, typically adopting two
symmetrical CNN backbones for multimodal feature extraction, which ignores the
substantial differences between modalities and brings great difficulty for the
reduction of the computational cost as well as effective crossmodal fusion. In
this work, we propose a novel and efficient framework named WCCNet that is able
to differentially extract rich features of different spectra with lower
computational complexity and semantically rearranges these features for
effective crossmodal fusion. Specifically, the discrete wavelet transform (DWT)
allowing fast inference and training speed is embedded to construct a
dual-stream backbone for efficient feature extraction. The DWT layers of WCCNet
extract frequency components for infrared modality, while the CNN layers
extract spatial-domain features for RGB modality. This methodology not only
significantly reduces the computational complexity, but also improves the
extraction of infrared features to facilitate the subsequent crossmodal fusion.
Based on the well extracted features, we elaborately design the crossmodal
rearranging fusion module (CMRF), which can mitigate spatial misalignment and
merge semantically complementary features of spatially-related local regions to
amplify the crossmodal complementary information. We conduct comprehensive
evaluations on KAIST and FLIR benchmarks, in which WCCNet outperforms
state-of-the-art methods with considerable computational efficiency and
competitive accuracy. We also perform the ablation study and analyze thoroughly
the impact of different components on the performance of WCCNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingjian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1&quot;&gt;Li Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01045">
<title>Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation. (arXiv:2308.01045v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01045</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers have achieved leading performance on various visual tasks
yet still suffer from high computational complexity. The situation deteriorates
in dense prediction tasks like semantic segmentation, as high-resolution inputs
and outputs usually imply more tokens involved in computations. Directly
removing the less attentive tokens has been discussed for the image
classification task but can not be extended to semantic segmentation since a
dense prediction is required for every patch. To this end, this work introduces
a Dynamic Token Pruning (DToP) method based on the early exit of tokens for
semantic segmentation. Motivated by the coarse-to-fine segmentation process by
humans, we naturally split the widely adopted auxiliary-loss-based network
architecture into several stages, where each auxiliary block grades every
token&apos;s difficulty level. We can finalize the prediction of easy tokens in
advance without completing the entire forward pass. Moreover, we keep $k$
highest confidence tokens for each semantic category to uphold the
representative context information. Thus, computational complexity will change
with the difficulty of the input, akin to the way humans do segmentation.
Experiments suggest that the proposed DToP architecture reduces on average
$20\% - 35\%$ of computational cost for current semantic segmentation methods
based on plain vision transformers without accuracy degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Quan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiajun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fagiu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01057">
<title>MammoDG: Generalisable Deep Learning Breaks the Limits of Cross-Domain Multi-Center Breast Cancer Screening. (arXiv:2308.01057v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01057</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer is a major cause of cancer death among women, emphasising the
importance of early detection for improved treatment outcomes and quality of
life. Mammography, the primary diagnostic imaging test, poses challenges due to
the high variability and patterns in mammograms. Double reading of mammograms
is recommended in many screening programs to improve diagnostic accuracy but
increases radiologists&apos; workload. Researchers explore Machine Learning models
to support expert decision-making. Stand-alone models have shown comparable or
superior performance to radiologists, but some studies note decreased
sensitivity with multiple datasets, indicating the need for high generalisation
and robustness models. This work devises MammoDG, a novel deep-learning
framework for generalisable and reliable analysis of cross-domain multi-center
mammography data. MammoDG leverages multi-view mammograms and a novel
contrastive mechanism to enhance generalisation capabilities. Extensive
validation demonstrates MammoDG&apos;s superiority, highlighting the critical
importance of domain generalisation for trustworthy mammography analysis in
imaging protocol variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shujun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hickman_S/0/1/0/all/0/1&quot;&gt;Sarah Hickman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_F/0/1/0/all/0/1&quot;&gt;Fiona J Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aviles_Rivero_A/0/1/0/all/0/1&quot;&gt;Angelica I. Aviles-Rivero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01058">
<title>Improving Generalization of Synthetically Trained Sonar Image Descriptors for Underwater Place Recognition. (arXiv:2308.01058v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01058</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous navigation in underwater environments presents challenges due to
factors such as light absorption and water turbidity, limiting the
effectiveness of optical sensors. Sonar systems are commonly used for
perception in underwater operations as they are unaffected by these
limitations. Traditional computer vision algorithms are less effective when
applied to sonar-generated acoustic images, while convolutional neural networks
(CNNs) typically require large amounts of labeled training data that are often
unavailable or difficult to acquire. To this end, we propose a novel compact
deep sonar descriptor pipeline that can generalize to real scenarios while
being trained exclusively on synthetic data. Our architecture is based on a
ResNet18 back-end and a properly parameterized random Gaussian projection
layer, whereas input sonar data is enhanced with standard ad-hoc
normalization/prefiltering techniques. A customized synthetic data generation
procedure is also presented. The proposed method has been evaluated extensively
using both synthetic and publicly available real data, demonstrating its
effectiveness compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donadi_I/0/1/0/all/0/1&quot;&gt;Ivano Donadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olivastri_E/0/1/0/all/0/1&quot;&gt;Emilio Olivastri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusaro_D/0/1/0/all/0/1&quot;&gt;Daniel Fusaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wanmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evangelista_D/0/1/0/all/0/1&quot;&gt;Daniele Evangelista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretto_A/0/1/0/all/0/1&quot;&gt;Alberto Pretto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01086">
<title>Homography Estimation in Complex Topological Scenes. (arXiv:2308.01086v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01086</link>
<description rdf:parseType="Literal">&lt;p&gt;Surveillance videos and images are used for a broad set of applications,
ranging from traffic analysis to crime detection. Extrinsic camera calibration
data is important for most analysis applications. However, security cameras are
susceptible to environmental conditions and small camera movements, resulting
in a need for an automated re-calibration method that can account for these
varying conditions. In this paper, we present an automated camera-calibration
process leveraging a dictionary-based approach that does not require prior
knowledge on any camera settings. The method consists of a custom
implementation of a Spatial Transformer Network (STN) and a novel topological
loss function. Experiments reveal that the proposed method improves the IoU
metric by up to 12% w.r.t. a state-of-the-art model across five synthetic
datasets and the World Cup 2014 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAmicantonio_G/0/1/0/all/0/1&quot;&gt;Giacomo D&amp;#x27;Amicantonio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondarau_E/0/1/0/all/0/1&quot;&gt;Egor Bondarau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1&quot;&gt;Peter H.N. De With&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01088">
<title>Hand tracking for clinical applications: validation of the Google MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks. (arXiv:2308.01088v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01088</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D tracking of hand and fingers movements poses significant
challenges in computer vision. The potential applications span across multiple
domains, including human-computer interaction, virtual reality, industry, and
medicine. While gesture recognition has achieved remarkable accuracy,
quantifying fine movements remains a hurdle, particularly in clinical
applications where the assessment of hand dysfunctions and rehabilitation
training outcomes necessitate precise measurements. Several novel and
lightweight frameworks based on Deep Learning have emerged to address this
issue; however, their performance in accurately and reliably measuring fingers
movements requires validation against well-established gold standard systems.
In this paper, the aim is to validate the handtracking framework implemented by
Google MediaPipe Hand (GMH) and an innovative enhanced version, GMH-D, that
exploits the depth estimation of an RGB-Depth camera to achieve more accurate
tracking of 3D movements. Three dynamic exercises commonly administered by
clinicians to assess hand dysfunctions, namely Hand Opening-Closing, Single
Finger Tapping and Multiple Finger Tapping are considered. Results demonstrate
high temporal and spectral consistency of both frameworks with the gold
standard. However, the enhanced GMH-D framework exhibits superior accuracy in
spatial measurements compared to the baseline GMH, for both slow and fast
movements. Overall, our study contributes to the advancement of hand tracking
technology, the establishment of a validation procedure as a good-practice to
prove efficacy of deep-learning-based hand-tracking, and proves the
effectiveness of GMH-D as a reliable framework for assessing 3D hand movements
in clinical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amprimo_G/0/1/0/all/0/1&quot;&gt;Gianluca Amprimo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masi_G/0/1/0/all/0/1&quot;&gt;Giulia Masi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pettiti_G/0/1/0/all/0/1&quot;&gt;Giuseppe Pettiti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olmo_G/0/1/0/all/0/1&quot;&gt;Gabriella Olmo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priano_L/0/1/0/all/0/1&quot;&gt;Lorenzo Priano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferraris_C/0/1/0/all/0/1&quot;&gt;Claudia Ferraris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01095">
<title>AutoPoster: A Highly Automatic and Content-aware Design System for Advertising Poster Generation. (arXiv:2308.01095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01095</link>
<description rdf:parseType="Literal">&lt;p&gt;Advertising posters, a form of information presentation, combine visual and
linguistic modalities. Creating a poster involves multiple steps and
necessitates design experience and creativity. This paper introduces
AutoPoster, a highly automatic and content-aware system for generating
advertising posters. With only product images and titles as inputs, AutoPoster
can automatically produce posters of varying sizes through four key stages:
image cleaning and retargeting, layout generation, tagline generation, and
style attribute prediction. To ensure visual harmony of posters, two
content-aware models are incorporated for layout and tagline generation.
Moreover, we propose a novel multi-task Style Attribute Predictor (SAP) to
jointly predict visual style attributes. Meanwhile, to our knowledge, we
propose the first poster generation dataset that includes visual attribute
annotations for over 76k posters. Qualitative and quantitative outcomes from
user studies and experiments substantiate the efficacy of our system and the
aesthetic superiority of the generated posters compared to other poster
generation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Ye Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_C/0/1/0/all/0/1&quot;&gt;Chenxi Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yangjian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01097">
<title>Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction (HMP) has emerged as a popular research topic due to
its diverse applications, but it remains a challenging task due to the
stochastic and aperiodic nature of future poses. Traditional methods rely on
hand-crafted features and machine learning techniques, which often struggle to
model the complex dynamics of human motion. Recent deep learning-based methods
have achieved success by learning spatio-temporal representations of motion,
but these models often overlook the reliability of motion data. Additionally,
the temporal and spatial dependencies of skeleton nodes are distinct. The
temporal relationship captures motion information over time, while the spatial
relationship describes body structure and the relationships between different
nodes. In this paper, we propose a novel spatio-temporal branching network
using incremental information for HMP, which decouples the learning of
temporal-domain and spatial-domain features, extracts more motion information,
and achieves complementary cross-domain knowledge learning through knowledge
distillation. Our approach effectively reduces noise interference and provides
more expressive information for characterizing motion by separately extracting
temporal and spatial features. We evaluate our approach on standard HMP
benchmarks and outperform state-of-the-art methods in terms of prediction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yujie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1&quot;&gt;Ying Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01119">
<title>Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image classification models are frequently trained using training
datasets derived from multiple data sources. While leveraging multiple data
sources is crucial for achieving model generalization, it is important to
acknowledge that the diverse nature of these sources inherently introduces
unintended confounders and other challenges that can impact both model accuracy
and transparency. A notable confounding factor in medical image classification,
particularly in musculoskeletal image classification, is skeletal
maturation-induced bone growth observed during adolescence. We train a deep
learning model using a Covid-19 chest X-ray dataset and we showcase how this
dataset can lead to spurious correlations due to unintended confounding
regions. eXplanation Based Learning (XBL) is a deep learning approach that goes
beyond interpretability by utilizing model explanations to interactively
unlearn spurious correlations. This is achieved by integrating interactive user
feedback, specifically feature annotations. In our study, we employed two
non-demanding manual feedback mechanisms to implement an XBL-based approach for
effectively eliminating these spurious correlations. Our results underscore the
promising potential of XBL in constructing robust models even in the presence
of confounding factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01125">
<title>Stereo Visual Odometry with Deep Learning-Based Point and Line Feature Matching using an Attention Graph Neural Network. (arXiv:2308.01125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01125</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust feature matching forms the backbone for most Visual Simultaneous
Localization and Mapping (vSLAM), visual odometry, 3D reconstruction, and
Structure from Motion (SfM) algorithms. However, recovering feature matches
from texture-poor scenes is a major challenge and still remains an open area of
research. In this paper, we present a Stereo Visual Odometry (StereoVO)
technique based on point and line features which uses a novel feature-matching
mechanism based on an Attention Graph Neural Network that is designed to
perform well even under adverse weather conditions such as fog, haze, rain, and
snow, and dynamic lighting conditions such as nighttime illumination and glare
scenarios. We perform experiments on multiple real and synthetic datasets to
validate the ability of our method to perform StereoVO under low visibility
weather and lighting conditions through robust point and line matches. The
results demonstrate that our method achieves more line feature matches than
state-of-the-art line matching algorithms, which when complemented with point
feature matches perform consistently well in adverse weather and dynamic
lighting conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kannapiran_S/0/1/0/all/0/1&quot;&gt;Shenbagaraj Kannapiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendapudi_N/0/1/0/all/0/1&quot;&gt;Nalin Bendapudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Ming-Yuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1&quot;&gt;Devarth Parikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berman_S/0/1/0/all/0/1&quot;&gt;Spring Berman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1&quot;&gt;Ankit Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1&quot;&gt;Gaurav Pandey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01126">
<title>Beyond Generic: Enhancing Image Captioning with Real-World Knowledge using Vision-Language Pre-Training Model. (arXiv:2308.01126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01126</link>
<description rdf:parseType="Literal">&lt;p&gt;Current captioning approaches tend to generate correct but &quot;generic&quot;
descriptions that lack real-world knowledge, e.g., named entities and
contextual information. Considering that Vision-Language Pre-Training (VLP)
models master massive such knowledge from large-scale web-harvested data, it is
promising to utilize the generalizability of VLP models to incorporate
knowledge into image descriptions. However, using VLP models faces challenges:
zero-shot inference suffers from knowledge hallucination that leads to
low-quality descriptions, but the generic bias in downstream task fine-tuning
hinders the VLP model from expressing knowledge. To address these concerns, we
propose a simple yet effective method called Knowledge-guided Replay
(K-Replay), which enables the retention of pre-training knowledge during
fine-tuning. Our approach consists of two parts: (1) a knowledge prediction
task on automatically collected replay exemplars to continuously awaken the VLP
model&apos;s memory about knowledge, thus preventing the model from collapsing into
the generic pattern; (2) a knowledge distillation constraint to improve the
faithfulness of generated descriptions hence alleviating the knowledge
hallucination. To evaluate knowledge-enhanced descriptions, we construct a
novel captioning benchmark KnowCap, containing knowledge of landmarks, famous
brands, special foods and movie characters. Experimental results show that our
approach effectively incorporates knowledge into descriptions, outperforming
strong VLP baseline by 20.9 points (78.7-&amp;gt;99.6) in CIDEr score and 20.5
percentage points (34.0%-&amp;gt;54.5%) in knowledge recognition accuracy. Our code
and data is available at https://github.com/njucckevin/KnowCap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kanzhi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Wenpo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianbing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01127">
<title>DiffusePast: Diffusion-based Generative Replay for Class Incremental Semantic Segmentation. (arXiv:2308.01127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01127</link>
<description rdf:parseType="Literal">&lt;p&gt;The Class Incremental Semantic Segmentation (CISS) extends the traditional
segmentation task by incrementally learning newly added classes. Previous work
has introduced generative replay, which involves replaying old class samples
generated from a pre-trained GAN, to address the issues of catastrophic
forgetting and privacy concerns. However, the generated images lack semantic
precision and exhibit out-of-distribution characteristics, resulting in
inaccurate masks that further degrade the segmentation performance. To tackle
these challenges, we propose DiffusePast, a novel framework featuring a
diffusion-based generative replay module that generates semantically accurate
images with more reliable masks guided by different instructions (e.g., text
prompts or edge maps). Specifically, DiffusePast introduces a dual-generator
paradigm, which focuses on generating old class images that align with the
distribution of downstream datasets while preserving the structure and layout
of the original images, enabling more precise masks. To adapt to the novel
visual concepts of newly added classes continuously, we incorporate class-wise
token embedding when updating the dual-generator. Moreover, we assign adequate
pseudo-labels of old classes to the background pixels in the new step images,
further mitigating the forgetting of previously learned knowledge. Through
comprehensive experiments, our method demonstrates competitive performance
across mainstream benchmarks, striking a better balance between the performance
of old and novel classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingfan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01136">
<title>Leveraging Expert Models for Training Deep Neural Networks in Scarce Data Domains: Application to Offline Handwritten Signature Verification. (arXiv:2308.01136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01136</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach to leverage the knowledge of existing
expert models for training new Convolutional Neural Networks, on domains where
task-specific data are limited or unavailable. The presented scheme is applied
in offline handwritten signature verification (OffSV) which, akin to other
biometric applications, suffers from inherent data limitations due to
regulatory restrictions. The proposed Student-Teacher (S-T) configuration
utilizes feature-based knowledge distillation (FKD), combining graph-based
similarity for local activations with global similarity measures to supervise
student&apos;s training, using only handwritten text data. Remarkably, the models
trained using this technique exhibit comparable, if not superior, performance
to the teacher model across three popular signature datasets. More importantly,
these results are attained without employing any signatures during the feature
extraction training process. This study demonstrates the efficacy of leveraging
existing expert models to overcome data scarcity challenges in OffSV and
potentially other related domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsourounis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Tsourounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorakopoulos_I/0/1/0/all/0/1&quot;&gt;Ilias Theodorakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zois_E/0/1/0/all/0/1&quot;&gt;Elias N. Zois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Economou_G/0/1/0/all/0/1&quot;&gt;George Economou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01137">
<title>Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans. (arXiv:2308.01137v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01137</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer and covid-19 have one of the highest morbidity and mortality
rates in the world. For physicians, the identification of lesions is difficult
in the early stages of the disease and time-consuming. Therefore, multi-task
learning is an approach to extracting important features, such as lesions, from
small amounts of medical data because it learns to generalize better. We
propose a novel multi-task framework for classification, segmentation,
reconstruction, and detection. To the best of our knowledge, we are the first
ones who added detection to the multi-task solution. Additionally, we checked
the possibility of using two different backbones and different loss functions
in the segmentation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hryniewska_Guzik_W/0/1/0/all/0/1&quot;&gt;Weronika Hryniewska-Guzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kedzierska_M/0/1/0/all/0/1&quot;&gt;Maria K&amp;#x119;dzierska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01140">
<title>DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01140</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary self-supervised contrastive algorithms like SimCLR, MoCo,
etc., the task of balancing attraction between two semantically similar samples
and repulsion between two samples from different classes is primarily affected
by the presence of hard negative samples. While the InfoNCE loss has been shown
to impose penalties based on hardness, the temperature hyper-parameter is the
key to regulating the penalties and the trade-off between uniformity and
tolerance. In this work, we focus our attention to improve the performance of
InfoNCE loss in SSL by studying the effect of temperature hyper-parameter
values. We propose a cosine similarity-dependent temperature scaling function
to effectively optimize the distribution of the samples in the feature space.
We further analyze the uniformity and tolerance metrics to investigate the
optimal regions in the cosine similarity space for better optimization.
Additionally, we offer a comprehensive examination of the behavior of local and
global structures in the feature space throughout the pre-training phase, as
the temperature varies. Experimental evidence shows that the proposed framework
outperforms or is at par with the contrastive loss-based SSL algorithms. We
believe our work (DySTreSS) on temperature scaling in SSL provides a foundation
for future research in contrastive learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1&quot;&gt;Siladittya Manna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Soumitri Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1&quot;&gt;Rakesh Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1&quot;&gt;Umapada Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01143">
<title>ADS-Cap: A Framework for Accurate and Diverse Stylized Captioning with Unpaired Stylistic Corpora. (arXiv:2308.01143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01143</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating visually grounded image captions with specific linguistic styles
using unpaired stylistic corpora is a challenging task, especially since we
expect stylized captions with a wide variety of stylistic patterns. In this
paper, we propose a novel framework to generate Accurate and Diverse Stylized
Captions (ADS-Cap). Our ADS-Cap first uses a contrastive learning module to
align the image and text features, which unifies paired factual and unpaired
stylistic corpora during the training process. A conditional variational
auto-encoder is then used to automatically memorize diverse stylistic patterns
in latent space and enhance diversity through sampling. We also design a simple
but effective recheck module to boost style accuracy by filtering
style-specific captions. Experimental results on two widely used stylized image
captioning datasets show that regarding consistency with the image, style
accuracy and diversity, ADS-Cap achieves outstanding performances compared to
various baselines. We finally conduct extensive analyses to understand the
effectiveness of our method. Our code is available at
https://github.com/njucckevin/ADS-Cap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kanzhi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1&quot;&gt;Shi Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianbing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xinyu Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01146">
<title>UCDFormer: Unsupervised Change Detection Using a Transformer-driven Image Translation. (arXiv:2308.01146v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01146</link>
<description rdf:parseType="Literal">&lt;p&gt;Change detection (CD) by comparing two bi-temporal images is a crucial task
in remote sensing. With the advantages of requiring no cumbersome labeled
change information, unsupervised CD has attracted extensive attention in the
community. However, existing unsupervised CD approaches rarely consider the
seasonal and style differences incurred by the illumination and atmospheric
conditions in multi-temporal images. To this end, we propose a change detection
with domain shift setting for remote sensing images. Furthermore, we present a
novel unsupervised CD method using a light-weight transformer, called
UCDFormer. Specifically, a transformer-driven image translation composed of a
light-weight transformer and a domain-specific affinity weight is first
proposed to mitigate domain shift between two images with real-time efficiency.
After image translation, we can generate the difference map between the
translated before-event image and the original after-event image. Then, a novel
reliable pixel extraction module is proposed to select significantly
changed/unchanged pixel positions by fusing the pseudo change maps of fuzzy
c-means clustering and adaptive threshold. Finally, a binary change map is
obtained based on these selected pixel pairs and a binary classifier.
Experimental results on different unsupervised CD tasks with seasonal and style
changes demonstrate the effectiveness of the proposed UCDFormer. For example,
compared with several other related methods, UCDFormer improves performance on
the Kappa coefficient by more than 12\%. In addition, UCDFormer achieves
excellent performance for earthquake-induced landslide detection when
considering large-scale applications. The code is available at
\url{https://github.com/zhu-xlab/UCDFormer}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingsong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yilei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianhua Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1&quot;&gt;Chaojun Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiao Xiang Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01147">
<title>Contrast-augmented Diffusion Model with Fine-grained Sequence Alignment for Markup-to-Image Generation. (arXiv:2308.01147v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01147</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently rising markup-to-image generation poses greater challenges as
compared to natural image generation, due to its low tolerance for errors as
well as the complex sequence and context correlations between markup and
rendered image. This paper proposes a novel model named &quot;Contrast-augmented
Diffusion Model with Fine-grained Sequence Alignment&quot; (FSA-CDM), which
introduces contrastive positive/negative samples into the diffusion model to
boost performance for markup-to-image generation. Technically, we design a
fine-grained cross-modal alignment module to well explore the sequence
similarity between the two modalities for learning robust feature
representations. To improve the generalization ability, we propose a
contrast-augmented diffusion model to explicitly explore positive and negative
samples by maximizing a novel contrastive variational objective, which is
mathematically inferred to provide a tighter bound for the model&apos;s
optimization. Moreover, the context-aware cross attention module is developed
to capture the contextual information within markup language during the
denoising process, yielding better noise prediction results. Extensive
experiments are conducted on four benchmark datasets from different domains,
and the experimental results demonstrate the effectiveness of the proposed
components in FSA-CDM, significantly exceeding state-of-the-art performance by
about 2%-12% DTW improvements. The code will be released at
https://github.com/zgj77/FSACDM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_G/0/1/0/all/0/1&quot;&gt;Guojin Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kailun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1&quot;&gt;Weili Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01175">
<title>Memory Encoding Model. (arXiv:2308.01175v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01175</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore a new class of brain encoding model by adding memory-related
information as input. Memory is an essential brain mechanism that works
alongside visual stimuli. During a vision-memory cognitive task, we found the
non-visual brain is largely predictable using previously seen images. Our
Memory Encoding Model (Mem) won the Algonauts 2023 visual brain competition
even without model ensemble (single model score 66.8, ensemble score 70.8). Our
ensemble model without memory input (61.4) can also stand a 3rd place.
Furthermore, we observe periodic delayed brain response correlated to 6th-7th
prior image, and hippocampus also showed correlated activity timed with this
periodicity. We conjuncture that the periodic replay could be related to memory
mechanism to enhance the working memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huzheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianbo Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01180">
<title>Interpretable End-to-End Driving Model for Implicit Scene Understanding. (arXiv:2308.01180v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01180</link>
<description rdf:parseType="Literal">&lt;p&gt;Driving scene understanding is to obtain comprehensive scene information
through the sensor data and provide a basis for downstream tasks, which is
indispensable for the safety of self-driving vehicles. Specific perception
tasks, such as object detection and scene graph generation, are commonly used.
However, the results of these tasks are only equivalent to the characterization
of sampling from high-dimensional scene features, which are not sufficient to
represent the scenario. In addition, the goal of perception tasks is
inconsistent with human driving that just focuses on what may affect the
ego-trajectory. Therefore, we propose an end-to-end Interpretable Implicit
Driving Scene Understanding (II-DSU) model to extract implicit high-dimensional
scene features as scene understanding results guided by a planning module and
to validate the plausibility of scene understanding using auxiliary perception
tasks for visualization. Experimental results on CARLA benchmarks show that our
approach achieves the new state-of-the-art and is able to obtain scene features
that embody richer scene information relevant to driving, enabling superior
performance of the downstream planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaonian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiagui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaqiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01184">
<title>Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01184</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning with noisy labels has been addressed with both discriminative
and generative models. Although discriminative models have dominated the field
due to their simpler modeling and more efficient computational training
processes, generative models offer a more effective means of disentangling
clean and noisy labels and improving the estimation of the label transition
matrix. However, generative approaches maximize the joint likelihood of noisy
labels and data using a complex formulation that only indirectly optimizes the
model of interest associating data and clean labels. Additionally, these
approaches rely on generative models that are challenging to train and tend to
use uninformative clean label priors. In this paper, we propose a new
generative noisy-label learning approach that addresses these three issues.
First, we propose a new model optimisation that directly associates data and
clean labels. Second, the generative model is implicitly estimated using a
discriminative model, eliminating the inefficient training of a generative
model. Third, we propose a new informative label prior inspired by partial
label learning as supervision signal for noisy label learning. Extensive
experiments on several noisy-label benchmarks demonstrate that our generative
model provides state-of-the-art results while maintaining a similar
computational complexity as discriminative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengbei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01189">
<title>Data-Centric Diet: Effective Multi-center Dataset Pruning for Medical Image Segmentation. (arXiv:2308.01189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01189</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper seeks to address the dense labeling problems where a significant
fraction of the dataset can be pruned without sacrificing much accuracy. We
observe that, on standard medical image segmentation benchmarks, the loss
gradient norm-based metrics of individual training examples applied in image
classification fail to identify the important samples. To address this issue,
we propose a data pruning method by taking into consideration the training
dynamics on target regions using Dynamic Average Dice (DAD) score. To the best
of our knowledge, we are among the first to address the data importance in
dense labeling tasks in the field of medical image analysis, making the
following contributions: (1) investigating the underlying causes with rigorous
empirical analysis, and (2) determining effective data pruning approach in
dense labeling problems. Our solution can be used as a strong yet simple
baseline to select important examples for medical image segmentation with
combined data sources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongkang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingjin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhijing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yongyi Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01194">
<title>Improving Generalization in Visual Reinforcement Learning via Conflict-aware Gradient Agreement Augmentation. (arXiv:2308.01194v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01194</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a policy with great generalization to unseen environments remains
challenging but critical in visual reinforcement learning. Despite the success
of augmentation combination in the supervised learning generalization, naively
applying it to visual RL algorithms may damage the training efficiency,
suffering from serve performance degradation. In this paper, we first conduct
qualitative analysis and illuminate the main causes: (i) high-variance gradient
magnitudes and (ii) gradient conflicts existed in various augmentation methods.
To alleviate these issues, we propose a general policy gradient optimization
framework, named Conflict-aware Gradient Agreement Augmentation (CG2A), and
better integrate augmentation combination into visual RL algorithms to address
the generalization bias. In particular, CG2A develops a Gradient Agreement
Solver to adaptively balance the varying gradient magnitudes, and introduces a
Soft Gradient Surgery strategy to alleviate the gradient conflicts. Extensive
experiments demonstrate that CG2A significantly improves the generalization
performance and sample efficiency of visual RL algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhile Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziqing Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1&quot;&gt;Xie Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1&quot;&gt;Zhongxue Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01196">
<title>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01196</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender Systems have become crucial in the modern world, commonly guiding
users towards relevant content or products, and having a large influence over
the decisions of users and citizens. However, ensuring transparency and user
trust in these systems remains a challenge; personalized explanations have
emerged as a solution, offering justifications for recommendations. Among the
existing approaches for generating personalized explanations, using visual
content created by the users is one particularly promising option, showing a
potential to maximize transparency and user trust. Existing models for
explaining recommendations in this context face limitations: sustainability has
been a critical concern, as they often require substantial computational
resources, leading to significant carbon emissions comparable to the
Recommender Systems where they would be integrated. Moreover, most models
employ surrogate learning goals that do not align with the objective of ranking
the most effective personalized explanations for a given recommendation,
leading to a suboptimal learning process and larger model sizes. To address
these limitations, we present BRIE, a novel model designed to tackle the
existing challenges by adopting a more adequate learning goal based on Bayesian
Pairwise Ranking, enabling it to achieve consistently superior performance than
state-of-the-art models in six real-world datasets, while exhibiting remarkable
efficiency, emitting up to 75% less CO${_2}$ during training and inference with
a model up to 64 times smaller than previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_Ruza_J/0/1/0/all/0/1&quot;&gt;Jorge Paz-Ruza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Betanzos_A/0/1/0/all/0/1&quot;&gt;Amparo Alonso-Betanzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1&quot;&gt;Berta Guijarro-Berdi&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancela_B/0/1/0/all/0/1&quot;&gt;Brais Cancela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiras_Franco_C/0/1/0/all/0/1&quot;&gt;Carlos Eiras-Franco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01217">
<title>TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval. (arXiv:2308.01217v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01217</link>
<description rdf:parseType="Literal">&lt;p&gt;For text-to-video retrieval (T2VR), which aims to retrieve unlabeled videos
by ad-hoc textual queries, CLIP-based methods are dominating. Compared to
CLIP4Clip which is efficient and compact, the state-of-the-art models tend to
compute video-text similarity by fine-grained cross-modal feature interaction
and matching, putting their scalability for large-scale T2VR into doubt. For
efficient T2VR, we propose TeachCLIP with multi-grained teaching to let a
CLIP4Clip based student network learn from more advanced yet computationally
heavy models such as X-CLIP, TS2-Net and X-Pool . To improve the student&apos;s
learning capability, we add an Attentional frame-Feature Aggregation (AFA)
block, which by design adds no extra storage/computation overhead at the
retrieval stage. While attentive weights produced by AFA are commonly used for
combining frame-level features, we propose a novel use of the weights to let
them imitate frame-text relevance estimated by the teacher network. As such,
AFA provides a fine-grained learning (teaching) channel for the student
(teacher). Extensive experiments on multiple public datasets justify the
viability of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1&quot;&gt;Kaibin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruixiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1&quot;&gt;Runquan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_F/0/1/0/all/0/1&quot;&gt;Fengzong Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1&quot;&gt;Zhanhui Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xirong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01236">
<title>Grounded Image Text Matching with Mismatched Relation Reasoning. (arXiv:2308.01236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01236</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Grounded Image Text Matching with Mismatched Relation
(GITM-MR), a novel visual-linguistic joint task that evaluates the relation
understanding capabilities of transformer-based pre-trained models. GITM-MR
requires a model to first determine if an expression describes an image, then
localize referred objects or ground the mismatched parts of the text. We
provide a benchmark for evaluating pre-trained models on this task, with a
focus on the challenging settings of limited data and out-of-distribution
sentence lengths. Our evaluation demonstrates that pre-trained models lack data
efficiency and length generalization ability. To address this, we propose the
Relation-sensitive Correspondence Reasoning Network (RCRN), which incorporates
relation-aware reasoning via bi-directional message propagation guided by
language structure. RCRN can be interpreted as a modular program and delivers
strong performance in both length generalization and data efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yana Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sibei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01239">
<title>CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion. (arXiv:2308.01239v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01239</link>
<description rdf:parseType="Literal">&lt;p&gt;The U-shaped architecture has emerged as a crucial paradigm in the design of
medical image segmentation networks. However, due to the inherent local
limitations of convolution, a fully convolutional segmentation network with
U-shaped architecture struggles to effectively extract global context
information, which is vital for the precise localization of lesions. While
hybrid architectures combining CNNs and Transformers can address these issues,
their application in real medical scenarios is limited due to the computational
resource constraints imposed by the environment and edge devices. In addition,
the convolutional inductive bias in lightweight networks adeptly fits the
scarce medical data, which is lacking in the Transformer based network. In
order to extract global context information while taking advantage of the
inductive bias, we propose CMUNeXt, an efficient fully convolutional
lightweight medical image segmentation network, which enables fast and accurate
auxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel and
inverted bottleneck design to thoroughly mix distant spatial and location
information, efficiently extracting global context information. We also
introduce the Skip-Fusion block, designed to enable smooth skip-connections and
ensure ample feature fusion. Experimental results on multiple medical image
datasets demonstrate that CMUNeXt outperforms existing heavyweight and
lightweight medical image segmentation networks in terms of segmentation
performance, while offering a faster inference speed, lighter weights, and a
reduced computational cost. The code is available at
https://github.com/FengheTan9/CMUNeXt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fenghe Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jianrui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingtao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chunping Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01246">
<title>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01246</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital preservation of Cultural Heritage (CH) sites is crucial to protect
them against damage from natural disasters or human activities. Creating 3D
models of CH sites has become a popular method of digital preservation thanks
to advancements in computer vision and photogrammetry. However, the process is
time-consuming, expensive, and typically requires specialized equipment and
expertise, posing challenges in resource-limited developing countries.
Additionally, the lack of an open repository for 3D models hinders research and
public engagement with their heritage. To address these issues, we propose
Tirtha, a web platform for crowdsourcing images of CH sites and creating their
3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and
Multi-View Stereo (MVS) techniques. It is modular, extensible and
cost-effective, allowing for the incorporation of new techniques as
photogrammetry advances. Tirtha is accessible through a web interface at
https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud
environment. In our case studies, we demonstrate the pipeline&apos;s effectiveness
by creating 3D models of temples in Odisha, India, using crowdsourced images.
These models are available for viewing, interaction, and download on the Tirtha
website. Our work aims to provide a dataset of crowdsourced images and 3D
reconstructions for research in computer vision, heritage conservation, and
related domains. Overall, Tirtha is a step towards democratizing digital
preservation, primarily in resource-limited developing countries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivottam_J/0/1/0/all/0/1&quot;&gt;Jyotirmaya Shivottam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Subhankar Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01248">
<title>A Hybrid Approach To Real-Time Multi-Object Tracking. (arXiv:2308.01248v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01248</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Object Tracking, also known as Multi-Target Tracking, is a significant
area of computer vision that has many uses in a variety of settings. The
development of deep learning, which has encouraged researchers to propose more
and more work in this direction, has significantly impacted the scientific
advancement around the study of tracking as well as many other domains related
to computer vision. In fact, all of the solutions that are currently
state-of-the-art in the literature and in the tracking industry, are built on
top of deep learning methodologies that produce exceptionally good results.
Deep learning is enabled thanks to the ever more powerful technology
researchers can use to handle the significant computational resources demanded
by these models. However, when real-time is a main requirement, developing a
tracking system without being constrained by expensive hardware support with
enormous computational resources is necessary to widen tracking applications in
real-world contexts. To this end, a compromise is to combine powerful deep
strategies with more traditional approaches to favor considerably lower
processing solutions at the cost of less accurate tracking results even though
suitable for real-time domains. Indeed, the present work goes in that
direction, proposing a hybrid strategy for real-time multi-target tracking that
combines effectively a classical optical flow algorithm with a deep learning
architecture, targeted to a human-crowd tracking system exhibiting a desirable
trade-off between performance in tracking precision and computational costs.
The developed architecture was experimented with different settings, and
yielded a MOTA of 0.608 out of the compared state-of-the-art 0.549 results, and
about half the running time when introducing the optical flow phase, achieving
almost the same performance in terms of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarrica_V/0/1/0/all/0/1&quot;&gt;Vincenzo Mariano Scarrica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panariello_C/0/1/0/all/0/1&quot;&gt;Ciro Panariello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferone_A/0/1/0/all/0/1&quot;&gt;Alessio Ferone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staiano_A/0/1/0/all/0/1&quot;&gt;Antonino Staiano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01251">
<title>A Hyper-pixel-wise Contrastive Learning Augmented Segmentation Network for Old Landslide Detection Using High-Resolution Remote Sensing Images and Digital Elevation Model Data. (arXiv:2308.01251v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01251</link>
<description rdf:parseType="Literal">&lt;p&gt;As a harzard disaster, landslide often brings tremendous losses to humanity,
so it&apos;s necessary to achieve reliable detection of landslide. However, the
problems of visual blur and small-sized dataset cause great challenges for old
landslide detection task when using remote sensing data. To reliably extract
semantic features, a hyper-pixel-wise contrastive learning augmented
segmentation network (HPCL-Net) is proposed, which augments the local salient
feature extraction from the boundaries of landslides through HPCL and fuses the
heterogeneous infromation in the semantic space from High-Resolution Remote
Sensing Images and Digital Elevation Model Data data. For full utilization of
the precious samples, a global hyper-pixel-wise sample pair queues-based
contrastive learning method, which includes the construction of global queues
that store hyper-pixel-wise samples and the updating scheme of a momentum
encoder, is developed, reliably enhancing the extraction ability of semantic
features. The proposed HPCL-Net is evaluated on a Loess Plateau old landslide
dataset and experiment results show that the model greatly improves the
reliablity of old landslide detection compared to the previous old landslide
segmentation model, where mIoU metric is increased from 0.620 to 0.651,
Landslide IoU metric is increased from 0.334 to 0.394 and F1-score metric is
increased from 0.501 to 0.565.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuexing Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junchuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_D/0/1/0/all/0/1&quot;&gt;Daqing Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wei Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01256">
<title>Learning Spatial Distribution of Long-Term Trackers Scores. (arXiv:2308.01256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01256</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-Term tracking is a hot topic in Computer Vision. In this context,
competitive models are presented every year, showing a constant growth rate in
performances, mainly measured in standardized protocols as Visual Object
Tracking (VOT) and Object Tracking Benchmark (OTB). Fusion-trackers strategy
has been applied over last few years for overcoming the known re-detection
problem, turning out to be an important breakthrough. Following this approach,
this work aims to generalize the fusion concept to an arbitrary number of
trackers used as baseline trackers in the pipeline, leveraging a learning phase
to better understand how outcomes correlate with each other, even when no
target is present. A model and data independence conjecture will be evidenced
in the manuscript, yielding a recall of 0.738 on LTB-50 dataset when learning
from VOT-LT2022, and 0.619 by reversing the two datasets. In both cases,
results are strongly competitive with state-of-the-art and recall turns out to
be the first on the podium.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarrica_V/0/1/0/all/0/1&quot;&gt;Vincenzo Mariano Scarrica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staiano_A/0/1/0/all/0/1&quot;&gt;Antonino Staiano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01262">
<title>Incorporating Season and Solar Specificity into Renderings made by a NeRF Architecture using Satellite Images. (arXiv:2308.01262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01262</link>
<description rdf:parseType="Literal">&lt;p&gt;As a result of Shadow NeRF and Sat-NeRF, it is possible to take the solar
angle into account in a NeRF-based framework for rendering a scene from a novel
viewpoint using satellite images for training. Our work extends those
contributions and shows how one can make the renderings season-specific. Our
main challenge was creating a Neural Radiance Field (NeRF) that could render
seasonal features independently of viewing angle and solar angle while still
being able to render shadows. We teach our network to render seasonal features
by introducing one more input variable -- time of the year. However, the small
training datasets typical of satellite imagery can introduce ambiguities in
cases where shadows are present in the same location for every image of a
particular season. We add additional terms to the loss function to discourage
the network from using seasonal features for accounting for shadows. We show
the performance of our network on eight Areas of Interest containing images
captured by the Maxar WorldView-3 satellite. This evaluation includes tests
measuring the ability of our framework to accurately render novel views,
generate height maps, predict shadows, and specify seasonal features
independently from shadows. Our ablation studies justify the choices made for
network design parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gableman_M/0/1/0/all/0/1&quot;&gt;Michael Gableman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1&quot;&gt;Avinash Kak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01265">
<title>Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives. (arXiv:2308.01265v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01265</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has demonstrated remarkable performance across various tasks in
medical imaging. However, these approaches primarily focus on supervised
learning, assuming that the training and testing data are drawn from the same
distribution. Unfortunately, this assumption may not always hold true in
practice. To address these issues, unsupervised domain adaptation (UDA)
techniques have been developed to transfer knowledge from a labeled domain to a
related but unlabeled domain. In recent years, significant advancements have
been made in UDA, resulting in a wide range of methodologies, including feature
alignment, image translation, self-supervision, and disentangled representation
methods, among others. In this paper, we provide a comprehensive literature
review of recent deep UDA approaches in medical imaging from a technical
perspective. Specifically, we categorize current UDA research in medical
imaging into six groups and further divide them into finer subcategories based
on the different tasks they perform. We also discuss the respective datasets
used in the studies to assess the divergence between the different domains.
Finally, we discuss emerging areas and provide insights and discussions on
future research directions to conclude this survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1&quot;&gt;Suruchi Kumari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pravendra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01300">
<title>Revisiting DETR Pre-training for Object Detection. (arXiv:2308.01300v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01300</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by that DETR-based approaches have established new records on COCO
detection and segmentation benchmarks, many recent endeavors show increasing
interest in how to further improve DETR-based approaches by pre-training the
Transformer in a self-supervised manner while keeping the backbone frozen. Some
studies already claimed significant improvements in accuracy. In this paper, we
take a closer look at their experimental methodology and check if their
approaches are still effective on the very recent state-of-the-art such as
$\mathcal{H}$-Deformable-DETR. We conduct thorough experiments on COCO object
detection tasks to study the influence of the choice of pre-training datasets,
localization, and classification target generation schemes. Unfortunately, we
find the previous representative self-supervised approach such as DETReg, fails
to boost the performance of the strong DETR-based approaches on full data
regimes. We further analyze the reasons and find that simply combining a more
accurate box predictor and Objects$365$ benchmark can significantly improve the
results in follow-up experiments. We demonstrate the effectiveness of our
approach by achieving strong object detection results of AP=$59.3\%$ on COCO
val set, which surpasses $\mathcal{H}$-Deformable-DETR + Swin-L by +$1.4\%$.
Last, we generate a series of synthetic pre-training datasets by combining the
very recent image-to-text captioning models (LLaVA) and text-to-image
generative models (SDXL). Notably, pre-training on these synthetic datasets
leads to notable improvements in object detection performance. Looking ahead,
we anticipate substantial advantages through the future expansion of the
synthetic pre-training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weicong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yiduo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01313">
<title>More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01313</link>
<description rdf:parseType="Literal">&lt;p&gt;CLIP, as a foundational vision language model, is widely used in zero-shot
image classification due to its ability to understand various visual concepts
and natural language descriptions. However, how to fully leverage CLIP&apos;s
unprecedented human-like understanding capabilities to achieve better zero-shot
classification is still an open question. This paper draws inspiration from the
human visual perception process: a modern neuroscience view suggests that in
classifying an object, humans first infer its class-independent attributes
(e.g., background and orientation) which help separate the foreground object
from the background, and then make decisions based on this information.
Inspired by this, we observe that providing CLIP with contextual attributes
improves zero-shot classification and mitigates reliance on spurious features.
We also observe that CLIP itself can reasonably infer the attributes from an
image. With these observations, we propose a training-free, two-step zero-shot
classification method named PerceptionCLIP. Given an image, it first infers
contextual attributes (e.g., background) and then performs object
classification conditioning on them. Our experiments show that PerceptionCLIP
achieves better generalization, group robustness, and better interpretability.
For example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by
16.5% on the Waterbirds dataset and by 3.5% on CelebA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panaitescu_Liess_M/0/1/0/all/0/1&quot;&gt;Michael-Andrei Panaitescu-Liess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1&quot;&gt;Chaithanya Kumar Mummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01316">
<title>Patched Denoising Diffusion Models For High-Resolution Image Synthesis. (arXiv:2308.01316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01316</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an effective denoising diffusion model for generating
high-resolution images (e.g., 1024$\times$512), trained on small-size image
patches (e.g., 64$\times$64). We name our algorithm Patch-DM, in which a new
feature collage strategy is designed to avoid the boundary artifact when
synthesizing large-size images. Feature collage systematically crops and
combines partial features of the neighboring patches to predict the features of
a shifted image patch, allowing the seamless generation of the entire image due
to the overlap in the patch feature space. Patch-DM produces high-quality image
synthesis results on our newly collected dataset of nature images
(1024$\times$512), as well as on standard benchmarks of smaller sizes
(256$\times$256), including LSUN-Bedroom, LSUN-Church, and FFHQ. We compare our
method with previous patch-based generation methods and achieve
state-of-the-art FID scores on all four datasets. Further, Patch-DM also
reduces memory complexity compared to the classic diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01317">
<title>ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders. (arXiv:2308.01317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01317</link>
<description rdf:parseType="Literal">&lt;p&gt;Our approach, which we call Embeddings for Language/Image-aligned X-Rays, or
ELIXR, leverages a language-aligned image encoder combined or grafted onto a
fixed LLM, PaLM 2, to perform a broad range of tasks. We train this lightweight
adapter architecture using images paired with corresponding free-text radiology
reports from the MIMIC-CXR dataset. ELIXR achieved state-of-the-art performance
on zero-shot chest X-ray (CXR) classification (mean AUC of 0.850 across 13
findings), data-efficient CXR classification (mean AUCs of 0.893 and 0.898
across five findings (atelectasis, cardiomegaly, consolidation, pleural
effusion, and pulmonary edema) for 1% (~2,200 images) and 10% (~22,000 images)
training data), and semantic search (0.76 normalized discounted cumulative gain
(NDCG) across nineteen queries, including perfect retrieval on twelve of them).
Compared to existing data-efficient methods including supervised contrastive
learning (SupCon), ELIXR required two orders of magnitude less data to reach
similar performance. ELIXR also showed promise on CXR vision-language tasks,
demonstrating overall accuracies of 58.7% and 62.5% on visual question
answering and report quality assurance tasks, respectively. These results
suggest that ELIXR is a robust and versatile approach to CXR AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shawn Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_C/0/1/0/all/0/1&quot;&gt;Christopher Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieniek_M/0/1/0/all/0/1&quot;&gt;Marcin Sieniek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohlberger_T/0/1/0/all/0/1&quot;&gt;Timo Kohlberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Martin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_W/0/1/0/all/0/1&quot;&gt;Wei-Hung Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiraly_A/0/1/0/all/0/1&quot;&gt;Attila Kiraly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemzadeh_S/0/1/0/all/0/1&quot;&gt;Sahar Kazemzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melamed_Z/0/1/0/all/0/1&quot;&gt;Zakkai Melamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jungyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strachan_P/0/1/0/all/0/1&quot;&gt;Patricia Strachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Chuck Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Preeti Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Christina Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1&quot;&gt;Mozziyar Etemadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalidindi_S/0/1/0/all/0/1&quot;&gt;Sreenivasa Raju Kalidindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yossi Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_K/0/1/0/all/0/1&quot;&gt;Katherine Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Greg S. Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Shravya Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tse_D/0/1/0/all/0/1&quot;&gt;Daniel Tse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakara_S/0/1/0/all/0/1&quot;&gt;Shruthi Prabhakara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golden_D/0/1/0/all/0/1&quot;&gt;Daniel Golden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilgrim_R/0/1/0/all/0/1&quot;&gt;Rory Pilgrim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eswaran_K/0/1/0/all/0/1&quot;&gt;Krish Eswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellergren_A/0/1/0/all/0/1&quot;&gt;Andrew Sellergren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.08657">
<title>Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.08657</link>
<description rdf:parseType="Literal">&lt;p&gt;In class-incremental learning, a learning agent faces a stream of data with
the goal of learning new classes while not forgetting previous ones. Neural
networks are known to suffer under this setting, as they forget previously
acquired knowledge. To address this problem, effective methods exploit past
data stored in an episodic memory while expanding the final classifier nodes to
accommodate the new classes.
&lt;/p&gt;
&lt;p&gt;In this work, we substitute the expanding classifier with a novel fixed
classifier in which a number of pre-allocated output nodes are subject to the
classification loss right from the beginning of the learning phase. Contrarily
to the standard expanding classifier, this allows: (a) the output nodes of
future unseen classes to firstly see negative samples since the beginning of
learning together with the positive samples that incrementally arrive; (b) to
learn features that do not change their geometric configuration as novel
classes are incorporated in the learning model.
&lt;/p&gt;
&lt;p&gt;Experiments with public datasets show that the proposed approach is as
effective as the expanding classifier while exhibiting novel intriguing
properties of the internal feature representation that are otherwise
not-existent. Our ablation study on pre-allocating a large number of classes
further validates the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pernici_F/0/1/0/all/0/1&quot;&gt;Federico Pernici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruni_M/0/1/0/all/0/1&quot;&gt;Matteo Bruni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baecchi_C/0/1/0/all/0/1&quot;&gt;Claudio Baecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turchini_F/0/1/0/all/0/1&quot;&gt;Francesco Turchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.10401">
<title>Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification. (arXiv:2104.10401v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.10401</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle re-identification helps in distinguishing between images of the same
and other vehicles. It is a challenging process because of significant
intra-instance differences between identical vehicles from different views and
subtle inter-instance differences between similar vehicles. To solve this
issue, researchers have extracted view-aware or part-specific features via
spatial attention mechanisms, which usually result in noisy attention maps or
otherwise require expensive additional annotation for metadata, such as key
points, to improve the quality. Meanwhile, based on the researchers&apos; insights,
various handcrafted multi-attention architectures for specific viewpoints or
vehicle parts have been proposed. However, this approach does not guarantee
that the number and nature of attention branches will be optimal for real-world
re-identification tasks. To address these problems, we proposed a new vehicle
re-identification network based on a multiple soft attention mechanism for
capturing various discriminative regions from different viewpoints more
efficiently. Furthermore, this model can significantly reduce the noise in
spatial attention maps by devising a new method for creating an attention map
for insignificant regions and then excluding it from generating the final
result. We also combined a channel-wise attention mechanism with a spatial
attention mechanism for the efficient selection of important semantic
attributes for vehicle re-identification. Our experiments showed that our
proposed model achieved a state-of-the-art performance among the
attention-based methods without metadata and was comparable to the approaches
using metadata for the VehicleID and VERI-Wild datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangrok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_T/0/1/0/all/0/1&quot;&gt;Taekang Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang Hun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02399">
<title>Learning Apparent Diffusion Coefficient Maps from Accelerated Radial k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model. (arXiv:2207.02399v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02399</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To accelerate radially sampled diffusion weighted spin-echo
(Rad-DW-SE) acquisition method for generating high quality apparent diffusion
coefficient (ADC) maps. Methods: A deep learning method was developed to
generate accurate ADC maps from accelerated DWI data acquired with the
Rad-DW-SE method. The deep learning method integrates convolutional neural
networks (CNNs) with vision transformers to generate high quality ADC maps from
accelerated DWI data, regularized by a monoexponential ADC model fitting term.
A model was trained on DWI data of 147 mice and evaluated on DWI data of 36
mice, with acceleration factors of 4x and 8x compared to the original
acquisition parameters. We have made our code publicly available at GitHub:
https://github.com/ymli39/DeepADC-Net-Learning-Apparent-Diffusion-Coefficient-Maps,
and our dataset can be downloaded at
https://pennpancreaticcancerimagingresource.github.io/data.html. Results:
Ablation studies and experimental results have demonstrated that the proposed
deep learning model generates higher quality ADC maps from accelerated DWI data
than alternative deep learning methods under comparison when their performance
is quantified in whole images as well as in regions of interest, including
tumors, kidneys, and muscles. Conclusions: The deep learning method with
integrated CNNs and transformers provides an effective means to accurately
compute ADC maps from accelerated DWI data acquired with the Rad-DW-SE method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuemeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joaquim_M/0/1/0/all/0/1&quot;&gt;Miguel Romanello Joaquim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pickup_S/0/1/0/all/0/1&quot;&gt;Stephen Pickup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hee Kwon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yong Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01072">
<title>Occlusion-Resistant LiDAR Fiducial Marker Detection. (arXiv:2209.01072v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01072</link>
<description rdf:parseType="Literal">&lt;p&gt;The LiDAR fiducial marker, akin to the well-known AprilTag used in camera
applications, serves as a convenient resource to impart artificial features to
the LiDAR sensor, facilitating robotics applications. Unfortunately, current
LiDAR fiducial marker detection methods are limited to occlusion-free point
clouds. In this work, we present a novel approach for occlusion-resistant LiDAR
fiducial marker detection. We first extract 3D points potentially corresponding
to the markers, leveraging the 3D intensity gradients. Afterward, we analyze
the 3D spatial distribution of the extracted points through clustering.
Subsequently, we determine the potential marker locations by examining the
geometric characteristics of these clusters. We then successively transfer the
3D points that fall within the candidate locations from the raw point cloud
onto a designed intermediate plane. Finally, using the intermediate plane, we
validate each location for the presence of a fiducial marker and compute the
marker&apos;s pose if found. We conduct both qualitative and quantitative
experiments to demonstrate that our approach is the first LiDAR fiducial marker
detection method applicable to point clouds with occlusion while achieving
better accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_J/0/1/0/all/0/1&quot;&gt;Jinjun Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schofield_H/0/1/0/all/0/1&quot;&gt;Hunter Schofield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.01769">
<title>B-CANF: Adaptive B-frame Coding with Conditional Augmented Normalizing Flows. (arXiv:2209.01769v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.01769</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, learning-based video compression has become an
active research area. However, most works focus on P-frame coding. Learned
B-frame coding is under-explored and more challenging. This work introduces a
novel B-frame coding framework, termed B-CANF, that exploits conditional
augmented normalizing flows for B-frame coding. B-CANF additionally features
two novel elements: frame-type adaptive coding and B*-frames. Our frame-type
adaptive coding learns better bit allocation for hierarchical B-frame coding by
dynamically adapting the feature distributions according to the B-frame type.
Our B*-frames allow greater flexibility in specifying the group-of-pictures
(GOP) structure by reusing the B-frame codec to mimic P-frame coding, without
the need for an additional, separate P-frame codec. On commonly used datasets,
B-CANF achieves the state-of-the-art compression performance as compared to the
other learned B-frame codecs and shows comparable BD-rate results to HM-16.23
under the random access configuration in terms of PSNR. When evaluated on
different GOP structures, our B*-frames achieve similar performance to the
additional use of a separate P-frame codec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mu-Jung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Hsin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wen-Hsiao Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11236">
<title>Boosting the Transferability of Adversarial Attacks with Global Momentum Initialization. (arXiv:2211.11236v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11236</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are vulnerable to adversarial examples, which attach
human invisible perturbations to benign inputs. Simultaneously, adversarial
examples exhibit transferability under different models, which makes practical
black-box attacks feasible. However, existing methods are still incapable of
achieving desired transfer attack performance. In this work, from the
perspective of gradient optimization and consistency, we analyze and discover
the gradient elimination phenomenon as well as the local momentum optimum
dilemma. To tackle these issues, we propose Global Momentum Initialization (GI)
to suppress gradient elimination and help search for the global optimum.
Specifically, we perform gradient pre-convergence before the attack and carry
out a global search during the pre-convergence stage. Our method can be easily
combined with almost all existing transfer methods, and we improve the success
rate of transfer attacks significantly by an average of 6.4% under various
advanced defense mechanisms compared to state-of-the-art methods. Eventually,
we achieve an attack success rate of 95.4%, fully illustrating the insecurity
of existing defense mechanisms. Code is available at
$\href{https://github.com/Omenzychen/Global-Momentum-Initialization}{this\
URL}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiafeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kaixun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pinxue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Haijing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14304">
<title>BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction. (arXiv:2211.14304v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14304</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic human motion prediction (HMP) has generally been tackled with
generative adversarial networks and variational autoencoders. Most prior works
aim at predicting highly diverse movements in terms of the skeleton joints&apos;
dispersion. This has led to methods predicting fast and motion-divergent
movements, which are often unrealistic and incoherent with past motion. Such
methods also neglect contexts that need to anticipate diverse low-range
behaviors, or actions, with subtle joint displacements. To address these
issues, we present BeLFusion, a model that, for the first time, leverages
latent diffusion models in HMP to sample from a latent space where behavior is
disentangled from pose and motion. As a result, diversity is encouraged from a
behavioral perspective. Thanks to our behavior coupler&apos;s ability to transfer
sampled behavior to ongoing motion, BeLFusion&apos;s predictions display a variety
of behaviors that are significantly more realistic than the state of the art.
To support it, we introduce two metrics, the Area of the Cumulative Motion
Distribution, and the Average Pairwise Distance Error, which are correlated to
our definition of realism according to a qualitative study with 126
participants. Finally, we prove BeLFusion&apos;s generalization power in a new
cross-dataset scenario for stochastic HMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barquero_G/0/1/0/all/0/1&quot;&gt;German Barquero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmero_C/0/1/0/all/0/1&quot;&gt;Cristina Palmero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01584">
<title>Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01584</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Learning (SSL) is a machine learning algorithm for
pretraining Deep Neural Networks (DNNs) without requiring manually labeled
data. The central idea of this learning technique is based on an auxiliary
stage aka pretext task in which labeled data are created automatically through
data augmentation and exploited for pretraining the DNN. However, the effect of
each pretext task is not well studied or compared in the literature. In this
paper, we study the contribution of augmentation operators on the performance
of self supervised learning algorithms in a constrained settings. We propose an
evolutionary search method for optimization of data augmentation pipeline in
pretext tasks and measure the impact of augmentation operators in several SOTA
SSL algorithms. By encoding different combination of augmentation operators in
chromosomes we seek the optimal augmentation policies through an evolutionary
optimization mechanism. We further introduce methods for analyzing and
explaining the performance of optimized SSL algorithms. Our results indicate
that our proposed method can find solutions that outperform the accuracy of
classification of SSL algorithms which confirms the influence of augmentation
policy choice on the overall performance of SSL algorithms. We also compare
optimal SSL solutions found by our evolutionary search mechanism and show the
effect of batch size in the pretext task on two visual datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_N/0/1/0/all/0/1&quot;&gt;Noah Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_Z/0/1/0/all/0/1&quot;&gt;Zahra Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02058">
<title>3D-Aware Object Localization using Gaussian Implicit Occupancy Function. (arXiv:2303.02058v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02058</link>
<description rdf:parseType="Literal">&lt;p&gt;To automatically localize a target object in an image is crucial for many
computer vision applications. To represent the 2D object, ellipse labels have
recently been identified as a promising alternative to axis-aligned bounding
boxes. This paper further considers 3D-aware ellipse labels, \textit{i.e.},
ellipses which are projections of a 3D ellipsoidal approximation of the object,
for 2D target localization. Indeed, projected ellipses carry more geometric
information about the object geometry and pose (3D awareness) than traditional
3D-agnostic bounding box labels. Moreover, such a generic 3D ellipsoidal model
allows for approximating known to coarsely known targets. We then propose to
have a new look at ellipse regression and replace the discontinuous geometric
ellipse parameters with the parameters of an implicit Gaussian distribution
encoding object occupancy in the image. The models are trained to regress the
values of this bivariate Gaussian distribution over the image pixels using a
statistical loss function. We introduce a novel non-trainable differentiable
layer, E-DSNT, to extract the distribution parameters. Also, we describe how to
readily generate consistent 3D-aware Gaussian occupancy parameters using only
coarse dimensions of the target and relative pose labels. We extend three
existing spacecraft pose estimation datasets with 3D-aware Gaussian occupancy
labels to validate our hypothesis. Labels and source code are publicly
accessible here: https://cvi2.uni.lu/3d-aware-obj-loc/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaudilliere_V/0/1/0/all/0/1&quot;&gt;Vincent Gaudilli&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauly_L/0/1/0/all/0/1&quot;&gt;Leo Pauly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathinam_A/0/1/0/all/0/1&quot;&gt;Arunkumar Rathinam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1&quot;&gt;Albert Garcia Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musallam_M/0/1/0/all/0/1&quot;&gt;Mohamed Adel Musallam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03651">
<title>F2BEV: Bird&apos;s Eye View Generation from Surround-View Fisheye Camera Images for Automated Driving. (arXiv:2303.03651v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03651</link>
<description rdf:parseType="Literal">&lt;p&gt;Bird&apos;s Eye View (BEV) representations are tremendously useful for
perception-related automated driving tasks. However, generating BEVs from
surround-view fisheye camera images is challenging due to the strong
distortions introduced by such wide-angle lenses. We take the first step in
addressing this challenge and introduce a baseline, F2BEV, to generate
discretized BEV height maps and BEV semantic segmentation maps from fisheye
images. F2BEV consists of a distortion-aware spatial cross attention module for
querying and consolidating spatial information from fisheye image features in a
transformer-style architecture followed by a task-specific head. We evaluate
single-task and multi-task variants of F2BEV on our synthetic FB-SSEM dataset,
all of which generate better BEV height and segmentation maps (in terms of the
IoU) than a state-of-the-art BEV generation method operating on undistorted
fisheye images. We also demonstrate discretized height map generation from
real-world fisheye images using F2BEV. Our dataset is publicly available at
https://github.com/volvo-cars/FB-SSEM-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samani_E/0/1/0/all/0/1&quot;&gt;Ekta U. Samani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1&quot;&gt;Feng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasari_H/0/1/0/all/0/1&quot;&gt;Harshavardhan R. Dasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Sihao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Ashis G. Banerjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06338">
<title>Learning Combinatorial Prompts for Universal Controllable Image Captioning. (arXiv:2303.06338v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06338</link>
<description rdf:parseType="Literal">&lt;p&gt;Controllable Image Captioning (CIC) -- generating natural language
descriptions about images under the guidance of given control signals -- is one
of the most promising directions towards next-generation captioning systems.
Till now, various kinds of control signals for CIC have been proposed, ranging
from content-related control to structure-related control. However, due to the
format and target gaps of different control signals, all existing CIC works (or
architectures) only focus on one certain control signal, and overlook the
human-like combinatorial ability. By ``combinatorial&quot;, we mean that our humans
can easily meet multiple needs (or constraints) simultaneously when generating
descriptions. To this end, we propose a novel prompt-based framework for CIC by
learning Combinatorial Prompts, dubbed as ComPro. Specifically, we directly
utilize a pretrained language model GPT-2 as our language model, which can help
to bridge the gap between different signal-specific CIC architectures. Then, we
reformulate the CIC as a prompt-guide sentence generation problem, and propose
a new lightweight prompt generation network to generate the combinatorial
prompts for different kinds of control signals. For different control signals,
we further design a new mask attention mechanism to realize the prompt-based
CIC. Due to its simplicity, our ComPro can be further extended to more kinds of
combined control signals by concatenating these prompts. Extensive experiments
on two prevalent CIC benchmarks have verified the effectiveness and efficiency
of our ComPro on both single and combined control signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Fei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jian Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10049">
<title>Uncertainty-informed Mutual Learning for Joint Medical Image Classification and Segmentation. (arXiv:2303.10049v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10049</link>
<description rdf:parseType="Literal">&lt;p&gt;Classification and segmentation are crucial in medical image analysis as they
enable accurate diagnosis and disease monitoring. However, current methods
often prioritize the mutual learning features and shared model parameters,
while neglecting the reliability of features and performances. In this paper,
we propose a novel Uncertainty-informed Mutual Learning (UML) framework for
reliable and interpretable medical image analysis. Our UML introduces
reliability to joint classification and segmentation tasks, leveraging mutual
learning with uncertainty to improve performance. To achieve this, we first use
evidential deep learning to provide image-level and pixel-wise confidences.
Then, an Uncertainty Navigator Decoder is constructed for better using mutual
features and generating segmentation results. Besides, an Uncertainty
Instructor is proposed to screen reliable masks for classification. Overall,
UML could produce confidence estimation in features and performance for each
link (classification and segmentation). The experiments on the public datasets
demonstrate that our UML outperforms existing methods in terms of both accuracy
and robustness. Our UML has the potential to explore the development of more
reliable and explainable medical image analysis models. We will release the
codes for reproduction after acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1&quot;&gt;Ke Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yidi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xuedong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaojing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15823">
<title>Automated wildlife image classification: An active learning tool for ecological applications. (arXiv:2303.15823v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15823</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildlife camera trap images are being used extensively to investigate animal
abundance, habitat associations, and behavior, which is complicated by the fact
that experts must first classify the images manually. Artificial intelligence
systems can take over this task but usually need a large number of
already-labeled training images to achieve sufficient performance. This
requirement necessitates human expert labor and poses a particular challenge
for projects with few cameras or short durations. We propose a label-efficient
learning strategy that enables researchers with small or medium-sized image
databases to leverage the potential of modern machine learning, thus freeing
crucial resources for subsequent analyses.
&lt;/p&gt;
&lt;p&gt;Our methodological proposal is two-fold: (1) We improve current strategies of
combining object detection and image classification by tuning the
hyperparameters of both models. (2) We provide an active learning (AL) system
that allows training deep learning models very efficiently in terms of required
human-labeled training images. We supply a software package that enables
researchers to use these methods directly and thereby ensure the broad
applicability of the proposed framework in ecological practice.
&lt;/p&gt;
&lt;p&gt;We show that our tuning strategy improves predictive performance. We
demonstrate how the AL pipeline reduces the amount of pre-labeled data needed
to achieve a specific predictive performance and that it is especially valuable
for improving out-of-sample predictive performance.
&lt;/p&gt;
&lt;p&gt;We conclude that the combination of tuning and AL increases predictive
performance substantially. Furthermore, we argue that our work can broadly
impact the community through the ready-to-use software package provided.
Finally, the publication of our models tailored to European wildlife data
enriches existing model bases mostly trained on data from Africa and North
America.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bothmann_L/0/1/0/all/0/1&quot;&gt;Ludwig Bothmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimmer_L/0/1/0/all/0/1&quot;&gt;Lisa Wimmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charrakh_O/0/1/0/all/0/1&quot;&gt;Omid Charrakh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1&quot;&gt;Tobias Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edelhoff_H/0/1/0/all/0/1&quot;&gt;Hendrik Edelhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_W/0/1/0/all/0/1&quot;&gt;Wibke Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benjamin_C/0/1/0/all/0/1&quot;&gt;Caryl Benjamin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzel_A/0/1/0/all/0/1&quot;&gt;Annette Menzel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03701">
<title>LMEye: An Interactive Perception Network for Large Language Models. (arXiv:2305.03701v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03701</link>
<description rdf:parseType="Literal">&lt;p&gt;Training a Large Visual Language Model (LVLM) from scratch, like GPT-4, is
resource-intensive. Our paper presents a play-and-plug module for Large
Language Models (LLMs), namely Interactive Perception Network (IPN), aiming to
achieve a LVLM by incorporating the image understanding capability into LLMs.
Previous methods incorporate visual information into LLMs with a simple visual
mapping network, where the image feature is projected into the embedding space
of LLMs via a linear layer. Such mapping network projects the image feature
once yet does not consider the interaction between the image and the human
input query. Hence, the obtained visual information with no connections with
human intention may be inadequate for LLMs to make intention-following
responses, which we term as static visual information. IPN addresses this issue
by allowing the LLM to request the desired visual information aligned with
various human instructions, which we term as the dynamic interaction between
the LLM and visual information. Specifically, IPN consists of a simple visual
mapping network to provide the basic perception of an image for LLMs. It also
contains additional modules responsible for acquiring requests from LLMs,
performing request-based visual information interaction, and transmitting the
resulting interacted visual information to LLMs, respectively. In this way,
LLMs act to understand the human query, deliver the corresponding request to
the request-based visual information interaction module, and generate the
response based on the interleaved multimodal information. We evaluate IPN
through extensive experiments on multimodal question answering, reasoning, and
so on, demonstrating that it significantly improves the zero-shot performance
of LVLMs on various multimodal tasks compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently learning visual representations of items is vital for large-scale
recommendations. In this article we compare several pretrained efficient
backbone architectures, both in the convolutional neural network (CNN) and in
the vision transformer (ViT) family. We describe challenges in e-commerce
vision applications at scale and highlight methods to efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating visual representations in several downstream tasks. To this end, we
present a novel multilingual text-to-image generative offline evaluation method
for visually similar recommendation systems. Finally, we include online results
from deployed machine learning systems in production on a large scale
e-commerce platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18948">
<title>Prompt-Based Tuning of Transformer Models for Multi-Center Medical Image Segmentation of Head and Neck Cancer. (arXiv:2305.18948v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18948</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation is a vital healthcare endeavor requiring precise
and efficient models for appropriate diagnosis and treatment. Vision
transformer (ViT)-based segmentation models have shown great performance in
accomplishing this task. However, to build a powerful backbone, the
self-attention block of ViT requires large-scale pre-training data. The present
method of modifying pre-trained models entails updating all or some of the
backbone parameters. This paper proposes a novel fine-tuning strategy for
adapting a pretrained transformer-based segmentation model on data from a new
medical center. This method introduces a small number of learnable parameters,
termed prompts, into the input space (less than 1\% of model parameters) while
keeping the rest of the model parameters frozen. Extensive studies employing
data from new unseen medical centers show that the prompt-based fine-tuning of
medical segmentation models provides excellent performance regarding the
new-center data with a negligible drop regarding the old centers. Additionally,
our strategy delivers great accuracy with minimum re-training on new-center
data, significantly decreasing the computational and time costs of fine-tuning
pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeed_N/0/1/0/all/0/1&quot;&gt;Numan Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ridzuan_M/0/1/0/all/0/1&quot;&gt;Muhammad Ridzuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majzoub_R/0/1/0/all/0/1&quot;&gt;Roba Al Majzoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaqub_M/0/1/0/all/0/1&quot;&gt;Mohammad Yaqub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01940">
<title>Sampling binary sparse coding QUBO models using a spiking neuromorphic processor. (arXiv:2306.01940v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01940</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of computing a sparse binary representation of an
image. To be precise, given an image and an overcomplete, non-orthonormal
basis, we aim to find a sparse binary vector indicating the minimal set of
basis vectors that when added together best reconstruct the given input. We
formulate this problem with an $L_2$ loss on the reconstruction error, and an
$L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing
sparsity. This yields a so-called Quadratic Unconstrained Binary Optimization
(QUBO) problem, whose solution is generally NP-hard to find. The contribution
of this work is twofold. First, the method of unsupervised and unnormalized
dictionary feature learning for a desired sparsity level to best match the data
is presented. Second, the binary sparse coding problem is then solved on the
Loihi 1 neuromorphic chip by the use of stochastic networks of neurons to
traverse the non-convex energy landscape. The solutions are benchmarked against
the classical heuristic simulated annealing. We demonstrate neuromorphic
computing is suitable for sampling low energy solutions of binary sparse coding
QUBO models, and although Loihi 1 is capable of sampling very sparse solutions
of the QUBO models, there needs to be improvement in the implementation in
order to be competitive with simulated annealing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henke_K/0/1/0/all/0/1&quot;&gt;Kyle Henke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelofske_E/0/1/0/all/0/1&quot;&gt;Elijah Pelofske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_G/0/1/0/all/0/1&quot;&gt;Georg Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1&quot;&gt;Garrett T. Kenyon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03504">
<title>Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis. (arXiv:2306.03504v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03504</link>
<description rdf:parseType="Literal">&lt;p&gt;We are interested in a novel task, namely low-resource text-to-talking
avatar. Given only a few-minute-long talking person video with the audio track
as the training data and arbitrary texts as the driving input, we aim to
synthesize high-quality talking portrait videos corresponding to the input
text. This task has broad application prospects in the digital human industry
but has not been technically achieved yet due to two challenges: (1) It is
challenging to mimic the timbre from out-of-domain audio for a traditional
multi-speaker Text-to-Speech system. (2) It is hard to render high-fidelity and
lip-synchronized talking avatars with limited training data. In this paper, we
introduce Adaptive Text-to-Talking Avatar (Ada-TTA), which (1) designs a
generic zero-shot multi-speaker TTS model that well disentangles the text
content, timbre, and prosody; and (2) embraces recent advances in neural
rendering to achieve realistic audio-driven talking face video generation. With
these designs, our method overcomes the aforementioned two challenges and
achieves to generate identity-preserving speech and realistic talking person
video. Experiments demonstrate that our method could synthesize realistic,
identity-preserving, and audio-visual synchronized talking avatar videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhenhui Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinglin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zejun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10940">
<title>TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting. (arXiv:2306.10940v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10940</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildfires are increasingly exacerbated as a result of climate change,
necessitating advanced proactive measures for effective mitigation. It is
important to forecast wildfires weeks and months in advance to plan forest fuel
management, resource procurement and allocation. To achieve such accurate
long-term forecasts at a global scale, it is crucial to employ models that
account for the Earth system&apos;s inherent spatio-temporal interactions, such as
memory effects and teleconnections. We propose a teleconnection-driven vision
transformer (TeleViT), capable of treating the Earth as one interconnected
system, integrating fine-grained local-scale inputs with global-scale inputs,
such as climate indices and coarse-grained global variables. Through
comprehensive experimentation, we demonstrate the superiority of TeleViT in
accurately predicting global burned area patterns for various forecasting
windows, up to four months in advance. The gain is especially pronounced in
larger forecasting windows, demonstrating the improved ability of deep learning
models that exploit teleconnections to capture Earth system dynamics. Code
available at https://github.com/Orion-Ai-Lab/TeleViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1&quot;&gt;Ioannis Prapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Ioannis Bountos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondylatos_S/0/1/0/all/0/1&quot;&gt;Spyros Kondylatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1&quot;&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;Gustau Camps-Valls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13720">
<title>Decoupled Diffusion Models with Explicit Transition Probability. (arXiv:2306.13720v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13720</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent diffusion probabilistic models (DPMs) have shown remarkable abilities
of generated content, however, they often suffer from complex forward
processes, resulting in inefficient solutions for the reversed process and
prolonged sampling times. In this paper, we aim to address the aforementioned
challenges by focusing on the diffusion process itself that we propose to
decouple the intricate diffusion process into two comparatively simpler process
to improve the generative efficacy and speed. In particular, we present a novel
diffusion paradigm named DDM (Decoupled Diffusion Models) based on the Ito
diffusion process, in which the image distribution is approximated by an
explicit transition probability while the noise path is controlled by the
standard Wiener process. We find that decoupling the diffusion process reduces
the learning difficulty and the explicit transition probability improves the
generative speed significantly. We prove a new training objective for DPM,
which enables the model to learn to predict the noise and image components
separately. Moreover, given the novel forward diffusion equation, we derive the
reverse denoising formula of DDM that naturally supports fewer steps of
generation without ordinary differential equation (ODE) based accelerators. Our
experiments demonstrate that DDM outperforms previous DPMs by a large margin in
fewer function evaluations setting and gets comparable performances in long
function evaluations setting. We also show that our framework can be applied to
image-conditioned generation and high-resolution image synthesis, and that it
can generate high-quality images with only 10 function evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zheng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14153">
<title>DomainStudio: Fine-Tuning Diffusion Models for Domain-Driven Image Generation using Limited Data. (arXiv:2306.14153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14153</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion probabilistic models (DDPMs) have been proven capable of
synthesizing high-quality images with remarkable diversity when trained on
large amounts of data. Typical diffusion models and modern large-scale
conditional generative models like text-to-image generative models are
vulnerable to overfitting when fine-tuned on extremely limited data. Existing
works have explored subject-driven generation using a reference set containing
a few images. However, few prior works explore DDPM-based domain-driven
generation, which aims to learn the common features of target domains while
maintaining diversity. This paper proposes a novel DomainStudio approach to
adapt DDPMs pre-trained on large-scale source datasets to target domains using
limited data. It is designed to keep the diversity of subjects provided by
source domains and get high-quality and diverse adapted samples in target
domains. We propose to keep the relative distances between adapted samples to
achieve considerable generation diversity. In addition, we further enhance the
learning of high-frequency details for better generation quality. Our approach
is compatible with both unconditional and conditional diffusion models. This
work makes the first attempt to realize unconditional few-shot image generation
with diffusion models, achieving better quality and greater diversity than
current state-of-the-art GAN-based approaches. Moreover, this work also
significantly relieves overfitting for conditional generation and realizes
high-quality domain-driven generation, further expanding the applicable
scenarios of modern large-scale text-to-image models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingyuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huimin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiansheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jian Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10636">
<title>Learning and Evaluating Human Preferences for Conversational Head Generation. (arXiv:2307.10636v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10636</link>
<description rdf:parseType="Literal">&lt;p&gt;A reliable and comprehensive evaluation metric that aligns with manual
preference assessments is crucial for conversational head video synthesis
methods development. Existing quantitative evaluations often fail to capture
the full complexity of human preference, as they only consider limited
evaluation dimensions. Qualitative evaluations and user studies offer a
solution but are time-consuming and labor-intensive. This limitation hinders
the advancement of conversational head generation algorithms and systems. In
this paper, we propose a novel learning-based evaluation metric named
Preference Score (PS) for fitting human preference according to the
quantitative evaluations across different dimensions. PS can serve as a
quantitative evaluation without the need for human annotation. Experimental
results validate the superiority of Preference Score in aligning with human
perception, and also demonstrate robustness and generalizability to unseen
data, making it a valuable tool for advancing conversation head generation. We
expect this metric could facilitate new advances in conversational head
generation. Project Page: https://https://github.com/dc3ea9f/PreferenceScore.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mohan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yalong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1&quot;&gt;Ting Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tiejun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1&quot;&gt;Tao Mei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10784">
<title>SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar. (arXiv:2307.10784v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10784</link>
<description rdf:parseType="Literal">&lt;p&gt;The 4D Millimeter wave (mmWave) radar is a promising technology for vehicle
sensing due to its cost-effectiveness and operability in adverse weather
conditions. However, the adoption of this technology has been hindered by
sparsity and noise issues in radar point cloud data. This paper introduces
spatial multi-representation fusion (SMURF), a novel approach to 3D object
detection using a single 4D imaging radar. SMURF leverages multiple
representations of radar detection points, including pillarization and density
features of a multi-dimensional Gaussian mixture distribution through kernel
density estimation (KDE). KDE effectively mitigates measurement inaccuracy
caused by limited angular resolution and multi-path propagation of radar
signals. Additionally, KDE helps alleviate point cloud sparsity by capturing
density features. Experimental evaluations on View-of-Delft (VoD) and
TJ4DRadSet datasets demonstrate the effectiveness and generalization ability of
SMURF, outperforming recently proposed 4D imaging radar-based
single-representation models. Moreover, while using 4D imaging radar only,
SMURF still achieves comparable performance to the state-of-the-art 4D imaging
radar and camera fusion-based method, with an increase of 1.22% in the mean
average precision on bird&apos;s-eye view of TJ4DRadSet dataset and 1.32% in the 3D
mean average precision on the entire annotated area of VoD dataset. Our
proposed method demonstrates impressive inference time and addresses the
challenges of real-time detection, with the inference time no more than 0.05
seconds for most scans on both datasets. This research highlights the benefits
of 4D mmWave radar and is a strong benchmark for subsequent works regarding 3D
object detection with 4D imaging radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qiuchi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Weiyi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12730">
<title>COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts. (arXiv:2307.12730v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12730</link>
<description rdf:parseType="Literal">&lt;p&gt;Practical object detection application can lose its effectiveness on image
inputs with natural distribution shifts. This problem leads the research
community to pay more attention on the robustness of detectors under
Out-Of-Distribution (OOD) inputs. Existing works construct datasets to
benchmark the detector&apos;s OOD robustness for a specific application scenario,
e.g., Autonomous Driving. However, these datasets lack universality and are
hard to benchmark general detectors built on common tasks such as COCO. To give
a more comprehensive robustness assessment, we introduce
COCO-O(ut-of-distribution), a test dataset based on COCO with 6 types of
natural distribution shifts. COCO-O has a large distribution gap with training
data and results in a significant 55.7% relative performance drop on a Faster
R-CNN detector. We leverage COCO-O to conduct experiments on more than 100
modern object detectors to investigate if their improvements are credible or
just over-fitting to the COCO test set. Unfortunately, most classic detectors
in early years do not exhibit strong OOD generalization. We further study the
robustness effect on recent breakthroughs of detector&apos;s architecture design,
augmentation and pre-training techniques. Some empirical findings are revealed:
1) Compared with detection head or neck, backbone is the most important part
for robustness; 2) An end-to-end detection transformer design brings no
enhancement, and may even reduce robustness; 3) Large-scale foundation models
have made a great leap on robust object detection. We hope our COCO-O could
provide a rich testbed for robustness study of object detection. The dataset
will be available at
https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuefeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Da Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hui Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16000">
<title>Automated Hit-frame Detection for Badminton Match Analysis. (arXiv:2307.16000v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16000</link>
<description rdf:parseType="Literal">&lt;p&gt;Sports professionals constantly under pressure to perform at the highest
level can benefit from sports analysis, which allows coaches and players to
reduce manual efforts and systematically evaluate their performance using
automated tools. This research aims to advance sports analysis in badminton,
systematically detecting hit-frames automatically from match videos using
modern deep learning techniques. The data included in hit-frames can
subsequently be utilized to synthesize players&apos; strokes and on-court movement,
as well as for other downstream applications such as analyzing training tasks
and competition strategy. The proposed approach in this study comprises several
automated procedures like rally-wise video trimming, player and court keypoints
detection, shuttlecock flying direction prediction, and hit-frame detection. In
the study, we achieved 99% accuracy on shot angle recognition for video
trimming, over 92% accuracy for applying player keypoints sequences on
shuttlecock flying direction prediction, and reported the evaluation results of
rally-wise video trimming and hit-frame detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_Y/0/1/0/all/0/1&quot;&gt;Yu-Hang Chien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16125">
<title>SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. (arXiv:2307.16125v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16125</link>
<description rdf:parseType="Literal">&lt;p&gt;Based on powerful Large Language Models (LLMs), recent generative Multimodal
Large Language Models (MLLMs) have gained prominence as a pivotal research
area, exhibiting remarkable capability for both comprehension and generation.
In this work, we address the evaluation of generative comprehension in MLLMs as
a preliminary step towards a comprehensive assessment of generative models, by
introducing a benchmark named SEED-Bench. SEED-Bench consists of 19K multiple
choice questions with accurate human annotations (x 6 larger than existing
benchmarks), which spans 12 evaluation dimensions including the comprehension
of both the image and video modality. We develop an advanced pipeline for
generating multiple-choice questions that target specific evaluation
dimensions, integrating both automatic filtering and manual verification
processes. Multiple-choice questions with groundtruth options derived from
human annotation enables an objective and efficient assessment of model
performance, eliminating the need for human or GPT intervention during
evaluation. We further evaluate the performance of 18 models across all 12
dimensions, covering both the spatial and temporal understanding. By revealing
the limitations of existing MLLMs through evaluation results, we aim for
SEED-Bench to provide insights for motivating future research. We will launch
and consistently maintain a leaderboard to provide a platform for the community
to assess and investigate model capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bohao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16865">
<title>Universal Adversarial Defense in Remote Sensing Based on Pre-trained Denoising Diffusion Models. (arXiv:2307.16865v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16865</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have achieved tremendous success in many remote
sensing (RS) applications, in which DNNs are vulnerable to adversarial
perturbations. Unfortunately, current adversarial defense approaches in RS
studies usually suffer from performance fluctuation and unnecessary re-training
costs due to the need for prior knowledge of the adversarial perturbations
among RS data. To circumvent these challenges, we propose a universal
adversarial defense approach in RS imagery (UAD-RS) using pre-trained diffusion
models to defend the common DNNs against multiple unknown adversarial attacks.
Specifically, the generative diffusion models are first pre-trained on
different RS datasets to learn generalized representations in various data
domains. After that, a universal adversarial purification framework is
developed using the forward and reverse process of the pre-trained diffusion
models to purify the perturbations from adversarial samples. Furthermore, an
adaptive noise level selection (ANLS) mechanism is built to capture the optimal
noise level of the diffusion model that can achieve the best purification
results closest to the clean samples according to their Frechet Inception
Distance (FID) in deep feature space. As a result, only a single pre-trained
diffusion model is needed for the universal purification of adversarial samples
on each dataset, which significantly alleviates the re-training efforts and
maintains high performance without prior knowledge of the adversarial
perturbations. Experiments on four heterogeneous RS datasets regarding scene
classification and semantic segmentation verify that UAD-RS outperforms
state-of-the-art adversarial purification approaches with a universal defense
against seven commonly existing adversarial perturbations. Codes and the
pre-trained models are available online (https://github.com/EricYu97/UAD-RS).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weikang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yonghao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00135">
<title>InFusion: Inject and Attention Fusion for Multi Concept Zero Shot Text based Video Editing. (arXiv:2308.00135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large text-to-image diffusion models have achieved remarkable success in
generating diverse high-quality images in alignment with text prompt used for
editing the input image. But, when these models applied to video the main
challenge is to ensure temporal consistency and coherence across frames. In
this paper, we proposed InFusion, a framework for zero-shot text-based video
editing leveraging large pre-trained image diffusion models. Our framework
specifically supports editing of multiple concepts with the pixel level control
over diverse concepts mentioned in the editing prompt. Specifically, we inject
the difference of features obtained with source and edit prompt from U-Net
residual blocks in decoder layers, this when combined with injected attention
features make it feasible to query the source contents and scale edited
concepts along with the injection of unedited parts. The editing is further
controlled in fine-grained manner with mask extraction and attention fusion
strategy which cuts the edited part from source and paste it into the denoising
pipeline for editing prompt. Our framework is a low cost alternative of
one-shot tuned models for editing since it does not require training. We
demonstrated the complex concept editing with generalised image model (Stable
Diffusion v1.5) using LoRA. Adaptation is compatible with all the existing
image diffusion techniques. Extensive experimental results demonstrate the
effectiveness over existing methods in rendering high-quality and temporally
consistent videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1&quot;&gt;Anant Khandelwal&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>