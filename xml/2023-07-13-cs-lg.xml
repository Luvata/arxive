<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06333" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1910.06151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.02613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2101.10870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.00479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.05942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.09035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.10865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.06672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.00129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.01179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.09387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11270" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05350" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.05486">
<title>Importance of equivariant and invariant symmetries for fluid flow modeling. (arXiv:2307.05486v1 [physics.flu-dyn])</title>
<link>http://arxiv.org/abs/2307.05486</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have shown promise in learning unstructured
mesh-based simulations of physical systems, including fluid dynamics. In
tandem, geometric deep learning principles have informed the development of
equivariant architectures respecting underlying physical symmetries. However,
the effect of rotational equivariance in modeling fluids remains unclear. We
build a multi-scale equivariant GNN to forecast fluid flow and study the effect
of modeling invariant and non-invariant representations of the flow state. We
evaluate the model performance of several equivariant and non-equivariant
architectures on predicting the evolution of two fluid flows, flow around a
cylinder and buoyancy-driven shear flow, to understand the effect of
equivariance and invariance on data-driven modeling approaches. Our results
show that modeling invariant quantities produces more accurate long-term
predictions and that these invariant quantities may be learned from the
velocity field using a data-driven encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Varun Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Barwey_S/0/1/0/all/0/1&quot;&gt;Shivam Barwey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kolter_Z/0/1/0/all/0/1&quot;&gt;Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Maulik_R/0/1/0/all/0/1&quot;&gt;Romit Maulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Viswanathan_V/0/1/0/all/0/1&quot;&gt;Venkatasubramanian Viswanathan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05501">
<title>HIVA: Holographic Intellectual Voice Assistant. (arXiv:2307.05501v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.05501</link>
<description rdf:parseType="Literal">&lt;p&gt;Holographic Intellectual Voice Assistant (HIVA) aims to facilitate human
computer interaction using audiovisual effects and 3D avatar. HIVA provides
complete information about the university, including requests of various
nature: admission, study issues, fees, departments, university structure and
history, canteen, human resources, library, student life and events,
information about the country and the city, etc. There are other ways for
receiving the data listed above: the university&apos;s official website and other
supporting apps, HEI (Higher Education Institution) official social media,
directly asking the HEI staff, and other channels. However, HIVA provides the
unique experience of &quot;face-to-face&quot; interaction with an animated 3D mascot,
helping to get a sense of &apos;real-life&apos; communication. The system includes many
sub-modules and connects a family of applications such as mobile applications,
Telegram chatbot, suggestion categorization, and entertainment services. The
Voice assistant uses Russian language NLP models and tools, which are pipelined
for the best user experience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isaev_R/0/1/0/all/0/1&quot;&gt;Ruslan Isaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gumerov_R/0/1/0/all/0/1&quot;&gt;Radmir Gumerov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esenalieva_G/0/1/0/all/0/1&quot;&gt;Gulzada Esenalieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekuria_R/0/1/0/all/0/1&quot;&gt;Remudin Reshid Mekuria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doszhanov_E/0/1/0/all/0/1&quot;&gt;Ermek Doszhanov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05506">
<title>Data-Driven Design for Metamaterials and Multiscale Systems: A Review. (arXiv:2307.05506v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2307.05506</link>
<description rdf:parseType="Literal">&lt;p&gt;Metamaterials are artificial materials designed to exhibit effective material
parameters that go beyond those found in nature. Composed of unit cells with
rich designability that are assembled into multiscale systems, they hold great
promise for realizing next-generation devices with exceptional, often exotic,
functionalities. However, the vast design space and intricate
structure-property relationships pose significant challenges in their design. A
compelling paradigm that could bring the full potential of metamaterials to
fruition is emerging: data-driven design. In this review, we provide a holistic
overview of this rapidly evolving field, emphasizing the general methodology
instead of specific domains and deployment contexts. We organize existing
research into data-driven modules, encompassing data acquisition, machine
learning-based unit cell design, and data-driven multiscale optimization. We
further categorize the approaches within each module based on shared
principles, analyze and compare strengths and applicability, explore
connections between different modules, and identify open research questions and
opportunities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Doksoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Wayne Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Y/0/1/0/all/0/1&quot;&gt;Yu-Chin Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05517">
<title>Adaptive Graph Convolution Networks for Traffic Flow Forecasting. (arXiv:2307.05517v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05517</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic flow forecasting is a highly challenging task due to the dynamic
spatial-temporal road conditions. Graph neural networks (GNN) has been widely
applied in this task. However, most of these GNNs ignore the effects of
time-varying road conditions due to the fixed range of the convolution
receptive field. In this paper, we propose a novel Adaptive Graph Convolution
Networks (AGC-net) to address this issue in GNN. The AGC-net is constructed by
the Adaptive Graph Convolution (AGC) based on a novel context attention
mechanism, which consists of a set of graph wavelets with various learnable
scales. The AGC transforms the spatial graph representations into
time-sensitive features considering the temporal context. Moreover, a shifted
graph convolution kernel is designed to enhance the AGC, which attempts to
correct the deviations caused by inaccurate topology. Experimental results on
two public traffic datasets demonstrate the effectiveness of the
AGC-net\footnote{Code is available at: https://github.com/zhengdaoli/AGC-net}
which outperforms other baseline models significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengdao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_K/0/1/0/all/0/1&quot;&gt;Kai Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05520">
<title>Do DL models and training environments have an impact on energy consumption?. (arXiv:2307.05520v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05520</link>
<description rdf:parseType="Literal">&lt;p&gt;Current research in the computer vision field mainly focuses on improving
Deep Learning (DL) correctness and inference time performance. However, there
is still little work on the huge carbon footprint that has training DL models.
This study aims to analyze the impact of the model architecture and training
environment when training greener computer vision models. We divide this goal
into two research questions. First, we analyze the effects of model
architecture on achieving greener models while keeping correctness at optimal
levels. Second, we study the influence of the training environment on producing
greener models. To investigate these relationships, we collect multiple metrics
related to energy efficiency and model correctness during the models&apos; training.
Then, we outline the trade-offs between the measured energy efficiency and the
models&apos; correctness regarding model architecture, and their relationship with
the training environment. We conduct this research in the context of a computer
vision system for image classification. In conclusion, we show that selecting
the proper model architecture and training environment can reduce energy
consumption dramatically (up to 98.83\%) at the cost of negligible decreases in
correctness. Also, we find evidence that GPUs should scale with the models&apos;
computational complexity for better energy efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rey_S/0/1/0/all/0/1&quot;&gt;Santiago del Rey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Fernandez_S/0/1/0/all/0/1&quot;&gt;Silverio Mart&amp;#xed;nez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_L/0/1/0/all/0/1&quot;&gt;Lu&amp;#xed;s Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franch_X/0/1/0/all/0/1&quot;&gt;Xavier Franch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05521">
<title>Toward High-Performance Energy and Power Battery Cells with Machine Learning-based Optimization of Electrode Manufacturing. (arXiv:2307.05521v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05521</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimization of the electrode manufacturing process is important for
upscaling the application of Lithium Ion Batteries (LIBs) to cater for growing
energy demand. In particular, LIB manufacturing is very important to be
optimized because it determines the practical performance of the cells when the
latter are being used in applications such as electric vehicles. In this study,
we tackled the issue of high-performance electrodes for desired battery
application conditions by proposing a powerful data-driven approach supported
by a deterministic machine learning (ML)-assisted pipeline for bi-objective
optimization of the electrochemical performance. This ML pipeline allows the
inverse design of the process parameters to adopt in order to manufacture
electrodes for energy or power applications. The latter work is an analogy to
our previous work that supported the optimization of the electrode
microstructures for kinetic, ionic, and electronic transport properties
improvement. An electrochemical pseudo-two-dimensional model is fed with the
electrode properties characterizing the electrode microstructures generated by
manufacturing simulations and used to simulate the electrochemical
performances. Secondly, the resulting dataset was used to train a deterministic
ML model to implement fast bi-objective optimizations to identify optimal
electrodes. Our results suggested a high amount of active material, combined
with intermediate values of solid content in the slurry and calendering degree,
to achieve the optimal electrodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duquesnoy_M/0/1/0/all/0/1&quot;&gt;Marc Duquesnoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chaoyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vishank Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayerbe_E/0/1/0/all/0/1&quot;&gt;Elixabete Ayerbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_A/0/1/0/all/0/1&quot;&gt;Alejandro A. Franco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05527">
<title>The Ethical Implications of Generative Audio Models: A Systematic Literature Review. (arXiv:2307.05527v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative audio models typically focus their applications in music and
speech generation, with recent models having human-like quality in their audio
output. This paper conducts a systematic literature review of 884 papers in the
area of generative audio models in order to both quantify the degree to which
researchers in the field are considering potential negative impacts and
identify the types of ethical implications researchers in this area need to
consider. Though 65% of generative audio research papers note positive
potential impacts of their work, less than 10% discuss any negative impacts.
This jarringly small percentage of papers considering negative impact is
particularly worrying because the issues brought to light by the few papers
doing so are raising serious ethical implications and concerns relevant to the
broader field such as the potential for fraud, deep-fakes, and copyright
infringement. By quantifying this lack of ethical consideration in generative
audio research and identifying key areas of potential harm, this paper lays the
groundwork for future work in the field at a critical point in time in order to
guide more conscientious research as this field progresses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnett_J/0/1/0/all/0/1&quot;&gt;Julia Barnett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05529">
<title>Keystroke Dynamics for User Identification. (arXiv:2307.05529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05529</link>
<description rdf:parseType="Literal">&lt;p&gt;In previous research, keystroke dynamics has shown promise for user
authentication, based on both fixed-text and free-text data. In this research,
we consider the more challenging multiclass user identification problem, based
on free-text data. We experiment with a complex image-like feature that has
previously been used to achieve state-of-the-art authentication results over
free-text data. Using this image-like feature and multiclass Convolutional
Neural Networks, we are able to obtain a classification (i.e., identification)
accuracy of 0.78 over a set of 148 users. However, we find that a Random Forest
classifier trained on a slightly modified version of this same feature yields
an accuracy of 0.93.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Atharva Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurecek_M/0/1/0/all/0/1&quot;&gt;Martin Jure&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1&quot;&gt;Mark Stamp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05537">
<title>NLP Meets RNA: Unsupervised Embedding Learning for Ribozymes with Word2Vec. (arXiv:2307.05537v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05537</link>
<description rdf:parseType="Literal">&lt;p&gt;Ribozymes, RNA molecules with distinct 3D structures and catalytic activity,
have widespread applications in synthetic biology and therapeutics. However,
relatively little research has focused on leveraging deep learning to enhance
our understanding of ribozymes. This study implements Word2Vec, an unsupervised
learning technique for natural language processing, to learn ribozyme
embeddings. Ribo2Vec was trained on over 9,000 diverse ribozymes, learning to
map sequences to 128 and 256-dimensional vector spaces. Using Ribo2Vec,
sequence embeddings for five classes of ribozymes (hatchet, pistol, hairpin,
hovlinc, and twister sister) were calculated. Principal component analysis
demonstrated the ability of these embeddings to distinguish between ribozyme
classes. Furthermore, a simple SVM classifier trained on ribozyme embeddings
showed promising results in accurately classifying ribozyme types. Our results
suggest that the embedding vectors contained meaningful information about
ribozymes. Interestingly, 256-dimensional embeddings behaved similarly to
128-dimensional embeddings, suggesting that a lower dimension vector space is
generally sufficient to capture ribozyme features. This approach demonstrates
the potential of Word2Vec for bioinformatics, opening new avenues for ribozyme
research. Future research includes using a Transformer-based method to learn
RNA embeddings, which can capture long-range interactions between nucleotides.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1&quot;&gt;Andrew Kean Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05551">
<title>Graph Neural Network-enabled Terahertz-based Flow-guided Nanoscale Localization. (arXiv:2307.05551v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05551</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific advancements in nanotechnology and advanced materials are paving
the way toward nanoscale devices for in-body precision medicine; comprising
integrated sensing, computing, communication, data and energy storage
capabilities. In the human cardiovascular system, such devices are envisioned
to be passively flowing and continuously sensing for detecting events of
diagnostic interest. The diagnostic value of detecting such events can be
enhanced by assigning to them their physical locations (e.g., body region),
which is the main proposition of flow-guided localization. Current flow-guided
localization approaches suffer from low localization accuracy and they are
by-design unable to localize events within the entire cardiovascular system.
Toward addressing this issue, we propose the utilization of Graph Neural
Networks (GNNs) for this purpose, and demonstrate localization accuracy and
coverage enhancements of our proposal over the existing State of the Art (SotA)
approaches. Based on our evaluation, we provide several design guidelines for
GNN-enabled flow-guided localization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartra_G/0/1/0/all/0/1&quot;&gt;Gerard Calvo Bartra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemic_F/0/1/0/all/0/1&quot;&gt;Filip Lemic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abadal_S/0/1/0/all/0/1&quot;&gt;Sergi Abadal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_X/0/1/0/all/0/1&quot;&gt;Xavier Costa Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05553">
<title>Review of feedback in Automated Essay Scoring. (arXiv:2307.05553v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05553</link>
<description rdf:parseType="Literal">&lt;p&gt;The first automated essay scoring system was developed 50 years ago.
Automated essay scoring systems are developing into systems with richer
functions than the previous simple scoring systems. Its purpose is not only to
score essays but also as a learning tool to improve the writing skill of users.
Feedback is the most important aspect of making an automated essay scoring
system useful in real life. The importance of feedback was already emphasized
in the first AES system. This paper reviews research on feedback including
different feedback types and essay traits on automated essay scoring. We also
reviewed the latest case studies of the automated essay scoring system that
provides feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jong_Y/0/1/0/all/0/1&quot;&gt;You-Jin Jong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ri_O/0/1/0/all/0/1&quot;&gt;Ok-Chol Ri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05563">
<title>RidgeBase: A Cross-Sensor Multi-Finger Contactless Fingerprint Dataset. (arXiv:2307.05563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05563</link>
<description rdf:parseType="Literal">&lt;p&gt;Contactless fingerprint matching using smartphone cameras can alleviate major
challenges of traditional fingerprint systems including hygienic acquisition,
portability and presentation attacks. However, development of practical and
robust contactless fingerprint matching techniques is constrained by the
limited availability of large scale real-world datasets. To motivate further
advances in contactless fingerprint matching across sensors, we introduce the
RidgeBase benchmark dataset. RidgeBase consists of more than 15,000 contactless
and contact-based fingerprint image pairs acquired from 88 individuals under
different background and lighting conditions using two smartphone cameras and
one flatbed contact sensor. Unlike existing datasets, RidgeBase is designed to
promote research under different matching scenarios that include Single Finger
Matching and Multi-Finger Matching for both contactless- to-contactless (CL2CL)
and contact-to-contactless (C2CL) verification and identification. Furthermore,
due to the high intra-sample variance in contactless fingerprints belonging to
the same finger, we propose a set-based matching protocol inspired by the
advances in facial recognition datasets. This protocol is specifically designed
for pragmatic contactless fingerprint matching that can account for variances
in focus, polarity and finger-angles. We report qualitative and quantitative
baseline results for different protocols using a COTS fingerprint matcher
(Verifinger) and a Deep CNN based approach on the RidgeBase dataset. The
dataset can be downloaded here:
https://www.buffalo.edu/cubs/research/datasets/ridgebase-benchmark-dataset.html
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1&quot;&gt;Bhavin Jawade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1&quot;&gt;Deen Dayal Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1&quot;&gt;Srirangaraj Setlur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratha_N/0/1/0/all/0/1&quot;&gt;Nalini Ratha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Govindaraju_V/0/1/0/all/0/1&quot;&gt;Venu Govindaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05582">
<title>DBFed: Debiasing Federated Learning Framework based on Domain-Independent. (arXiv:2307.05582v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05582</link>
<description rdf:parseType="Literal">&lt;p&gt;As digital transformation continues, enterprises are generating, managing,
and storing vast amounts of data, while artificial intelligence technology is
rapidly advancing. However, it brings challenges in information security and
data security. Data security refers to the protection of digital information
from unauthorized access, damage, theft, etc. throughout its entire life cycle.
With the promulgation and implementation of data security laws and the emphasis
on data security and data privacy by organizations and users,
Privacy-preserving technology represented by federated learning has a wide
range of application scenarios. Federated learning is a distributed machine
learning computing framework that allows multiple subjects to train joint
models without sharing data to protect data privacy and solve the problem of
data islands. However, the data among multiple subjects are independent of each
other, and the data differences in quality may cause fairness issues in
federated learning modeling, such as data bias among multiple subjects,
resulting in biased and discriminatory models. Therefore, we propose DBFed, a
debiasing federated learning framework based on domain-independent, which
mitigates model bias by explicitly encoding sensitive attributes during
client-side training. This paper conducts experiments on three real datasets
and uses five evaluation metrics of accuracy and fairness to quantify the
effect of the model. Most metrics of DBFed exceed those of the other three
comparative methods, fully demonstrating the debiasing effect of DBFed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiale Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05587">
<title>Active Learning for Video Classification with Frame Level Queries. (arXiv:2307.05587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms have pushed the boundaries of computer vision
research and have depicted commendable performance in a variety of
applications. However, training a robust deep neural network necessitates a
large amount of labeled training data, acquiring which involves significant
time and human effort. This problem is even more serious for an application
like video classification, where a human annotator has to watch an entire video
end-to-end to furnish a label. Active learning algorithms automatically
identify the most informative samples from large amounts of unlabeled data;
this tremendously reduces the human annotation effort in inducing a machine
learning model, as only the few samples that are identified by the algorithm,
need to be labeled manually. In this paper, we propose a novel active learning
framework for video classification, with the goal of further reducing the
labeling onus on the human annotators. Our framework identifies a batch of
exemplar videos, together with a set of informative frames for each video; the
human annotator needs to merely review the frames and provide a label for each
video. This involves much less manual work than watching the complete video to
come up with a label. We formulate a criterion based on uncertainty and
diversity to identify the informative videos and exploit representative
sampling techniques to extract a set of exemplar frames from each video. To the
best of our knowledge, this is the first research effort to develop an active
learning framework for video classification, where the annotators need to
inspect only a few frames to produce a label, rather than watching the
end-to-end video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1&quot;&gt;Debanjan Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Shayok Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05591">
<title>SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05591</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual and semantic comprehension of images is essential for generating
proper captions. The comprehension requires detection of objects, modeling of
relations between them, an assessment of the semantics of the scene and,
finally, representing the extracted knowledge in a language space. To achieve
rich language capabilities while ensuring good image-language mappings,
pretrained language models (LMs) were conditioned on pretrained multi-modal
(image-text) models that allow for image inputs. This requires an alignment of
the image representation of the multi-modal model with the language
representations of a generative LM. However, it is not clear how to best
transfer semantics detected by the vision encoder of the multi-modal model to
the LM. We introduce two novel ways of constructing a linear mapping that
successfully transfers semantics between the embedding spaces of the two
pretrained models. The first aligns the embedding space of the multi-modal
language encoder with the embedding space of the pretrained LM via token
correspondences. The latter leverages additional data that consists of
image-text pairs to construct the mapping directly from vision to language
space. Using our semantic mappings, we unlock image captioning for LMs without
access to gradient information. By using different sources of data we achieve
strong captioning performance on MS-COCO and Flickr30k datasets. Even in the
face of limited data, our method partly exceeds the performance of other
zero-shot and even finetuned competitors. Our ablation studies show that even
LMs at a scale of merely 250M parameters can generate decent captions employing
our semantic mappings. Our approach makes image captioning more accessible for
institutions with restricted computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paischer_F/0/1/0/all/0/1&quot;&gt;Fabian Paischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adler_T/0/1/0/all/0/1&quot;&gt;Thomas Adler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hofmarcher_M/0/1/0/all/0/1&quot;&gt;Markus Hofmarcher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1&quot;&gt;Sepp Hochreiter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05592">
<title>Functional PCA and Deep Neural Networks-based Bayesian Inverse Uncertainty Quantification with Transient Experimental Data. (arXiv:2307.05592v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.05592</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse UQ is the process to inversely quantify the model input uncertainties
based on experimental data. This work focuses on developing an inverse UQ
process for time-dependent responses, using dimensionality reduction by
functional principal component analysis (PCA) and deep neural network
(DNN)-based surrogate models. The demonstration is based on the inverse UQ of
TRACE physical model parameters using the FEBA transient experimental data. The
measurement data is time-dependent peak cladding temperature (PCT). Since the
quantity-of-interest (QoI) is time-dependent that corresponds to
infinite-dimensional responses, PCA is used to reduce the QoI dimension while
preserving the transient profile of the PCT, in order to make the inverse UQ
process more efficient. However, conventional PCA applied directly to the PCT
time series profiles can hardly represent the data precisely due to the sudden
temperature drop at the time of quenching. As a result, a functional alignment
method is used to separate the phase and amplitude information of the transient
PCT profiles before dimensionality reduction. DNNs are then trained using PC
scores from functional PCA to build surrogate models of TRACE in order to
reduce the computational cost in Markov Chain Monte Carlo sampling. Bayesian
neural networks are used to estimate the uncertainties of DNN surrogate model
predictions. In this study, we compared four different inverse UQ processes
with different dimensionality reduction methods and surrogate models. The
proposed approach shows an improvement in reducing the dimension of the TRACE
transient simulations, and the forward propagation of inverse UQ results has a
better agreement with the experimental data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Ziyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yaseen_M/0/1/0/all/0/1&quot;&gt;Mahmoud Yaseen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05596">
<title>Compositional Generalization from First Principles. (arXiv:2307.05596v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05596</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging the compositional nature of our world to expedite learning and
facilitate generalization is a hallmark of human perception. In machine
learning, on the other hand, achieving compositional generalization has proven
to be an elusive goal, even for models with explicit compositional priors. To
get a better handle on compositional generalization, we here approach it from
the bottom up: Inspired by identifiable representation learning, we investigate
compositionality as a property of the data-generating process rather than the
data itself. This reformulation enables us to derive mild conditions on only
the support of the training distribution and the model architecture, which are
sufficient for compositional generalization. We further demonstrate how our
theoretical framework applies to real-world scenarios and validate our findings
empirically. Our results set the stage for a principled theoretical study of
compositional generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiedemer_T/0/1/0/all/0/1&quot;&gt;Thadd&amp;#xe4;us Wiedemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayilvahanan_P/0/1/0/all/0/1&quot;&gt;Prasanna Mayilvahanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05601">
<title>Unsupervised Domain Adaptation with Deep Neural-Network. (arXiv:2307.05601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05601</link>
<description rdf:parseType="Literal">&lt;p&gt;This report contributes to the field of unsupervised domain adaptation by
providing an analysis of existing methods, introducing a new approach, and
demonstrating the potential for improving visual recognition tasks across
different domains. The results of this study open up opportunities for further
study and development of advanced methods in the field of domain adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bituitskii_A/0/1/0/all/0/1&quot;&gt;Artem Bituitskii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05603">
<title>Can You Improve My Code? Optimizing Programs with Local Search. (arXiv:2307.05603v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.05603</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a local search method for improving an existing program
with respect to a measurable objective. Program Optimization with Locally
Improving Search (POLIS) exploits the structure of a program, defined by its
lines. POLIS improves a single line of the program while keeping the remaining
lines fixed, using existing brute-force synthesis algorithms, and continues
iterating until it is unable to improve the program&apos;s performance. POLIS was
evaluated with a 27-person user study, where participants wrote programs
attempting to maximize the score of two single-agent games: Lunar Lander and
Highway. POLIS was able to substantially improve the participants&apos; programs
with respect to the game scores. A proof-of-concept demonstration on existing
Stack Overflow code measures applicability in real-world problems. These
results suggest that POLIS could be used as a helpful programming assistant for
programming problems with measurable objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdollahi_F/0/1/0/all/0/1&quot;&gt;Fatemeh Abdollahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ameen_S/0/1/0/all/0/1&quot;&gt;Saqib Ameen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lelis_L/0/1/0/all/0/1&quot;&gt;Levi H. S. Lelis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05610">
<title>Substance or Style: What Does Your Image Embedding Know?. (arXiv:2307.05610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05610</link>
<description rdf:parseType="Literal">&lt;p&gt;Probes are small networks that predict properties of underlying data from
embeddings, and they provide a targeted, effective way to illuminate the
information contained in embeddings. While analysis through the use of probes
has become standard in NLP, there has been much less exploration in vision.
Image foundation models have primarily been evaluated for semantic content.
Better understanding the non-semantic information in popular embeddings (e.g.,
MAE, SimCLR, or CLIP) will shed new light both on the training algorithms and
on the uses for these foundation models. We design a systematic transformation
prediction task and measure the visual content of embeddings along many axes,
including image style, quality, and a range of natural and artificial
transformations. Surprisingly, six embeddings (including SimCLR) encode enough
non-semantic information to identify dozens of transformations. We also
consider a generalization task, where we group similar transformations and hold
out several for testing. We find that image-text models (CLIP and ALIGN) are
better at recognizing new examples of style transfer than masking-based models
(CAN and MAE). Overall, our results suggest that the choice of pre-training
algorithm impacts the types of information in the embedding, and certain models
are better than others for non-semantic downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashtchian_C/0/1/0/all/0/1&quot;&gt;Cyrus Rashtchian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_C/0/1/0/all/0/1&quot;&gt;Charles Herrmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferng_C/0/1/0/all/0/1&quot;&gt;Chun-Sung Ferng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1&quot;&gt;Ayan Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnan_D/0/1/0/all/0/1&quot;&gt;Dilip Krishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Deqing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juan_D/0/1/0/all/0/1&quot;&gt;Da-Cheng Juan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomkins_A/0/1/0/all/0/1&quot;&gt;Andrew Tomkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05614">
<title>Impact of Feature Encoding on Malware Classification Explainability. (arXiv:2307.05614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05614</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the impact of feature encoding techniques on the
explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a
malware classification dataset, we trained an XGBoost model and compared the
performance of two feature encoding methods: Label Encoding (LE) and One Hot
Encoding (OHE). Our findings reveal a marginal performance loss when using OHE
instead of LE. However, the more detailed explanations provided by OHE
compensated for this loss. We observed that OHE enables deeper exploration of
details in both global and local contexts, facilitating more comprehensive
answers. Additionally, we observed that using OHE resulted in smaller
explanation files and reduced analysis time for human analysts. These findings
emphasize the significance of considering feature encoding techniques in XAI
research and suggest potential for further exploration by incorporating
additional encoding methods and innovative visualization approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manai_E/0/1/0/all/0/1&quot;&gt;Elyes Manai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejri_M/0/1/0/all/0/1&quot;&gt;Mohamed Mejri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fattahi_J/0/1/0/all/0/1&quot;&gt;Jaouhar Fattahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05620">
<title>Latent Space Perspicacity and Interpretation Enhancement (LS-PIE) Framework. (arXiv:2307.05620v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.05620</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear latent variable models such as principal component analysis (PCA),
independent component analysis (ICA), canonical correlation analysis (CCA), and
factor analysis (FA) identify latent directions (or loadings) either ordered or
unordered. The data is then projected onto the latent directions to obtain
their projected representations (or scores). For example, PCA solvers usually
rank the principal directions by explaining the most to least variance, while
ICA solvers usually return independent directions unordered and often with
single sources spread across multiple directions as multiple sub-sources, which
is of severe detriment to their usability and interpretability.
&lt;/p&gt;
&lt;p&gt;This paper proposes a general framework to enhance latent space
representations for improving the interpretability of linear latent spaces.
Although the concepts in this paper are language agnostic, the framework is
written in Python. This framework automates the clustering and ranking of
latent vectors to enhance the latent information per latent vector, as well as,
the interpretation of latent vectors. Several innovative enhancements are
incorporated including latent ranking (LR), latent scaling (LS), latent
clustering (LC), and latent condensing (LCON).
&lt;/p&gt;
&lt;p&gt;For a specified linear latent variable model, LR ranks latent directions
according to a specified metric, LS scales latent directions according to a
specified metric, LC automatically clusters latent directions into a specified
number of clusters, while, LCON automatically determines an appropriate number
of clusters into which to condense the latent directions for a given metric.
Additional functionality of the framework includes single-channel and
multi-channel data sources, data preprocessing strategies such as Hankelisation
to seamlessly expand the applicability of linear latent variable models (LLVMs)
to a wider variety of data.
&lt;/p&gt;
&lt;p&gt;The effectiveness of LR, LS, and LCON are showcased on two crafted
foundational problems with two applied latent variable models, namely, PCA and
ICA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stevens_J/0/1/0/all/0/1&quot;&gt;Jesse Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wilke_D/0/1/0/all/0/1&quot;&gt;Daniel N. Wilke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Setshedi_I/0/1/0/all/0/1&quot;&gt;Itumeleng Setshedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05623">
<title>A DeepLearning Framework for Dynamic Estimation of Origin-Destination Sequence. (arXiv:2307.05623v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05623</link>
<description rdf:parseType="Literal">&lt;p&gt;OD matrix estimation is a critical problem in the transportation domain. The
principle method uses the traffic sensor measured information such as traffic
counts to estimate the traffic demand represented by the OD matrix. The problem
is divided into two categories: static OD matrix estimation and dynamic OD
matrices sequence(OD sequence for short) estimation. The above two face the
underdetermination problem caused by abundant estimated parameters and
insufficient constraint information. In addition, OD sequence estimation also
faces the lag challenge: due to different traffic conditions such as
congestion, identical vehicle will appear on different road sections during the
same observation period, resulting in identical OD demands correspond to
different trips. To this end, this paper proposes an integrated method, which
uses deep learning methods to infer the structure of OD sequence and uses
structural constraints to guide traditional numerical optimization. Our
experiments show that the neural network(NN) can effectively infer the
structure of the OD sequence and provide practical constraints for numerical
optimization to obtain better results. Moreover, the experiments show that
provided structural information contains not only constraints on the spatial
structure of OD matrices but also provides constraints on the temporal
structure of OD sequence, which solve the effect of the lagging problem well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zheli Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1&quot;&gt;Defu Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiaomin Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05624">
<title>CILF:Causality Inspired Learning Framework for Out-of-Distribution Vehicle Trajectory Prediction. (arXiv:2307.05624v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05624</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction is critical for autonomous driving vehicles. Most
existing methods tend to model the correlation between history trajectory
(input) and future trajectory (output). Since correlation is just a superficial
description of reality, these methods rely heavily on the i.i.d. assumption and
evince a heightened susceptibility to out-of-distribution data. To address this
problem, we propose an Out-of- Distribution Causal Graph (OOD-CG), which
explicitly defines the underlying causal structure of the data with three
entangled latent features: 1) domain-invariant causal feature (IC), 2)
domain-variant causal feature (VC), and 3) domain-variant non-causal feature
(VN ). While these features are confounded by confounder (C) and domain
selector (D). To leverage causal features for prediction, we propose a Causal
Inspired Learning Framework (CILF), which includes three steps: 1) extracting
domain-invariant causal feature by means of an invariance loss, 2) extracting
domain variant feature by domain contrastive learning, and 3) separating
domain-variant causal and non-causal feature by encouraging causal sufficiency.
We evaluate the performance of CILF in different vehicle trajectory prediction
models on the mainstream datasets NGSIM and INTERACTION. Experiments show
promising improvements in CILF on domain generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Q/0/1/0/all/0/1&quot;&gt;Qifan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yezhuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanpeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05628">
<title>DNAGPT: A Generalized Pretrained Tool for Multiple DNA Sequence Analysis Tasks. (arXiv:2307.05628v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2307.05628</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of the GPT series proves that GPT can extract general information
from sequences, thereby benefiting all downstream tasks. This motivates us to
use pre-trained models to explore the hidden information in DNA sequences.
However, data and task requirements in DNA sequence analysis are complexity and
diversity as DNA relevant data includes different types of information, such as
sequences, expression levels, etc, while there is currently no model
specifically designed for these characteristics. Hereby, we present DNAGPT, a
generalized foundation model pre-trained on over 10 billion base pairs from 9
species which can be fine-tuned for any DNA sequence analysis task. Our model
can simultaneously process or output DNA sequences and numbers. In addition,
our unique token design allows users to design prompts according to their own
task requirements, making it applicable to any type of task. We have evaluated
our model on classification, regression, and generation tasks. We demonstrate
that DNAGPT benefits from pre-training, and therefore can bring performance
gains to any downstream task. Our model is not only a new attempt in the field
of genomes analysis, but also provides a new direction for the application of
foundation models in biology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Daoan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chenchen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jianhua Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05633">
<title>Transaction Fraud Detection via an Adaptive Graph Neural Network. (arXiv:2307.05633v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05633</link>
<description rdf:parseType="Literal">&lt;p&gt;Many machine learning methods have been proposed to achieve accurate
transaction fraud detection, which is essential to the financial security of
individuals and banks. However, most existing methods leverage original
features only or require manual feature engineering. They lack the ability to
learn discriminative representations from transaction data. Moreover, criminals
often commit fraud by imitating cardholders&apos; behaviors, which causes the poor
performance of existing detection models. In this paper, we propose an Adaptive
Sampling and Aggregation-based Graph Neural Network (ASA-GNN) that learns
discriminative representations to improve the performance of transaction fraud
detection. A neighbor sampling strategy is performed to filter noisy nodes and
supplement information for fraudulent nodes. Specifically, we leverage cosine
similarity and edge weights to adaptively select neighbors with similar
behavior patterns for target nodes and then find multi-hop neighbors for
fraudulent nodes. A neighbor diversity metric is designed by calculating the
entropy among neighbors to tackle the camouflage issue of fraudsters and
explicitly alleviate the over-smoothing phenomena. Extensive experiments on
three real financial datasets demonstrate that the proposed method ASA-GNN
outperforms state-of-the-art ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yue Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mengchu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05635">
<title>Fundamental limits of overparametrized shallow neural networks for supervised learning. (arXiv:2307.05635v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05635</link>
<description rdf:parseType="Literal">&lt;p&gt;We carry out an information-theoretical analysis of a two-layer neural
network trained from input-output pairs generated by a teacher network with
matching architecture, in overparametrized regimes. Our results come in the
form of bounds relating i) the mutual information between training data and
network weights, or ii) the Bayes-optimal generalization error, to the same
quantities but for a simpler (generalized) linear model for which explicit
expressions are rigorously known. Our bounds, which are expressed in terms of
the number of training samples, input dimension and number of hidden units,
thus yield fundamental performance limits for any neural network (and actually
any learning procedure) trained from limited data generated according to our
two-layer teacher neural network model. The proof relies on rigorous tools from
spin glasses and is guided by ``Gaussian equivalence principles&apos;&apos; lying at the
core of numerous recent analyses of neural networks. With respect to the
existing literature, which is either non-rigorous or restricted to the case of
the learning of the readout weights only, our results are information-theoretic
(i.e. are not specific to any learning algorithm) and, importantly, cover a
setting where all the network parameters are trained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilli_F/0/1/0/all/0/1&quot;&gt;Francesco Camilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tieplova_D/0/1/0/all/0/1&quot;&gt;Daria Tieplova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbier_J/0/1/0/all/0/1&quot;&gt;Jean Barbier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05637">
<title>Speech Diarization and ASR with GMM. (arXiv:2307.05637v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.05637</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research paper, we delve into the topics of Speech Diarization and
Automatic Speech Recognition (ASR). Speech diarization involves the separation
of individual speakers within an audio stream. By employing the ASR transcript,
the diarization process aims to segregate each speaker&apos;s utterances, grouping
them based on their unique audio characteristics. On the other hand, Automatic
Speech Recognition refers to the capability of a machine or program to identify
and convert spoken words and phrases into a machine-readable format. In our
speech diarization approach, we utilize the Gaussian Mixer Model (GMM) to
represent speech segments. The inter-cluster distance is computed based on the
GMM parameters, and the distance threshold serves as the stopping criterion.
ASR entails the conversion of an unknown speech waveform into a corresponding
written transcription. The speech signal is analyzed using synchronized
algorithms, taking into account the pitch frequency. Our primary objective
typically revolves around developing a model that minimizes the Word Error Rate
(WER) metric during speech transcription.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aayush Kumar Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhavikatti_V/0/1/0/all/0/1&quot;&gt;Vineet Bhavikatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nidawani_A/0/1/0/all/0/1&quot;&gt;Amogh Nidawani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siddappaji_D/0/1/0/all/0/1&quot;&gt;Dr. Siddappaji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+P_S/0/1/0/all/0/1&quot;&gt;Sanath P&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mishra_D/0/1/0/all/0/1&quot;&gt;Dr Geetishree Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05638">
<title>A Comprehensive Survey of Deep Transfer Learning for Anomaly Detection in Industrial Time Series: Methods, Applications, and Directions. (arXiv:2307.05638v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05638</link>
<description rdf:parseType="Literal">&lt;p&gt;Automating the monitoring of industrial processes has the potential to
enhance efficiency and optimize quality by promptly detecting abnormal events
and thus facilitating timely interventions. Deep learning, with its capacity to
discern non-trivial patterns within large datasets, plays a pivotal role in
this process. Standard deep learning methods are suitable to solve a specific
task given a specific type of data. During training, the algorithms demand
large volumes of labeled training data. However, due to the dynamic nature of
processes and the environment, it is impractical to acquire the needed data for
standard deep learning training for every slightly different case anew. Deep
transfer learning offers a solution to this problem. By leveraging knowledge
from related tasks and accounting for variations in data distributions, this
learning framework solves new tasks even with little or no additional labeled
data. The approach bypasses the need to retrain a model from scratch for every
new setup and dramatically reduces the labeled data requirement. This survey
provides an in-depth review of deep transfer learning, examining the problem
settings of transfer learning and classifying the prevailing deep transfer
learning methods. Moreover, we delve into applying deep transfer learning in
the context of a broad spectrum of time series anomaly detection tasks
prevalent in primary industrial domains, e.g., manufacturing process
monitoring, predictive maintenance, energy management, and infrastructure
facility monitoring. We conclude this survey by underlining the challenges and
limitations of deep transfer learning in industrial contexts. We also provide
practical directions for solution design and implementation for these tasks,
leading to specific, actionable suggestions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Peng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulkadir_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdulkadir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenthal_M/0/1/0/all/0/1&quot;&gt;Matthias Rosenthal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schatte_G/0/1/0/all/0/1&quot;&gt;Gerrit A. Schatte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewe_B/0/1/0/all/0/1&quot;&gt;Benjamin F. Grewe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1&quot;&gt;Thilo Stadelmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05639">
<title>Learning Active Subspaces and Discovering Important Features with Gaussian Radial Basis Functions Neural Networks. (arXiv:2307.05639v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05639</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing a model that achieves a strong predictive performance and at the
same time is interpretable by humans is one of the most difficult challenges in
machine learning research due to the conflicting nature of these two
objectives. To address this challenge, we propose a modification of the Radial
Basis Function Neural Network model by equipping its Gaussian kernel with a
learnable precision matrix. We show that precious information is contained in
the spectrum of the precision matrix that can be extracted once the training of
the model is completed. In particular, the eigenvectors explain the directions
of maximum sensitivity of the model revealing the active subspace and
suggesting potential applications for supervised dimensionality reduction. At
the same time, the eigenvectors highlight the relationship in terms of absolute
variation between the input and the latent variables, thereby allowing us to
extract a ranking of the input variables based on their importance to the
prediction task enhancing the model interpretability. We conducted numerical
experiments for regression, classification, and feature selection tasks,
comparing our model against popular machine learning models and the
state-of-the-art deep learning-based embedding feature selection techniques.
Our results demonstrate that the proposed model does not only yield an
attractive prediction performance with respect to the competitors but also
provides meaningful and interpretable results that potentially could assist the
decision-making process in real-world applications. A PyTorch implementation of
the model is available on GitHub at the following link.
https://github.com/dannyzx/GRBF-NNs
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAgostino_D/0/1/0/all/0/1&quot;&gt;Danny D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilievski_I/0/1/0/all/0/1&quot;&gt;Ilija Ilievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoemaker_C/0/1/0/all/0/1&quot;&gt;Christine Annette Shoemaker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05642">
<title>ConFL: Constraint-guided Fuzzing for Machine Learning Framework. (arXiv:2307.05642v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.05642</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning gains prominence in various sectors of society for
automated decision-making, concerns have risen regarding potential
vulnerabilities in machine learning (ML) frameworks. Nevertheless, testing
these frameworks is a daunting task due to their intricate implementation.
Previous research on fuzzing ML frameworks has struggled to effectively extract
input constraints and generate valid inputs, leading to extended fuzzing
durations for deep execution or revealing the target crash.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose ConFL, a constraint-guided fuzzer for ML
frameworks. ConFL automatically extracting constraints from kernel codes
without the need for any prior knowledge. Guided by the constraints, ConFL is
able to generate valid inputs that can pass the verification and explore deeper
paths of kernel codes. In addition, we design a grouping technique to boost the
fuzzing efficiency.
&lt;/p&gt;
&lt;p&gt;To demonstrate the effectiveness of ConFL, we evaluated its performance
mainly on Tensorflow. We find that ConFL is able to cover more code lines, and
generate more valid inputs than state-of-the-art (SOTA) fuzzers. More
importantly, ConFL found 84 previously unknown vulnerabilities in different
versions of Tensorflow, all of which were assigned with new CVE ids, of which 3
were critical-severity and 13 were high-severity. We also extended ConFL to
test PyTorch and Paddle, 7 vulnerabilities are found to date.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Quanchen Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1&quot;&gt;Guozhu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Deyue Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05643">
<title>Multiobjective Hydropower Reservoir Operation Optimization with Transformer-Based Deep Reinforcement Learning. (arXiv:2307.05643v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05643</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to shortage of water resources and increasing water demands, the joint
operation of multireservoir systems for balancing power generation, ecological
protection, and the residential water supply has become a critical issue in
hydropower management. However, the numerous constraints and nonlinearity of
multiple reservoirs make solving this problem time-consuming. To address this
challenge, a deep reinforcement learning approach that incorporates a
transformer framework is proposed. The multihead attention mechanism of the
encoder effectively extracts information from reservoirs and residential areas,
and the multireservoir attention network of the decoder generates suitable
operational decisions. The proposed method is applied to Lake Mead and Lake
Powell in the Colorado River Basin. The experimental results demonstrate that
the transformer-based deep reinforcement learning approach can produce
appropriate operational outcomes. Compared to a state-of-the-art method, the
operation strategies produced by the proposed approach generate 10.11% more
electricity, reduce the amended annual proportional flow deviation by 39.69%,
and increase water supply revenue by 4.10%. Consequently, the proposed approach
offers an effective method for the multiobjective operation of multihydropower
reservoir systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Rixin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jie Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05694">
<title>A Survey on Figure Classification Techniques in Scientific Documents. (arXiv:2307.05694v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05694</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures visually represent an essential piece of information and provide an
effective means to communicate scientific facts. Recently there have been many
efforts toward extracting data directly from figures, specifically from tables,
diagrams, and plots, using different Artificial Intelligence and Machine
Learning techniques. This is because removing information from figures could
lead to deeper insights into the concepts highlighted in the scientific
documents. In this survey paper, we systematically categorize figures into five
classes - tables, photos, diagrams, maps, and plots, and subsequently present a
critical review of the existing methodologies and data sets that address the
problem of figure classification. Finally, we identify the current research
gaps and provide possible directions for further research on figure
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhote_A/0/1/0/all/0/1&quot;&gt;Anurag Dhote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05695">
<title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1&quot;&gt;Vladislav Lialin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1&quot;&gt;Namrata Shivagunde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1&quot;&gt;Sherin Muckatira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1&quot;&gt;Anna Rumshisky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05697">
<title>A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car Pooling Services. (arXiv:2307.05697v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Car pooling is expected to significantly help in reducing traffic congestion
and pollution in cities by enabling drivers to share their cars with travellers
with similar itineraries and time schedules. A number of car pooling matching
services have been designed in order to efficiently find successful ride
matches in a given pool of drivers and potential passengers. However, it is now
recognised that many non-monetary aspects and social considerations, besides
simple mobility needs, may influence the individual willingness of sharing a
ride, which are difficult to predict. To address this problem, in this study we
propose GoTogether, a recommender system for car pooling services that
leverages on learning-to-rank techniques to automatically derive the
personalised ranking model of each user from the history of her choices (i.e.,
the type of accepted or rejected shared rides). Then, GoTogether builds the
list of recommended rides in order to maximise the success rate of the offered
matches. To test the performance of our scheme we use real data from Twitter
and Foursquare sources in order to generate a dataset of plausible mobility
patterns and ride requests in a metropolitan area. The results show that the
proposed solution quickly obtain an accurate prediction of the personalised
user&apos;s choice model both in static and dynamic conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campana_M/0/1/0/all/0/1&quot;&gt;Mattia Giovanni Campana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmastro_F/0/1/0/all/0/1&quot;&gt;Franca Delmastro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruno_R/0/1/0/all/0/1&quot;&gt;Raffaele Bruno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05698">
<title>Online Ad Procurement in Non-stationary Autobidding Worlds. (arXiv:2307.05698v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05698</link>
<description rdf:parseType="Literal">&lt;p&gt;Today&apos;s online advertisers procure digital ad impressions through interacting
with autobidding platforms: advertisers convey high level procurement goals via
setting levers such as budget, target return-on-investment, max cost per click,
etc.. Then ads platforms subsequently procure impressions on advertisers&apos;
behalf, and report final procurement conversions (e.g. click) to advertisers.
In practice, advertisers may receive minimal information on platforms&apos;
procurement details, and procurement outcomes are subject to non-stationary
factors like seasonal patterns, occasional system corruptions, and market
trends which make it difficult for advertisers to optimize lever decisions
effectively. Motivated by this, we present an online learning framework that
helps advertisers dynamically optimize ad platform lever decisions while
subject to general long-term constraints in a realistic bandit feedback
environment with non-stationary procurement outcomes. In particular, we
introduce a primal-dual algorithm for online decision making with
multi-dimension decision variables, bandit feedback and long-term uncertain
constraints. We show that our algorithm achieves low regret in many worlds when
procurement outcomes are generated through procedures that are stochastic,
adversarial, adversarially corrupted, periodic, and ergodic, respectively,
without having to know which procedure is the ground truth. Finally, we
emphasize that our proposed algorithm and theoretical results extend beyond the
applications of online advertising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jason Cheuk Nam Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haihao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Baoyu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05704">
<title>A Causal Ordering Prior for Unsupervised Representation Learning. (arXiv:2307.05704v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05704</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised representation learning with variational inference relies
heavily on independence assumptions over latent variables. Causal
representation learning (CRL), however, argues that factors of variation in a
dataset are, in fact, causally related. Allowing latent variables to be
correlated, as a consequence of causal relationships, is more realistic and
generalisable. So far, provably identifiable methods rely on: auxiliary
information, weak labels, and interventional or even counterfactual data.
Inspired by causal discovery with functional causal models, we propose a fully
unsupervised representation learning method that considers a data generation
process with a latent additive noise model (ANM). We encourage the latent space
to follow a causal ordering via loss function based on the Hessian of the
latent distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilouras_K/0/1/0/all/0/1&quot;&gt;Konstantinos Vilouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1&quot;&gt;Sotirios A. Tsaftaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05707">
<title>MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning. (arXiv:2307.05707v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05707</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent progress in incremental learning, addressing catastrophic
forgetting under distributional drift is still an open and important problem.
Indeed, while state-of-the-art domain incremental learning (DIL) methods
perform satisfactorily within known domains, their performance largely degrades
in the presence of novel domains. This limitation hampers their
generalizability, and restricts their scalability to more realistic settings
where train and test data are drawn from different distributions. To address
these limitations, we present a novel DIL approach based on a mixture of
prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of
S-Prompting to handle both in-distribution and out-of-distribution data at
inference. In particular, at the training stage we model the features
distribution of every class in each domain, learning individual text and visual
prompts to adapt to a given domain. At inference, the learned distributions
allow us to identify whether a given test sample belongs to a known domain,
selecting the correct prompt for the classification task, or from an unseen
domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical
evaluation reveals the poor performance of existing DIL methods under domain
shift, and suggests that the proposed MoP-CLIP performs competitively in the
standard DIL settings while outperforming state-of-the-art methods in OOD
scenarios. These results demonstrate the superiority of MoP-CLIP, offering a
robust and general solution to the problem of domain incremental learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolas_J/0/1/0/all/0/1&quot;&gt;Julien Nicolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiaroni_F/0/1/0/all/0/1&quot;&gt;Florent Chiaroni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziko_I/0/1/0/all/0/1&quot;&gt;Imtiaz Ziko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_O/0/1/0/all/0/1&quot;&gt;Ola Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1&quot;&gt;Christian Desrosiers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05728">
<title>Towards A Scalable Solution for Improving Multi-Group Fairness in Compositional Classification. (arXiv:2307.05728v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05728</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the rich literature on machine learning fairness, relatively little
attention has been paid to remediating complex systems, where the final
prediction is the combination of multiple classifiers and where multiple groups
are present. In this paper, we first show that natural baseline approaches for
improving equal opportunity fairness scale linearly with the product of the
number of remediated groups and the number of remediated prediction labels,
rendering them impractical. We then introduce two simple techniques, called
{\em task-overconditioning} and {\em group-interleaving}, to achieve a constant
scaling in this multi-group multi-label setup. Our experimental results in
academic and real-world environments demonstrate the effectiveness of our
proposal at mitigation within this environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atwood_J/0/1/0/all/0/1&quot;&gt;James Atwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_T/0/1/0/all/0/1&quot;&gt;Tina Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1&quot;&gt;Ben Packer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deodhar_M/0/1/0/all/0/1&quot;&gt;Meghana Deodhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jilin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1&quot;&gt;Alex Beutel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1&quot;&gt;Flavien Prost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1&quot;&gt;Ahmad Beirami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05735">
<title>GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting for Continuous-time Generative Models. (arXiv:2307.05735v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05735</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. The GOKU-UI broadens the original
model&apos;s spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e.
ubiquitous, inference through attention mechanisms and a novel multiple
shooting training strategy in the latent space. These enhancements have led to
a significant increase in its performance in both reconstruction and forecast
tasks, as demonstrated by our evaluation of simulated and empirical data.
Specifically, GOKU-UI outperformed all baseline models on synthetic datasets
even with a training set 32-fold smaller, underscoring its remarkable data
efficiency. Furthermore, when applied to empirical human brain data, while
incorporating stochastic Stuart-Landau oscillators into its dynamical core, it
not only surpassed state-of-the-art baseline methods in the reconstruction
task, but also demonstrated better prediction of future brain activity up to 12
seconds ahead. By training GOKU-UI on resting-state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning an effective
low-dimensional dynamical system model that could offer insights into brain
functionality and open avenues for practical applications such as mental state
or psychiatric condition classification. Ultimately, our research provides
further impetus for the field of Scientific Machine Learning, showcasing the
potential for advancements when established scientific insights are interwoven
with modern machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrevaya_G/0/1/0/all/0/1&quot;&gt;Germ&amp;#xe1;n Abrevaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezanian_Panahi_M/0/1/0/all/0/1&quot;&gt;Mahta Ramezanian-Panahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1&quot;&gt;Jean-Christophe Gagnon-Audet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polosecki_P/0/1/0/all/0/1&quot;&gt;Pablo Polosecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawson_S/0/1/0/all/0/1&quot;&gt;Silvina Ponce Dawson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1&quot;&gt;Guillermo Cecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1&quot;&gt;Guillaume Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05747">
<title>Integrating Curricula with Replays: Its Effects on Continual Learning. (arXiv:2307.05747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05747</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans engage in learning and reviewing processes with curricula when
acquiring new skills or knowledge. This human learning behavior has inspired
the integration of curricula with replay methods in continual learning agents.
The goal is to emulate the human learning process, thereby improving knowledge
retention and facilitating learning transfer. Existing replay methods in
continual learning agents involve the random selection and ordering of data
from previous tasks, which has shown to be effective. However, limited research
has explored the integration of different curricula with replay methods to
enhance continual learning. Our study takes initial steps in examining the
impact of integrating curricula with replay methods on continual learning in
three specific aspects: the interleaved frequency of replayed exemplars with
training data, the sequence in which exemplars are replayed, and the strategy
for selecting exemplars into the replay buffer. These aspects of curricula
design align with cognitive psychology principles and leverage the benefits of
interleaved practice during replays, easy-to-hard rehearsal, and exemplar
selection strategy involving exemplars from a uniform distribution of
difficulties. Based on our results, these three curricula effectively mitigated
catastrophic forgetting and enhanced positive knowledge transfer, demonstrating
the potential of curricula in advancing continual learning methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tee_R/0/1/0/all/0/1&quot;&gt;Ren Jie Tee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengmi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05750">
<title>Fermat Distances: Metric Approximation, Spectral Convergence, and Clustering Algorithms. (arXiv:2307.05750v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.05750</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the convergence properties of Fermat distances, a family of
density-driven metrics defined on Riemannian manifolds with an associated
probability measure. Fermat distances may be defined either on discrete samples
from the underlying measure, in which case they are random, or in the continuum
setting, in which they are induced by geodesics under a density-distorted
Riemannian metric. We prove that discrete, sample-based Fermat distances
converge to their continuum analogues in small neighborhoods with a precise
rate that depends on the intrinsic dimensionality of the data and the parameter
governing the extent of density weighting in Fermat distances. This is done by
leveraging novel geometric and statistical arguments in percolation theory that
allow for non-uniform densities and curved domains. Our results are then used
to prove that discrete graph Laplacians based on discrete, sample-driven Fermat
distances converge to corresponding continuum operators. In particular, we show
the discrete eigenvalues and eigenvectors converge to their continuum analogues
at a dimension-dependent rate, which allows us to interpret the efficacy of
discrete spectral clustering using Fermat distances in terms of the resulting
continuum limit. The perspective afforded by our discrete-to-continuum Fermat
distance analysis leads to new clustering algorithms for data and related
insights into efficient computations associated to density-driven spectral
clustering. Our theoretical analysis is supported with numerical simulations
and experiments on synthetic and real image data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Trillos_N/0/1/0/all/0/1&quot;&gt;Nicol&amp;#xe1;s Garc&amp;#xed;a Trillos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Little_A/0/1/0/all/0/1&quot;&gt;Anna Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+McKenzie_D/0/1/0/all/0/1&quot;&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murphy_J/0/1/0/all/0/1&quot;&gt;James M. Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05763">
<title>Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison Between Q-Learning and Heuristic Methods. (arXiv:2307.05763v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.05763</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to technological advances in the field of radio technology and its
availability, the number of interference signals in the radio spectrum is
continuously increasing. Interference signals must be detected in a timely
fashion, in order to maintain standards and keep emergency frequencies open. To
this end, specialized (multi-channel) receivers are used for spectrum
monitoring. In this paper, the performances of two different approaches for
controlling the available receiver resources are compared. The methods used for
resource management (ReMa) are linear frequency tuning as a heuristic approach
and a Q-learning algorithm from the field of reinforcement learning. To test
the methods to be investigated, a simplified scenario was designed with two
receiver channels monitoring ten non-overlapping frequency bands with
non-uniform signal activity. For this setting, it is shown that the Q-learning
algorithm used has a significantly higher detection rate than the heuristic
approach at the expense of a smaller exploration rate. In particular, the
Q-learning approach can be parameterized to allow for a suitable trade-off
between detection and exploration rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braun_T/0/1/0/all/0/1&quot;&gt;Tobias Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korzyzkowske_T/0/1/0/all/0/1&quot;&gt;Tobias Korzyzkowske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Putzar_L/0/1/0/all/0/1&quot;&gt;Larissa Putzar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mietzner_J/0/1/0/all/0/1&quot;&gt;Jan Mietzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoeher_P/0/1/0/all/0/1&quot;&gt;Peter A. Hoeher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05772">
<title>Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05772</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is increasingly deployed in safety-critical domains where
robustness against adversarial attacks is crucial and erroneous predictions
could lead to potentially catastrophic consequences. This highlights the need
for learning systems to be equipped with the means to determine a model&apos;s
confidence in its prediction and the epistemic uncertainty associated with it,
&apos;to know when a model does not know&apos;. In this paper, we propose a novel
Random-Set Convolutional Neural Network (RS-CNN) for classification which
predicts belief functions rather than probability vectors over the set of
classes, using the mathematics of random sets, i.e., distributions over the
power set of the sample space. Based on the epistemic deep learning approach,
random-set models are capable of representing the &apos;epistemic&apos; uncertainty
induced in machine learning by limited training sets. We estimate epistemic
uncertainty by approximating the size of credal sets associated with the
predicted belief functions, and experimentally demonstrate how our approach
outperforms competing uncertainty-aware approaches in a classical evaluation
setting. The performance of RS-CNN is best demonstrated on OOD samples where it
manages to capture the true prediction while standard CNNs fail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchingal_S/0/1/0/all/0/1&quot;&gt;Shireen Kudukkil Manchingal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mubashar_M/0/1/0/all/0/1&quot;&gt;Muhammad Mubashar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaizheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariatmadar_K/0/1/0/all/0/1&quot;&gt;Keivan Shariatmadar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05775">
<title>Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of the WL Test. (arXiv:2307.05775v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05775</link>
<description rdf:parseType="Literal">&lt;p&gt;The expressive power of graph neural networks is usually measured by
comparing how many pairs of graphs or nodes an architecture can possibly
distinguish as non-isomorphic to those distinguishable by the $k$-dimensional
Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments
between practitioners&apos; conceptualizations of expressive power and $k$-WL
through a systematic analysis of the reliability and validity of $k$-WL. We
further conduct a survey ($n = 18$) of practitioners to surface their
conceptualizations of expressive power and their assumptions about $k$-WL. In
contrast to practitioners&apos; opinions, our analysis (which draws from graph
theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry,
can be irrelevant to real-world graph tasks, and may not promote generalization
or trustworthiness. We argue for extensional definitions and measurement of
expressive power based on benchmarks; we further contribute guiding questions
for constructing such benchmarks, which is critical for progress in graph
machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1&quot;&gt;Arjun Subramonian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Adina Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1&quot;&gt;Maximilian Nickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1&quot;&gt;Levent Sagun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05785">
<title>Making the Nystr\&quot;om method highly accurate for low-rank approximations. (arXiv:2307.05785v1 [math.NA])</title>
<link>http://arxiv.org/abs/2307.05785</link>
<description rdf:parseType="Literal">&lt;p&gt;The Nystr\&quot;om method is a convenient heuristic method to obtain low-rank
approximations to kernel matrices in nearly linear complexity. Existing studies
typically use the method to approximate positive semidefinite matrices with low
or modest accuracies. In this work, we propose a series of heuristic strategies
to make the Nystr\&quot;om method reach high accuracies for nonsymmetric and/or
rectangular matrices. The resulting methods (called high-accuracy Nystr\&quot;om
methods) treat the Nystr\&quot;om method and a skinny rank-revealing factorization
as a fast pivoting strategy in a progressive alternating direction refinement
process. Two refinement mechanisms are used: alternating the row and column
pivoting starting from a small set of randomly chosen columns, and adaptively
increasing the number of samples until a desired rank or accuracy is reached. A
fast subset update strategy based on the progressive sampling of Schur
complements is further proposed to accelerate the refinement process. Efficient
randomized accuracy control is also provided. Relevant accuracy and singular
value analysis is given to support some of the heuristics. Extensive tests with
various kernel functions and data sets show how the methods can quickly reach
prespecified high accuracies in practice, sometimes with quality close to SVDs,
using only small numbers of progressive sampling steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jianlin Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05789">
<title>Implicit regularisation in stochastic gradient descent: from single-objective to two-player games. (arXiv:2307.05789v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.05789</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen many insights on deep learning optimisation being
brought forward by finding implicit regularisation effects of commonly used
gradient-based optimisers. Understanding implicit regularisation can not only
shed light on optimisation dynamics, but it can also be used to improve
performance and stability across problem domains, from supervised learning to
two-player games such as Generative Adversarial Networks. An avenue for finding
such implicit regularisation effects has been quantifying the discretisation
errors of discrete optimisers via continuous-time flows constructed by backward
error analysis (BEA). The current usage of BEA is not without limitations,
since not all the vector fields of continuous-time flows obtained using BEA can
be written as a gradient, hindering the construction of modified losses
revealing implicit regularisers. In this work, we provide a novel approach to
use BEA, and show how our approach can be used to construct continuous-time
flows with vector fields that can be written as gradients. We then use this to
find previously unknown implicit regularisation effects, such as those induced
by multiple stochastic gradient descent steps while accounting for the exact
data batches used in the updates, and in generally differentiable two-player
games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1&quot;&gt;Mihaela Rosca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1&quot;&gt;Marc Peter Deisenroth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05794">
<title>Machine Learning Study of the Extended Drug-target Interaction Network informed by Pain Related Voltage-Gated Sodium Channels. (arXiv:2307.05794v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2307.05794</link>
<description rdf:parseType="Literal">&lt;p&gt;Pain is a significant global health issue, and the current treatment options
for pain management have limitations in terms of effectiveness, side effects,
and potential for addiction. There is a pressing need for improved pain
treatments and the development of new drugs. Voltage-gated sodium channels,
particularly Nav1.3, Nav1.7, Nav1.8, and Nav1.9, play a crucial role in
neuronal excitability and are predominantly expressed in the peripheral nervous
system. Targeting these channels may provide a means to treat pain while
minimizing central and cardiac adverse effects. In this study, we construct
protein-protein interaction (PPI) networks based on pain-related sodium
channels and develop a corresponding drug-target interaction (DTI) network to
identify potential lead compounds for pain management. To ensure reliable
machine learning predictions, we carefully select 111 inhibitor datasets from a
pool of over 1,000 targets in the PPI network. We employ three distinct machine
learning algorithms combined with advanced natural language processing
(NLP)-based embeddings, specifically pre-trained transformer and autoencoder
representations. Through a systematic screening process, we evaluate the side
effects and repurposing potential of over 150,000 drug candidates targeting
Nav1.7 and Nav1.8 sodium channels. Additionally, we assess the ADMET
(absorption, distribution, metabolism, excretion, and toxicity) properties of
these candidates to identify leads with near-optimal characteristics. Our
strategy provides an innovative platform for the pharmacological development of
pain treatments, offering the potential for improved efficacy and reduced side
effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dou_B/0/1/0/all/0/1&quot;&gt;Bozheng Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hongsong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yueying Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bengong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianshou Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wei_G/0/1/0/all/0/1&quot;&gt;Guo-Wei Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05801">
<title>Differentiable Forward Projector for X-ray Computed Tomography. (arXiv:2307.05801v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05801</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven deep learning has been successfully applied to various computed
tomographic reconstruction problems. The deep inference models may outperform
existing analytical and iterative algorithms, especially in ill-posed CT
reconstruction. However, those methods often predict images that do not agree
with the measured projection data. This paper presents an accurate
differentiable forward and back projection software library to ensure the
consistency between the predicted images and the original measurements. The
software library efficiently supports various projection geometry types while
minimizing the GPU memory footprint requirement, which facilitates seamless
integration with existing deep learning training and inference pipelines. The
proposed software is available as open source: https://github.com/LLNL/LEAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyojin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Champley_K/0/1/0/all/0/1&quot;&gt;Kyle Champley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05812">
<title>Safe Reinforcement Learning for Strategic Bidding of Virtual Power Plants in Day-Ahead Markets. (arXiv:2307.05812v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.05812</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel safe reinforcement learning algorithm for
strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity
markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient
(DDPG) method to learn competitive bidding policies without requiring an
accurate market model. Furthermore, to account for the complex internal
physical constraints of VPPs we introduce two enhancements to the DDPG method.
Firstly, a projection-based safety shield that restricts the agent&apos;s actions to
the feasible space defined by the non-linear power flow equations and operating
constraints of distributed energy resources is derived. Secondly, a penalty for
the shield activation in the reward function that incentivizes the agent to
learn a safer policy is introduced. A case study based on the IEEE 13-bus
network demonstrates the effectiveness of the proposed approach in enabling the
agent to learn a highly competitive, safe strategic policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stanojev_O/0/1/0/all/0/1&quot;&gt;Ognjen Stanojev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitridati_L/0/1/0/all/0/1&quot;&gt;Lesia Mitridati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prata_R/0/1/0/all/0/1&quot;&gt;Riccardo de Nardis di Prata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hug_G/0/1/0/all/0/1&quot;&gt;Gabriela Hug&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05825">
<title>Bayesian taut splines for estimating the number of modes. (arXiv:2307.05825v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2307.05825</link>
<description rdf:parseType="Literal">&lt;p&gt;The number of modes in a probability density function is representative of
the model&apos;s complexity and can also be viewed as the number of existing
subpopulations. Despite its relevance, little research has been devoted to its
estimation. Focusing on the univariate setting, we propose a novel approach
targeting prediction accuracy inspired by some overlooked aspects of the
problem. We argue for the need for structure in the solutions, the subjective
and uncertain nature of modes, and the convenience of a holistic view blending
global and local density properties. Our method builds upon a combination of
flexible kernel estimators and parsimonious compositional splines. Feature
exploration, model selection and mode testing are implemented in the Bayesian
inference paradigm, providing soft solutions and allowing to incorporate expert
judgement in the process. The usefulness of our proposal is illustrated through
a case study in sports analytics, showcasing multiple companion visualisation
tools. A thorough simulation study demonstrates that traditional
modality-driven approaches paradoxically struggle to provide accurate results.
In this context, our method emerges as a top-tier alternative offering
innovative solutions for analysts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chacon_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; E. Chac&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Serrano_J/0/1/0/all/0/1&quot;&gt;Javier Fern&amp;#xe1;ndez Serrano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05827">
<title>Relational Extraction on Wikipedia Tables using Convolutional and Memory Networks. (arXiv:2307.05827v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05827</link>
<description rdf:parseType="Literal">&lt;p&gt;Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_A/0/1/0/all/0/1&quot;&gt;Arif Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1&quot;&gt;Rohan Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbosa_D/0/1/0/all/0/1&quot;&gt;Denilson Barbosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05831">
<title>Memorization Through the Lens of Curvature of Loss Function Around Samples. (arXiv:2307.05831v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05831</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are overparametrized and easily overfit the datasets they
train on. In the extreme case, it is shown that they can memorize a training
set with fully randomized labels. We propose using the curvature of loss
function around the training sample as a measure of its memorization, averaged
over all training epochs. We use this to study the generalization versus
memorization properties of different samples in popular image datasets. We
visualize samples with the highest curvature of loss around them, and show that
these visually correspond to long-tailed, mislabeled or conflicting samples.
This analysis helps us find a, to the best of our knowledge, novel failure
model on the CIFAR100 dataset, that of duplicated images with different labels.
We also synthetically mislabel a proportion of the dataset by randomly
corrupting the labels of a few samples, and show that sorting by curvature
yields high AUROC values for identifying the mislabeled samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_I/0/1/0/all/0/1&quot;&gt;Isha Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1&quot;&gt;Kaushik Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05834">
<title>Scaling Distributed Multi-task Reinforcement Learning with Experience Sharing. (arXiv:2307.05834v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05834</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, DARPA launched the ShELL program, which aims to explore how
experience sharing can benefit distributed lifelong learning agents in adapting
to new challenges. In this paper, we address this issue by conducting both
theoretical and empirical research on distributed multi-task reinforcement
learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks
without prior knowledge of their identities. We approach the problem by
formulating it as linearly parameterized contextual Markov decision processes
(MDPs), where each task is represented by a context that specifies the
transition dynamics and rewards. To tackle this problem, we propose an
algorithm called DistMT-LSVI. First, the agents identify the tasks, and then
they exchange information through a central server to derive $\epsilon$-optimal
policies for the tasks. Our research demonstrates that to achieve
$\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI
needs to run a total number of episodes that is at most
$\tilde{\mathcal{O}}({d^3H^6(\epsilon^{-2}+c_{\rm sep}^{-2})}\cdot M/N)$, where
$c_{\rm sep}&amp;gt;0$ is a constant representing task separability, $H$ is the
horizon of each episode, and $d$ is the feature dimension of the dynamics and
rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed
settings by a factor of $1/N$, as each agent independently learns
$\epsilon$-optimal policies for all $M$ tasks using
$\tilde{\mathcal{O}}(d^3H^6M\epsilon^{-2})$ episodes. Additionally, we provide
numerical experiments conducted on OpenAI Gym Atari environments that validate
our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amani_S/0/1/0/all/0/1&quot;&gt;Sanae Amani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pahwa_K/0/1/0/all/0/1&quot;&gt;Khushbu Pahwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1&quot;&gt;Vladimir Braverman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin F. Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05857">
<title>FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for Human-in-the-Loop Systems. (arXiv:2307.05857v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05857</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application&apos;s utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taherisadr_M/0/1/0/all/0/1&quot;&gt;Mojtaba Taherisadr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmalaki_S/0/1/0/all/0/1&quot;&gt;Salma Elmalaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05862">
<title>Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes. (arXiv:2307.05862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05862</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning is traditionally studied at the model level: researchers
measure and improve the accuracy, robustness, bias, efficiency, and other
dimensions of specific models. In practice, the societal impact of machine
learning is determined by the surrounding context of machine learning
deployments. To capture this, we introduce ecosystem-level analysis: rather
than analyzing a single model, we consider the collection of models that are
deployed in a given context. For example, ecosystem-level analysis in hiring
recognizes that a job candidate&apos;s outcomes are not only determined by a single
hiring algorithm or firm but instead by the collective decisions of all the
firms they applied to. Across three modalities (text, images, speech) and 11
datasets, we establish a clear trend: deployed machine learning is prone to
systemic failure, meaning some users are exclusively misclassified by all
models available. Even when individual models improve at the population level
over time, we find these improvements rarely reduce the prevalence of systemic
failure. Instead, the benefits of these improvements predominantly accrue to
individuals who are already correctly classified by other models. In light of
these trends, we consider medical imaging for dermatology where the costs of
systemic failure are especially high. While traditional analyses reveal racial
performance disparities for both models and humans, ecosystem-level analysis
reveals new forms of racial disparity in model predictions that do not present
in human predictions. These examples demonstrate ecosystem-level analysis has
unique strengths for characterizing the societal impact of machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toups_C/0/1/0/all/0/1&quot;&gt;Connor Toups&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bommasani_R/0/1/0/all/0/1&quot;&gt;Rishi Bommasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creel_K/0/1/0/all/0/1&quot;&gt;Kathleen A. Creel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bana_S/0/1/0/all/0/1&quot;&gt;Sarah H. Bana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1&quot;&gt;Dan Jurafsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05881">
<title>Dynamic Prediction using Time-Dependent Cox Survival Neural Network. (arXiv:2307.05881v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.05881</link>
<description rdf:parseType="Literal">&lt;p&gt;The target of dynamic prediction is to provide individualized risk
predictions over time which can be updated as new data become available.
Motivated by establishing a dynamic prediction model for the progressive eye
disease, age-related macular degeneration (AMD), we proposed a time-dependent
Cox model-based survival neural network (tdCoxSNN) to predict its progression
on a continuous time scale using longitudinal fundus images. tdCoxSNN extends
the time-dependent Cox model by utilizing a neural network to model the
non-linear effect of the time-dependent covariates on the survival outcome.
Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN
can take the longitudinal raw images as input. We evaluate and compare our
proposed method with joint modeling and landmarking approaches through
comprehensive simulations using two time-dependent accuracy metrics, the Brier
Score and dynamic AUC. We applied the proposed approach to two real datasets.
One is a large AMD study, the Age-Related Eye Disease Study (AREDS), in which
more than 50,000 fundus images were captured over a period of 12 years for more
than 4,000 participants. Another is a public dataset of the primary biliary
cirrhosis (PBC) disease, in which multiple lab tests were longitudinally
collected to predict the time-to-liver transplant. Our approach achieves
satisfactory prediction performance in both simulation studies and the two real
data analyses. tdCoxSNN was implemented in PyTorch, Tensorflow, and
R-Tensorflow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Lang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Ying Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05888">
<title>Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud Computing Environment. (arXiv:2307.05888v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05888</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to
empower various areas as a bridge between physical objects and the digital
world. Through virtualization and simulation techniques, multiple functions can
be achieved by leveraging computing resources. In this process, Mobile Cloud
Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key
factors to achieve real-time feedback. However, current works only considered
edge servers or cloud servers in the DT system models. Besides, The models
ignore the DT with not only one data resource. In this paper, we propose a new
DT system model considering a heterogeneous MEC/MCC environment. Each DT in the
model is maintained in one of the servers via multiple data collection devices.
The offloading decision-making problem is also considered and a new offloading
scheme is proposed based on Distributed Deep Learning (DDL). Simulation results
demonstrate that our proposed algorithm can effectively and efficiently
decrease the system&apos;s average latency and energy consumption. Significant
improvement is achieved compared with the baselines under the dynamic
environment of DTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuling Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1&quot;&gt;Guangzhi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1&quot;&gt;Pan Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05891">
<title>PID-Inspired Inductive Biases for Deep Reinforcement Learning in Partially Observable Control Tasks. (arXiv:2307.05891v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05891</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller&apos;s success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of high dimensional control tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Char_I/0/1/0/all/0/1&quot;&gt;Ian Char&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05893">
<title>Deep Unrolling for Nonconvex Robust Principal Component Analysis. (arXiv:2307.05893v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.05893</link>
<description rdf:parseType="Literal">&lt;p&gt;We design algorithms for Robust Principal Component Analysis (RPCA) which
consists in decomposing a matrix into the sum of a low rank matrix and a sparse
matrix. We propose a deep unrolled algorithm based on an accelerated
alternating projection algorithm which aims to solve RPCA in its nonconvex
form. The proposed procedure combines benefits of deep neural networks and the
interpretability of the original algorithm and it automatically learns
hyperparameters. We demonstrate the unrolled algorithm&apos;s effectiveness on
synthetic datasets and also on a face modeling problem, where it leads to both
better numerical and visual performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_E/0/1/0/all/0/1&quot;&gt;Elizabeth Z. C. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chaux_C/0/1/0/all/0/1&quot;&gt;Caroline Chaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soubies_E/0/1/0/all/0/1&quot;&gt;Emmanuel Soubies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05902">
<title>Stability Guarantees for Feature Attributions with Multiplicative Smoothing. (arXiv:2307.05902v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05902</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanation methods for machine learning models tend to not provide any
formal guarantees and may not reflect the underlying decision-making process.
In this work, we analyze stability as a property for reliable feature
attribution methods. We prove that relaxed variants of stability are guaranteed
if the model is sufficiently Lipschitz with respect to the masking of features.
To achieve such a model, we develop a smoothing method called Multiplicative
Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard
smoothing techniques and can be integrated with any classifier and feature
attribution method. We evaluate MuS on vision and language models with a
variety of feature attribution methods, such as LIME and SHAP, and demonstrate
that MuS endows feature attributions with non-trivial stability guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_A/0/1/0/all/0/1&quot;&gt;Anton Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1&quot;&gt;Rajeev Alur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05906">
<title>Mini-Batch Optimization of Contrastive Loss. (arXiv:2307.05906v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05906</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has gained significant attention as a method for
self-supervised learning. The contrastive loss function ensures that embeddings
of positive sample pairs (e.g., different samples from the same class or
different views of the same object) are similar, while embeddings of negative
pairs are dissimilar. Practical constraints such as large memory requirements
make it challenging to consider all possible positive and negative pairs,
leading to the use of mini-batch optimization. In this paper, we investigate
the theoretical aspects of mini-batch optimization in contrastive learning. We
show that mini-batch optimization is equivalent to full-batch optimization if
and only if all $\binom{N}{B}$ mini-batches are selected, while sub-optimality
may arise when examining only a subset. We then demonstrate that utilizing
high-loss mini-batches can speed up SGD convergence and propose a spectral
clustering-based approach for identifying these high-loss mini-batches. Our
experimental results validate our theoretical findings and demonstrate that our
proposed algorithm outperforms vanilla SGD in practically relevant settings,
providing a better understanding of mini-batch optimization in contrastive
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1&quot;&gt;Kartik Sreenivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Keon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mun_K/0/1/0/all/0/1&quot;&gt;Kyunghoo Mun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Soheun Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jeong-Gwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1&quot;&gt;Anna Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1&quot;&gt;Jy-yong Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kangwook Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05908">
<title>Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding. (arXiv:2307.05908v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05908</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents &quot;Predictive Pipelined Decoding (PPD),&quot; an approach that
speeds up greedy decoding in Large Language Models (LLMs) while maintaining the
exact same output as the original decoding. Unlike conventional strategies, PPD
employs additional compute resources to parallelize the initiation of
subsequent token decoding during the current token decoding. This innovative
method reduces decoding latency and reshapes the understanding of trade-offs in
LLM decoding strategies. We have developed a theoretical framework that allows
us to analyze the trade-off between computation and latency. Using this
framework, we can analytically estimate the potential reduction in latency
associated with our proposed method, achieved through the assessment of the
match rate, represented as p_correct. The results demonstrate that the use of
extra computational resources has the potential to accelerate LLM greedy
decoding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Seongjun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gibbeum Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1&quot;&gt;Dimitris Papailiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kangwook Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05911">
<title>Grain and Grain Boundary Segmentation using Machine Learning with Real and Generated Datasets. (arXiv:2307.05911v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2307.05911</link>
<description rdf:parseType="Literal">&lt;p&gt;We report significantly improved accuracy of grain boundary segmentation
using Convolutional Neural Networks (CNN) trained on a combination of real and
generated data. Manual segmentation is accurate but time-consuming, and
existing computational methods are faster but often inaccurate. To combat this
dilemma, machine learning models can be used to achieve the accuracy of manual
segmentation and have the efficiency of a computational method. An extensive
dataset of from 316L stainless steel samples is additively manufactured,
prepared, polished, etched, and then microstructure grain images were
systematically collected. Grain segmentation via existing computational methods
and manual (by-hand) were conducted, to create &quot;real&quot; training data. A Voronoi
tessellation pattern combined with random synthetic noise and simulated
defects, is developed to create a novel artificial grain image fabrication
method. This provided training data supplementation for data-intensive machine
learning methods. The accuracy of the grain measurements from microstructure
images segmented via computational methods and machine learning methods
proposed in this work are calculated and compared to provide much benchmarks in
grain segmentation. Over 400 images of the microstructure of stainless steel
samples were manually segmented for machine learning training applications.
This data and the artificial data is available on Kaggle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Warren_P/0/1/0/all/0/1&quot;&gt;Peter Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Raju_N/0/1/0/all/0/1&quot;&gt;Nandhini Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Prasad_A/0/1/0/all/0/1&quot;&gt;Abhilash Prasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hossain_S/0/1/0/all/0/1&quot;&gt;Shajahan Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Subramanian_R/0/1/0/all/0/1&quot;&gt;Ramesh Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kapat_J/0/1/0/all/0/1&quot;&gt;Jayanta Kapat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Manjooran_N/0/1/0/all/0/1&quot;&gt;Navin Manjooran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ghosh_R/0/1/0/all/0/1&quot;&gt;Ranajay Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05914">
<title>FIS-ONE: Floor Identification System with One Label for Crowdsourced RF Signals. (arXiv:2307.05914v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.05914</link>
<description rdf:parseType="Literal">&lt;p&gt;Floor labels of crowdsourced RF signals are crucial for many smart-city
applications, such as multi-floor indoor localization, geofencing, and robot
surveillance. To build a prediction model to identify the floor number of a new
RF signal upon its measurement, conventional approaches using the crowdsourced
RF signals assume that at least few labeled signal samples are available on
each floor. In this work, we push the envelope further and demonstrate that it
is technically feasible to enable such floor identification with only one
floor-labeled signal sample on the bottom floor while having the rest of signal
samples unlabeled.
&lt;/p&gt;
&lt;p&gt;We propose FIS-ONE, a novel floor identification system with only one labeled
sample. FIS-ONE consists of two steps, namely signal clustering and cluster
indexing. We first build a bipartite graph to model the RF signal samples and
obtain a latent representation of each node (each signal sample) using our
attention-based graph neural network model so that the RF signal samples can be
clustered more accurately. Then, we tackle the problem of indexing the clusters
with proper floor labels, by leveraging the observation that signals from an
access point can be detected on different floors, i.e., signal spillover.
Specifically, we formulate a cluster indexing problem as a combinatorial
optimization problem and show that it is equivalent to solving a traveling
salesman problem, whose (near-)optimal solution can be found efficiently. We
have implemented FIS-ONE and validated its effectiveness on the Microsoft
dataset and in three large shopping malls. Our results show that FIS-ONE
outperforms other baseline algorithms significantly, with up to 23% improvement
in adjusted rand index and 25% improvement in normalized mutual information
using only one floor-labeled signal sample.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1&quot;&gt;Weipeng Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_K/0/1/0/all/0/1&quot;&gt;Ka Ho Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jierun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziqi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;S.-H. Gary Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Sangtae Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chul-Ho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05915">
<title>Prompt Generate Train (PGT): A framework for few-shot domain adaptation, alignment, and uncertainty calibration of a retriever augmented generation (RAG) model for domain specific open book question-answering. (arXiv:2307.05915v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05915</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation model to the target domain using supervised
finetuning and reinforcement learning with synthetic feedback in a few-shot
setting. This yields an aligned, uncertainty calibrated model that is
competitive with GPT-4 based in-context retrieval augmented generation in
generating relevant answers at lower serving costs. The synthetic generation
pipeline generates high quality synthetic training data musing a medium sized
LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is
designed to generate both abstractive and extractive questions that span the
entire corpus. Using samples from this dataset, the framework fine-tunes a
smaller RAG model comprising a dense retriever and a smaller sized LLM on
samples from the dataset. In parallel, the framework trains a Reward model to
score domain grounded answers higher than hallucinated answers. In the next
phase, the framework aligns to the RAG model with the target domain using
reinforcement learning. This step improves the RAG model&apos;s ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework calibrates the model uncertainty for extractive question-answers.
This is a desirable feature since the model can be integrated into a cascading
system where the RAG model&apos;s answer is surfaced only when the model is
confident of its answer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_C/0/1/0/all/0/1&quot;&gt;C. S. Krishna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05920">
<title>Unified Medical Image-Text-Label Contrastive Learning With Continuous Prompt. (arXiv:2307.05920v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.05920</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive language-image Pre-training (CLIP) [13] can leverage large
datasets of unlabeled Image-Text pairs, which have demonstrated impressive
performance in various downstream tasks. Given that annotating medical data is
time-consuming and laborious, Image-Text Pre-training has promising
applications in exploiting large-scale medical image and radiology report
datasets. However, medical Image-Text Pre-training faces several challenges, as
follows: (1) Due to privacy concerns, the amount of available medical data is
relatively small compared to natural data, leading to weaker generalization
ability of the model. (2) Medical images are highly similar with only
fine-grained differences in subtleties, resulting in a large number of
false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt
usually differs from the natural medical image report, Subtle changes in
wording can lead to significant differences in performance. In this paper, we
propose a unified Image-Text-Label contrastive learning framework based on
continuous prompts, with three main contributions. First, We unified the data
of images, text, and labels, which greatly expanded the training data that the
model could utilize. Second, we address the issue of data diversity and the
impact of hand-crafted prompts on model performance by introducing continuous
implicit prompts. Lastly, we propose a ImageText-Label contrastive Training to
mitigate the problem of too many false-negative samples. We demonstrate through
sufficient experiments that the Unified Medical Contrastive Learning (UMCL)
framework exhibits excellent performance on several downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05926">
<title>Filling time-series gaps using image techniques: Multidimensional context autoencoder approach for building energy data imputation. (arXiv:2307.05926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05926</link>
<description rdf:parseType="Literal">&lt;p&gt;Building energy prediction and management has become increasingly important
in recent decades, driven by the growth of Internet of Things (IoT) devices and
the availability of more energy data. However, energy data is often collected
from multiple sources and can be incomplete or inconsistent, which can hinder
accurate predictions and management of energy systems and limit the usefulness
of the data for decision-making and research. To address this issue, past
studies have focused on imputing missing gaps in energy data, including random
and continuous gaps. One of the main challenges in this area is the lack of
validation on a benchmark dataset with various building and meter types, making
it difficult to accurately evaluate the performance of different imputation
methods. Another challenge is the lack of application of state-of-the-art
imputation methods for missing gaps in energy data. Contemporary
image-inpainting methods, such as Partial Convolution (PConv), have been widely
used in the computer vision domain and have demonstrated their effectiveness in
dealing with complex missing patterns. To study whether energy data imputation
can benefit from the image-based deep learning method, this study compared
PConv, Convolutional neural networks (CNNs), and weekly persistence method
using one of the biggest publicly available whole building energy datasets,
consisting of 1479 power meters worldwide, as the benchmark. The results show
that, compared to the CNN with the raw time series (1D-CNN) and the weekly
persistence method, neural network models with reshaped energy data with two
dimensions reduced the Mean Squared Error (MSE) by 10% to 30%. The advanced
deep learning method, Partial convolution (PConv), has further reduced the MSE
by 20-30% than 2D-CNN and stands out among all models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chun Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quintana_M/0/1/0/all/0/1&quot;&gt;Matias Quintana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagy_Z/0/1/0/all/0/1&quot;&gt;Zoltan Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1&quot;&gt;Clayton Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05945">
<title>YOGA: Deep Object Detection in the Wild with Lightweight Feature Learning and Multiscale Attention. (arXiv:2307.05945v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05945</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce YOGA, a deep learning based yet lightweight object detection
model that can operate on low-end edge devices while still achieving
competitive accuracy. The YOGA architecture consists of a two-phase feature
learning pipeline with a cheap linear transformation, which learns feature maps
using only half of the convolution filters required by conventional
convolutional neural networks. In addition, it performs multi-scale feature
fusion in its neck using an attention mechanism instead of the naive
concatenation used by conventional detectors. YOGA is a flexible model that can
be easily scaled up or down by several orders of magnitude to fit a broad range
of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets
with other over 10 state-of-the-art object detectors. The results show that
YOGA strikes the best trade-off between model size and accuracy (up to 22%
increase of AP and 23-34% reduction of parameters and FLOPs), making it an
ideal choice for deployment in the wild on low-end edge devices. This is
further affirmed by our hardware implementation and evaluation on NVIDIA Jetson
Nano.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunkara_R/0/1/0/all/0/1&quot;&gt;Raja Sunkara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05946">
<title>A Bayesian approach to quantifying uncertainties and improving generalizability in traffic prediction models. (arXiv:2307.05946v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05946</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep-learning models for traffic data prediction can have superior
performance in modeling complex functions using a multi-layer architecture.
However, a major drawback of these approaches is that most of these approaches
do not offer forecasts with uncertainty estimates, which are essential for
traffic operations and control. Without uncertainty estimates, it is difficult
to place any level of trust to the model predictions, and operational
strategies relying on overconfident predictions can lead to worsening traffic
conditions. In this study, we propose a Bayesian recurrent neural network
framework for uncertainty quantification in traffic prediction with higher
generalizability by introducing spectral normalization to its hidden layers. In
our paper, we have shown that normalization alters the training process of deep
neural networks by controlling the model&apos;s complexity and reducing the risk of
overfitting to the training data. This, in turn, helps improve the
generalization performance of the model on out-of-distribution datasets.
Results demonstrate that spectral normalization improves uncertainty estimates
and significantly outperforms both the layer normalization and model without
normalization in single-step prediction horizons. This improved performance can
be attributed to the ability of spectral normalization to better localize the
feature space of the data under perturbations. Our findings are especially
relevant to traffic management applications, where predicting traffic
conditions across multiple locations is the goal, but the availability of
training data from multiple locations is limited. Spectral normalization,
therefore, provides a more generalizable approach that can effectively capture
the underlying patterns in traffic data without requiring location-specific
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1&quot;&gt;Sudeepta Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Adway Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1&quot;&gt;S. Ilgin Guler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05948">
<title>Diversity-enhancing Generative Network for Few-shot Hypothesis Adaptation. (arXiv:2307.05948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05948</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating unlabeled data has been recently shown to help address the
few-shot hypothesis adaptation (FHA) problem, where we aim to train a
classifier for the target domain with a few labeled target-domain data and a
well-trained source-domain classifier (i.e., a source hypothesis), for the
additional information of the highly-compatible unlabeled data. However, the
generated data of the existing methods are extremely similar or even the same.
The strong dependency among the generated data will lead the learning to fail.
In this paper, we propose a diversity-enhancing generative network (DEG-Net)
for the FHA problem, which can generate diverse unlabeled data with the help of
a kernel independence measure: the Hilbert-Schmidt independence criterion
(HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value
(i.e., maximizing the independence) among the semantic features of the
generated data. By DEG-Net, the generated unlabeled data are more diverse and
more effective for addressing the FHA problem. Experimental results show that
the DEG-Net outperforms existing FHA baselines and further verifies that
generating diverse data plays a vital role in addressing the FHA problem
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Ruijiang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1&quot;&gt;Haoang Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Mingming Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1&quot;&gt;Gang Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05949">
<title>Newell&apos;s theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05949</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models for spatio-temporal traffic flow forecasting employ
convolutional or graph-convolutional filters along with recurrent neural
networks to capture spatial and temporal dependencies in traffic data. These
models, such as CNN-LSTM, utilize traffic flows from neighboring detector
stations to predict flows at a specific location of interest. However, these
models are limited in their ability to capture the broader dynamics of the
traffic system, as they primarily learn features specific to the detector
configuration and traffic characteristics at the target location. Hence, the
transferability of these models to different locations becomes challenging,
particularly when data is unavailable at the new location for model training.
To address this limitation, we propose a traffic flow physics-based feature
transformation for spatio-temporal DL models. This transformation incorporates
Newell&apos;s uncongested and congested-state estimators of traffic flows at the
target locations, enabling the models to learn broader dynamics of the system.
Our methodology is empirically validated using traffic data from two different
locations. The results demonstrate that the proposed feature transformation
improves the models&apos; performance in predicting traffic flows over different
prediction horizons, as indicated by better goodness-of-fit statistics. An
important advantage of our framework is its ability to be transferred to new
locations where data is unavailable. This is achieved by appropriately
accounting for spatial dependencies based on station distances and various
traffic parameters. In contrast, regular DL models are not easily transferable
as their inputs remain fixed. It should be noted that due to data limitations,
we were unable to perform spatial sensitivity analysis, which calls for further
research using simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1&quot;&gt;S. Ilgin Guler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05959">
<title>Giving Robots a Hand: Learning Generalizable Manipulation with Eye-in-Hand Human Video Demonstrations. (arXiv:2307.05959v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.05959</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye-in-hand cameras have shown promise in enabling greater sample efficiency
and generalization in vision-based robotic manipulation. However, for robotic
imitation, it is still expensive to have a human teleoperator collect large
amounts of expert demonstrations with a real robot. Videos of humans performing
tasks, on the other hand, are much cheaper to collect since they eliminate the
need for expertise in robotic teleoperation and can be quickly captured in a
wide range of scenarios. Therefore, human video demonstrations are a promising
data source for learning generalizable robotic manipulation policies at scale.
In this work, we augment narrow robotic imitation datasets with broad unlabeled
human video demonstrations to greatly enhance the generalization of eye-in-hand
visuomotor policies. Although a clear visual domain gap exists between human
and robot data, our framework does not need to employ any explicit domain
adaptation method, as we leverage the partial observability of eye-in-hand
cameras as well as a simple fixed image masking scheme. On a suite of eight
real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method
improves the success rates of eye-in-hand manipulation policies by 58%
(absolute) on average, enabling robots to generalize to both new environment
configurations and new tasks that are unseen in the robot demonstration data.
See video results at https://giving-robots-a-hand.github.io/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Moo Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05972">
<title>Self-Distilled Quantization: Achieving High Compression Rates in Transformer-Based Language Models. (arXiv:2307.05972v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.05972</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neill_J/0/1/0/all/0/1&quot;&gt;James O&amp;#x27; Neill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Sourav Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05973">
<title>VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models. (arXiv:2307.05973v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.05973</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenlong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruohan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunzhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05974">
<title>Contrastive Learning for Conversion Rate Prediction. (arXiv:2307.05974v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.05974</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversion rate (CVR) prediction plays an important role in advertising
systems. Recently, supervised deep neural network-based models have shown
promising performance in CVR prediction. However, they are data hungry and
require an enormous amount of training data. In online advertising systems,
although there are millions to billions of ads, users tend to click only a
small set of them and to convert on an even smaller set. This data sparsity
issue restricts the power of these deep models. In this paper, we propose the
Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the
supervised CVR prediction task with a contrastive learning task, which can
learn better data representations exploiting abundant unlabeled data and
improve the CVR prediction performance. To tailor the contrastive learning task
to the CVR prediction problem, we propose embedding masking (EM), rather than
feature masking, to create two views of augmented samples. We also propose a
false negative elimination (FNE) component to eliminate samples with the same
feature as the anchor sample, to account for the natural property in user
behavior data. We further propose a supervised positive inclusion (SPI)
component to include additional positive samples for each anchor sample, in
order to make full use of sparse but precious user conversion events.
Experimental results on two real-world conversion datasets demonstrate the
superior performance of CL4CVR. The source code is available at
https://github.com/DongRuiHust/CL4CVR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wentao Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Rui Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiuwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chaofeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jinmei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangzheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yanlong Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05975">
<title>Outlier detection in regression: conic quadratic formulations. (arXiv:2307.05975v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.05975</link>
<description rdf:parseType="Literal">&lt;p&gt;In many applications, when building linear regression models, it is important
to account for the presence of outliers, i.e., corrupted input data points.
Such problems can be formulated as mixed-integer optimization problems
involving cubic terms, each given by the product of a binary variable and a
quadratic term of the continuous variables. Existing approaches in the
literature, typically relying on the linearization of the cubic terms using
big-M constraints, suffer from weak relaxation and poor performance in
practice. In this work we derive stronger second-order conic relaxations that
do not involve big-M constraints. Our computational experiments indicate that
the proposed formulations are several orders-of-magnitude faster than existing
big-M formulations in the literature for this problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05977">
<title>Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. (arXiv:2307.05977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.05977</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Seohyeon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Balhae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Moonseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05979">
<title>Transformers in Reinforcement Learning: A Survey. (arXiv:2307.05979v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05979</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1&quot;&gt;Pranav Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Aamer Abdul Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+St_Charles_P/0/1/0/all/0/1&quot;&gt;Pierre-Luc St-Charles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prince_S/0/1/0/all/0/1&quot;&gt;Simon J.D. Prince&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1&quot;&gt;Samira Ebrahimi Kahou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05988">
<title>A Comprehensive Review of Automated Data Annotation Techniques in Human Activity Recognition. (arXiv:2307.05988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.05988</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Activity Recognition (HAR) has become one of the leading research
topics of the last decade. As sensing technologies have matured and their
economic costs have declined, a host of novel applications, e.g., in
healthcare, industry, sports, and daily life activities have become popular.
The design of HAR systems requires different time-consuming processing steps,
such as data collection, annotation, and model training and optimization. In
particular, data annotation represents the most labor-intensive and cumbersome
step in HAR, since it requires extensive and detailed manual work from human
annotators. Therefore, different methodologies concerning the automation of the
annotation procedure in HAR have been proposed. The annotation problem occurs
in different notions and scenarios, which all require individual solutions. In
this paper, we provide the first systematic review on data annotation
techniques for HAR. By grouping existing approaches into classes and providing
a taxonomy, our goal is to support the decision on which techniques can be
beneficially used in a given scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demrozi_F/0/1/0/all/0/1&quot;&gt;Florenc Demrozi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turetta_C/0/1/0/all/0/1&quot;&gt;Cristian Turetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machot_F/0/1/0/all/0/1&quot;&gt;Fadi Al Machot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pravadelli_G/0/1/0/all/0/1&quot;&gt;Graziano Pravadelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindt_P/0/1/0/all/0/1&quot;&gt;Philipp H. Kindt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06005">
<title>DDNAS: Discretized Differentiable Neural Architecture Search for Text Classification. (arXiv:2307.06005v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06005</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) has shown promising capability in learning
text representation. However, existing text-based NAS neither performs a
learnable fusion of neural operations to optimize the architecture, nor encodes
the latent hierarchical categorization behind text input. This paper presents a
novel NAS method, Discretized Differentiable Neural Architecture Search
(DDNAS), for text representation learning and classification. With the
continuous relaxation of architecture representation, DDNAS can use gradient
descent to optimize the search. We also propose a novel discretization layer
via mutual information maximization, which is imposed on every search node to
model the latent hierarchical categorization in text representation. Extensive
experiments conducted on eight diverse real datasets exhibit that DDNAS can
consistently outperform the state-of-the-art NAS methods. While DDNAS relies on
only three basic operations, i.e., convolution, pooling, and none, to be the
candidates of NAS building blocks, its promising performance is noticeable and
extensible to obtain further improvement by adding more different operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kuan-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Cheng-Te Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuo-Jung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06006">
<title>What Happens During Finetuning of Vision Transformers: An Invariance Based Investigation. (arXiv:2307.06006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06006</link>
<description rdf:parseType="Literal">&lt;p&gt;The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merlin_G/0/1/0/all/0/1&quot;&gt;Gabriele Merlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_V/0/1/0/all/0/1&quot;&gt;Vedant Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_R/0/1/0/all/0/1&quot;&gt;Ruchit Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1&quot;&gt;Mariya Toneva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06013">
<title>An Effective and Efficient Time-aware Entity Alignment Framework via Two-aspect Three-view Label Propagation. (arXiv:2307.06013v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06013</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity alignment (EA) aims to find the equivalent entity pairs between
different knowledge graphs (KGs), which is crucial to promote knowledge fusion.
With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA)
methods appear to enhance EA. Existing TEA models are based on Graph Neural
Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is
difficult to transfer them to large-scale TKGs due to the scalability issue of
GNN. In this paper, we propose an effective and efficient non-neural EA
framework between TKGs, namely LightTEA, which consists of four essential
components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity
with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative
Learning. All of these modules work together to improve the performance of EA
while reducing the time consumption of the model. Extensive experiments on
public datasets indicate that our proposed model significantly outperforms the
SOTA methods for EA between TKGs, and the time consumed by LightTEA is only
dozens of seconds at most, no more than 10% of the most efficient TEA method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Li Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xin Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Youshao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Changxu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Man Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06026">
<title>Learning from Exemplary Explanations. (arXiv:2307.06026v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06026</link>
<description rdf:parseType="Literal">&lt;p&gt;eXplanation Based Learning (XBL) is a form of Interactive Machine Learning
(IML) that provides a model refining approach via user feedback collected on
model explanations. Although the interactivity of XBL promotes model
transparency, XBL requires a huge amount of user interaction and can become
expensive as feedback is in the form of detailed annotation rather than simple
category labelling which is more common in IML. This expense is exacerbated in
high stakes domains such as medical image classification. To reduce the effort
and expense of XBL we introduce a new approach that uses two input instances
and their corresponding Gradient Weighted Class Activation Mapping (GradCAM)
model explanations as exemplary explanations to implement XBL. Using a medical
image classification task, we demonstrate that, using minimal human input, our
approach produces improved explanations (+0.02, +3%) and achieves reduced
classification performance (-0.04, -4%) when compared against a model trained
without interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06040">
<title>Rhythm Modeling for Voice Conversion. (arXiv:2307.06040v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.06040</link>
<description rdf:parseType="Literal">&lt;p&gt;Voice conversion aims to transform source speech into a different target
voice. However, typical voice conversion systems do not account for rhythm,
which is an important factor in the perception of speaker identity. To bridge
this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion
that does not require parallel data or text transcriptions. Using
self-supervised representations, we first divide source audio into segments
approximating sonorants, obstruents, and silences. Then we model rhythm by
estimating speaking rate or the duration distribution of each segment type.
Finally, we match the target speaking rate or rhythm by time-stretching the
speech segments. Experiments show that Urhythmic outperforms existing
unsupervised methods in terms of quality and prosody. Code and checkpoints:
https://github.com/bshall/urhythmic. Audio demo page:
https://ubisoft-laforge.github.io/speech/urhythmic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Niekerk_B/0/1/0/all/0/1&quot;&gt;Benjamin van Niekerk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carbonneau_M/0/1/0/all/0/1&quot;&gt;Marc-Andr&amp;#xe9; Carbonneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1&quot;&gt;Herman Kamper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06046">
<title>An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06046</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of inductive link prediction in (discrete) attributed multigraphs
infers missing attributed links (relations) between nodes in new test
multigraphs. Traditional relational learning methods face the challenge of
limited generalization to OOD test multigraphs containing both novel nodes and
novel relation types not seen in training. Recently, under the only assumption
that all relation types share the same structural predictive patterns (single
task), Gao et al. (2023) proposed an OOD link prediction method using the
theoretical concept of double exchangeability (for nodes &amp;amp; relation types), in
contrast to the (single) exchangeability (only for nodes) used to design Graph
Neural Networks (GNNs). In this work we further extend the double
exchangeability concept to multi-task double exchangeability, where we define
link prediction in attributed multigraphs that can have distinct and
potentially conflicting predictive patterns for different sets of relation
types (multiple tasks). Our empirical results on real-world datasets
demonstrate that our approach can effectively generalize to entirely new
relation types in test, without access to additional information, yielding
significant performance improvements over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jincheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1&quot;&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1&quot;&gt;Bruno Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06048">
<title>Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex Optimization. (arXiv:2307.06048v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.06048</link>
<description rdf:parseType="Literal">&lt;p&gt;We study multi-product inventory control problems where a manager makes
sequential replenishment decisions based on partial historical information in
order to minimize its cumulative losses. Our motivation is to consider general
demands, losses and dynamics to go beyond standard models which usually rely on
newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand
assumptions. We propose MaxCOSD, an online algorithm that has provable
guarantees even for problems with non-i.i.d. demands and stateful dynamics,
including for instance perishability. We consider what we call non-degeneracy
assumptions on the demand process, and argue that they are necessary to allow
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hihat_M/0/1/0/all/0/1&quot;&gt;Massil Hihat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gaiffas_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ga&amp;#xef;ffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Garrigos_G/0/1/0/all/0/1&quot;&gt;Guillaume Garrigos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bussy_S/0/1/0/all/0/1&quot;&gt;Simon Bussy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06055">
<title>Function-Space Regularization for Deep Bayesian Classification. (arXiv:2307.06055v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06055</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian deep learning approaches assume model parameters to be latent random
variables and infer posterior distributions to quantify uncertainty, increase
safety and trust, and prevent overconfident and unpredictable behavior.
However, weight-space priors are model-specific, can be difficult to interpret
and are hard to specify. Instead, we apply a Dirichlet prior in predictive
space and perform approximate function-space variational inference. To this
end, we interpret conventional categorical predictions from stochastic neural
network classifiers as samples from an implicit Dirichlet distribution. By
adapting the inference, the same function-space prior can be combined with
different models without affecting model architecture or size. We illustrate
the flexibility and efficacy of such a prior with toy experiments and
demonstrate scalability, improved uncertainty quantification and adversarial
robustness with large-scale image classification experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Andreas Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1&quot;&gt;Joe Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klink_P/0/1/0/all/0/1&quot;&gt;Pascal Klink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06060">
<title>Interpreting deep embeddings for disease progression clustering. (arXiv:2307.06060v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.06060</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel approach for interpreting deep embeddings in the context
of patient clustering. We evaluate our approach on a dataset of participants
with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful
insights into disease progression patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Munoz_Farre_A/0/1/0/all/0/1&quot;&gt;Anna Munoz-Farre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Poulakakis_Daktylidis_A/0/1/0/all/0/1&quot;&gt;Antonios Poulakakis-Daktylidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kothalawala_D/0/1/0/all/0/1&quot;&gt;Dilini Mahesha Kothalawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rodriguez_Martinez_A/0/1/0/all/0/1&quot;&gt;Andrea Rodriguez-Martinez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06092">
<title>Quantitative CLTs in Deep Neural Networks. (arXiv:2307.06092v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06092</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show, both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma&amp;gt;0,$ with the exponent
depending on the metric used to measure discrepancy. Our bounds are stronger in
terms of their dependence on network width than any previously available in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1&quot;&gt;Stefano Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinucci_D/0/1/0/all/0/1&quot;&gt;Domenico Marinucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourdin_I/0/1/0/all/0/1&quot;&gt;Ivan Nourdin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peccati_G/0/1/0/all/0/1&quot;&gt;Giovanni Peccati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06093">
<title>Online Laplace Model Selection Revisited. (arXiv:2307.06093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06093</link>
<description rdf:parseType="Literal">&lt;p&gt;The Laplace approximation provides a closed-form model selection objective
for neural networks (NN). Online variants, which optimise NN parameters jointly
with hyperparameters, like weight decay strength, have seen renewed interest in
the Bayesian deep learning community. However, these methods violate Laplace&apos;s
method&apos;s critical assumption that the approximation is performed around a mode
of the loss, calling into question their soundness. This work re-derives online
Laplace methods, showing them to target a variational bound on a mode-corrected
variant of the Laplace evidence which does not make stationarity assumptions.
Online Laplace and its mode-corrected counterpart share stationary points where
1. the NN parameters are a maximum a posteriori, satisfying the Laplace
method&apos;s assumption, and 2. the hyperparameters maximise the Laplace evidence,
motivating online methods. We demonstrate that these optima are roughly
attained in practise by online algorithms using full-batch gradient descent on
UCI regression datasets. The optimised hyperparameters prevent overfitting and
outperform validation-based early stopping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Andreas Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antoran_J/0/1/0/all/0/1&quot;&gt;Javier Antor&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06097">
<title>Learning Stochastic Dynamical Systems as an Implicit Regularization with Graph Neural Networks. (arXiv:2307.06097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06097</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gumbel graph networks are proposed to learn high-dimensional time
series, where the observed dimensions are often spatially correlated. To that
end, the observed randomness and spatial-correlations are captured by learning
the drift and diffusion terms of the stochastic differential equation with a
Gumble matrix embedding, respectively. In particular, this novel framework
enables us to investigate the implicit regularization effect of the noise terms
in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by
deriving the difference between the two corresponding loss functions in a small
neighborhood of weight. Then, we employ Kuramoto&apos;s model to generate data for
comparing the spectral density from the Hessian Matrix of the two loss
functions. Experimental results on real-world data, demonstrate that S-GGNs
exhibit superior convergence, robustness, and generalization, compared with
state-of-the-arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Ting Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yufu Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sikun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinqiao Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06104">
<title>Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06104</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in research on Deep Graph Networks (DGNs) has led to a
maturation of the domain of learning on graphs. Despite the growth of this
research field, there are still important challenges that are yet unsolved.
Specifically, there is an urge of making DGNs suitable for predictive tasks on
realworld systems of interconnected entities, which evolve over time. With the
aim of fostering research in the domain of dynamic graphs, at first, we survey
recent advantages in learning both temporal and spatial information, providing
a comprehensive overview of the current state-of-the-art in the domain of
representation learning for dynamic graphs. Secondly, we conduct a fair
performance comparison among the most popular proposed approaches, leveraging
rigorous model selection and assessment for all the methods, thus establishing
a sound baseline for evaluating new architectures and approaches
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gravina_A/0/1/0/all/0/1&quot;&gt;Alessio Gravina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1&quot;&gt;Davide Bacciu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06123">
<title>SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark. (arXiv:2307.06123v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.06123</link>
<description rdf:parseType="Literal">&lt;p&gt;Membership inference (MI) attacks threaten user privacy through determining
if a given data example has been used to train a target model. However, it has
been increasingly recognized that the &quot;comparing different MI attacks&quot;
methodology used in the existing works has serious limitations. Due to these
limitations, we found (through the experiments in this work) that some
comparison results reported in the literature are quite misleading. In this
paper, we seek to develop a comprehensive benchmark for comparing different MI
attacks, called MIBench, which consists not only the evaluation metrics, but
also the evaluation scenarios. And we design the evaluation scenarios from four
perspectives: the distance distribution of data samples in the target dataset,
the distance between data samples of the target dataset, the differential
distance between two datasets (i.e., the target dataset and a generated dataset
with only nonmembers), and the ratio of the samples that are made no inferences
by an MI attack. The evaluation metrics consist of ten typical evaluation
metrics. We have identified three principles for the proposed &quot;comparing
different MI attacks&quot; methodology, and we have designed and implemented the
MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we
have used our benchmark to fairly and systematically compare 15
state-of-the-art MI attack algorithms across 588 evaluation scenarios, and
these evaluation scenarios cover 7 widely used datasets and 7 representative
types of models. All codes and evaluations of MIBench are publicly available at
https://github.com/MIBench/MIBench.github.io/blob/main/README.md.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1&quot;&gt;Jun Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Moxuan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chunhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yangming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Suyu An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yangzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xinghui Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhipeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Weihao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1&quot;&gt;Kuo Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yulong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaohong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianfeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06125">
<title>Learning Hierarchical Interactive Multi-Object Search for Mobile Manipulation. (arXiv:2307.06125v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06125</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real-world that demonstrate that HIMOS
effectively transfers to new environments in a zero-shot manner. It shows
robustness to unseen subpolicies, failures in their execution, and different
robot kinematics. These capabilities open the door to a wide range of
downstream tasks across embodied AI and real-world use cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalstieg_F/0/1/0/all/0/1&quot;&gt;Fabian Schmalstieg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honerkamp_D/0/1/0/all/0/1&quot;&gt;Daniel Honerkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welschehold_T/0/1/0/all/0/1&quot;&gt;Tim Welschehold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1&quot;&gt;Abhinav Valada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06148">
<title>NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06148</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have triggered tremendous success to empower
daily life by generative information, and the personalization of LLMs could
further contribute to their applications due to better alignment with human
intents. Towards personalized generative services, a collaborative cloud-edge
methodology sounds promising, as it facilitates the effective orchestration of
heterogeneous distributed communication and computing resources. In this
article, after discussing the pros and cons of several candidate cloud-edge
collaboration techniques, we put forward NetGPT to capably deploy appropriate
LLMs at the edge and the cloud in accordance with their computing capacity. In
addition, edge LLMs could efficiently leverage location-based information for
personalized prompt completion, thus benefiting the interaction with cloud
LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and
LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on
the basis of low-rank adaptation-based light-weight fine-tuning. Subsequently,
we highlight substantial essential changes required for a native artificial
intelligence (AI) network architecture towards NetGPT, with special emphasis on
deeper integration of communications and computing resources and careful
calibration of logical AI workflow. Furthermore, we demonstrate several
by-product benefits of NetGPT, given edge LLM&apos;s astonishing capability to
predict trends and infer intents, which possibly leads to a unified solution
for intelligent network management \&amp;amp; orchestration. In a nutshell, we argue
that NetGPT is a promising native-AI network architecture beyond provisioning
personalized generative services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chenghui Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1&quot;&gt;Ekram Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honggang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06152">
<title>Maneuver Decision-Making Through Automatic Curriculum Reinforcement Learning Without Handcrafted Reward functions. (arXiv:2307.06152v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06152</link>
<description rdf:parseType="Literal">&lt;p&gt;Maneuver decision-making is the core of unmanned combat aerial vehicle for
autonomous air combat. To solve this problem, we propose an automatic
curriculum reinforcement learning method, which enables agents to learn
effective decisions in air combat from scratch. The range of initial states are
used for distinguishing curricula of different difficulty levels, thereby
maneuver decision is divided into a series of sub-tasks from easy to difficult,
and test results are used to change sub-tasks. As sub-tasks change, agents
gradually learn to complete a series of sub-tasks from easy to difficult,
enabling them to make effective maneuvering decisions to cope with various
states without the need to spend effort designing reward functions. The
ablation studied show that the automatic curriculum learning proposed in this
article is an essential component for training through reinforcement learning,
namely, agents cannot complete effective decisions without curriculum learning.
Simulation experiments show that, after training, agents are able to make
effective decisions given different states, including tracking, attacking and
escaping, which are both rational and interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Peng_Z/0/1/0/all/0/1&quot;&gt;Zhang Hong-Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06162">
<title>Deep Generative Models for Physiological Signals: A Systematic Literature Review. (arXiv:2307.06162v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram,
electroencephalogram, photoplethysmogram and electromyogram. Compared to the
existing review papers, we present the first review that summarizes the recent
state-of-the-art deep generative models. By analysing the state-of-the-art
research related to deep generative models along with their main applications
and challenges, this review contributes to the overall understanding of these
models applied to physiological signals. Additionally, by highlighting the
employed evaluation protocol and the most used physiological databases, this
review facilitates the assessment and benchmarking of deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neifar_N/0/1/0/all/0/1&quot;&gt;Nour Neifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mdhaffar_A/0/1/0/all/0/1&quot;&gt;Afef Mdhaffar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Hamadou_A/0/1/0/all/0/1&quot;&gt;Achraf Ben-Hamadou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jmaiel_M/0/1/0/all/0/1&quot;&gt;Mohamed Jmaiel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06167">
<title>Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based Partial Differential Equations Solving. (arXiv:2307.06167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06167</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs) have emerged as promising surrogate
modes for solving partial differential equations (PDEs). Their effectiveness
lies in the ability to capture solution-related features through neural
networks. However, original PINNs often suffer from bottlenecks, such as low
accuracy and non-convergence, limiting their applicability in complex physical
contexts. To alleviate these issues, we proposed auxiliary-task learning-based
physics-informed neural networks (ATL-PINNs), which provide four different
auxiliary-task learning modes and investigate their performance compared with
original PINNs. We also employ the gradient cosine similarity algorithm to
integrate auxiliary problem loss with the primary problem loss in ATL-PINNs,
which aims to enhance the effectiveness of the auxiliary-task learning modes.
To the best of our knowledge, this is the first study to introduce
auxiliary-task learning modes in the context of physics-informed learning. We
conduct experiments on three PDE problems across different fields and
scenarios. Our findings demonstrate that the proposed auxiliary-task learning
modes can significantly improve solution accuracy, achieving a maximum
performance boost of 96.62% (averaging 28.23%) compared to the original
single-task PINNs. The code and dataset are open source at
https://github.com/junjun-yan/ATL-PINN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junjun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinhai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enqiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06175">
<title>Learning Decentralized Partially Observable Mean Field Control for Artificial Collective Behavior. (arXiv:2307.06175v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06175</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent reinforcement learning (RL) methods have achieved success in various
domains. However, multi-agent RL (MARL) remains a challenge in terms of
decentralization, partial observability and scalability to many agents.
Meanwhile, collective behavior requires resolution of the aforementioned
challenges, and remains of importance to many state-of-the-art applications
such as active matter physics, self-organizing systems, opinion dynamics, and
biological or robotic swarms. Here, MARL via mean field control (MFC) offers a
potential solution to scalability, but fails to consider decentralized and
partially observable systems. In this paper, we enable decentralized behavior
of agents under partial information by proposing novel models for decentralized
partially observable MFC (Dec-POMFC), a broad class of problems with
permutation-invariant agents allowing for reduction to tractable single-agent
Markov decision processes (MDP) with single-agent RL solution. We provide
rigorous theoretical results, including a dynamic programming principle,
together with optimality guarantees for Dec-POMFC solutions applied to finite
swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient
methods for MARL via centralized training and decentralized execution, together
with policy gradient approximation guarantees. In addition, we improve upon
state-of-the-art histogram-based MFC by kernel methods, which is of separate
interest also for fully observable MFC. We evaluate numerically on
representative collective behavior tasks such as adapted Kuramoto and Vicsek
swarming models, being on par with state-of-the-art MARL. Overall, our
framework takes a step towards RL-based engineering of artificial collective
behavior via MFC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kai Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauck_S/0/1/0/all/0/1&quot;&gt;Sascha Hauck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_C/0/1/0/all/0/1&quot;&gt;Christian Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06235">
<title>Unified Molecular Modeling via Modality Blending. (arXiv:2307.06235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06235</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised molecular representation learning is critical for
molecule-based tasks such as AI-assisted drug discovery. Recent studies
consider leveraging both 2D and 3D information for representation learning,
with straightforward alignment strategies that treat each modality separately.
In this work, we introduce a novel &quot;blend-then-predict&quot; self-supervised
learning method (MoleBLEND), which blends atom relations from different
modalities into one unified relation matrix for encoding, then recovers
modality-specific information for both 2D and 3D structures. By treating atom
relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned
and integrated at fine-grained relation-level organically. Extensive
experiments show that MoleBLEND achieves state-of-the-art performance across
major 2D/3D benchmarks. We further provide theoretical insights from the
perspective of mutual-information maximization, demonstrating that our method
unifies contrastive, generative (inter-modal prediction) and mask-then-predict
(intra-modal prediction) objectives into a single cohesive blend-then-predict
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yudi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuyan Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shikun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06240">
<title>DSSE: a drone swarm search environment. (arXiv:2307.06240v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06240</link>
<description rdf:parseType="Literal">&lt;p&gt;The Drone Swarm Search project is an environment, based on PettingZoo, that
is to be used in conjunction with multi-agent (or single-agent) reinforcement
learning algorithms. It is an environment in which the agents (drones), have to
find the targets (shipwrecked people). The agents do not know the position of
the target and do not receive rewards related to their own distance to the
target(s). However, the agents receive the probabilities of the target(s) being
in a certain cell of the map. The aim of this project is to aid in the study of
reinforcement learning algorithms that require dynamic probabilities as inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castanares_M/0/1/0/all/0/1&quot;&gt;Manuel Castanares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrete_L/0/1/0/all/0/1&quot;&gt;Luis F. S. Carrete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damiani_E/0/1/0/all/0/1&quot;&gt;Enrico F. Damiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abreu_L/0/1/0/all/0/1&quot;&gt;Leonardo D. M. de Abreu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brancalion_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Fernando B. Brancalion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barth_F/0/1/0/all/0/1&quot;&gt;Fabr&amp;#xed;cio J. Barth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06243">
<title>Reconstructing Spatiotemporal Data with C-VAEs. (arXiv:2307.06243v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2307.06243</link>
<description rdf:parseType="Literal">&lt;p&gt;The continuous representation of spatiotemporal data commonly relies on using
abstract data types, such as \textit{moving regions}, to represent entities
whose shape and position continuously change over time. Creating this
representation from discrete snapshots of real-world entities requires using
interpolation methods to compute in-between data representations and estimate
the position and shape of the object of interest at arbitrary temporal points.
Existing region interpolation methods often fail to generate smooth and
realistic representations of a region&apos;s evolution. However, recent advancements
in deep learning techniques have revealed the potential of deep models trained
on discrete observations to capture spatiotemporal dependencies through
implicit feature learning.
&lt;/p&gt;
&lt;p&gt;In this work, we explore the capabilities of Conditional Variational
Autoencoder (C-VAE) models to generate smooth and realistic representations of
the spatiotemporal evolution of moving regions. We evaluate our proposed
approach on a sparsely annotated dataset on the burnt area of a forest fire. We
apply compression operations to sample from the dataset and use the C-VAE model
and other commonly used interpolation algorithms to generate in-between region
representations. To evaluate the performance of the methods, we compare their
interpolation results with manually annotated data and regions generated by a
U-Net model. We also assess the quality of generated data considering temporal
consistency metrics.
&lt;/p&gt;
&lt;p&gt;The proposed C-VAE-based approach demonstrates competitive results in
geometric similarity metrics. It also exhibits superior temporal consistency,
suggesting that C-VAE models may be a viable alternative to modelling the
spatiotemporal evolution of 2D moving regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_T/0/1/0/all/0/1&quot;&gt;Tiago F. R. Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1&quot;&gt;Fernando Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_R/0/1/0/all/0/1&quot;&gt;Rog&amp;#xe9;rio Lu&amp;#xed;s de C. Costa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06244">
<title>Diffusion Based Multi-Agent Adversarial Tracking. (arXiv:2307.06244v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06244</link>
<description rdf:parseType="Literal">&lt;p&gt;Target tracking plays a crucial role in real-world scenarios, particularly in
drug-trafficking interdiction, where the knowledge of an adversarial target&apos;s
location is often limited. Improving autonomous tracking systems will enable
unmanned aerial, surface, and underwater vehicles to better assist in
interdicting smugglers that use manned surface, semi-submersible, and aerial
vessels. As unmanned drones proliferate, accurate autonomous target estimation
is even more crucial for security and safety. This paper presents Constrained
Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach
aimed at generating comprehensive predictions of adversary locations by
leveraging past sparse state information. To assess the effectiveness of this
approach, we evaluate predictions on single-target and multi-target pursuit
environments, employing Monte-Carlo sampling of the diffusion model to estimate
the probability associated with each generated trajectory. We propose a novel
cross-attention based diffusion model that utilizes constraint-based sampling
to generate multimodal track hypotheses. Our single-target model surpasses the
performance of all baseline methods on Average Displacement Error (ADE) for
predictions across all time horizons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1&quot;&gt;Sean Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_M/0/1/0/all/0/1&quot;&gt;Manisha Natarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1&quot;&gt;Matthew Gombolay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06250">
<title>Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.06250</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal disentanglement aims to uncover a representation of data using latent
variables that are interrelated through a causal model. Such a representation
is identifiable if the latent model that explains the data is unique. In this
paper, we focus on the scenario where unpaired observational and interventional
data are available, with each intervention changing the mechanism of a latent
variable. When the causal variables are fully observed, statistically
consistent algorithms have been developed to identify the causal model under
faithfulness assumptions. We here show that identifiability can still be
achieved with unobserved causal variables, given a generalized notion of
faithfulness. Our results guarantee that we can recover the latent causal model
up to an equivalence class and predict the effect of unseen combinations of
interventions, in the limit of infinite data. We implement our causal
disentanglement framework by developing an autoencoding variational Bayes
algorithm and apply it to the problem of predicting combinatorial perturbation
effects in genomics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1&quot;&gt;Chandler Squires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greenewald_K/0/1/0/all/0/1&quot;&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1&quot;&gt;Caroline Uhler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06255">
<title>Machine learning and Topological data analysis identify unique features of human papillae in 3D scans. (arXiv:2307.06255v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06255</link>
<description rdf:parseType="Literal">&lt;p&gt;The tongue surface houses a range of papillae that are integral to the
mechanics and chemistry of taste and textural sensation. Although gustatory
function of papillae is well investigated, the uniqueness of papillae within
and across individuals remains elusive. Here, we present the first machine
learning framework on 3D microscopic scans of human papillae (n = 2092),
uncovering the uniqueness of geometric and topological features of papillae.
The finer differences in shapes of papillae are investigated computationally
based on a number of features derived from discrete differential geometry and
computational topology. Interpretable machine learning techniques show that
persistent homology features of the papillae shape are the most effective in
predicting the biological variables. Models trained on these features with
small volumes of data samples predict the type of papillae with an accuracy of
85%. The papillae type classification models can map the spatial arrangement of
filiform and fungiform papillae on a surface. Remarkably, the papillae are
found to be distinctive across individuals and an individual can be identified
with an accuracy of 48% among the 15 participants from a single papillae.
Collectively, this is the first unprecedented evidence demonstrating that
tongue papillae can serve as a unique identifier inspiring new research
direction for food preferences and oral diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreeva_R/0/1/0/all/0/1&quot;&gt;Rayna Andreeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Anwesha Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_R/0/1/0/all/0/1&quot;&gt;Rik Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06263">
<title>On the hierarchical Bayesian modelling of frequency response functions. (arXiv:2307.06263v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06263</link>
<description rdf:parseType="Literal">&lt;p&gt;Population-based structural health monitoring (PBSHM) aims to share valuable
information among members of a population, such as normal- and damage-condition
data, to improve inferences regarding the health states of the members. Even
when the population is comprised of nominally-identical structures, benign
variations among the members will exist as a result of slight differences in
material properties, geometry, boundary conditions, or environmental effects
(e.g., temperature changes). These discrepancies can affect modal properties
and present as changes in the characteristics of the resonance peaks of the
frequency response function (FRF). Many SHM strategies depend on monitoring the
dynamic properties of structures, so benign variations can be challenging for
the practical implementation of these systems. Another common challenge with
vibration-based SHM is data loss, which may result from transmission issues,
sensor failure, a sample-rate mismatch between sensors, and other causes.
Missing data in the time domain will result in decreased resolution in the
frequency domain, which can impair dynamic characterisation. The hierarchical
Bayesian approach provides a useful modelling structure for PBSHM, because
statistical distributions at the population and individual (or domain) level
are learnt simultaneously to bolster statistical strength among the parameters.
As a result, variance is reduced among the parameter estimates, particularly
when data are limited. In this paper, combined probabilistic FRF models are
developed for a small population of nominally-identical helicopter blades under
varying temperature conditions, using a hierarchical Bayesian structure. These
models address critical challenges in SHM, by accommodating benign variations
that present as differences in the underlying dynamics, while also considering
(and utilising), the similarities among the blades.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dardeno_T/0/1/0/all/0/1&quot;&gt;T.A. Dardeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_R/0/1/0/all/0/1&quot;&gt;R.S. Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1&quot;&gt;N. Dervilis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1&quot;&gt;K. Worden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1&quot;&gt;L.A. Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06267">
<title>Physics-informed Machine Learning for Calibrating Macroscopic Traffic Flow Models. (arXiv:2307.06267v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06267</link>
<description rdf:parseType="Literal">&lt;p&gt;Well-calibrated traffic flow models are fundamental to understanding traffic
phenomena and designing control strategies. Traditional calibration has been
developed base on optimization methods. In this paper, we propose a novel
physics-informed, learning-based calibration approach that achieves
performances comparable to and even better than those of optimization-based
methods. To this end, we combine the classical deep autoencoder, an
unsupervised machine learning model consisting of one encoder and one decoder,
with traffic flow models. Our approach informs the decoder of the physical
traffic flow models and thus induces the encoder to yield reasonable traffic
parameters given flow and speed measurements. We also introduce the denoising
autoencoder into our method so that it can handles not only with normal data
but also with corrupted data with missing values. We verified our approach with
a case study of I-210 E in California.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Li Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozbay_K/0/1/0/all/0/1&quot;&gt;Kaan Ozbay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06272">
<title>Exposing the Fake: Effective Diffusion-Generated Images Detection. (arXiv:2307.06272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06272</link>
<description rdf:parseType="Literal">&lt;p&gt;Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}_{\text{Stat}}$ and neural network-based
$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID&apos;s superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruipeng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fei Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoshuang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06279">
<title>SpreadNUTS -- Moderate Dynamic Extension of Paths for No-U-Turn Sampling &amp; Partitioning Visited Regions. (arXiv:2307.06279v1 [stat.CO])</title>
<link>http://arxiv.org/abs/2307.06279</link>
<description rdf:parseType="Literal">&lt;p&gt;Markov chain Monte Carlo (MCMC) methods have existed for a long time and the
field is well-explored. The purpose of MCMC methods is to approximate a
distribution through repeated sampling; most MCMC algorithms exhibit
asymptotically optimal behavior in that they converge to the true distribution
at the limit. However, what differentiates these algorithms are their practical
convergence guarantees and efficiency. While a sampler may eventually
approximate a distribution well, because it is used in the real world it is
necessary that the point at which the sampler yields a good estimate of the
distribution is reachable in a reasonable amount of time. Similarly, if it is
computationally difficult or intractable to produce good samples from a
distribution for use in estimation, then there is no real-world utility
afforded by the sampler. Thus, most MCMC methods these days focus on improving
efficiency and speeding up convergence. However, many MCMC algorithms suffer
from random walk behavior and often only mitigate such behavior as outright
erasing random walks is difficult. Hamiltonian Monte Carlo (HMC) is a class of
MCMC methods that theoretically exhibit no random walk behavior because of
properties related to Hamiltonian dynamics. This paper introduces modifications
to a specific HMC algorithm known as the no-U-turn sampler (NUTS) that aims to
explore the sample space faster than NUTS, yielding a sampler that has faster
convergence to the true distribution than NUTS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheriff_F/0/1/0/all/0/1&quot;&gt;Fareed Sheriff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06283">
<title>Tackling Computational Heterogeneity in FL: A Few Theoretical Insights. (arXiv:2307.06283v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06283</link>
<description rdf:parseType="Literal">&lt;p&gt;The future of machine learning lies in moving data collection along with
training to the edge. Federated Learning, for short FL, has been recently
proposed to achieve this goal. The principle of this approach is to aggregate
models learned over a large number of distributed clients, i.e.,
resource-constrained mobile devices that collect data from their environment,
to obtain a new more general model. The latter is subsequently redistributed to
clients for further training. A key feature that distinguishes federated
learning from data-center-based distributed training is the inherent
heterogeneity. In this work, we introduce and analyse a novel aggregation
framework that allows for formalizing and tackling computational heterogeneity
in federated optimization, in terms of both heterogeneous data and local
updates. Proposed aggregation algorithms are extensively analyzed from a
theoretical, and an experimental prospective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansour_A/0/1/0/all/0/1&quot;&gt;Adnan Ben Mansour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1&quot;&gt;Gaia Carenini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duplessis_A/0/1/0/all/0/1&quot;&gt;Alexandre Duplessis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06287">
<title>Rational Neural Network Controllers. (arXiv:2307.06287v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.06287</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks have shown great success in many machine learning related
tasks, due to their ability to act as general function approximators. Recent
work has demonstrated the effectiveness of neural networks in control systems
(known as neural feedback loops), most notably by using a neural network as a
controller. However, one of the big challenges of this approach is that neural
networks have been shown to be sensitive to adversarial attacks. This means
that, unless they are designed properly, they are not an ideal candidate for
controllers due to issues with robustness and uncertainty, which are pivotal
aspects of control systems. There has been initial work on robustness to both
analyse and design dynamical systems with neural network controllers. However,
one prominent issue with these methods is that they use existing neural network
architectures tailored for traditional machine learning tasks. These structures
may not be appropriate for neural network controllers and it is important to
consider alternative architectures. This paper considers rational neural
networks and presents novel rational activation functions, which can be used
effectively in robustness problems for neural feedback loops. Rational
activation functions are replaced by a general rational neural network
structure, which is convex in the neural network&apos;s parameters. A method is
proposed to recover a stabilising controller from a Sum of Squares feasibility
test. This approach is then applied to a refined rational neural network which
is more compatible with Sum of Squares programming. Numerical examples show
that this method can successfully recover stabilising rational neural network
controllers for neural feedback loops with non-linear plants with noise and
parametric uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Newton_M/0/1/0/all/0/1&quot;&gt;Matthew Newton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Papachristodoulou_A/0/1/0/all/0/1&quot;&gt;Antonis Papachristodoulou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06290">
<title>Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06290</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models&apos; ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yihan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yanbin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06299">
<title>Towards a Certified Proof Checker for Deep Neural Network Verification. (arXiv:2307.06299v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.06299</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in deep neural networks (DNNs) have led to their adoption
in safety-critical systems, which in turn has heightened the need for
guaranteeing their safety. These safety properties of DNNs can be proven using
tools developed by the verification community. However, these tools are
themselves prone to implementation bugs and numerical stability problems, which
make their reliability questionable. To overcome this, some verifiers produce
proofs of their results which can be checked by a trusted checker. In this
work, we present a novel implementation of a proof checker for DNN
verification. It improves on existing implementations by offering numerical
stability and greater verifiability. To achieve this, we leverage two key
capabilities of Imandra, an industrial theorem prover: its support of infinite
precision real arithmetic and its formal verification infrastructure. So far,
we have implemented a proof checker in Imandra, specified its correctness
properties and started to verify the checker&apos;s compliance with them. Our
ongoing work focuses on completing the formal verification of the checker and
further optimizing its performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desmartin_R/0/1/0/all/0/1&quot;&gt;Remi Desmartin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isac_O/0/1/0/all/0/1&quot;&gt;Omri Isac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passmore_G/0/1/0/all/0/1&quot;&gt;Grant Passmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stark_K/0/1/0/all/0/1&quot;&gt;Kathrin Stark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1&quot;&gt;Guy Katz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Komendantskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06304">
<title>Patch n&apos; Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution. (arXiv:2307.06304v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06304</link>
<description rdf:parseType="Literal">&lt;p&gt;The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Djolonga_J/0/1/0/all/0/1&quot;&gt;Josip Djolonga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heek_J/0/1/0/all/0/1&quot;&gt;Jonathan Heek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1&quot;&gt;Matthias Minderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1&quot;&gt;Mathilde Caron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiner_A/0/1/0/all/0/1&quot;&gt;Andreas Steiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1&quot;&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_A/0/1/0/all/0/1&quot;&gt;Avital Oliver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padlewski_P/0/1/0/all/0/1&quot;&gt;Piotr Padlewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1&quot;&gt;Alexey Gritsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1&quot;&gt;Mario Lu&amp;#x10d;i&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06306">
<title>Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes. (arXiv:2307.06306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06306</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art federated learning algorithms such as FedAvg require
carefully tuned stepsizes to achieve their best performance. The improvements
proposed by existing adaptive federated methods involve tuning of additional
hyperparameters such as momentum parameters, and consider adaptivity only in
the server aggregation round, but not locally. These methods can be inefficient
in many practical scenarios because they require excessive tuning of
hyperparameters and do not capture local geometric information. In this work,
we extend the recently proposed stochastic Polyak stepsize (SPS) to the
federated learning setting, and propose new locally adaptive and nearly
parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that
FedSPS converges linearly in strongly convex and sublinearly in convex settings
when the interpolation condition (overparametrization) is satisfied, and
converges to a neighborhood of the solution in the general case. We extend our
proposed method to a decreasing stepsize version FedDecSPS, that converges also
when the interpolation condition does not hold. We validate our theoretical
claims by performing illustrative convex experiments. Our proposed algorithms
match the optimization performance of FedAvg with the best tuned
hyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d.
case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Sohom Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loizou_N/0/1/0/all/0/1&quot;&gt;Nicolas Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stich_S/0/1/0/all/0/1&quot;&gt;Sebastian U. Stich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06307">
<title>Facial Reenactment Through a Personalized Generator. (arXiv:2307.06307v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06307</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the role of image generative models in facial reenactment
has been steadily increasing. Such models are usually subject-agnostic and
trained on domain-wide datasets. The appearance of the reenacted individual is
learned from a single image, and hence, the entire breadth of the individual&apos;s
appearance is not entirely captured, leading these methods to resort to
unfaithful hallucination. Thanks to recent advancements, it is now possible to
train a personalized generative model tailored specifically to a given
individual. In this paper, we propose a novel method for facial reenactment
using a personalized generator. We train the generator using frames from a
short, yet varied, self-scan video captured using a simple commodity camera.
Images synthesized by the personalized generator are guaranteed to preserve
identity. The premise of our work is that the task of reenactment is thus
reduced to accurately mimicking head poses and expressions. To this end, we
locate the desired frames in the latent space of the personalized generator
using carefully designed latent optimization. Through extensive evaluation, we
demonstrate state-of-the-art performance for facial reenactment. Furthermore,
we show that since our reenactment takes place in a semantic latent space, it
can be semantically edited and stylized in post-processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazary_A/0/1/0/all/0/1&quot;&gt;Ariel Elazary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitzan_Y/0/1/0/all/0/1&quot;&gt;Yotam Nitzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06324">
<title>Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.06324</link>
<description rdf:parseType="Literal">&lt;p&gt;This work establishes provably faster convergence rates for gradient descent
via a computer-assisted analysis technique. Our theory allows nonconstant
stepsize policies with frequent long steps potentially violating descent by
analyzing the overall effect of many iterations at once rather than the typical
one-iteration inductions used in most first-order method analyses. We show that
long steps, which may increase the objective value in the short term, lead to
provably faster convergence in the long term. A conjecture towards proving a
faster $O(1/T\log T)$ rate for gradient descent is also motivated along with
simple numerical validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grimmer_B/0/1/0/all/0/1&quot;&gt;Benjamin Grimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06328">
<title>Budgeting Counterfactual for Offline RL. (arXiv:2307.06328v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06328</link>
<description rdf:parseType="Literal">&lt;p&gt;The main challenge of offline reinforcement learning, where data is limited,
arises from a sequence of counterfactual reasoning dilemmas within the realm of
potential actions: What if we were to choose a different course of action?
These circumstances frequently give rise to extrapolation errors, which tend to
accumulate exponentially with the problem horizon. Hence, it becomes crucial to
acknowledge that not all decision steps are equally important to the final
outcome, and to budget the number of counterfactual decisions a policy make in
order to control the extrapolation. Contrary to existing approaches that use
regularization on either the policy or value function, we propose an approach
to explicitly bound the amount of out-of-distribution actions during training.
Specifically, our method utilizes dynamic programming to decide where to
extrapolate and where not to, with an upper bound on the decisions different
from behavior policy. It balances between the potential for improvement from
taking out-of-distribution actions and the risk of making errors due to
extrapolation. Theoretically, we justify our method by the constrained
optimality of the fixed point solution to our $Q$ updating rules. Empirically,
we show that the overall performance of our method is better than the
state-of-the-art offline RL methods on tasks in the widely-used D4RL
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakoor_R/0/1/0/all/0/1&quot;&gt;Rasool Fakoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06333">
<title>Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for Test-Time Policy Adaptation. (arXiv:2307.06333v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06333</link>
<description rdf:parseType="Literal">&lt;p&gt;Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent&apos;s observation. However, designers don&apos;t
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_A/0/1/0/all/0/1&quot;&gt;Andi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netanyahu_A/0/1/0/all/0/1&quot;&gt;Aviv Netanyahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1&quot;&gt;Mark Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Tianmin Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobu_A/0/1/0/all/0/1&quot;&gt;Andreea Bobu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1&quot;&gt;Julie Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1910.06151">
<title>Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. (arXiv:1910.06151v4 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/1910.06151</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an algorithmic framework for quantum-inspired classical algorithms
on close-to-low-rank matrices, generalizing the series of results started by
Tang&apos;s breakthrough quantum-inspired algorithm for recommendation systems
[STOC&apos;19]. Motivated by quantum linear algebra algorithms and the quantum
singular value transformation (SVT) framework of Gily\&apos;en, Su, Low, and Wiebe
[STOC&apos;19], we develop classical algorithms for SVT that run in time independent
of input dimension, under suitable quantum-inspired sampling assumptions. Our
results give compelling evidence that in the corresponding QRAM data structure
input model, quantum SVT does not yield exponential quantum speedups. Since the
quantum SVT framework generalizes essentially all known techniques for quantum
linear algebra, our results, combined with sampling lemmas from previous work,
suffice to generalize all recent results about dequantizing quantum machine
learning algorithms. In particular, our classical SVT framework recovers and
often improves the dequantization results on recommendation systems, principal
component analysis, supervised clustering, support vector machines, low-rank
regression, and semidefinite program solving. We also give additional
dequantization results on low-rank Hamiltonian simulation and discriminant
analysis. Our improvements come from identifying the key feature of the
quantum-inspired input model that is at the core of all prior quantum-inspired
results: $\ell^2$-norm sampling can approximate matrix products in time
independent of their dimension. We reduce all our main results to this fact,
making our exposition concise, self-contained, and intuitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chia_N/0/1/0/all/0/1&quot;&gt;Nai-Hui Chia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilyen_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Gily&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Han-Hsuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_E/0/1/0/all/0/1&quot;&gt;Ewin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.02613">
<title>Deep Learning based Uncertainty Decomposition for Real-time Control. (arXiv:2010.02613v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.02613</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-driven control in unknown environments requires a clear understanding of
the involved uncertainties for ensuring safety and efficient exploration. While
aleatoric uncertainty that arises from measurement noise can often be
explicitly modeled given a parametric description, it can be harder to model
epistemic uncertainty, which describes the presence or absence of training
data. The latter can be particularly useful for implementing exploratory
control strategies when system dynamics are unknown. We propose a novel method
for detecting the absence of training data using deep learning, which gives a
continuous valued scalar output between $0$ (indicating low uncertainty) and
$1$ (indicating high uncertainty). We utilize this detector as a proxy for
epistemic uncertainty and show its advantages over existing approaches on
synthetic and real-world datasets. Our approach can be directly combined with
aleatoric uncertainty estimates and allows for uncertainty estimation in
real-time as the inference is sample-free unlike existing approaches for
uncertainty modeling. We further demonstrate the practicality of this
uncertainty estimate in deploying online data-efficient control on a simulated
quadcopter acted upon by an unknown disturbance model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1&quot;&gt;Neha Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umlauft_J/0/1/0/all/0/1&quot;&gt;Jonas Umlauft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lederer_A/0/1/0/all/0/1&quot;&gt;Armin Lederer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beckers_T/0/1/0/all/0/1&quot;&gt;Thomas Beckers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirche_S/0/1/0/all/0/1&quot;&gt;Sandra Hirche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2101.10870">
<title>B-HAR: an open-source baseline framework for in depth study of human activity recognition datasets and workflows. (arXiv:2101.10870v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2101.10870</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Activity Recognition (HAR), based on machine and deep learning
algorithms is considered one of the most promising technologies to monitor
professional and daily life activities for different categories of people
(e.g., athletes, elderly, kids, employers) in order to provide a variety of
services related, for example to well-being, empowering of technical
performances, prevention of risky situation, and educational purposes. However,
the analysis of the effectiveness and the efficiency of HAR methodologies
suffers from the lack of a standard workflow, which might represent the
baseline for the estimation of the quality of the developed pattern recognition
models. This makes the comparison among different approaches a challenging
task. In addition, researchers can make mistakes that, when not detected,
definitely affect the achieved results. To mitigate such issues, this paper
proposes an open-source automatic and highly configurable framework, named
B-HAR, for the definition, standardization, and development of a baseline
framework in order to evaluate and compare HAR methodologies. It implements the
most popular data processing methods for data preparation and the most commonly
used machine and deep learning pattern recognition models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Demrozi_F/0/1/0/all/0/1&quot;&gt;Florenc Demrozi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turetta_C/0/1/0/all/0/1&quot;&gt;Cristian Turetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pravadelli_G/0/1/0/all/0/1&quot;&gt;Graziano Pravadelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.00479">
<title>Fast Rates for the Regret of Offline Reinforcement Learning. (arXiv:2102.00479v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2102.00479</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the regret of reinforcement learning from offline data generated by
a fixed behavior policy in an infinite-horizon discounted Markov decision
process (MDP). While existing analyses of common approaches, such as fitted
$Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret,
empirical behavior exhibits \emph{much} faster convergence. In this paper, we
present a finer regret analysis that exactly characterizes this phenomenon by
providing fast rates for the regret convergence. First, we show that given any
estimate for the optimal quality function $Q^*$, the regret of the policy it
defines converges at a rate given by the exponentiation of the $Q^*$-estimate&apos;s
pointwise convergence rate, thus speeding it up. The level of exponentiation
depends on the level of noise in the \emph{decision-making} problem, rather
than the estimation problem. We establish such noise levels for linear and
tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman
residual minimization to establish the correct pointwise convergence
guarantees. As specific cases, our results imply $O(1/n)$ regret rates in
linear cases and $\exp(-\Omega(n))$ regret rates in tabular cases. We extend
our findings to general function approximation by extending our results to
regret guarantees based on $L_p$-convergence rates for estimating $Q^*$ rather
than pointwise rates, where $L_2$ guarantees for nonparametric $Q^*$-estimation
can be ensured under mild conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yichun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1&quot;&gt;Nathan Kallus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1&quot;&gt;Masatoshi Uehara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.05942">
<title>Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed Stability and Robustness. (arXiv:2104.05942v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2104.05942</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces recurrent equilibrium networks (RENs), a new class of
nonlinear dynamical models} for applications in machine learning, system
identification and control. The new model class admits ``built in&apos;&apos; behavioural
guarantees of stability and robustness. All models in the proposed class are
contracting -- a strong form of nonlinear stability -- and models can satisfy
prescribed incremental integral quadratic constraints (IQC), including
Lipschitz bounds and incremental passivity. RENs are otherwise very flexible:
they can represent all stable linear systems, all previously-known sets of
contracting recurrent neural networks and echo state networks, all deep
feedforward neural networks, and all stable Wiener/Hammerstein models, and can
approximate all fading-memory and contracting nonlinear systems. RENs are
parameterized directly by a vector in R^N, i.e. stability and robustness are
ensured without parameter constraints, which simplifies learning since
\HL{generic methods for unconstrained optimization such as stochastic gradient
descent and its variants can be used}. The performance and robustness of the
new model set is evaluated on benchmark nonlinear system identification
problems, and the paper also presents applications in data-driven nonlinear
observer design and control with stability guarantees.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Revay_M/0/1/0/all/0/1&quot;&gt;Max Revay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchester_I/0/1/0/all/0/1&quot;&gt;Ian R. Manchester&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.09035">
<title>Regularization of Mixture Models for Robust Principal Graph Learning. (arXiv:2106.09035v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.09035</link>
<description rdf:parseType="Literal">&lt;p&gt;A regularized version of Mixture Models is proposed to learn a principal
graph from a distribution of $D$-dimensional data points. In the particular
case of manifold learning for ridge detection, we assume that the underlying
manifold can be modeled as a graph structure acting like a topological prior
for the Gaussian clusters turning the problem into a maximum a posteriori
estimation. Parameters of the model are iteratively estimated through an
Expectation-Maximization procedure making the learning of the structure
computationally efficient with guaranteed convergence for any graph prior in a
polynomial time. We also embed in the formalism a natural way to make the
algorithm robust to outliers of the pattern and heteroscedasticity of the
manifold sampling coherently with the graph structure. The method uses a graph
prior given by the minimum spanning tree that we extend using random
sub-samplings of the dataset to take into account cycles that can be observed
in the spatial distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonnaire_T/0/1/0/all/0/1&quot;&gt;Tony Bonnaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghanim_N/0/1/0/all/0/1&quot;&gt;Nabila Aghanim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.10865">
<title>Benign Overfitting in Multiclass Classification: All Roads Lead to Interpolation. (arXiv:2106.10865v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2106.10865</link>
<description rdf:parseType="Literal">&lt;p&gt;The literature on &quot;benign overfitting&quot; in overparameterized models has been
mostly restricted to regression or binary classification; however, modern
machine learning operates in the multiclass setting. Motivated by this
discrepancy, we study benign overfitting in multiclass linear classification.
Specifically, we consider the following training algorithms on separable data:
(i) empirical risk minimization (ERM) with cross-entropy loss, which converges
to the multiclass support vector machine (SVM) solution; (ii) ERM with
least-squares loss, which converges to the min-norm interpolating (MNI)
solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple
sufficient deterministic condition under which all three algorithms lead to
classifiers that interpolate the training data and have equal accuracy. When
the data is generated from Gaussian mixtures or a multinomial logistic model,
this condition holds under high enough effective overparameterization. We also
show that this sufficient condition is satisfied under &quot;neural collapse&quot;, a
phenomenon that is observed in training deep neural networks. Second, we derive
novel bounds on the accuracy of the MNI classifier, thereby showing that all
three training algorithms lead to benign overfitting under sufficient
overparameterization. Ultimately, our analysis shows that good generalization
is possible for SVM solutions beyond the realm in which typical margin-based
bounds apply.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Muthukumar_V/0/1/0/all/0/1&quot;&gt;Vidya Muthukumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.06672">
<title>The Deep Generative Decoder: MAP estimation of representations improves modeling of single-cell RNA data. (arXiv:2110.06672v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.06672</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning low-dimensional representations of single-cell transcriptomics has
become instrumental to its downstream analysis. The state of the art is
currently represented by neural network models such as variational autoencoders
(VAEs) which use a variational approximation of the likelihood for inference.
We here present the Deep Generative Decoder (DGD), a simple generative model
that computes model parameters and representations directly via maximum a
posteriori (MAP) estimation. The DGD handles complex parameterized latent
distributions naturally unlike VAEs which typically use a fixed Gaussian
distribution, because of the complexity of adding other types. We first show
its general functionality on a commonly used benchmark set, Fashion-MNIST.
Secondly, we apply the model to multiple single-cell data sets. Here the DGD
learns low-dimensional, meaningful and well-structured latent representations
with sub-clustering beyond the provided labels. The advantages of this approach
are its simplicity and its capability to provide representations of much
smaller dimensionality than a comparable VAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuster_V/0/1/0/all/0/1&quot;&gt;Viktoria Schuster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krogh_A/0/1/0/all/0/1&quot;&gt;Anders Krogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11337">
<title>Empowering General-purpose User Representation with Full-life Cycle Behavior Modeling. (arXiv:2110.11337v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11337</link>
<description rdf:parseType="Literal">&lt;p&gt;User Modeling plays an essential role in industry. In this field,
task-agnostic approaches, which generate general-purpose representation
applicable to diverse downstream user cognition tasks, is a promising direction
being more valuable and economical than task-specific representation learning.
With the rapid development of Internet service platforms, user behaviors have
been accumulated continuously. However, existing general-purpose user
representation researches have little ability for full-life cycle modeling on
extremely long behavior sequences since user registration. In this study, we
propose a novel framework called full- Life cycle User Representation Model
(LURM) to tackle this challenge. Specifically, LURM consists of two cascaded
sub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time
period into a sparse vector with super-high dimension (e.g., 10^5); (II)
Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI
features to multiple low-dimensional user representations. Specially, SMEN
achieves almost lossless dimensionality reduction, benefiting from a novel
multi-anchor module which can learn different aspects of user interests.
Experiments on several benchmark datasets show that our approach outperforms
state-of-the-art general-purpose representation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Ke Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qinghui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03222">
<title>On Complexity of 1-Center in Various Metrics. (arXiv:2112.03222v3 [cs.CC] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03222</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the classic 1-center problem: Given a set $P$ of $n$ points in a
metric space find the point in $P$ that minimizes the maximum distance to the
other points of $P$. We study the complexity of this problem in $d$-dimensional
$\ell_p$-metrics and in edit and Ulam metrics over strings of length $d$. Our
results for the 1-center problem may be classified based on $d$ as follows.
&lt;/p&gt;
&lt;p&gt;$\bullet$ Small $d$: Assuming the hitting set conjecture (HSC), we show that
when $d=\omega(\log n)$, no subquadratic algorithm can solve 1-center problem
in any of the $\ell_p$-metrics, or in edit or Ulam metrics.
&lt;/p&gt;
&lt;p&gt;$\bullet$ Large $d$: When $d=\Omega(n)$, we extend our conditional lower
bound to rule out subquartic algorithms for 1-center problem in edit metric
(assuming Quantified SETH). On the other hand, we give a
$(1+\epsilon)$-approximation for 1-center in Ulam metric with running time
$\tilde{O_{\varepsilon}}(nd+n^2\sqrt{d})$.
&lt;/p&gt;
&lt;p&gt;We also strengthen some of the above lower bounds by allowing approximations
or by reducing the dimension $d$, but only against a weaker class of algorithms
which list all requisite solutions. Moreover, we extend one of our hardness
results to rule out subquartic algorithms for the well-studied 1-median problem
in the edit metric, where given a set of $n$ strings each of length $n$, the
goal is to find a string in the set that minimizes the sum of the edit
distances to the rest of the strings in the set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abboud_A/0/1/0/all/0/1&quot;&gt;Amir Abboud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bateni_M/0/1/0/all/0/1&quot;&gt;Mohammad Hossein Bateni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1&quot;&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S%2E_K/0/1/0/all/0/1&quot;&gt;Karthik C. S.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seddighin_S/0/1/0/all/0/1&quot;&gt;Saeed Seddighin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.00129">
<title>Fundamental Limits for Sensor-Based Robot Control. (arXiv:2202.00129v5 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2202.00129</link>
<description rdf:parseType="Literal">&lt;p&gt;Our goal is to develop theory and algorithms for establishing fundamental
limits on performance imposed by a robot&apos;s sensors for a given task. In order
to achieve this, we define a quantity that captures the amount of task-relevant
information provided by a sensor. Using a novel version of the generalized Fano
inequality from information theory, we demonstrate that this quantity provides
an upper bound on the highest achievable expected reward for one-step decision
making tasks. We then extend this bound to multi-step problems via a dynamic
programming approach. We present algorithms for numerically computing the
resulting bounds, and demonstrate our approach on three examples: (i) the lava
problem from the literature on partially observable Markov decision processes,
(ii) an example with continuous state and observation spaces corresponding to a
robot catching a freely-falling object, and (iii) obstacle avoidance using a
depth sensor with non-Gaussian noise. We demonstrate the ability of our
approach to establish strong limits on achievable performance for these
problems by comparing our upper bounds with achievable lower bounds (computed
by synthesizing or learning concrete control policies).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumdar_A/0/1/0/all/0/1&quot;&gt;Anirudha Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Z/0/1/0/all/0/1&quot;&gt;Zhiting Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacelli_V/0/1/0/all/0/1&quot;&gt;Vincent Pacelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13739">
<title>High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v4 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13739</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum computers hold great promise to enhance machine learning, but their
current qubit counts restrict the realisation of this promise. In an attempt to
placate this limitation techniques can be applied for evaluating a quantum
circuit using a machine with fewer qubits than the circuit naively requires.
These techniques work by evaluating many smaller circuits on the smaller
machine, that are then combined in a polynomial to replicate the output of the
larger machine. This scheme requires more circuit evaluations than are
practical for general circuits. However, we investigate the possibility that
for certain applications many of these subcircuits are superfluous, and that a
much smaller sum is sufficient to estimate the full circuit. We construct a
machine learning model that may be capable of approximating the outputs of the
larger circuit with much fewer circuit evaluations. We successfully apply our
model to the task of digit recognition, using simulated quantum computers much
smaller than the data dimension. The model is also applied to the task of
approximating a random 10 qubit PQC with simulated access to a 5 qubit
computer, even with only relatively modest number of circuits our model
provides an accurate approximation of the 10 qubit PQCs output, superior to a
neural network attempt. The developed method might be useful for implementing
quantum models on larger data throughout the NISQ era.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Marshall_S/0/1/0/all/0/1&quot;&gt;Simon C. Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gyurik_C/0/1/0/all/0/1&quot;&gt;Casper Gyurik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.01179">
<title>VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait Representation. (arXiv:2205.01179v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2205.01179</link>
<description rdf:parseType="Literal">&lt;p&gt;Quadruped locomotion is rapidly maturing to a degree where robots are able to
realise highly dynamic manoeuvres. However, current planners are unable to vary
key gait parameters of the in-swing feet midair. In this work we address this
limitation and show that it is pivotal in increasing controller robustness by
learning a latent space capturing the key stance phases constituting a
particular gait. This is achieved via a generative model trained on a single
trot style, which encourages disentanglement such that application of a drive
signal to a single dimension of the latent state induces holistic plans
synthesising a continuous variety of trot styles. We demonstrate that specific
properties of the drive signal map directly to gait parameters such as cadence,
footstep height and full stance duration. Due to the nature of our approach
these synthesised gaits are continuously variable online during robot
operation. The use of a generative model facilitates the detection and
mitigation of disturbances to provide a versatile and robust planning
framework. We evaluate our approach on two versions of the real ANYmal
quadruped robots and demonstrate that our method achieves a continuous blend of
dynamic trot styles whilst being robust and reactive to external perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_A/0/1/0/all/0/1&quot;&gt;Alexander L. Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merkt_W/0/1/0/all/0/1&quot;&gt;Wolfgang Merkt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geisert_M/0/1/0/all/0/1&quot;&gt;Mathieu Geisert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangapurwala_S/0/1/0/all/0/1&quot;&gt;Siddhant Gangapurwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelcke_M/0/1/0/all/0/1&quot;&gt;Martin Engelcke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_O/0/1/0/all/0/1&quot;&gt;Oiwi Parker Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havoutis_I/0/1/0/all/0/1&quot;&gt;Ioannis Havoutis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05173">
<title>Self-Supervised Anomaly Detection: A Survey and Outlook. (arXiv:2205.05173v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) plays a crucial role in various domains, including
cybersecurity, finance, and healthcare, by identifying patterns or events that
deviate from normal behaviour. In recent years, significant progress has been
made in this field due to the remarkable growth of deep learning models.
Notably, the advent of self-supervised learning has sparked the development of
novel AD algorithms that outperform the existing state-of-the-art approaches by
a considerable margin. This paper aims to provide a comprehensive review of the
current methodologies in self-supervised anomaly detection. We present
technical details of the standard methods and discuss their strengths and
drawbacks. We also compare the performance of these models against each other
and other state-of-the-art anomaly detection models. Finally, the paper
concludes with a discussion of future directions for self-supervised anomaly
detection, including the development of more effective and efficient algorithms
and the integration of these techniques with other related fields, such as
multi-modal learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hojjati_H/0/1/0/all/0/1&quot;&gt;Hadi Hojjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Thi Kieu Khanh Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05200">
<title>Dynamic mean field programming. (arXiv:2206.05200v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05200</link>
<description rdf:parseType="Literal">&lt;p&gt;A dynamic mean field theory is developed for finite state and action Bayesian
reinforcement learning in the large state space limit. In an analogy with
statistical physics, the Bellman equation is studied as a disordered dynamical
system; the Markov decision process transition probabilities are interpreted as
couplings and the value functions as deterministic spins that evolve
dynamically. Thus, the mean-rewards and transition probabilities are considered
to be quenched random variables. The theory reveals that, under certain
assumptions, the state-action values are statistically independent across
state-action pairs in the asymptotic state space limit, and provides the form
of the distribution exactly. The results hold in the finite and discounted
infinite horizon settings, for both value iteration and policy evaluation. The
state-action value statistics can be computed from a set of mean field
equations, which we call dynamic mean field programming (DMFP). For policy
evaluation the equations are exact. For value iteration, approximate equations
are obtained by appealing to extreme value theory or bounds. The result
provides analytic insight into the statistical structure of tabular
reinforcement learning, for example revealing the conditions under which
reinforcement learning is equivalent to a set of independent multi-armed bandit
problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stamatescu_G/0/1/0/all/0/1&quot;&gt;George Stamatescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04104">
<title>Towards a More Rigorous Science of Blindspot Discovery in Image Classification Models. (arXiv:2207.04104v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04104</link>
<description rdf:parseType="Literal">&lt;p&gt;A growing body of work studies Blindspot Discovery Methods (&quot;BDM&quot;s): methods
that use an image embedding to find semantically meaningful (i.e., united by a
human-understandable concept) subsets of the data where an image classifier
performs significantly worse. Motivated by observed gaps in prior work, we
introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic
image datasets to train models with known blindspots and a new BDM, PlaneSpot,
that uses a 2D image representation. We use SpotCheck to run controlled
experiments that identify factors that influence BDM performance (e.g., the
number of blindspots in a model, or features used to define the blindspot) and
show that PlaneSpot is competitive with and in many cases outperforms existing
BDMs. Importantly, we validate these findings by designing additional
experiments that use real image data from MS-COCO, a large image benchmark
dataset. Our findings suggest several promising directions for future work on
BDM design and evaluation. Overall, we hope that the methodology and analyses
presented in this work will help facilitate a more rigorous science of
blindspot discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1&quot;&gt;Gregory Plumb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_N/0/1/0/all/0/1&quot;&gt;Nari Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel Alexander Cabrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1&quot;&gt;Ameet Talwalkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.09387">
<title>Green, Quantized Federated Learning over Wireless Networks: An Energy-Efficient Design. (arXiv:2207.09387v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.09387</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a green-quantized FL framework, which represents data with a
finite precision level in both local training and uplink transmission, is
proposed. Here, the finite precision level is captured through the use of
quantized neural networks (QNNs) that quantize weights and activations in
fixed-precision format. In the considered FL model, each device trains its QNN
and transmits a quantized training result to the base station. Energy models
for the local training and the transmission with quantization are rigorously
derived. To minimize the energy consumption and the number of communication
rounds simultaneously, a multi-objective optimization problem is formulated
with respect to the number of local iterations, the number of selected devices,
and the precision levels for both local training and transmission while
ensuring convergence under a target accuracy constraint. To solve this problem,
the convergence rate of the proposed FL system is analytically derived with
respect to the system control variables. Then, the Pareto boundary of the
problem is characterized to provide efficient solutions using the normal
boundary inspection method. Design insights on balancing the tradeoff between
the two objectives while achieving a target accuracy are drawn from using the
Nash bargaining solution and analyzing the derived convergence rate. Simulation
results show that the proposed FL framework can reduce energy consumption until
convergence by up to 70\% compared to a baseline FL algorithm that represents
data with full precision without damaging the convergence rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozaffari_M/0/1/0/all/0/1&quot;&gt;Mohammad Mozaffari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1&quot;&gt;Merouane Debbah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00736">
<title>A large sample theory for infinitesimal gradient boosting. (arXiv:2210.00736v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00736</link>
<description rdf:parseType="Literal">&lt;p&gt;Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the
vanishing-learning-rate limit of the popular tree-based gradient boosting
algorithm from machine learning. It is characterized as the solution of a
nonlinear ordinary differential equation in a infinite-dimensional function
space where the infinitesimal boosting operator driving the dynamics depends on
the training sample. We consider the asymptotic behavior of the model in the
large sample limit and prove its convergence to a deterministic process. This
population limit is again characterized by a differential equation that depends
on the population distribution. We explore some properties of this population
limit: we prove that the dynamics makes the test error decrease and we consider
its long time behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dombry_C/0/1/0/all/0/1&quot;&gt;Clement Dombry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duchamps_J/0/1/0/all/0/1&quot;&gt;Jean-Jil Duchamps&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01212">
<title>spred: Solving $L_1$ Penalty with SGD. (arXiv:2210.01212v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01212</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to minimize a generic differentiable objective with $L_1$
constraint using a simple reparametrization and straightforward stochastic
gradient descent. Our proposal is the direct generalization of previous ideas
that the $L_1$ penalty may be equivalent to a differentiable reparametrization
with weight decay. We prove that the proposed method, \textit{spred}, is an
exact differentiable solver of $L_1$ and that the reparametrization trick is
completely ``benign&quot; for a generic nonconvex function. Practically, we
demonstrate the usefulness of the method in (1) training sparse neural networks
to perform gene selection tasks, which involves finding relevant features in a
very high dimensional space, and (2) neural network compression task, to which
previous attempts at applying the $L_1$-penalty have been unsuccessful.
Conceptually, our result bridges the gap between the sparsity in deep learning
and conventional statistical learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1&quot;&gt;Liu Ziyin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01892">
<title>Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01892</link>
<description rdf:parseType="Literal">&lt;p&gt;Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherlis_A/0/1/0/all/0/1&quot;&gt;Adam Scherlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1&quot;&gt;Kshitij Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1&quot;&gt;Adam S. Jermyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benton_J/0/1/0/all/0/1&quot;&gt;Joe Benton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1&quot;&gt;Buck Shlegeris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03104">
<title>Distributionally Adaptive Meta Reinforcement Learning. (arXiv:2210.03104v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03104</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-reinforcement learning algorithms provide a data-driven way to acquire
policies that quickly adapt to many tasks with varying rewards or dynamics
functions. However, learned meta-policies are often effective only on the exact
task distribution on which they were trained and struggle in the presence of
distribution shift of test-time rewards or transition dynamics. In this work,
we develop a framework for meta-RL algorithms that are able to behave
appropriately under test-time distribution shifts in the space of tasks. Our
framework centers on an adaptive approach to distributional robustness that
trains a population of meta-policies to be robust to varying levels of
distribution shift. When evaluated on a potentially shifted test-time
distribution of tasks, this allows us to choose the meta-policy with the most
appropriate level of robustness, and use it to perform fast adaptation. We
formally show how our framework allows for improved regret under distribution
shift, and empirically show its efficacy on simulated robotics problems under a
wide range of distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajay_A/0/1/0/all/0/1&quot;&gt;Anurag Ajay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dibya Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09452">
<title>Multiple Instance Learning via Iterative Self-Paced Supervised Contrastive Learning. (arXiv:2210.09452v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09452</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning representations for individual instances when only bag-level labels
are available is a fundamental challenge in multiple instance learning (MIL).
Recent works have shown promising results using contrastive self-supervised
learning (CSSL), which learns to push apart representations corresponding to
two different randomly-selected instances. Unfortunately, in real-world
applications such as medical image classification, there is often class
imbalance, so randomly-selected instances mostly belong to the same majority
class, which precludes CSSL from learning inter-class differences. To address
this issue, we propose a novel framework, Iterative Self-paced Supervised
Contrastive Learning for MIL Representations (ItS2CLR), which improves the
learned representation by exploiting instance-level pseudo labels derived from
the bag-level labels. The framework employs a novel self-paced sampling
strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three
medical datasets, showing that it improves the quality of instance-level pseudo
labels and representations, and outperforms existing MIL methods in terms of
both bag and instance level accuracy. Code is available at
https://github.com/Kangningthu/ItS2CLR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Weicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqiu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razavian_N/0/1/0/all/0/1&quot;&gt;Narges Razavian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1&quot;&gt;Krzysztof J. Geras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1&quot;&gt;Carlos Fernandez-Granda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13601">
<title>Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13601</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of active learning for single neuron models, also
sometimes called ``ridge functions&apos;&apos;, in the agnostic setting (under
adversarial label noise). Such models have been shown to be broadly effective
in modeling physical phenomena, and for constructing surrogate data-driven
models for partial differential equations.
&lt;/p&gt;
&lt;p&gt;Surprisingly, we show that for a single neuron model with any Lipschitz
non-linearity (such as the ReLU, sigmoid, absolute value, low-degree
polynomial, among others), strong provable approximation guarantees can be
obtained using a well-known active learning strategy for fitting \emph{linear
functions} in the agnostic setting. % -- i.e. for the case when there is no
non-linearity. Namely, we can collect samples via statistical \emph{leverage
score sampling}, which has been shown to be near-optimal in other active
learning scenarios. We support our theoretical results with empirical
simulations showing that our proposed active learning strategy based on
leverage score sampling outperforms (ordinary) uniform sampling when fitting
single neuron models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gajjar_A/0/1/0/all/0/1&quot;&gt;Aarshvi Gajjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1&quot;&gt;Christopher Musco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15097">
<title>Contrastive Decoding: Open-ended Text Generation as Optimization. (arXiv:2210.15097v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15097</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Lisa Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1&quot;&gt;Ari Holtzman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1&quot;&gt;Daniel Fried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_J/0/1/0/all/0/1&quot;&gt;Jason Eisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1&quot;&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Mike Lewis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01413">
<title>Harnessing the Power of Explanations for Incremental Training: A LIME-Based Approach. (arXiv:2211.01413v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01413</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability of neural network prediction is essential to understand
feature importance and gain interpretable insight into neural network
performance. However, explanations of neural network outcomes are mostly
limited to visualization, and there is scarce work that looks to use these
explanations as feedback to improve model performance. In this work, model
explanations are fed back to the feed-forward training to help the model
generalize better. To this extent, a custom weighted loss where the weights are
generated by considering the Euclidean distances between true LIME (Local
Interpretable Model-Agnostic Explanations) explanations and model-predicted
LIME explanations is proposed. Also, in practical training scenarios,
developing a solution that can help the model learn sequentially without losing
information on previous data distribution is imperative due to the
unavailability of all the training data at once. Thus, the framework
incorporates the custom weighted loss with Elastic Weight Consolidation (EWC)
to maintain performance in sequential testing sets. The proposed custom
training procedure results in a consistent enhancement of accuracy ranging from
0.5% to 1.5% throughout all phases of the incremental learning setup compared
to traditional loss-based training methods for the keyword spotting task using
the Google Speech Commands dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1&quot;&gt;Arnab Neelim Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyons_N/0/1/0/all/0/1&quot;&gt;Niall Lyons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_A/0/1/0/all/0/1&quot;&gt;Ashutosh Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santra_A/0/1/0/all/0/1&quot;&gt;Avik Santra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohsenin_T/0/1/0/all/0/1&quot;&gt;Tinoosh Mohsenin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01753">
<title>Looking Beyond IoCs: Automatically Extracting Attack Patterns from External CTI. (arXiv:2211.01753v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01753</link>
<description rdf:parseType="Literal">&lt;p&gt;Public and commercial organizations extensively share cyberthreat
intelligence (CTI) to prepare systems to defend against existing and emerging
cyberattacks. However, traditional CTI has primarily focused on tracking known
threat indicators such as IP addresses and domain names, which may not provide
long-term value in defending against evolving attacks. To address this
challenge, we propose to use more robust threat intelligence signals called
attack patterns. LADDER is a knowledge extraction framework that can extract
text-based attack patterns from CTI reports at scale. The framework
characterizes attack patterns by capturing the phases of an attack in Android
and enterprise networks and systematically maps them to the MITRE ATT\&amp;amp;CK
pattern framework. LADDER can be used by security analysts to determine the
presence of attack vectors related to existing and emerging threats, enabling
them to prepare defenses proactively. We also present several use cases to
demonstrate the application of LADDER in real-world scenarios. Finally, we
provide a new, open-access benchmark malware dataset to train future
cyberthreat intelligence models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md Tanvirul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhusal_D/0/1/0/all/0/1&quot;&gt;Dipkamal Bhusal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Youngja Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1&quot;&gt;Nidhi Rastogi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04125">
<title>Efficacy of MRI data harmonization in the age of machine learning. A multicenter study across 36 datasets. (arXiv:2211.04125v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04125</link>
<description rdf:parseType="Literal">&lt;p&gt;Pooling publicly-available MRI data from multiple sites allows to assemble
extensive groups of subjects, increase statistical power, and promote data
reuse with machine learning techniques. The harmonization of multicenter data
is necessary to reduce the confounding effect associated with non-biological
sources of variability in the data. However, when applied to the entire dataset
before machine learning, the harmonization leads to data leakage, because
information outside the training set may affect model building, and potentially
falsely overestimate performance. We propose a 1) measurement of the efficacy
of data harmonization; 2) harmonizer transformer, i.e., an implementation of
the ComBat harmonization allowing its encapsulation among the preprocessing
steps of a machine learning pipeline, avoiding data leakage. We tested these
tools using brain T1-weighted MRI data from 1740 healthy subjects acquired at
36 sites. After harmonization, the site effect was removed or reduced, and we
showed the data leakage effect in predicting individual age from MRI data,
highlighting that introducing the harmonizer transformer into a machine
learning pipeline allows for avoiding data leakage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marzi_C/0/1/0/all/0/1&quot;&gt;Chiara Marzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannelli_M/0/1/0/all/0/1&quot;&gt;Marco Giannelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barucci_A/0/1/0/all/0/1&quot;&gt;Andrea Barucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessa_C/0/1/0/all/0/1&quot;&gt;Carlo Tessa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascalchi_M/0/1/0/all/0/1&quot;&gt;Mario Mascalchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diciotti_S/0/1/0/all/0/1&quot;&gt;Stefano Diciotti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07625">
<title>What Images are More Memorable to Machines?. (arXiv:2211.07625v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07625</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of measuring and predicting how memorable an
image is to pattern recognition machines, as a path to explore machine
intelligence. Firstly, we propose a self-supervised machine memory
quantification pipeline, dubbed ``MachineMem measurer&apos;&apos;, to collect machine
memorability scores of images. Similar to humans, machines also tend to
memorize certain kinds of images, whereas the types of images that machines and
humans memorize are different. Through in-depth analysis and comprehensive
visualizations, we gradually unveil that``complex&quot; images are usually more
memorable to machines. We further conduct extensive experiments across 11
different machines (from linear classifiers to modern ViTs) and 9 pre-training
methods to analyze and understand machine memory. This work proposes the
concept of machine memorability and opens a new research direction at the
interface between machine memory and visual data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junlin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Huangying Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jie Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1&quot;&gt;Pengfei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10590">
<title>Bidirectional Generation of Structure and Properties Through a Single Molecular Foundation Model. (arXiv:2211.10590v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10590</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of large foundation models in artificial intelligence has
prompted the emergence of chemical pre-trained models. Despite the growing
interest in large molecular pre-trained models that provide informative
representations for downstream tasks, attempts for multimodal pre-training
approaches on the molecule domain were limited. To address this, we present a
novel multimodal molecular pre-trained model that incorporates the modalities
of structure and biochemical properties, drawing inspiration from recent
advances in multimodal learning techniques. Our proposed model pipeline of data
handling and training objectives aligns the structure/property features in a
common embedding space, which enables the model to regard bidirectional
information between the molecules&apos; structure and properties. These
contributions emerge synergistic knowledge, allowing us to tackle both
multimodal and unimodal downstream tasks through a single model. Through
extensive experiments, we demonstrate that our model shows remarkable
capabilities in solving various meaningful chemical challenges, including
conditional molecule generation, property prediction, molecule classification,
and reaction prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Jinho Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16237">
<title>Closing the gap between SVRG and TD-SVRG with Gradient Splitting. (arXiv:2211.16237v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16237</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal difference (TD) learning is a policy evaluation in reinforcement
learning whose performance can be enhanced by variance reduction techniques.
Recently, multiple works have sought to fuse TD learning with SVRG to obtain a
policy evaluation method with a geometric rate of convergence. However, the
resulting convergence rate is significantly weaker than what is achieved by
SVRG in the setting of convex optimization. In this work we utilize a recent
interpretation of TD-learning as the splitting of the gradient of an
appropriately chosen function, thus simplifying the algorithm and fusing TD
with SVRG. Our main result is a geometric convergence bound with predetermined
learning rate of $1/8$, which is identical to the convergence bound available
for SVRG in the convex setting. Our theoretical findings are supported by a set
of experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafin_A/0/1/0/all/0/1&quot;&gt;Arsenii Mustafin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olshevsky_A/0/1/0/all/0/1&quot;&gt;Alex Olshevsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschalidis_I/0/1/0/all/0/1&quot;&gt;Ioannis Ch. Paschalidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03529">
<title>Towards Fleet-wide Sharing of Wind Turbine Condition Information through Privacy-preserving Federated Learning. (arXiv:2212.03529v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03529</link>
<description rdf:parseType="Literal">&lt;p&gt;Terabytes of data are collected by wind turbine manufacturers from their
fleets every day. And yet, a lack of data access and sharing impedes exploiting
the full potential of the data. We present a distributed machine learning
approach that preserves the data privacy by leaving the data on the wind
turbines while still enabling fleet-wide learning on those local data. We show
that through federated fleet-wide learning, turbines with little or no
representative training data can benefit from more accurate normal behavior
models. Customizing the global federated model to individual turbines yields
the highest fault detection accuracy in cases where the monitored target
variable is distributed heterogeneously across the fleet. We demonstrate this
for bearing temperatures, a target variable whose normal behavior can vary
widely depending on the turbine. We show that no turbine experiences a loss in
model performance from participating in the federated learning process,
resulting in superior performance of the federated learning strategy in our
case studies. The distributed learning increases the normal behavior model
training times by about a factor of ten due to increased communication overhead
and slower model convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenkel_L/0/1/0/all/0/1&quot;&gt;Lorin Jenkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonas_S/0/1/0/all/0/1&quot;&gt;Stefan Jonas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1&quot;&gt;Angela Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10258">
<title>In and Out-of-Domain Text Adversarial Robustness via Label Smoothing. (arXiv:2212.10258v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10258</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yahan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1&quot;&gt;Soham Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Insup Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06024">
<title>A data science and machine learning approach to continuous analysis of Shakespeare&apos;s plays. (arXiv:2301.06024v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06024</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of quantitative text analysis methods has provided new ways
of analyzing literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear changes in the style
of writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare&apos;s writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swisher_C/0/1/0/all/0/1&quot;&gt;Charles Swisher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1&quot;&gt;Lior Shamir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11873">
<title>A Deep Learning Method for Comparing Bayesian Hierarchical Models. (arXiv:2301.11873v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11873</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian model comparison (BMC) offers a principled approach for assessing
the relative merits of competing computational models and propagating
uncertainty into model selection decisions. However, BMC is often intractable
for the popular class of hierarchical models due to their high-dimensional
nested parameter structure. To address this intractability, we propose a deep
learning method for performing BMC on any set of hierarchical models which can
be instantiated as probabilistic programs. Since our method enables amortized
inference, it allows efficient re-estimation of posterior model probabilities
and fast performance validation prior to any real-data application. In a series
of extensive validation studies, we benchmark the performance of our method
against the state-of-the-art bridge sampling method and demonstrate excellent
amortized inference across all BMC settings. We then showcase our method by
comparing four hierarchical evidence accumulation models that have previously
been deemed intractable for BMC due to partly implicit likelihoods. In this
application, we corroborate evidence for the recently proposed L\&apos;evy flight
model of decision-making and show how transfer learning can be leveraged to
enhance training efficiency. We provide reproducible code for all analyses and
an open-source implementation of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Elsemuller_L/0/1/0/all/0/1&quot;&gt;Lasse Elsem&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schnuerch_M/0/1/0/all/0/1&quot;&gt;Martin Schnuerch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Burkner_P/0/1/0/all/0/1&quot;&gt;Paul-Christian B&amp;#xfc;rkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Radev_S/0/1/0/all/0/1&quot;&gt;Stefan T. Radev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12386">
<title>Plugin estimators for selective classification with out-of-distribution detection. (arXiv:2301.12386v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12386</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world classifiers can benefit from the option of abstaining from
predicting on samples where they have low confidence. Such abstention is
particularly useful on samples which are close to the learned decision
boundary, or which are outliers with respect to the training sample. These
settings have been the subject of extensive but disjoint study in the selective
classification (SC) and out-of-distribution (OOD) detection literature. Recent
work on selective classification with OOD detection (SCOD) has argued for the
unified study of these problems; however, the formal underpinnings of this
problem are still nascent, and existing techniques are heuristic in nature. In
this paper, we propose new plugin estimators for SCOD that are theoretically
grounded, effective, and generalise existing approaches from the SC and OOD
detection literature. In the course of our analysis, we formally explicate how
na\&quot;{i}ve use of existing SC and OOD detection baselines may be inadequate for
SCOD. We empirically demonstrate that our approaches yields competitive SC and
OOD detection performance compared to baselines from both literatures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1&quot;&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1&quot;&gt;Wittawat Jitkrittum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00766">
<title>Privacy Risk for anisotropic Langevin dynamics using relative entropy bounds. (arXiv:2302.00766v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00766</link>
<description rdf:parseType="Literal">&lt;p&gt;The privacy preserving properties of Langevin dynamics with additive
isotropic noise have been extensively studied. However, the isotropic noise
assumption is very restrictive: (a) when adding noise to existing learning
algorithms to preserve privacy and maintain the best possible accuracy one
should take into account the relative magnitude of the outputs and their
correlations; (b) popular algorithms such as stochastic gradient descent (and
their continuous time limits) appear to possess anisotropic covariance
properties. To study the privacy risks for the anisotropic noise case, one
requires general results on the relative entropy between the laws of two
Stochastic Differential Equations with different drifts and diffusion
coefficients. Our main contribution is to establish such a bound using
stability estimates for solutions to the Fokker-Planck equations via functional
inequalities. With additional assumptions, the relative entropy bound implies
an $(\epsilon,\delta)$-differential privacy bound or translates to bounds on
the membership inference attack success and we show how anisotropic noise can
lead to better privacy-accuracy trade-offs. Finally, the benefits of
anisotropic noise are illustrated using numerical results in quadratic loss and
neural network setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borovykh_A/0/1/0/all/0/1&quot;&gt;Anastasia Borovykh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kantas_N/0/1/0/all/0/1&quot;&gt;Nikolas Kantas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parpas_P/0/1/0/all/0/1&quot;&gt;Panos Parpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavliotis_G/0/1/0/all/0/1&quot;&gt;Greg Pavliotis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01075">
<title>MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein Gradient Flows. (arXiv:2302.01075v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01075</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional understanding of adversarial training in generative
adversarial networks (GANs) is that the discriminator is trained to estimate a
divergence, and the generator learns to minimize this divergence. We argue that
despite the fact that many variants of GANs were developed following this
paradigm, the current theoretical understanding of GANs and their practical
algorithms are inconsistent. In this paper, we leverage Wasserstein gradient
flows which characterize the evolution of particles in the sample space, to
gain theoretical insights and algorithmic inspiration of GANs. We introduce a
unified generative modeling framework - MonoFlow: the particle evolution is
rescaled via a monotonically increasing mapping of the log density ratio. Under
our framework, adversarial training can be viewed as a procedure first
obtaining MonoFlow&apos;s vector field via training the discriminator and the
generator learns to draw the particle flow defined by the corresponding vector
field. We also reveal the fundamental difference between variational divergence
minimization and adversarial training. This analysis helps us to identify what
types of generator loss functions can lead to the successful training of GANs
and suggest that GANs may have more loss designs beyond the literature (e.g.,
non-saturated loss), as long as they realize MonoFlow. Consistent empirical
studies are included to validate the effectiveness of our framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yi_M/0/1/0/all/0/1&quot;&gt;Mingxuan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Song Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01241">
<title>Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01241</link>
<description rdf:parseType="Literal">&lt;p&gt;Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1&quot;&gt;Brian Y. Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cahaly_J/0/1/0/all/0/1&quot;&gt;Joseph P. Cahaly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sng_C/0/1/0/all/0/1&quot;&gt;Chester Y. F. Sng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chew_A/0/1/0/all/0/1&quot;&gt;Adam Chew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02988">
<title>Asymptotically Optimal Fixed-Budget Best Arm Identification with Variance-Dependent Bounds. (arXiv:2302.02988v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02988</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of fixed-budget best arm identification (BAI) for
minimizing expected simple regret. In an adaptive experiment, a decision maker
draws one of multiple treatment arms based on past observations and observes
the outcome of the drawn arm. After the experiment, the decision maker
recommends the treatment arm with the highest expected outcome. We evaluate the
decision based on the expected simple regret, which is the difference between
the expected outcomes of the best arm and the recommended arm. Due to inherent
uncertainty, we evaluate the regret using the minimax criterion. First, we
derive asymptotic lower bounds for the worst-case expected simple regret, which
are characterized by the variances of potential outcomes (leading factor).
Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder
(HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in
recommending the best arm. Our theoretical analysis shows that the TS-HIR
strategy is asymptotically minimax optimal, meaning that the leading factor of
its worst-case expected simple regret matches our derived worst-case lower
bound. Additionally, we consider extensions of our method, such as the
asymptotic optimality for the probability of misidentification. Finally, we
validate the proposed method&apos;s effectiveness through simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1&quot;&gt;Masahiro Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imaizumi_M/0/1/0/all/0/1&quot;&gt;Masaaki Imaizumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishihara_T/0/1/0/all/0/1&quot;&gt;Takuya Ishihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitagawa_T/0/1/0/all/0/1&quot;&gt;Toru Kitagawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06375">
<title>One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06375</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luetto_S/0/1/0/all/0/1&quot;&gt;Simone Luetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garuti_F/0/1/0/all/0/1&quot;&gt;Fabrizio Garuti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1&quot;&gt;Enver Sangineto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forni_L/0/1/0/all/0/1&quot;&gt;Lorenzo Forni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10873">
<title>Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction. (arXiv:2302.10873v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10873</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time, accurate prediction of human steering behaviors has wide
applications, from developing intelligent traffic systems to deploying
autonomous driving systems in both real and simulated worlds. In this paper, we
present ContextVAE, a context-aware approach for multi-modal vehicle trajectory
prediction. Built upon the backbone architecture of a timewise variational
autoencoder, ContextVAE observation encoding employs a dual attention mechanism
that accounts for the environmental context and the dynamic agents&apos; states, in
a unified way. By utilizing features extracted from semantic maps during agent
state encoding, our approach takes into account both the social features
exhibited by agents on the scene and the physical environment constraints to
generate map-compliant and socially-aware trajectories. We perform extensive
testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo
Open Motion Dataset to show the effectiveness of our approach and its
state-of-the-art performance. In all tested datasets, ContextVAE models are
fast to train and provide high-quality multi-modal predictions in real-time.
Our code is available at: https://github.com/xupei0610/ContextVAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1&quot;&gt;Jean-Bernard Hayet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1&quot;&gt;Ioannis Karamouzas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10975">
<title>Improved uncertainty quantification for neural networks with Bayesian last layer. (arXiv:2302.10975v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10975</link>
<description rdf:parseType="Literal">&lt;p&gt;Uncertainty quantification is an essential task in machine learning - a task
in which neural networks (NNs) have traditionally not excelled. This can be a
limitation for safety-critical applications, where uncertainty-aware methods
like Gaussian processes or Bayesian linear regression are often preferred.
Bayesian neural networks are an approach to address this limitation. They
assume probability distributions for all parameters and yield distributed
predictions. However, training and inference are typically intractable and
approximations must be employed. A promising approximation is NNs with Bayesian
last layer (BLL). They assume distributed weights only in the last linear layer
and yield a normally distributed prediction. NNs with BLL can be seen as a
Bayesian linear regression model with learned nonlinear features. To
approximate the intractable Bayesian neural network, point estimates of the
distributed weights in all but the last layer should be obtained by maximizing
the marginal likelihood. This has previously been challenging, as the marginal
likelihood is expensive to evaluate in this setting and prohibits direct
training through backpropagation. We present a reformulation of the
log-marginal likelihood of a NN with BLL which allows for efficient training
using backpropagation. Furthermore, we address the challenge of quantifying
uncertainty for extrapolation points. We provide a metric to quantify the
degree of extrapolation and derive a method to improve the uncertainty
quantification for these points. Our methods are derived for the multivariate
case and demonstrated in a simulation study, where we compare Bayesian linear
regression applied to a previously trained neural network with our proposed
algorithm
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiedler_F/0/1/0/all/0/1&quot;&gt;Felix Fiedler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucia_S/0/1/0/all/0/1&quot;&gt;Sergio Lucia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12559">
<title>From Noisy Fixed-Point Iterations to Private ADMM for Centralized and Federated Learning. (arXiv:2302.12559v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12559</link>
<description rdf:parseType="Literal">&lt;p&gt;We study differentially private (DP) machine learning algorithms as instances
of noisy fixed-point iterations, in order to derive privacy and utility results
from this well-studied framework. We show that this new perspective recovers
popular private gradient-based methods like DP-SGD and provides a principled
way to design and analyze new private optimization algorithms in a flexible
manner. Focusing on the widely-used Alternating Directions Method of
Multipliers (ADMM) method, we use our general framework to derive novel private
ADMM algorithms for centralized, federated and fully decentralized learning.
For these three algorithms, we establish strong privacy guarantees leveraging
privacy amplification by iteration and by subsampling. Finally, we provide
utility guarantees using a unified analysis that exploits a recent linear
convergence result for noisy fixed-point iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cyffers_E/0/1/0/all/0/1&quot;&gt;Edwige Cyffers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_D/0/1/0/all/0/1&quot;&gt;Debabrota Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12808">
<title>Linearization Algorithms for Fully Composite Optimization. (arXiv:2302.12808v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12808</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies first-order algorithms for solving fully composite
optimization problems over convex and compact sets. We leverage the structure
of the objective by handling its differentiable and non-differentiable
components separately, linearizing only the smooth parts. This provides us with
new generalizations of the classical Frank-Wolfe method and the Conditional
Gradient Sliding algorithm, that cater to a subclass of non-differentiable
problems. Our algorithms rely on a stronger version of the linear minimization
oracle, which can be efficiently implemented in several practical applications.
We provide the basic version of our method with an affine-invariant analysis
and prove global convergence rates for both convex and non-convex objectives.
Furthermore, in the convex case, we propose an accelerated method with
correspondingly improved complexity. Finally, we provide illustrative
experiments to support our theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Vladarean_M/0/1/0/all/0/1&quot;&gt;Maria-Luiza Vladarean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doikov_N/0/1/0/all/0/1&quot;&gt;Nikita Doikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Flammarion_N/0/1/0/all/0/1&quot;&gt;Nicolas Flammarion&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13152">
<title>On Bellman&apos;s principle of optimality and Reinforcement learning for safety-constrained Markov decision process. (arXiv:2302.13152v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13152</link>
<description rdf:parseType="Literal">&lt;p&gt;We study optimality for the safety-constrained Markov decision process which
is the underlying framework for safe reinforcement learning. Specifically, we
consider a constrained Markov decision process (with finite states and finite
actions) where the goal of the decision maker is to reach a target set while
avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the
underlying Markov chain for any control policy will be multichain since by
definition there exists a target set and an unsafe set. The decision maker also
has to be optimal (with respect to a cost function) while navigating to the
target set. This gives rise to a multi-objective optimization problem. We
highlight the fact that Bellman&apos;s principle of optimality may not hold for
constrained Markov decision problems with an underlying multichain structure
(as shown by the counterexample due to Haviv. We resolve the counterexample by
formulating the aforementioned multi-objective optimization problem as a
zero-sum game and thereafter construct an asynchronous value iteration scheme
for the Lagrangian (similar to Shapley&apos;s algorithm). Finally, we consider the
reinforcement learning problem for the same and construct a modified
$Q$-learning algorithm for learning the Lagrangian from data. We also provide a
lower bound on the number of iterations required for learning the Lagrangian
and corresponding error bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Misra_R/0/1/0/all/0/1&quot;&gt;Rahul Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wisniewski_R/0/1/0/all/0/1&quot;&gt;Rafa&amp;#x142; Wisniewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kallesoe_C/0/1/0/all/0/1&quot;&gt;Carsten Skovmose Kalles&amp;#xf8;e&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13948">
<title>Supervised topological data analysis for MALDI mass spectrometry imaging applications. (arXiv:2302.13948v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13948</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Matrix-assisted laser desorption/ionization mass spectrometry
imaging (MALDI MSI) displays significant potential for applications in cancer
research, especially in tumor typing and subtyping. Lung cancer is the primary
cause of tumor-related deaths, where the most lethal entities are
adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between
these two common subtypes is crucial for therapy decisions and successful
patient management.
&lt;/p&gt;
&lt;p&gt;Results: We propose a new algebraic topological framework, which obtains
intrinsic information from MALDI data and transforms it to reflect topological
persistence. Our framework offers two main advantages. Firstly, topological
persistence aids in distinguishing the signal from noise. Secondly, it
compresses the MALDI data, saving storage space and optimizes computational
time for subsequent classification tasks. We present an algorithm that
efficiently implements our topological framework, relying on a single tuning
parameter. Afterwards, logistic regression and random forest classifiers are
employed on the extracted persistence features, thereby accomplishing an
automated tumor (sub-)typing process. To demonstrate the competitiveness of our
proposed framework, we conduct experiments on a real-world MALDI dataset using
cross-validation. Furthermore, we showcase the effectiveness of the single
denoising parameter by evaluating its performance on synthetic MALDI images
with varying levels of noise.
&lt;/p&gt;
&lt;p&gt;Conclusion: Our empirical experiments demonstrate that the proposed algebraic
topological framework successfully captures and leverages the intrinsic
spectral information from MALDI data, leading to competitive results in
classifying lung cancer subtypes. Moreover, the frameworks ability to be
fine-tuned for denoising highlights its versatility and potential for enhancing
data analysis in MALDI applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klaila_G/0/1/0/all/0/1&quot;&gt;Gideon Klaila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vutov_V/0/1/0/all/0/1&quot;&gt;Vladimir Vutov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stefanou_A/0/1/0/all/0/1&quot;&gt;Anastasios Stefanou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01388">
<title>Reinforced Labels: Multi-Agent Deep Reinforcement Learning for Point-Feature Label Placement. (arXiv:2303.01388v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01388</link>
<description rdf:parseType="Literal">&lt;p&gt;Over recent years, Reinforcement Learning combined with Deep Learning
techniques has successfully proven to solve complex problems in various
domains, including robotics, self-driving cars, and finance. In this paper, we
are introducing Reinforcement Learning (RL) to label placement, a complex task
in data visualization that seeks optimal positioning for labels to avoid
overlap and ensure legibility. Our novel point-feature label placement method
utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label
placement strategy, which is the first machine-learning-driven labeling method
in contrast to existing hand-crafted algorithms designed by human experts. To
facilitate RL learning, we developed an environment where an agent acts as a
proxy for a label, a short textual annotation that augments visualization. Our
results show that the strategy trained by our method significantly outperforms
the random strategy of an untrained agent and compared methods designed by
human experts in terms of completeness (i.e., the number of placed labels). The
trade-off is increased computation time, making the proposed method slower than
compared methods. Nevertheless, our method is ideal for scenarios where the
labeling can be computed in advance, and completeness is essential, such as
cartographic maps, technical drawings, and medical atlases. Additionally, we
conducted a user study to assess the perceived performance. The outcomes
revealed that the participants considered the proposed method to be
significantly better than the other examined methods. This indicates that the
improved completeness is not just reflected in the quantitative metrics but
also in the subjective evaluation of the participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobak_P/0/1/0/all/0/1&quot;&gt;Petr Bob&amp;#xe1;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cmolik_L/0/1/0/all/0/1&quot;&gt;Ladislav &amp;#x10c;mol&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cadik_M/0/1/0/all/0/1&quot;&gt;Martin &amp;#x10c;ad&amp;#xed;k&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04878">
<title>DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks. (arXiv:2303.04878v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04878</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are widely used in various application domains
such as image processing, speech recognition, and natural language processing.
However, testing DNN models may be challenging due to the complexity and size
of their input domain. Particularly, testing DNN models often requires
generating or exploring large unlabeled datasets. In practice, DNN test
oracles, which identify the correct outputs for inputs, often require expensive
manual effort to label test data, possibly involving multiple experts to ensure
labeling correctness. In this paper, we propose DeepGD, a black-box
multi-objective test selection approach for DNN models. It reduces the cost of
labeling by prioritizing the selection of test inputs with high fault revealing
power from large unlabeled datasets. DeepGD not only selects test inputs with
high uncertainty scores to trigger as many mispredicted inputs as possible but
also maximizes the probability of revealing distinct faults in the DNN model by
selecting diverse mispredicted inputs. The experimental results conducted on
four widely used datasets and five DNN models show that in terms of
fault-revealing ability: (1) White-box, coverage-based approaches fare poorly,
(2) DeepGD outperforms existing black-box test selection approaches in terms of
fault detection, and (3) DeepGD also leads to better guidance for DNN model
retraining when using selected inputs to augment the training set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghababaeyan_Z/0/1/0/all/0/1&quot;&gt;Zohreh Aghababaeyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdellatif_M/0/1/0/all/0/1&quot;&gt;Manel Abdellatif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadkhah_M/0/1/0/all/0/1&quot;&gt;Mahboubeh Dadkhah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1&quot;&gt;Lionel Briand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02819">
<title>GPT detectors are biased against non-native English writers. (arXiv:2304.02819v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02819</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse. The published
version of this study can be accessed at:
www.cell.com/patterns/fulltext/S2666-3899(23)00130-7
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weixin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuksekgonul_M/0/1/0/all/0/1&quot;&gt;Mert Yuksekgonul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yining Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1&quot;&gt;Eric Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06848">
<title>CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06848</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of &quot;anytime
regularized determinized sparse partially observable tree&quot; (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannizzaro_R/0/1/0/all/0/1&quot;&gt;Ricardo Cannizzaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1&quot;&gt;Lars Kunze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08172">
<title>Pointwise convergence theorem of gradient descent in sparse deep neural network. (arXiv:2304.08172v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08172</link>
<description rdf:parseType="Literal">&lt;p&gt;The theoretical structure of deep neural network (DNN) has been clarified
gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the
learning ability of DNN is superior to the previous theories when the target
function is non-smooth functions. However, as far as the author is aware, none
of the numerous works to date attempted to mathematically investigate what kind
of DNN architectures really induce pointwise convergence of gradient descent
(without any statistical argument), and this attempt seems to be closer to the
practical DNNs. In this paper we restrict target functions to non-smooth
indicator functions, and construct a deep neural network inducing pointwise
convergence provided by gradient descent process in ReLU-DNN. The DNN has a
sparse and a special shape, with certain variable transformations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoneda_T/0/1/0/all/0/1&quot;&gt;Tsuyoshi Yoneda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08309">
<title>Promises and Pitfalls of the Linearized Laplace in Bayesian Optimization. (arXiv:2304.08309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08309</link>
<description rdf:parseType="Literal">&lt;p&gt;The linearized-Laplace approximation (LLA) has been shown to be effective and
efficient in constructing Bayesian neural networks. It is theoretically
compelling since it can be seen as a Gaussian process posterior with the mean
function given by the neural network&apos;s maximum-a-posteriori predictive function
and the covariance function induced by the empirical neural tangent kernel.
However, while its efficacy has been studied in large-scale tasks like image
classification, it has not been studied in sequential decision-making problems
like Bayesian optimization where Gaussian processes -- with simple mean
functions and kernels such as the radial basis function -- are the de-facto
surrogate models. In this work, we study the usefulness of the LLA in Bayesian
optimization and highlight its strong performance and flexibility. However, we
also present some pitfalls that might arise and a potential problem with the
LLA when the search space is unbounded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1&quot;&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Immer_A/0/1/0/all/0/1&quot;&gt;Alexander Immer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eschenhagen_R/0/1/0/all/0/1&quot;&gt;Runa Eschenhagen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10701">
<title>Matching-based Data Valuation for Generative Model. (arXiv:2304.10701v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10701</link>
<description rdf:parseType="Literal">&lt;p&gt;Data valuation is critical in machine learning, as it helps enhance model
transparency and protect data properties. Existing data valuation methods have
primarily focused on discriminative models, neglecting deep generative models
that have recently gained considerable attention. Similar to discriminative
models, there is an urgent need to assess data contributions in deep generative
models as well. However, previous data valuation approaches mainly relied on
discriminative model performance metrics and required model retraining.
Consequently, they cannot be applied directly and efficiently to recent deep
generative models, such as generative adversarial networks and diffusion
models, in practice. To bridge this gap, we formulate the data valuation
problem in generative models from a similarity-matching perspective.
Specifically, we introduce Generative Model Valuator (GMValuator), the first
model-agnostic approach for any generative models, designed to provide data
valuation for generation tasks. We have conducted extensive experiments to
demonstrate the effectiveness of the proposed method. To the best of their
knowledge, GMValuator is the first work that offers a training-free, post-hoc
data valuation strategy for deep generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaxi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wenglong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Benlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yangsibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14108">
<title>DataComp: In search of the next generation of multimodal datasets. (arXiv:2304.14108v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14108</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal datasets are a critical component in recent breakthroughs such as
Stable Diffusion and GPT-4, yet their design does not receive the same research
attention as model architectures or training algorithms. To address this
shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset
experiments centered around a new candidate pool of 12.8 billion image-text
pairs from Common Crawl. Participants in our benchmark design new filtering
techniques or curate new data sources and then evaluate their new dataset by
running our standardized CLIP training code and testing the resulting model on
38 downstream test sets. Our benchmark consists of multiple compute scales
spanning four orders of magnitude, which enables the study of scaling trends
and makes the benchmark accessible to researchers with varying resources. Our
baseline experiments show that the DataComp workflow leads to better training
sets. In particular, our best baseline, DataComp-1B, enables training a CLIP
ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming
OpenAI&apos;s CLIP ViT-L/14 by 3.7 percentage points while using the same training
procedure and compute. We release DataComp and all accompanying code at
www.datacomp.ai.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Yitzhak Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_A/0/1/0/all/0/1&quot;&gt;Alex Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayase_J/0/1/0/all/0/1&quot;&gt;Jonathan Hayase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smyrnis_G/0/1/0/all/0/1&quot;&gt;Georgios Smyrnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marten_R/0/1/0/all/0/1&quot;&gt;Ryan Marten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1&quot;&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1&quot;&gt;Dhruba Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orgad_E/0/1/0/all/0/1&quot;&gt;Eyal Orgad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1&quot;&gt;Rahim Entezari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1&quot;&gt;Giannis Daras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratt_S/0/1/0/all/0/1&quot;&gt;Sarah Pratt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1&quot;&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1&quot;&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vencu_R/0/1/0/all/0/1&quot;&gt;Richard Vencu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1&quot;&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1&quot;&gt;Olga Saukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaumont_R/0/1/0/all/0/1&quot;&gt;Romain Beaumont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sewoong Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1&quot;&gt;Alex Dimakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1&quot;&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1&quot;&gt;Yair Carmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shankar_V/0/1/0/all/0/1&quot;&gt;Vaishaal Shankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02309">
<title>CodeGen2: Lessons for Training LLMs on Programming and Natural Languages. (arXiv:2305.02309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02309</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable abilities in
representation learning for program synthesis and understanding tasks. The
quality of the learned representations appears to be dictated by the neural
scaling laws as a function of the number of model parameters and observations,
while imposing upper bounds on the model performance by the amount of available
data and compute, which is costly.
&lt;/p&gt;
&lt;p&gt;In this study, we attempt to render the training of LLMs for program
synthesis more efficient by unifying four key components: (1) model
architectures, (2) learning methods, (3) infill sampling, and, (4) data
distributions. Specifically, for the model architecture, we attempt to unify
encoder and decoder-based models into a single prefix-LM. For learning methods,
(i) causal language modeling, (ii) span corruption, (iii) infilling are unified
into a simple learning algorithm. For infill sampling, we explore the claim of
a &quot;free lunch&quot; hypothesis. For data distributions, the effect of a mixture
distribution and multi-epoch training of programming and natural languages on
model performance is explored.
&lt;/p&gt;
&lt;p&gt;We conduct a comprehensive series of empirical experiments on 1B LLMs, for
which failures and successes of this exploration are distilled into five
lessons. We will provide a final recipe for training and release CodeGen2
models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training
framework as open-source: https://github.com/salesforce/CodeGen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1&quot;&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1&quot;&gt;Hiroaki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04003">
<title>ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for Verification. (arXiv:2305.04003v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04003</link>
<description rdf:parseType="Literal">&lt;p&gt;Verification of machine learning models used in Natural Language Processing
(NLP) is known to be a hard problem. In particular, many known neural network
verification methods that work for computer vision and other numeric datasets
do not work for NLP. Here, we study technical reasons that underlie this
problem. Based on this analysis, we propose practical methods and heuristics
for preparing NLP datasets and models in a way that renders them amenable to
known verification methods based on abstract interpretation. We implement these
methods as a Python library called ANTONIO that links to the neural network
verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP
dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP
applications. We hope that, thanks to its general applicability, this work will
open novel possibilities for including NLP verification problems into neural
network verification competitions, and will popularise NLP problems within this
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casadio_M/0/1/0/all/0/1&quot;&gt;Marco Casadio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaboldi_L/0/1/0/all/0/1&quot;&gt;Luca Arnaboldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1&quot;&gt;Matthew L. Daggitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isac_O/0/1/0/all/0/1&quot;&gt;Omri Isac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1&quot;&gt;Tanvi Dinkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienitz_D/0/1/0/all/0/1&quot;&gt;Daniel Kienitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1&quot;&gt;Verena Rieser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Komendantskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04492">
<title>MGR: Multi-generator Based Rationalization. (arXiv:2305.04492v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04492</link>
<description rdf:parseType="Literal">&lt;p&gt;Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuankai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yang Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06802">
<title>Physics-Informed Neural Networks for Discovering Localised Eigenstates in Disordered Media. (arXiv:2305.06802v2 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06802</link>
<description rdf:parseType="Literal">&lt;p&gt;The Schr\&quot;{o}dinger equation with random potentials is a fundamental model
for understanding the behaviour of particles in disordered systems. Disordered
media are characterised by complex potentials that lead to the localisation of
wavefunctions, also called Anderson localisation. These wavefunctions may have
similar scales of eigenenergies which poses difficulty in their discovery. It
has been a longstanding challenge due to the high computational cost and
complexity of solving the Schr\&quot;{o}dinger equation. Recently, machine-learning
tools have been adopted to tackle these challenges. In this paper, based upon
recent advances in machine learning, we present a novel approach for
discovering localised eigenstates in disordered media using physics-informed
neural networks (PINNs). We focus on the spectral approximation of Hamiltonians
in one dimension with potentials that are randomly generated according to the
Bernoulli, normal, and uniform distributions. We introduce a novel feature to
the loss function that exploits known physical phenomena occurring in these
regions to scan across the domain and successfully discover these eigenstates,
regardless of the similarity of their eigenenergies. We present various
examples to demonstrate the performance of the proposed approach and compare it
with isogeometric analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Harcombe_L/0/1/0/all/0/1&quot;&gt;Liam Harcombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Deng_Q/0/1/0/all/0/1&quot;&gt;Quanling Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12529">
<title>DreamWaltz: Make a Scene with Complex 3D Animatable Avatars. (arXiv:2305.12529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12529</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DreamWaltz, a novel framework for generating and animating complex
3D avatars given text guidance and parametric human body prior. While recent
methods have shown encouraging results for text-to-3D generation of common
objects, creating high-quality and animatable 3D avatars remains challenging.
To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent
occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural
representations with canonical poses. It provides view-aligned supervision via
3D-aware skeleton conditioning which enables complex avatar generation without
artifacts and multiple faces. For animation, our method learns an animatable
and generalizable avatar representation which could map arbitrary poses to the
canonical pose representation. Extensive evaluations demonstrate that
DreamWaltz is an effective and robust approach for creating 3D avatars that can
take on complex shapes and appearances as well as novel poses for animation.
The proposed framework further enables the creation of complex scenes with
diverse compositions, including avatar-avatar, avatar-object and avatar-scene
interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and
animation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yukun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;He Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xianbiao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yukai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14961">
<title>Deep Learning for Survival Analysis: A Review. (arXiv:2305.14961v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14961</link>
<description rdf:parseType="Literal">&lt;p&gt;The influx of deep learning (DL) techniques into the field of survival
analysis in recent years has led to substantial methodological progress; for
instance, learning from unstructured or high-dimensional data such as images,
text or omics data. In this work, we conduct a comprehensive systematic review
of DL-based methods for time-to-event analysis, characterizing them according
to both survival- and DL-related attributes. In summary, the reviewed methods
often address only a small subset of tasks relevant to time-to-event data -
e.g., single-risk right-censored data - and neglect to incorporate more complex
settings. Our findings are summarized in an editable, open-source, interactive
table: https://survival-org.github.io/DL4Survival. As this research area is
advancing rapidly, we encourage community contribution in order to keep this
database up to date.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wiegrebe_S/0/1/0/all/0/1&quot;&gt;Simon Wiegrebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kopper_P/0/1/0/all/0/1&quot;&gt;Philipp Kopper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sonabend_R/0/1/0/all/0/1&quot;&gt;Raphael Sonabend&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bender_A/0/1/0/all/0/1&quot;&gt;Andreas Bender&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19779">
<title>Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19779</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based disease mapping remains a fundamental policy-informing tool in
public health and disease surveillance. Hierarchical Bayesian models have
become the state-of-the-art approach for disease mapping since they are able to
capture structure in the data, as well as to characterise uncertainty. When
working with areal data, e.g.~aggregates at the administrative unit level such
as district or province, routinely used models rely on the adjacency structure
of areal units to account for spatial correlations. The goal of disease
surveillance systems is to track disease outcomes over time. This task provides
challenging in situations of crises, such as political changes, leading to
changes of administrative boundaries. Kenya is an example of a country where
change of boundaries took place in 2010. Moreover, the adjacency-based approach
ignores the continuous nature of spatial processes and cannot solve the
change-of-support problem, i.e.~when administrative boundaries change or when
estimates must be produced at a different administrative level. We present a
novel, practical, and easy to implement solution relying on a methodology
combining deep generative modelling and fully Bayesian inference: we build on
the recently proposed PriorVAE method able to encode spatial priors over small
areas with variational autoencoders, to map malaria prevalence in Kenya.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Semenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Swapnil Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1&quot;&gt;Samir Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unwin_H/0/1/0/all/0/1&quot;&gt;H Juliette T Unwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02121">
<title>Identifying Subgroups of ICU Patients Using End-to-End Multivariate Time-Series Clustering Algorithm Based on Real-World Vital Signs Data. (arXiv:2306.02121v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02121</link>
<description rdf:parseType="Literal">&lt;p&gt;This study employed the MIMIC-IV database as data source to investigate the
use of dynamic, high-frequency, multivariate time-series vital signs data,
including temperature, heart rate, mean blood pressure, respiratory rate, and
SpO2, monitored first 8 hours data in the ICU stay. Various clustering
algorithms were compared, and an end-to-end multivariate time series clustering
system called Time2Feat, combined with K-Means, was chosen as the most
effective method to cluster patients in the ICU. In clustering analysis, data
of 8,080 patients admitted between 2008 and 2016 was used for model development
and 2,038 patients admitted between 2017 and 2019 for model validation. By
analyzing the differences in clinical mortality prognosis among different
categories, varying risks of ICU mortality and hospital mortality were found
between different subgroups. Furthermore, the study visualized the trajectory
of vital signs changes. The findings of this study provide valuable insights
into the potential use of multivariate time-series clustering systems in
patient management and monitoring in the ICU setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tongyue Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Junhua Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianguo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shuai Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huiying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_G/0/1/0/all/0/1&quot;&gt;Guilan Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02422">
<title>A Generalized Alternating Method for Bilevel Learning under the Polyak-{\L}ojasiewicz Condition. (arXiv:2306.02422v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02422</link>
<description rdf:parseType="Literal">&lt;p&gt;Bilevel optimization has recently regained interest owing to its applications
in emerging machine learning fields such as hyperparameter optimization,
meta-learning, and reinforcement learning. Recent results have shown that
simple alternating (implicit) gradient-based algorithms can achieve the same
convergence rate of single-level gradient descent (GD) for bilevel problems
with a strongly convex lower-level objective. However, it remains unclear
whether this result can be generalized to bilevel problems beyond this basic
setting. In this paper, we propose a Generalized ALternating mEthod for bilevel
opTimization (GALET) with a nonconvex lower-level objective that satisfies the
Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric
for the considered bilevel problems, which generalizes the existing metric. We
then establish that GALET achieves an $\epsilon$-stationary metric for the
considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which
matches the iteration complexity of GD for smooth nonconvex problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Quan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Songtao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03372">
<title>Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03372</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate a generalized framework for estimating latent low-rank tensors
in an online setting, encompassing both linear and generalized linear models.
This framework offers a flexible approach for handling continuous or
categorical variables. Additionally, we investigate two specific applications:
online tensor completion and online binary tensor learning. To address these
challenges, we propose the online Riemannian gradient descent algorithm, which
demonstrates linear convergence and the ability to recover the low-rank
component under appropriate conditions in all applications. Furthermore, we
establish a precise entry-wise error bound for online tensor completion.
Notably, our work represents the first attempt to incorporate noise in the
online low-rank tensor recovery task. Intriguingly, we observe a surprising
trade-off between computational and statistical aspects in the presence of
noise. Increasing the step size accelerates convergence but leads to higher
statistical error, whereas a smaller step size yields a statistically optimal
estimator at the expense of slower convergence. Moreover, we conduct regret
analysis for online tensor regression. Under the fixed step size regime, a
fascinating trilemma concerning the convergence rate, statistical error rate,
and regret is observed. With an optimal choice of step size we achieve an
optimal regret of $O(\sqrt{T})$. Furthermore, we extend our analysis to the
adaptive setting where the horizon T is unknown. In this case, we demonstrate
that by employing different step sizes, we can attain a statistically optimal
error rate along with a regret of $O(\log T)$. To validate our theoretical
claims, we provide numerical results that corroborate our findings and support
our assertions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jian-Feng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04026">
<title>Value Functions are Control Barrier Functions: Verification of Safe Policies using Control Theory. (arXiv:2306.04026v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04026</link>
<description rdf:parseType="Literal">&lt;p&gt;Guaranteeing safe behaviour of reinforcement learning (RL) policies poses
significant challenges for safety-critical applications, despite RL&apos;s
generality and scalability. To address this, we propose a new approach to apply
verification methods from control theory to learned value functions. By
analyzing task structures for safety preservation, we formalize original
theorems that establish links between value functions and control barrier
functions. Further, we propose novel metrics for verifying value functions in
safe control tasks and practical implementation details to improve learning.
Our work presents a novel method for certificate learning, which unlocks a
diversity of verification techniques from control theory for RL policies, and
marks a significant step towards a formal framework for the general, scalable,
and verifiable design of RL-based control systems. Code and videos are
available at this https url: https://rl-cbf.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_D/0/1/0/all/0/1&quot;&gt;Daniel C.H. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acero_F/0/1/0/all/0/1&quot;&gt;Fernando Acero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_R/0/1/0/all/0/1&quot;&gt;Robert McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanoulas_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kanoulas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06331">
<title>Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and Problem Solving: Evidence from the Vietnamese National High School Graduation Examination. (arXiv:2306.06331v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06331</link>
<description rdf:parseType="Literal">&lt;p&gt;This study offers a complete analysis of ChatGPT&apos;s mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT&apos;s performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_X/0/1/0/all/0/1&quot;&gt;Xuan-Quy Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngoc-Bich Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07129">
<title>Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip Force Feedback. (arXiv:2306.07129v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07129</link>
<description rdf:parseType="Literal">&lt;p&gt;The diagnostic value of biopsies is highly dependent on the placement of
needles. Robotic trajectory guidance has been shown to improve needle
positioning, but feedback for real-time navigation is limited. Haptic display
of needle tip forces can provide rich feedback for needle navigation by
enabling localization of tissue structures along the insertion path. We present
a collaborative robotic biopsy system that combines trajectory guidance with
kinesthetic feedback to assist the physician in needle placement. The robot
aligns the needle while the insertion is performed in collaboration with a
medical expert who controls the needle position on site. We present a needle
design that senses forces at the needle tip based on optical coherence
tomography and machine learning for real-time data processing. Our robotic
setup allows operators to sense deep tissue interfaces independent of
frictional forces to improve needle placement relative to a desired target
structure. We first evaluate needle tip force sensing in ex-vivo tissue in a
phantom study. We characterize the tip forces during insertions with constant
velocity and demonstrate the ability to detect tissue interfaces in a
collaborative user study. Participants are able to detect 91% of ex-vivo tissue
interfaces based on needle tip force feedback alone. Finally, we demonstrate
that even smaller, deep target structures can be accurately sampled by
performing post-mortem in situ biopsies of the pancreas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mieling_R/0/1/0/all/0/1&quot;&gt;Robin Mieling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neidhardt_M/0/1/0/all/0/1&quot;&gt;Maximilian Neidhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latus_S/0/1/0/all/0/1&quot;&gt;Sarah Latus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stapper_C/0/1/0/all/0/1&quot;&gt;Carolin Stapper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerlach_S/0/1/0/all/0/1&quot;&gt;Stefan Gerlach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kniep_I/0/1/0/all/0/1&quot;&gt;Inga Kniep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinemann_A/0/1/0/all/0/1&quot;&gt;Axel Heinemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ondruschka_B/0/1/0/all/0/1&quot;&gt;Benjamin Ondruschka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlaefer_A/0/1/0/all/0/1&quot;&gt;Alexander Schlaefer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08810">
<title>Deep Generative Models for Decision-Making and Control. (arXiv:2306.08810v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08810</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep model-based reinforcement learning methods offer a conceptually simple
approach to the decision-making and control problem: use learning for the
purpose of estimating an approximate dynamics model, and offload the rest of
the work to classical trajectory optimization. However, this combination has a
number of empirical shortcomings, limiting the usefulness of model-based
methods in practice. The dual purpose of this thesis is to study the reasons
for these shortcomings and to propose solutions for the uncovered problems.
Along the way, we highlight how inference techniques from the contemporary
generative modeling toolbox, including beam search, classifier-guided sampling,
and image inpainting, can be reinterpreted as viable planning strategies for
reinforcement learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1&quot;&gt;Michael Janner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10453">
<title>Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking. (arXiv:2306.10453v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10453</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction attempts to predict whether an unseen edge exists based on
only a portion of edges of a graph. A flurry of methods have been introduced in
recent years that attempt to make use of graph neural networks (GNNs) for this
task. Furthermore, new and diverse datasets have also been created to better
evaluate the effectiveness of these new models. However, multiple pitfalls
currently exist that hinder our ability to properly evaluate these new methods.
These pitfalls mainly include: (1) Lower than actual performance on multiple
baselines, (2) A lack of a unified data split and evaluation metric on some
datasets, and (3) An unrealistic evaluation setting that uses easy negative
samples. To overcome these challenges, we first conduct a fair comparison
across prominent methods and datasets, utilizing the same dataset and
hyperparameter search settings. We then create a more practical evaluation
setting based on a Heuristic Related Sampling Technique (HeaRT), which samples
hard negative samples via multiple heuristics. The new evaluation setting helps
promote new challenges and opportunities in link prediction by aligning the
evaluation with real-world situations. Our implementation and data are
available at https://github.com/Juanhui28/HeaRT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juanhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shomer_H/0/1/0/all/0/1&quot;&gt;Harry Shomer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shenglai Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10548">
<title>MARBLE: Music Audio Representation Benchmark for Universal Evaluation. (arXiv:2306.10548v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10548</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1&quot;&gt;Ruibin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yinghao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hanzhi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_L/0/1/0/all/0/1&quot;&gt;Le Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zeyue Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Binyue Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ningzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenghua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1&quot;&gt;Emmanouil Benetos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1&quot;&gt;Anton Ragni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1&quot;&gt;Norbert Gyenge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1&quot;&gt;Roger Dannenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gus Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11197">
<title>Sparse Modular Activation for Efficient Sequence Modeling. (arXiv:2306.11197v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11197</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liliang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuohang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yichong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1&quot;&gt;ChengXiang Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11270">
<title>Evaluating the Zero-shot Robustness of Instruction-tuned Language Models. (arXiv:2306.11270v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11270</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction fine-tuning has recently emerged as a promising approach for
improving the zero-shot capabilities of Large Language Models (LLMs) on new
tasks. This technique has shown particular strength in improving the
performance of modestly sized LLMs, sometimes inducing performance competitive
with much larger model variants. In this paper we ask two questions: (1) How
sensitive are instruction-tuned models to the particular phrasings of
instructions, and, (2) How can we make them more robust to such natural
language variation? To answer the former, we collect a set of 319 instructions
manually written by NLP practitioners for over 80 unique tasks included in
widely used benchmarks, and we evaluate the variance and average performance of
these instructions as compared to instruction phrasings observed during
instruction fine-tuning. We find that using novel (unobserved) but appropriate
instruction phrasings consistently degrades model performance, sometimes
substantially so. Further, such natural instructions yield a wide variance in
downstream performance, despite their semantic equivalence. Put another way,
instruction-tuned models are not especially robust to instruction re-phrasings.
We propose a simple method to mitigate this issue by introducing ``soft
prompt&apos;&apos; embedding parameters and optimizing these to maximize the similarity
between representations of semantically equivalent instructions. We show that
this method consistently improves the robustness of instruction-tuned models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiuding Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaib_C/0/1/0/all/0/1&quot;&gt;Chantal Shaib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Byron C. Wallace&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11380">
<title>A Bayesian Take on Gaussian Process Networks. (arXiv:2306.11380v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11380</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian Process Networks (GPNs) are a class of directed graphical models
which employ Gaussian processes as priors for the conditional expectation of
each variable given its parents in the network. The model allows describing
continuous joint distributions in a compact but flexible manner with minimal
parametric assumptions on the dependencies between variables. Bayesian
structure learning of GPNs requires computing the posterior over graphs of the
network and is computationally infeasible even in low dimensions. This work
implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the
posterior distribution of network structures. As such, the approach follows the
Bayesian paradigm, comparing models via their marginal likelihood and computing
the posterior probability of the GPN features. Simulation studies show that our
method outperforms state-of-the-art algorithms in recovering the graphical
structure of the network and provides an accurate approximation of its
posterior distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Giudice_E/0/1/0/all/0/1&quot;&gt;Enrico Giudice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kuipers_J/0/1/0/all/0/1&quot;&gt;Jack Kuipers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moffa_G/0/1/0/all/0/1&quot;&gt;Giusi Moffa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11941">
<title>Efficient Dynamics Modeling in Interactive Environments with Koopman Theory. (arXiv:2306.11941v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11941</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate modeling of dynamics in interactive environments is critical for
successful long-range prediction. Such a capability could advance Reinforcement
Learning (RL) and Planning algorithms, but achieving it is challenging.
Inaccuracies in model estimates can compound, resulting in increased errors
over long horizons. We approach this problem from the lens of Koopman theory,
where the nonlinear dynamics of the environment can be linearized in a
high-dimensional latent space. This allows us to efficiently parallelize the
sequential problem of long-range prediction using convolution, while accounting
for the agent&apos;s action at every time step. Our approach also enables stability
analysis and better control over gradients through time. Taken together, these
advantages result in significant improvement over the existing approaches, both
in the efficiency and the accuracy of modeling dynamics over extended horizons.
We also report promising experimental results in dynamics modeling for the
scenarios of both model-based planning and model-free RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Arnab Kumar Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panigrahi_S/0/1/0/all/0/1&quot;&gt;Siba Smarak Panigrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1&quot;&gt;Sai Rajeswar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqi_K/0/1/0/all/0/1&quot;&gt;Kaleem Siddiqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravanbakhsh_S/0/1/0/all/0/1&quot;&gt;Siamak Ravanbakhsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12001">
<title>An Overview of Catastrophic AI Risks. (arXiv:2306.12001v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12001</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose catastrophic risks. Although
numerous risks have been detailed separately, there is a pressing need for a
systematic discussion and illustration of the potential dangers to better
inform efforts to mitigate them. This paper provides an overview of the main
sources of catastrophic AI risks, which we organize into four categories:
malicious use, in which individuals or groups intentionally use AIs to cause
harm; AI race, in which competitive environments compel actors to deploy unsafe
AIs or cede control to AIs; organizational risks, highlighting how human
factors and complex systems can increase the chances of catastrophic accidents;
and rogue AIs, describing the inherent difficulty in controlling agents far
more intelligent than humans. For each category of risk, we describe specific
hazards, present illustrative stories, envision ideal scenarios, and propose
practical suggestions for mitigating these dangers. Our goal is to foster a
comprehensive understanding of these risks and inspire collective and proactive
efforts to ensure that AIs are developed and deployed in a safe manner.
Ultimately, we hope this will allow us to realize the benefits of this powerful
technology while minimizing the potential for catastrophic outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodside_T/0/1/0/all/0/1&quot;&gt;Thomas Woodside&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15951">
<title>Reduce Computational Complexity for Convolutional Layers by Skipping Zeros. (arXiv:2306.15951v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15951</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks rely on parallel processors for acceleration. To design
operators for them, it requires not only good algorithm to reduce complexity,
but also sufficient utilization of hardwares. Convolutional layers mainly
contain 3 kinds of operators: convolution in forward propagation, deconvolution
and dilated-convolution in backward propagation. When executing these
operators, 0s are always added to tensors, causing redundant calculations. This
paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these
0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors
to dense tensors, to avoid inserted 0s in deconvolution and
dilated-convolution. In contrast to regular convolution, deconvolution is hard
to accelerate due to its complicacy. This paper provides high-performance GPU
implementations of C-K-S, and verifies their effectiveness with comparison to
PyTorch. According to the experiments, C-K-S has advantages over PyTorch in
certain cases, especially in deconvolution on small feature-maps. Further
enhancement of C-K-S can be done by making full optimizations oriented at
specific GPU architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuopin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00721">
<title>Neural Polytopes. (arXiv:2307.00721v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00721</link>
<description rdf:parseType="Literal">&lt;p&gt;We find that simple neural networks with ReLU activation generate polytopes
as an approximation of a unit sphere in various dimensions. The species of
polytopes are regulated by the network architecture, such as the number of
units and layers. For a variety of activation functions, generalization of
polytopes is obtained, which we call neural polytopes. They are a smooth
analogue of polytopes, exhibiting geometric duality. This finding initiates
research of generative discrete geometry to approximate surfaces by machine
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1&quot;&gt;Koji Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naito_T/0/1/0/all/0/1&quot;&gt;Tomoya Naito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naito_H/0/1/0/all/0/1&quot;&gt;Hisashi Naito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01227">
<title>ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01227</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic forecasting is a highly challenging task owing to the dynamical
spatio-temporal dependencies of traffic flows. To handle this, we focus on
modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze
Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple
regions. ESGCN consists of two modules: W-module and ES module. W-module is a
fully node-wise convolutional network. It encodes the time-series of each
traffic region separately and decomposes the time-series at various scales to
capture fine and coarse features. The ES module models the spatio-temporal
dynamics using Graph Convolutional Network (GCN) and generates an Adaptive
Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,
we introduce three key concepts. 1) Using edge features to directly capture the
spatiotemporal flow representation among regions. 2) Applying an edge attention
mechanism to GCN to extract the AAM from the edge features. Here, the attention
mechanism can effectively determine important spatio-temporal adjacency
relations. 3) Proposing a novel node contrastive loss to suppress obstructed
connections and emphasize related connections. Experimental results show that
ESGCN achieves state-of-the-art performance by a large margin on four
real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangrok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Ha Young Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01719">
<title>MOPO-LSI: A User Guide. (arXiv:2307.01719v2 [q-fin.PM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01719</link>
<description rdf:parseType="Literal">&lt;p&gt;MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shukla_K/0/1/0/all/0/1&quot;&gt;Kumar Neelotpal Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jasmine Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+David/0/1/0/all/0/1&quot;&gt;David&lt;/a&gt; (Xuejun) &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang/0/1/0/all/0/1&quot;&gt;Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+OLeary_M/0/1/0/all/0/1&quot;&gt;Michael O&amp;#x27;Leary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03042">
<title>Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03042</link>
<description rdf:parseType="Literal">&lt;p&gt;Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
However, this approach is increasingly proven to be impractical owing to the
substantial computational requirements associated with training such large
language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
techniques offer a viable solution by selectively fine-tuning a small subset of
additional parameters, significantly reducing the computational requirements
for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT
adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is
trained using clinical notes obtained from the MIMIC-IV database, thereby
creating a specialised adapter designed for the clinical domain. Additionally,
we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with
Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.
We evaluate this framework on multiple clinical outcome prediction datasets,
comparing it to clinically trained language models. Our proposed framework
achieves a state-of-the-art AUROC score averaged across all clinical downstream
tasks. We observe substantial improvements of 6-9% AUROC score in the
large-scale multilabel classification tasks, such as diagnoses and procedures
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gema_A/0/1/0/all/0/1&quot;&gt;Aryo Pradipta Gema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daines_L/0/1/0/all/0/1&quot;&gt;Luke Daines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1&quot;&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1&quot;&gt;Beatrice Alex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03190">
<title>Synthesizing Artistic Cinemagraphs from Text. (arXiv:2307.03190v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03190</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Text2Cinemagraph, a fully automated method for creating
cinemagraphs from text descriptions - an especially challenging task when
prompts feature imaginary elements and artistic styles, given the complexity of
interpreting the semantics and motions of these images. Existing single-image
animation methods fall short on artistic inputs, and recent text-based video
methods frequently introduce temporal inconsistencies, struggling to keep
certain regions static. To address these challenges, we propose an idea of
synthesizing image twins from a single text prompt - a pair of an artistic
image and its pixel-aligned corresponding natural-looking twin. While the
artistic image depicts the style and appearance detailed in our text prompt,
the realistic counterpart greatly simplifies layout and motion analysis.
Leveraging existing natural image and video datasets, we can accurately segment
the realistic image and predict plausible motion given the semantic
information. The predicted motion can then be transferred to the artistic image
to create the final cinemagraph. Our method outperforms existing approaches in
creating cinemagraphs for natural landscapes as well as artistic and
other-worldly scenes, as validated by automated metrics and user studies.
Finally, we demonstrate two extensions: animating existing paintings and
controlling motion directions using text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1&quot;&gt;Aniruddha Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1&quot;&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hsin-Ying Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1&quot;&gt;Sergey Tulyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03364">
<title>Distilled Pruning: Using Synthetic Data to Win the Lottery. (arXiv:2307.03364v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03364</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a novel approach to pruning deep learning models by
using distilled data. Unlike conventional strategies which primarily focus on
architectural or algorithmic optimization, our method reconsiders the role of
data in these scenarios. Distilled datasets capture essential patterns from
larger datasets, and we demonstrate how to leverage this capability to enable a
computationally efficient pruning process. Our approach can find sparse,
trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative
Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results
highlight the potential of using distilled data for resource-efficient neural
network pruning, model compression, and neural architecture search.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDermott_L/0/1/0/all/0/1&quot;&gt;Luke McDermott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummings_D/0/1/0/all/0/1&quot;&gt;Daniel Cummings&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03544">
<title>Roman Numeral Analysis with Graph Neural Networks: Onset-wise Predictions from Note-wise Features. (arXiv:2307.03544v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03544</link>
<description rdf:parseType="Literal">&lt;p&gt;Roman Numeral analysis is the important task of identifying chords and their
functional context in pieces of tonal music. This paper presents a new approach
to automatic Roman Numeral analysis in symbolic music. While existing
techniques rely on an intermediate lossy representation of the score, we
propose a new method based on Graph Neural Networks (GNNs) that enable the
direct description and processing of each individual note in the score. The
proposed architecture can leverage notewise features and interdependencies
between notes but yield onset-wise representation by virtue of our novel edge
contraction algorithm. Our results demonstrate that ChordGNN outperforms
existing state-of-the-art models, achieving higher accuracy in Roman Numeral
analysis on the reference datasets. In addition, we investigate variants of our
model using proposed techniques such as NADE, and post-processing of the chord
predictions. The full source code for this work is available at
https://github.com/manoskary/chordgnn
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karystinaios_E/0/1/0/all/0/1&quot;&gt;Emmanouil Karystinaios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widmer_G/0/1/0/all/0/1&quot;&gt;Gerhard Widmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04149">
<title>Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04149</link>
<description rdf:parseType="Literal">&lt;p&gt;Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ayush Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambhu_Y/0/1/0/all/0/1&quot;&gt;Yash Bhambhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckchash_H/0/1/0/all/0/1&quot;&gt;Himanshu Buckchash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1&quot;&gt;Dilip K. Prasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04603">
<title>Solvent: A Framework for Protein Folding. (arXiv:2307.04603v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04603</link>
<description rdf:parseType="Literal">&lt;p&gt;Consistency and reliability are crucial for conducting AI research. Many
famous research fields, such as object detection, have been compared and
validated with solid benchmark frameworks. After AlphaFold2, the protein
folding task has entered a new phase, and many methods are proposed based on
the component of AlphaFold2. The importance of a unified research framework in
protein folding contains implementations and benchmarks to consistently and
fairly compare various approaches. To achieve this, we present Solvent, an
protein folding framework that supports significant components of
state-of-th-arts models in the manner of off-the-shelf interface Solvent
contains different models implemented in a unified codebase and supports
training and evaluation for defined models on the same dataset. We benchmark
well-known algorithms and their components and provide experiments that give
helpful insights into the protein structure modeling field. We hope that
Solvent will increase the reliability and consistency of proposed models and
gives efficiency in both speed and costs, resulting in acceleration on protein
folding modeling research. The code is available at
https://github.com/kakaobrain/solvent, and the project will continue to be
developed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaemyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kyeongtak Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hasun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Youhan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04617">
<title>Weakly-supervised positional contrastive learning: application to cirrhosis classification. (arXiv:2307.04617v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04617</link>
<description rdf:parseType="Literal">&lt;p&gt;Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfati_E/0/1/0/all/0/1&quot;&gt;Emma Sarfati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bone_A/0/1/0/all/0/1&quot;&gt;Alexandre B&amp;#xf4;ne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohe_M/0/1/0/all/0/1&quot;&gt;Marc-Michel Roh&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1&quot;&gt;Pietro Gori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1&quot;&gt;Isabelle Bloch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04927">
<title>Probabilistic Counterexample Guidance for Safer Reinforcement Learning (Extended Version). (arXiv:2307.04927v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04927</link>
<description rdf:parseType="Literal">&lt;p&gt;Safe exploration aims at addressing the limitations of Reinforcement Learning
(RL) in safety-critical scenarios, where failures during trial-and-error
learning may incur high costs. Several methods exist to incorporate external
knowledge or to use proximal sensor data to limit the exploration of unsafe
states. However, reducing exploration risks in unknown environments, where an
agent must discover safety threats during exploration, remains challenging. In
this paper, we target the problem of safe exploration by guiding the training
with counterexamples of the safety requirement. Our method abstracts both
continuous and discrete state-space systems into compact abstract models
representing the safety-relevant knowledge acquired by the agent during
exploration. We then exploit probabilistic counterexample generation to
construct minimal simulation submodels eliciting safety requirement violations,
where the agent can efficiently train offline to refine its policy towards
minimising the risk of safety violations during the subsequent online
exploration. We demonstrate our method&apos;s effectiveness in reducing safety
violations during online exploration in preliminary experiments by an average
of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with
previous related work, while achieving comparable cumulative rewards with
respect to unrestricted exploration and alternative approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiaotong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1&quot;&gt;Antonio Filieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05014">
<title>Test-Time Training on Video Streams. (arXiv:2307.05014v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05014</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1&quot;&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05350">
<title>Dividing and Conquering a BlackBox to a Mixture of Interpretable Models: Route, Interpret, Repeat. (arXiv:2307.05350v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05350</link>
<description rdf:parseType="Literal">&lt;p&gt;ML model design either starts with an interpretable model or a Blackbox and
explains it post hoc. Blackbox models are flexible but difficult to explain,
while interpretable models are inherently explainable. Yet, interpretable
models require extensive ML knowledge and tend to be less flexible and
underperforming than their Blackbox variants. This paper aims to blur the
distinction between a post hoc explanation of a Blackbox and constructing
interpretable models. Beginning with a Blackbox, we iteratively carve out a
mixture of interpretable experts (MoIE) and a residual network. Each
interpretable model specializes in a subset of samples and explains them using
First Order Logic (FOL), providing basic reasoning on concepts from the
Blackbox. We route the remaining samples through a flexible residual. We repeat
the method on the residual network until all the interpretable models explain
the desired proportion of data. Our extensive experiments show that our route,
interpret, and repeat approach (1) identifies a diverse set of
instance-specific concepts with high concept completeness via MoIE without
compromising in performance, (2) identifies the relatively ``harder&apos;&apos; samples
to explain via residuals, (3) outperforms the interpretable by-design models by
significant margins during test-time interventions, and (4) fixes the shortcut
learned by the original Blackbox. The code for MoIE is publicly available at:
\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Ke Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1&quot;&gt;Forough Arabshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>