<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07298" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1810.07287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.10113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.08756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.12319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.08768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.09874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.07014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06283" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06913" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.06950">
<title>Pathway toward prior knowledge-integrated machine learning in engineering. (arXiv:2307.06950v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06950</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the digitalization trend and data volume surge, first-principles
models (also known as logic-driven, physics-based, rule-based, or
knowledge-based models) and data-driven approaches have existed in parallel,
mirroring the ongoing AI debate on symbolism versus connectionism. Research for
process development to integrate both sides to transfer and utilize domain
knowledge in the data-driven process is rare. This study emphasizes efforts and
prevailing trends to integrate multidisciplinary domain professions into
machine acknowledgeable, data-driven processes in a two-fold organization:
examining information uncertainty sources in knowledge representation and
exploring knowledge decomposition with a three-tier knowledge-integrated
machine learning paradigm. This approach balances holist and reductionist
perspectives in the engineering domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_P/0/1/0/all/0/1&quot;&gt;Philipp Geyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06951">
<title>AI For Global Climate Cooperation 2023 Competition Proceedings. (arXiv:2307.06951v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;The international community must collaborate to mitigate climate change and
sustain economic growth. However, collaboration is hard to achieve, partly
because no global authority can ensure compliance with international climate
agreements. Combining AI with climate-economic simulations offers a promising
solution to design international frameworks, including negotiation protocols
and climate agreements, that promote and incentivize collaboration. In
addition, these frameworks should also have policy goals fulfillment, and
sustained commitment, taking into account climate-economic dynamics and
strategic behaviors. These challenges require an interdisciplinary approach
across machine learning, economics, climate science, law, policy, ethics, and
other fields.
&lt;/p&gt;
&lt;p&gt;Towards this objective, we organized AI for Global Climate Cooperation, a
Mila competition in which teams submitted proposals and analyses of
international frameworks, based on (modifications of) RICE-N, an AI-driven
integrated assessment model (IAM). In particular, RICE-N supports modeling
regional decision-making using AI agents. Furthermore, the IAM then models the
climate-economic impact of those decisions into the future.
&lt;/p&gt;
&lt;p&gt;Whereas the first track focused only on performance metrics, the proposals
submitted to the second track were evaluated both quantitatively and
qualitatively. The quantitative evaluation focused on a combination of (i) the
degree of mitigation of global temperature rise and (ii) the increase in
economic productivity. On the other hand, an interdisciplinary panel of human
experts in law, policy, sociology, economics and environmental science,
evaluated the solutions qualitatively. In particular, the panel considered the
effectiveness, simplicity, feasibility, ethics, and notions of climate justice
of the protocols. In the third track, the participants were asked to critique
and improve RICE-N.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Prateek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phade_S/0/1/0/all/0/1&quot;&gt;Soham Phade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1&quot;&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Stephan Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06957">
<title>Embracing the chaos: analysis and diagnosis of numerical instability in variational flows. (arXiv:2307.06957v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.06957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the impact of numerical instability on the
reliability of sampling, density evaluation, and evidence lower bound (ELBO)
estimation in variational flows. We first empirically demonstrate that common
flows can exhibit a catastrophic accumulation of error: the numerical flow map
deviates significantly from the exact map -- which affects sampling -- and the
numerical inverse flow map does not accurately recover the initial input --
which affects density and ELBO computations. Surprisingly though, we find that
results produced by flows are often accurate enough for applications despite
the presence of serious numerical instability. In this work, we treat
variational flows as dynamical systems, and leverage shadowing theory to
elucidate this behavior via theoretical guarantees on the error of sampling,
density evaluation, and ELBO estimation. Finally, we develop and empirically
test a diagnostic procedure that can be used to validate results produced by
numerically unstable flows in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zuheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Campbell_T/0/1/0/all/0/1&quot;&gt;Trevor Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06963">
<title>Is Task-Agnostic Explainable AI a Myth?. (arXiv:2307.06963v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06963</link>
<description rdf:parseType="Literal">&lt;p&gt;Our work serves as a framework for unifying the challenges of contemporary
explainable AI (XAI). We demonstrate that while XAI methods provide
supplementary and potentially useful output for machine learning models,
researchers and decision-makers should be mindful of their conceptual and
technical limitations, which frequently result in these methods themselves
becoming black boxes. We examine three XAI research avenues spanning image,
textual, and graph data, covering saliency, attention, and graph-type
explainers. Despite the varying contexts and timeframes of the mentioned cases,
the same persistent roadblocks emerge, highlighting the need for a conceptual
breakthrough in the field to address the challenge of compatibility between XAI
methods and application tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaszczewicz_A/0/1/0/all/0/1&quot;&gt;Alicja Chaszczewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06966">
<title>Layerwise Linear Mode Connectivity. (arXiv:2307.06966v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06966</link>
<description rdf:parseType="Literal">&lt;p&gt;In the federated setup one performs an aggregation of separate local models
multiple times during training in order to obtain a stronger global model; most
often aggregation is a simple averaging of the parameters. Understanding when
and why averaging works in a non-convex setup, such as federated deep learning,
is an open challenge that hinders obtaining highly performant global models. On
i.i.d.~datasets federated deep learning with frequent averaging is successful.
The common understanding, however, is that during the independent training
models are drifting away from each other and thus averaging may not work
anymore after many local parameter updates. The problem can be seen from the
perspective of the loss surface: for points on a non-convex surface the average
can become arbitrarily bad. The assumption of local convexity, often used to
explain the success of federated averaging, contradicts to the empirical
evidence showing that high loss barriers exist between models from the very
beginning of the learning, even when training on the same data. Based on the
observation that the learning process evolves differently in different layers,
we investigate the barrier between models in a layerwise fashion. Our
conjecture is that barriers preventing from successful federated training are
caused by a particular layer or group of layers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adilova_L/0/1/0/all/0/1&quot;&gt;Linara Adilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1&quot;&gt;Asja Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1&quot;&gt;Martin Jaggi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06970">
<title>Machine Learning-Assisted Pattern Recognition Algorithms for Estimating Ultimate Tensile Strength in Fused Deposition Modeled Polylactic Acid Specimens. (arXiv:2307.06970v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06970</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigate the application of supervised machine learning
algorithms for estimating the Ultimate Tensile Strength (UTS) of Polylactic
Acid (PLA) specimens fabricated using the Fused Deposition Modeling (FDM)
process. A total of 31 PLA specimens were prepared, with Infill Percentage,
Layer Height, Print Speed, and Extrusion Temperature serving as input
parameters. The primary objective was to assess the accuracy and effectiveness
of four distinct supervised classification algorithms, namely Logistic
Classification, Gradient Boosting Classification, Decision Tree, and K-Nearest
Neighbor, in predicting the UTS of the specimens. The results revealed that
while the Decision Tree and K-Nearest Neighbor algorithms both achieved an F1
score of 0.71, the KNN algorithm exhibited a higher Area Under the Curve (AUC)
score of 0.79, outperforming the other algorithms. This demonstrates the
superior ability of the KNN algorithm in differentiating between the two
classes of ultimate tensile strength within the dataset, rendering it the most
favorable choice for classification in the context of this research. This study
represents the first attempt to estimate the UTS of PLA specimens using machine
learning-based classification algorithms, and the findings offer valuable
insights into the potential of these techniques in improving the performance
and accuracy of predictive models in the domain of additive manufacturing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Akshansh Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1&quot;&gt;Vijaykumar S Jatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06971">
<title>Short Boolean Formulas as Explanations in Practice. (arXiv:2307.06971v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2307.06971</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate explainability via short Boolean formulas in the data model
based on unary relations. As an explanation of length k, we take a Boolean
formula of length k that minimizes the error with respect to the target
attribute to be explained. We first provide novel quantitative bounds for the
expected error in this scenario. We then also demonstrate how the setting works
in practice by studying three concrete data sets. In each case, we calculate
explanation formulas of different lengths using an encoding in Answer Set
Programming. The most accurate formulas we obtain achieve errors similar to
other methods on the same data sets. However, due to overfitting, these
formulas are not necessarily ideal explanations, so we use cross validation to
identify a suitable length for explanations. By limiting to shorter formulas,
we obtain explanations that avoid overfitting but are still reasonably accurate
and also, importantly, human interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaakkola_R/0/1/0/all/0/1&quot;&gt;Reijo Jaakkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janhunen_T/0/1/0/all/0/1&quot;&gt;Tomi Janhunen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuusisto_A/0/1/0/all/0/1&quot;&gt;Antti Kuusisto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rankooh_M/0/1/0/all/0/1&quot;&gt;Masood Feyzbakhsh Rankooh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vilander_M/0/1/0/all/0/1&quot;&gt;Miikka Vilander&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06975">
<title>Neuro-symbolic Empowered Denoising Diffusion Probabilistic Models for Real-time Anomaly Detection in Industry 4.0. (arXiv:2307.06975v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06975</link>
<description rdf:parseType="Literal">&lt;p&gt;Industry 4.0 involves the integration of digital technologies, such as IoT,
Big Data, and AI, into manufacturing and industrial processes to increase
efficiency and productivity. As these technologies become more interconnected
and interdependent, Industry 4.0 systems become more complex, which brings the
difficulty of identifying and stopping anomalies that may cause disturbances in
the manufacturing process. This paper aims to propose a diffusion-based model
for real-time anomaly prediction in Industry 4.0 processes. Using a
neuro-symbolic approach, we integrate industrial ontologies in the model,
thereby adding formal knowledge on smart manufacturing. Finally, we propose a
simple yet effective way of distilling diffusion models through Random Fourier
Features for deployment on an embedded system for direct integration into the
manufacturing process. To the best of our knowledge, this approach has never
been explored before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capogrosso_L/0/1/0/all/0/1&quot;&gt;Luigi Capogrosso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascolini_A/0/1/0/all/0/1&quot;&gt;Alessio Mascolini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girella_F/0/1/0/all/0/1&quot;&gt;Federico Girella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1&quot;&gt;Geri Skenderi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaiardelli_S/0/1/0/all/0/1&quot;&gt;Sebastiano Gaiardelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallOra_N/0/1/0/all/0/1&quot;&gt;Nicola Dall&amp;#x27;Ora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponzio_F/0/1/0/all/0/1&quot;&gt;Francesco Ponzio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fraccaroli_E/0/1/0/all/0/1&quot;&gt;Enrico Fraccaroli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cataldo_S/0/1/0/all/0/1&quot;&gt;Santa Di Cataldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinco_S/0/1/0/all/0/1&quot;&gt;Sara Vinco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macii_E/0/1/0/all/0/1&quot;&gt;Enrico Macii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fummi_F/0/1/0/all/0/1&quot;&gt;Franco Fummi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1&quot;&gt;Marco Cristani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06978">
<title>A decision framework for selecting information-transfer strategies in population-based SHM. (arXiv:2307.06978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06978</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-support for the operation and maintenance of structures provides
significant motivation for the development and implementation of structural
health monitoring (SHM) systems. Unfortunately, the limited availability of
labelled training data hinders the development of the statistical models on
which these decision-support systems rely. Population-based SHM seeks to
mitigate the impact of data scarcity by using transfer learning techniques to
share information between individual structures within a population. The
current paper proposes a decision framework for selecting transfer strategies
based upon a novel concept -- the expected value of information transfer --
such that negative transfer is avoided. By avoiding negative transfer, and by
optimising information transfer strategies using the transfer-decision
framework, one can reduce the costs associated with operating and maintaining
structures, and improve safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_A/0/1/0/all/0/1&quot;&gt;Aidan J. Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poole_J/0/1/0/all/0/1&quot;&gt;Jack Poole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dervilis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Dervilis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_P/0/1/0/all/0/1&quot;&gt;Paul Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worden_K/0/1/0/all/0/1&quot;&gt;Keith Worden&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06984">
<title>Data Augmentation for Mathematical Objects. (arXiv:2307.06984v1 [cs.SC])</title>
<link>http://arxiv.org/abs/2307.06984</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses and evaluates ideas of data balancing and data
augmentation in the context of mathematical objects: an important topic for
both the symbolic computation and satisfiability checking communities, when
they are making use of machine learning techniques to optimise their tools. We
consider a dataset of non-linear polynomial problems and the problem of
selecting a variable ordering for cylindrical algebraic decomposition to tackle
these with. By swapping the variable names in already labelled problems, we
generate new problem instances that do not require any further labelling when
viewing the selection as a classification problem. We find this augmentation
increases the accuracy of ML models by 63% on average. We study what part of
this improvement is due to the balancing of the dataset and what is achieved
thanks to further increasing the size of the dataset, concluding that both have
a very significant effect. We finish the paper by reflecting on how this idea
could be applied in other uses of machine learning in mathematics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_T/0/1/0/all/0/1&quot;&gt;Tereso del Rio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+England_M/0/1/0/all/0/1&quot;&gt;Matthew England&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07011">
<title>Impact of Free-carrier Nonlinearities on Silicon Microring-based Reservoir Computing. (arXiv:2307.07011v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2307.07011</link>
<description rdf:parseType="Literal">&lt;p&gt;We quantify the impact of thermo-optic and free-carrier effects on time-delay
reservoir computing using a silicon microring resonator. We identify pump power
and frequency detuning ranges with NMSE less than 0.05 for the NARMA-10 task
depending on the time constants of the two considered effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_B/0/1/0/all/0/1&quot;&gt;Bernard J. Giron Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peucheret_C/0/1/0/all/0/1&quot;&gt;Christophe Peucheret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zibar_D/0/1/0/all/0/1&quot;&gt;Darko Zibar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ros_F/0/1/0/all/0/1&quot;&gt;Francesco Da Ros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07014">
<title>Leveraging Factored Action Spaces for Off-Policy Evaluation. (arXiv:2307.07014v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07014</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-policy evaluation (OPE) aims to estimate the benefit of following a
counterfactual sequence of actions, given data collected from executed
sequences. However, existing OPE estimators often exhibit high bias and high
variance in problems involving large, combinatorial action spaces. We
investigate how to mitigate this issue using factored action spaces i.e.
expressing each action as a combination of independent sub-actions from smaller
action spaces. This approach facilitates a finer-grained analysis of how
actions differ in their effects. In this work, we propose a new family of
&quot;decomposed&quot; importance sampling (IS) estimators based on factored action
spaces. Given certain assumptions on the underlying problem structure, we prove
that the decomposed IS estimators have less variance than their original
non-decomposed versions, while preserving the property of zero bias. Through
simulations, we empirically verify our theoretical results, probing the
validity of various assumptions. Provided with a technique that can derive the
action space factorisation for a given problem, our work shows that OPE can be
improved &quot;for free&quot; by utilising this inherent problem structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebello_A/0/1/0/all/0/1&quot;&gt;Aaman Rebello&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shengpu Tang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1&quot;&gt;Jenna Wiens&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parbhoo_S/0/1/0/all/0/1&quot;&gt;Sonali Parbhoo&lt;/a&gt; (1) ((1) Department of Engineering, Imperial College London, (2) Division of Computer Science &amp;amp; Engineering, University of Michigan)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07030">
<title>Accelerated gradient methods for nonconvex optimization: Escape trajectories from strict saddle points and convergence to local minima. (arXiv:2307.07030v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.07030</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the problem of understanding the behavior of a general
class of accelerated gradient methods on smooth nonconvex functions. Motivated
by some recent works that have proposed effective algorithms, based on Polyak&apos;s
heavy ball method and the Nesterov accelerated gradient method, to achieve
convergence to a local minimum of nonconvex functions, this work proposes a
broad class of Nesterov-type accelerated methods and puts forth a rigorous
study of these methods encompassing the escape from saddle-points and
convergence to local minima through a both asymptotic and a non-asymptotic
analysis. In the asymptotic regime, this paper answers an open question of
whether Nesterov&apos;s accelerated gradient method (NAG) with variable momentum
parameter avoids strict saddle points almost surely. This work also develops
two metrics of asymptotic rate of convergence and divergence, and evaluates
these two metrics for several popular standard accelerated methods such as the
NAG, and Nesterov&apos;s accelerated gradient with constant momentum (NCM) near
strict saddle points. In the local regime, this work provides an analysis that
leads to the &quot;linear&quot; exit time estimates from strict saddle neighborhoods for
trajectories of these accelerated methods as well the necessary conditions for
the existence of such trajectories. Finally, this work studies a sub-class of
accelerated methods that can converge in convex neighborhoods of nonconvex
functions with a near optimal rate to a local minima and at the same time this
sub-class offers superior saddle-escape behavior compared to that of NAG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Dixit_R/0/1/0/all/0/1&quot;&gt;Rishabh Dixit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gurbuzbalaban_M/0/1/0/all/0/1&quot;&gt;Mert Gurbuzbalaban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Bajwa_W/0/1/0/all/0/1&quot;&gt;Waheed U. Bajwa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07044">
<title>AnyStar: Domain randomized universal star-convex 3D instance segmentation. (arXiv:2307.07044v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07044</link>
<description rdf:parseType="Literal">&lt;p&gt;Star-convex shapes arise across bio-microscopy and radiology in the form of
nuclei, nodules, metastases, and other units. Existing instance segmentation
networks for such structures train on densely labeled instances for each
dataset, which requires substantial and often impractical manual annotation
effort. Further, significant reengineering or finetuning is needed when
presented with new datasets and imaging modalities due to changes in contrast,
shape, orientation, resolution, and density. We present AnyStar, a
domain-randomized generative model that simulates synthetic training data of
blob-like objects with randomized appearance, environments, and imaging physics
to train general-purpose star-convex instance segmentation networks. As a
result, networks trained using our generative model do not require annotated
images from unseen datasets. A single network trained on our synthesized data
accurately 3D segments C. elegans and P. dumerilii nuclei in fluorescence
microscopy, mouse cortical nuclei in micro-CT, zebrafish brain nuclei in EM,
and placental cotyledons in human fetal MRI, all without any retraining,
finetuning, transfer learning, or domain adaptation. Code is available at
https://github.com/neel-dey/AnyStar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1&quot;&gt;Neel Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abulnaga_S/0/1/0/all/0/1&quot;&gt;S. Mazdak Abulnaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Billot_B/0/1/0/all/0/1&quot;&gt;Benjamin Billot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turk_E/0/1/0/all/0/1&quot;&gt;Esra Abaci Turk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1&quot;&gt;P. Ellen Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1&quot;&gt;Adrian V. Dalca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1&quot;&gt;Polina Golland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07050">
<title>Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\&quot;odinger Equation. (arXiv:2307.07050v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2307.07050</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving the quantum many-body Schr\&quot;odinger equation is a fundamental and
challenging problem in the fields of quantum physics, quantum chemistry, and
material sciences. One of the common computational approaches to this problem
is Quantum Variational Monte Carlo (QVMC), in which ground-state solutions are
obtained by minimizing the energy of the system within a restricted family of
parameterized wave functions. Deep learning methods partially address the
limitations of traditional QVMC by representing a rich family of wave functions
in terms of neural networks. However, the optimization objective in QVMC
remains notoriously hard to minimize and requires second-order optimization
methods such as natural gradient. In this paper, we first reformulate energy
functional minimization in the space of Born distributions corresponding to
particle-permutation (anti-)symmetric wave functions, rather than the space of
wave functions. We then interpret QVMC as the Fisher--Rao gradient flow in this
distributional space, followed by a projection step onto the variational
manifold. This perspective provides us with a principled framework to derive
new QMC algorithms, by endowing the distributional space with better metrics,
and following the projected gradient flow induced by those metrics. More
specifically, we propose &quot;Wasserstein Quantum Monte Carlo&quot; (WQMC), which uses
the gradient flow induced by the Wasserstein metric, rather than Fisher--Rao
metric, and corresponds to transporting the probability mass, rather than
teleporting it. We demonstrate empirically that the dynamics of WQMC results in
faster convergence to the ground state of molecular systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Neklyudov_K/0/1/0/all/0/1&quot;&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nys_J/0/1/0/all/0/1&quot;&gt;Jannes Nys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Thiede_L/0/1/0/all/0/1&quot;&gt;Luca Thiede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Carrasquilla_J/0/1/0/all/0/1&quot;&gt;Juan Carrasquilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Welling_M/0/1/0/all/0/1&quot;&gt;Max Welling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Makhzani_A/0/1/0/all/0/1&quot;&gt;Alireza Makhzani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07051">
<title>Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07051</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models have led to renewed interest in
natural language processing in healthcare using the free text of clinical
notes. One distinguishing characteristic of clinical notes is their long time
span over multiple long documents. The unique structure of clinical notes
creates a new design choice: when the context length for a language model
predictor is limited, which part of clinical notes should we choose as the
input? Existing studies either choose the inputs with domain knowledge or
simply truncate them. We propose a framework to analyze the sections with high
predictive power. Using MIMIC-III, we show that: 1) predictive power
distribution is different between nursing notes and discharge notes and 2)
combining different types of notes could improve performance when the context
length is large. Our findings suggest that a carefully selected sampling
function could enable more efficient information extraction from clinical
notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hongyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lavender Yao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1&quot;&gt;Eric Karl Oermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07055">
<title>Reward-Directed Conditional Diffusion: Provable Distribution Estimation and Reward Improvement. (arXiv:2307.07055v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07055</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the methodology and theory of reward-directed generation via
conditional diffusion models. Directed generation aims to generate samples with
desired properties as measured by a reward function, which has broad
applications in generative AI, reinforcement learning, and computational
biology. We consider the common learning scenario where the data set consists
of unlabeled data along with a smaller set of data with noisy reward labels.
Our approach leverages a learned reward function on the smaller data set as a
pseudolabeler. From a theoretical standpoint, we show that this directed
generator can effectively learn and sample from the reward-conditioned data
distribution. Additionally, our model is capable of recovering the latent
subspace representation of data. Moreover, we establish that the model
generates a new population that moves closer to a user-specified target reward
value, where the optimality gap aligns with the off-policy bandit regret in the
feature subspace. The improvement in rewards obtained is influenced by the
interplay between the strength of the reward signal, the distribution shift,
and the cost of off-support extrapolation. We provide empirical results to
validate our theory and highlight the relationship between the strength of
extrapolation and the quality of generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaixuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_C/0/1/0/all/0/1&quot;&gt;Chengzhuo Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minshuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07062">
<title>Controllable Emphasis with zero data for text-to-speech. (arXiv:2307.07062v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.07062</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a scalable method to produce high quality emphasis for
text-to-speech (TTS) that does not require recordings or annotations. Many TTS
models include a phoneme duration model. A simple but effective method to
achieve emphasized speech consists in increasing the predicted duration of the
emphasised word. We show that this is significantly better than spectrogram
modification techniques improving naturalness by $7.3\%$ and correct testers&apos;
identification of the emphasized word in a sentence by $40\%$ on a reference
female en-US voice. We show that this technique significantly closes the gap to
methods that require explicit recordings. The method proved to be scalable and
preferred in all four languages tested (English, Spanish, Italian, German), for
different voices and multiple speaking styles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1&quot;&gt;Arnaud Joly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nicolis_M/0/1/0/all/0/1&quot;&gt;Marco Nicolis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peterova_E/0/1/0/all/0/1&quot;&gt;Ekaterina Peterova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lombardi_A/0/1/0/all/0/1&quot;&gt;Alessandro Lombardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1&quot;&gt;Ammar Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korlaar_A/0/1/0/all/0/1&quot;&gt;Arent van Korlaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hussain_A/0/1/0/all/0/1&quot;&gt;Aman Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Parul Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1&quot;&gt;Alexis Moinet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lajszczak_M/0/1/0/all/0/1&quot;&gt;Mateusz Lajszczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1&quot;&gt;Penny Karanasou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonafonte_A/0/1/0/all/0/1&quot;&gt;Antonio Bonafonte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1&quot;&gt;Thomas Drugman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sokolova_E/0/1/0/all/0/1&quot;&gt;Elena Sokolova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07063">
<title>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code is available at
https://github.com/yiren-jian/BLIText
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1&quot;&gt;Yiren Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chongyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07066">
<title>Proof of Training (PoT): Harnessing Crypto Mining Power for Distributed AI Training. (arXiv:2307.07066v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.07066</link>
<description rdf:parseType="Literal">&lt;p&gt;In the midst of the emerging trend of integrating artificial intelligence
(AI) with crypto mining, we identify three major challenges that create a gap
between these two fields. To bridge this gap, we introduce the
proof-of-training (PoT) protocol, an approach that combines the strengths of
both AI and blockchain technology. The PoT protocol utilizes the practical
Byzantine fault tolerance (PBFT) consensus mechanism to synchronize global
states. To evaluate the performance of the protocol design, we present an
implementation of a decentralized training network (DTN) that adopts the PoT
protocol. Our results indicate that the protocol exhibits considerable
potential in terms of task throughput, system robustness, and network security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peihao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07072">
<title>Rician likelihood loss for quantitative MRI using self-supervised deep learning. (arXiv:2307.07072v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07072</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Previous quantitative MR imaging studies using self-supervised deep
learning have reported biased parameter estimates at low SNR. Such systematic
errors arise from the choice of Mean Squared Error (MSE) loss function for
network training, which is incompatible with Rician-distributed MR magnitude
signals. To address this issue, we introduce the negative log Rician likelihood
(NLR) loss. Methods: A numerically stable and accurate implementation of the
NLR loss was developed to estimate quantitative parameters of the apparent
diffusion coefficient (ADC) model and intra-voxel incoherent motion (IVIM)
model. Parameter estimation accuracy, precision and overall error were
evaluated in terms of bias, variance and root mean squared error and compared
against the MSE loss over a range of SNRs (5 - 30). Results: Networks trained
with NLR loss show higher estimation accuracy than MSE for the ADC and IVIM
diffusion coefficients as SNR decreases, with minimal loss of precision or
total error. At high effective SNR (high SNR and small diffusion coefficients),
both losses show comparable accuracy and precision for all parameters of both
models. Conclusion: The proposed NLR loss is numerically stable and accurate
across the full range of tested SNRs and improves parameter estimation accuracy
of diffusion coefficients using self-supervised deep learning. We expect the
development to benefit quantitative MR imaging techniques broadly, enabling
more accurate parameter estimation from noisy data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_C/0/1/0/all/0/1&quot;&gt;Christopher S. Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroder_A/0/1/0/all/0/1&quot;&gt;Anna Schroder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Sean C. Epstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1&quot;&gt;James Cole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1&quot;&gt;Daniel C. Alexander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07081">
<title>Kernel t-distributed stochastic neighbor embedding. (arXiv:2307.07081v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07081</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a kernelized version of the t-SNE algorithm, capable of
mapping high-dimensional data to a low-dimensional space while preserving the
pairwise distances between the data points in a non-Euclidean metric. This can
be achieved using a kernel trick only in the high dimensional space or in both
spaces, leading to an end-to-end kernelized version. The proposed kernelized
version of the t-SNE algorithm can offer new views on the relationships between
data points, which can improve performance and accuracy in particular
applications, such as classification problems involving kernel methods. The
differences between t-SNE and its kernelized version are illustrated for
several datasets, showing a neater clustering of points belonging to different
classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilie_Ablachim_D/0/1/0/all/0/1&quot;&gt;Denis C. Ilie-Ablachim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumitrescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Dumitrescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusu_C/0/1/0/all/0/1&quot;&gt;Cristian Rusu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07083">
<title>A Scenario-Based Functional Testing Approach to Improving DNN Performance. (arXiv:2307.07083v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07083</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a scenario-based functional testing approach for
enhancing the performance of machine learning (ML) applications. The proposed
method is an iterative process that starts with testing the ML model on various
scenarios to identify areas of weakness. It follows by a further testing on the
suspected weak scenarios and statistically evaluate the model&apos;s performance on
the scenarios to confirm the diagnosis. Once the diagnosis of weak scenarios is
confirmed by test results, the treatment of the model is performed by
retraining the model using a transfer learning technique with the original
model as the base and applying a set of training data specifically targeting
the treated scenarios plus a subset of training data selected at random from
the original train dataset to prevent the so-call catastrophic forgetting
effect. Finally, after the treatment, the model is assessed and evaluated again
by testing on the treated scenarios as well as other scenarios to check if the
treatment is effective and no side effect caused. The paper reports a case
study with a real ML deep neural network (DNN) model, which is the perception
system of an autonomous racing car. It is demonstrated that the method is
effective in the sense that DNN model&apos;s performance can be improved. It
provides an efficient method of enhancing ML model&apos;s performance with much less
human and compute resource than retrain from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thi Minh Tam Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benjumea_A/0/1/0/all/0/1&quot;&gt;Aduen Benjumea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1&quot;&gt;Andrew Bradley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07084">
<title>Safe Reinforcement Learning as Wasserstein Variational Inference: Formal Methods for Interpretability. (arXiv:2307.07084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07084</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning or optimal control can provide effective reasoning for
sequential decision-making problems with variable dynamics. Such reasoning in
practical implementation, however, poses a persistent challenge in interpreting
the reward function and corresponding optimal policy. Consequently, formalizing
the sequential decision-making problems as inference has a considerable value,
as probabilistic inference in principle offers diverse and powerful
mathematical tools to infer the stochastic dynamics whilst suggesting a
probabilistic interpretation of the reward design and policy convergence. In
this study, we propose a novel Adaptive Wasserstein Variational Optimization
(AWaVO) to tackle these challenges in sequential decision-making. Our approach
utilizes formal methods to provide interpretations of reward design,
transparency of training convergence, and probabilistic interpretation of
sequential decisions. To demonstrate practicality, we show convergent training
with guaranteed global convergence rates not only in simulation but also in
real robot tasks, and empirically verify a reasonable tradeoff between high
performance and conservative interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyle_D/0/1/0/all/0/1&quot;&gt;David Boyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07090">
<title>Choice Models and Permutation Invariance. (arXiv:2307.07090v1 [econ.EM])</title>
<link>http://arxiv.org/abs/2307.07090</link>
<description rdf:parseType="Literal">&lt;p&gt;Choice Modeling is at the core of many economics, operations, and marketing
problems. In this paper, we propose a fundamental characterization of choice
functions that encompasses a wide variety of extant choice models. We
demonstrate how nonparametric estimators like neural nets can easily
approximate such functionals and overcome the curse of dimensionality that is
inherent in the non-parametric estimation of choice functions. We demonstrate
through extensive simulations that our proposed functionals can flexibly
capture underlying consumer behavior in a completely data-driven fashion and
outperform traditional parametric models. As demand settings often exhibit
endogenous features, we extend our framework to incorporate estimation under
endogenous features. Further, we also describe a formal inference procedure to
construct valid confidence intervals on objects of interest like price
elasticity. Finally, to assess the practical applicability of our estimator, we
utilize a real-world dataset from S. Berry, Levinsohn, and Pakes (1995). Our
empirical analysis confirms that the estimator generates realistic and
comparable own- and cross-price elasticities that are consistent with the
observations reported in the existing literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amandeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Yoganarasimhan_H/0/1/0/all/0/1&quot;&gt;Hema Yoganarasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07091">
<title>Robotic Manipulation Datasets for Offline Compositional Reinforcement Learning. (arXiv:2307.07091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07091</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline reinforcement learning (RL) is a promising direction that allows RL
agents to pre-train on large datasets, avoiding the recurrence of expensive
data collection. To advance the field, it is crucial to generate large-scale
datasets. Compositional RL is particularly appealing for generating such large
datasets, since 1) it permits creating many tasks from few components, 2) the
task structure may enable trained agents to solve new tasks by combining
relevant learned components, and 3) the compositional dimensions provide a
notion of task relatedness. This paper provides four offline RL datasets for
simulated robotic manipulation created using the 256 tasks from CompoSuite
[Mendez et al., 2022a]. Each dataset is collected from an agent with a
different degree of performance, and consists of 256 million transitions. We
provide training and evaluation settings for assessing an agent&apos;s ability to
learn compositional task policies. Our benchmarking experiments on each setting
show that current offline RL methods can learn the training tasks to some
extent and that compositional methods significantly outperform
non-compositional methods. However, current methods are still unable to extract
the tasks&apos; compositional structure to generalize to unseen tasks, showing a
need for further research in offline compositional RL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussing_M/0/1/0/all/0/1&quot;&gt;Marcel Hussing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendez_J/0/1/0/all/0/1&quot;&gt;Jorge A. Mendez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singrodia_A/0/1/0/all/0/1&quot;&gt;Anisha Singrodia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1&quot;&gt;Cassandra Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1&quot;&gt;Eric Eaton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07093">
<title>MaxCorrMGNN: A Multi-Graph Neural Network Framework for Generalized Multimodal Fusion of Medical Data for Outcome Prediction. (arXiv:2307.07093v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07093</link>
<description rdf:parseType="Literal">&lt;p&gt;With the emergence of multimodal electronic health records, the evidence for
an outcome may be captured across multiple modalities ranging from clinical to
imaging and genomic data. Predicting outcomes effectively requires fusion
frameworks capable of modeling fine-grained and multi-faceted complex
interactions between modality features within and across patients. We develop
an innovative fusion approach called MaxCorr MGNN that models non-linear
modality correlations within and across patients through
Hirschfeld-Gebelein-Renyi maximal correlation (MaxCorr) embeddings, resulting
in a multi-layered graph that preserves the identities of the modalities and
patients. We then design, for the first time, a generalized multi-layered graph
neural network (MGNN) for task-informed reasoning in multi-layered graphs, that
learns the parameters defining patient-modality graph connectivity and message
passing in an end-to-end fashion. We evaluate our model an outcome prediction
task on a Tuberculosis (TB) dataset consistently outperforming several
state-of-the-art neural, graph-based and traditional fusion techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DSouza_N/0/1/0/all/0/1&quot;&gt;Niharika S. D&amp;#x27;Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1&quot;&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foncubierta_Rodriguez_A/0/1/0/all/0/1&quot;&gt;Antonio Foncubierta-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beck_K/0/1/0/all/0/1&quot;&gt;Kristen L. Beck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyko_O/0/1/0/all/0/1&quot;&gt;Orest Boyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1&quot;&gt;Tanveer Syeda-Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07107">
<title>Graph Positional and Structural Encoder. (arXiv:2307.07107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07107</link>
<description rdf:parseType="Literal">&lt;p&gt;Positional and structural encodings (PSE) enable better identifiability of
nodes within a graph, as in general graphs lack a canonical node ordering. This
renders PSEs essential tools for empowering modern GNNs, and in particular
graph Transformers. However, designing PSEs that work optimally for a variety
of graph prediction tasks is a challenging and unsolved problem. Here, we
present the graph positional and structural encoder (GPSE), a first-ever
attempt to train a graph encoder that captures rich PSE representations for
augmenting any GNN. GPSE can effectively learn a common latent representation
for multiple PSEs, and is highly transferable. The encoder trained on a
particular graph dataset can be used effectively on datasets drawn from
significantly different distributions and even modalities. We show that across
a wide range of benchmarks, GPSE-enhanced models can significantly improve the
performance in certain tasks, while performing on par with those that employ
explicitly computed PSEs in other cases. Our results pave the way for the
development of large pre-trained models for extracting graph positional and
structural information and highlight their potential as a viable alternative to
explicitly computed PSEs as well as to existing self-supervised pre-training
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Renming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canturk_S/0/1/0/all/0/1&quot;&gt;Semih Cant&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapointe_Gagne_O/0/1/0/all/0/1&quot;&gt;Olivier Lapointe-Gagn&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letourneau_V/0/1/0/all/0/1&quot;&gt;Vincent L&amp;#xe9;tourneau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1&quot;&gt;Guy Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beaini_D/0/1/0/all/0/1&quot;&gt;Dominique Beaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampasek_L/0/1/0/all/0/1&quot;&gt;Ladislav Ramp&amp;#xe1;&amp;#x161;ek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07113">
<title>Variance-reduced accelerated methods for decentralized stochastic double-regularized nonconvex strongly-concave minimax problems. (arXiv:2307.07113v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider the decentralized, stochastic nonconvex
strongly-concave (NCSC) minimax problem with nonsmooth regularization terms on
both primal and dual variables, wherein a network of $m$ computing agents
collaborate via peer-to-peer communications. We consider when the coupling
function is in expectation or finite-sum form and the double regularizers are
convex functions, applied separately to the primal and dual variables. Our
algorithmic framework introduces a Lagrangian multiplier to eliminate the
consensus constraint on the dual variable. Coupling this with
variance-reduction (VR) techniques, our proposed method, entitled VRLM, by a
single neighbor communication per iteration, is able to achieve an
$\mathcal{O}(\kappa^3\varepsilon^{-3})$ sample complexity under the general
stochastic setting, with either a big-batch or small-batch VR option, where
$\kappa$ is the condition number of the problem and $\varepsilon$ is the
desired solution accuracy. With a big-batch VR, we can additionally achieve
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity. Under the
special finite-sum setting, our method with a big-batch VR can achieve an
$\mathcal{O}(n + \sqrt{n} \kappa^2\varepsilon^{-2})$ sample complexity and
$\mathcal{O}(\kappa^2\varepsilon^{-2})$ communication complexity, where $n$ is
the number of components in the finite sum. All complexity results match the
best-known results achieved by a few existing methods for solving special cases
of the problem we consider. To the best of our knowledge, this is the first
work which provides convergence guarantees for NCSC minimax problems with
general convex nonsmooth regularizers applied to both the primal and dual
variables in the decentralized stochastic setting. Numerical experiments are
conducted on two machine learning problems. Our code is downloadable from
https://github.com/RPI-OPT/VRLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mancino_Ball_G/0/1/0/all/0/1&quot;&gt;Gabriel Mancino-Ball&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07119">
<title>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07119</link>
<description rdf:parseType="Literal">&lt;p&gt;Current automated machine learning (ML) tools are model-centric, focusing on
model selection and parameter optimization. However, the majority of the time
in data analysis is devoted to data cleaning and wrangling, for which limited
tools are available. Here we present DataAssist, an automated data preparation
and cleaning platform that enhances dataset quality using ML-informed methods.
We show that DataAssist provides a pipeline for exploratory data analysis and
data cleaning, including generating visualization for user-selected variables,
unifying data annotation, suggesting anomaly removal, and preprocessing data.
The exported dataset can be readily integrated with other autoML tools or
user-specified model for downstream analysis. Our data-centric tool is
applicable to a variety of fields, including economics, business, and
forecasting applications saving over 50\% time of the time spent on data
cleansing and preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1&quot;&gt;Kartikay Goyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1&quot;&gt;Vakul Goyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07134">
<title>Multi-Dimensional Ability Diagnosis for Machine Learning Algorithms. (arXiv:2307.07134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07134</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning algorithms have become ubiquitous in a number of
applications (e.g. image classification). However, due to the insufficient
measurement of traditional metrics (e.g. the coarse-grained Accuracy of each
classifier), substantial gaps are usually observed between the real-world
performance of these algorithms and their scores in standardized evaluations.
In this paper, inspired by the psychometric theories from human measurement, we
propose a task-agnostic evaluation framework Camilla, where a multi-dimensional
diagnostic metric Ability is defined for collaboratively measuring the
multifaceted strength of each machine learning algorithm. Specifically, given
the response logs from different algorithms to data samples, we leverage
cognitive diagnosis assumptions and neural networks to learn the complex
interactions among algorithms, samples and the skills (explicitly or implicitly
pre-defined) of each sample. In this way, both the abilities of each algorithm
on multiple skills and some of the sample factors (e.g. sample difficulty) can
be simultaneously quantified. We conduct extensive experiments with hundreds of
machine learning algorithms on four public datasets, and our experimental
results demonstrate that Camilla not only can capture the pros and cons of each
algorithm more precisely, but also outperforms state-of-the-art baselines on
the metric reliability, rank consistency and rank stability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zheng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhenya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanren Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07160">
<title>Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07160</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel task-agnostic in-domain pre-training method that sits
between generic pre-training and fine-tuning. Our approach selectively masks
in-domain keywords, i.e., words that provide a compact representation of the
target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We
evaluate our approach using six different settings: three datasets combined
with two distinct pre-trained language models (PLMs). Our results reveal that
the fine-tuned PLMs adapted using our in-domain pre-training strategy
outperform PLMs that used in-domain pre-training with random masking as well as
those that followed the common pre-train-then-fine-tune paradigm. Further, the
overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the
pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1&quot;&gt;Shahriar Golchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavabi_N/0/1/0/all/0/1&quot;&gt;Nazgol Tavabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiapour_A/0/1/0/all/0/1&quot;&gt;Ata Kiapour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07167">
<title>Vulnerability-Aware Instance Reweighting For Adversarial Training. (arXiv:2307.07167v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07167</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial Training (AT) has been found to substantially improve the
robustness of deep learning classifiers against adversarial attacks. AT
involves obtaining robustness by including adversarial examples in training a
classifier. Most variants of AT algorithms treat every training example
equally. However, recent works have shown that better performance is achievable
by treating them unequally. In addition, it has been observed that AT exerts an
uneven influence on different classes in a training set and unfairly hurts
examples corresponding to classes that are inherently harder to classify.
Consequently, various reweighting schemes have been proposed that assign
unequal weights to robust losses of individual examples in a training set. In
this work, we propose a novel instance-wise reweighting scheme. It considers
the vulnerability of each natural example and the resulting information loss on
its adversarial counterpart occasioned by adversarial attacks. Through
extensive experiments, we show that our proposed method significantly improves
over existing reweighting schemes, especially against strong white and
black-box attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fakorede_O/0/1/0/all/0/1&quot;&gt;Olukorede Fakorede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nirala_A/0/1/0/all/0/1&quot;&gt;Ashutosh Kumar Nirala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atsague_M/0/1/0/all/0/1&quot;&gt;Modeste Atsague&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jin Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07171">
<title>Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07171</link>
<description rdf:parseType="Literal">&lt;p&gt;Although large language models (LLMs) have achieved great success in vast
real-world applications, their vulnerabilities towards noisy inputs have
significantly limited their uses, especially in high-stake environments. In
these contexts, it is crucial to ensure that every prediction made by large
language models is stable, i.e., LLM predictions should be consistent given
minor differences in the input. This largely falls into the study of certified
robust LLMs, i.e., all predictions of LLM are certified to be correct in a
local region around the input. Randomized smoothing has demonstrated great
potential in certifying the robustness and prediction stability of LLMs.
However, randomized smoothing requires adding noise to the input before model
prediction, and its certification performance depends largely on the model&apos;s
performance on corrupted data. As a result, its direct application to LLMs
remains challenging and often results in a small certification radius. To
address this issue, we take advantage of the multitasking nature of LLMs and
propose to denoise the corrupted inputs with LLMs in a self-denoising manner.
Different from previous works like denoised smoothing, which requires training
a separate model to robustify LLM, our method enjoys far better efficiency and
flexibility. Our experiment results show that our method outperforms the
existing certification methods under both certified robustness and empirical
robustness. The codes are available at
https://github.com/UCSB-NLP-Chang/SelfDenoise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bairu Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07172">
<title>FedBIAD: Communication-Efficient and Accuracy-Guaranteed Federated Learning with Bayesian Inference-Based Adaptive Dropout. (arXiv:2307.07172v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.07172</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) emerges as a distributed machine learning paradigm
without end-user data transmission, effectively avoiding privacy leakage.
Participating devices in FL are usually bandwidth-constrained, and the uplink
is much slower than the downlink in wireless networks, which causes a severe
uplink communication bottleneck. A prominent direction to alleviate this
problem is federated dropout, which drops fractional weights of local models.
However, existing federated dropout studies focus on random or ordered dropout
and lack theoretical support, resulting in unguaranteed performance. In this
paper, we propose Federated learning with Bayesian Inference-based Adaptive
Dropout (FedBIAD), which regards weight rows of local models as probability
distributions and adaptively drops partial weight rows based on importance
indicators correlated with the trend of local training loss. By applying
FedBIAD, each client adaptively selects a high-quality dropping pattern with
accurate approximations and only transmits parameters of non-dropped weight
rows to mitigate uplink costs while improving accuracy. Theoretical analysis
demonstrates that the convergence rate of the average generalization error of
FedBIAD is minimax optimal up to a squared logarithmic factor. Extensive
experiments on image classification and next-word prediction show that compared
with status quo approaches, FedBIAD provides 2x uplink reduction with an
accuracy increase of up to 2.41% even on non-Independent and Identically
Distributed (non-IID) data, which brings up to 72% decrease in training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jingjing Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuefeng Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07176">
<title>Safe DreamerV3: Safe Reinforcement Learning with World Models. (arXiv:2307.07176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07176</link>
<description rdf:parseType="Literal">&lt;p&gt;The widespread application of Reinforcement Learning (RL) in real-world
situations is yet to come to fruition, largely as a result of its failure to
satisfy the essential safety demands of such systems. Existing safe
reinforcement learning (SafeRL) methods, employing cost functions to enhance
safety, fail to achieve zero-cost in complex scenarios, including vision-only
tasks, even with comprehensive data sampling and training. To address this, we
introduce Safe DreamerV3, a novel algorithm that integrates both
Lagrangian-based and planning-based methods within a world model. Our
methodology represents a significant advancement in SafeRL as the first
algorithm to achieve nearly zero-cost in both low-dimensional and vision-only
tasks within the Safety-Gymnasium benchmark. Our project website can be found
in: https://sites.google.com/view/safedreamerv3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiaming Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chunhe Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07178">
<title>A Surrogate Data Assimilation Model for the Estimation of Dynamical System in a Limited Area. (arXiv:2307.07178v1 [math.NA])</title>
<link>http://arxiv.org/abs/2307.07178</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel learning-based surrogate data assimilation (DA) model for
efficient state estimation in a limited area. Our model employs a feedforward
neural network for online computation, eliminating the need for integrating
high-dimensional limited-area models. This approach offers significant
computational advantages over traditional DA algorithms. Furthermore, our
method avoids the requirement of lateral boundary conditions for the
limited-area model in both online and offline computations. The design of our
surrogate DA model is built upon a robust theoretical framework that leverages
two fundamental concepts: observability and effective region. The concept of
observability enables us to quantitatively determine the optimal amount of
observation data necessary for accurate DA. Meanwhile, the concept of effective
region substantially reduces the computational burden associated with computing
observability and generating training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kang_W/0/1/0/all/0/1&quot;&gt;Wei Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07181">
<title>DISPEL: Domain Generalization via Domain-Specific Liberating. (arXiv:2307.07181v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07181</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization aims to learn a generalization model that can perform
well on unseen test domains by only training on limited source domains.
However, existing domain generalization approaches often bring in
prediction-irrelevant noise or require the collection of domain labels. To
address these challenges, we consider the domain generalization problem from a
different perspective by categorizing underlying feature groups into
domain-shared and domain-specific features. Nevertheless, the domain-specific
features are difficult to be identified and distinguished from the input data.
In this work, we propose DomaIn-SPEcific Liberating (DISPEL), a post-processing
fine-grained masking approach that can filter out undefined and
indistinguishable domain-specific features in the embedding space.
Specifically, DISPEL utilizes a mask generator that produces a unique mask for
each input data to filter domain-specific features. The DISPEL framework is
highly flexible to be applied to any fine-tuned models. We derive a
generalization error bound to guarantee the generalization performance by
optimizing a designed objective loss. The experimental results on five
benchmarks demonstrate DISPEL outperforms existing methods and can further
generalize various algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Yuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yu-Neng Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanchu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_Z/0/1/0/all/0/1&quot;&gt;Zou Na&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07189">
<title>Multiplicative update rules for accelerating deep learning training and increasing robustness. (arXiv:2307.07189v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07189</link>
<description rdf:parseType="Literal">&lt;p&gt;Even nowadays, where Deep Learning (DL) has achieved state-of-the-art
performance in a wide range of research domains, accelerating training and
building robust DL models remains a challenging task. To this end, generations
of researchers have pursued to develop robust methods for training DL
architectures that can be less sensitive to weight distributions, model
architectures and loss landscapes. However, such methods are limited to
adaptive learning rate optimizers, initialization schemes, and clipping
gradients without investigating the fundamental rule of parameters update.
Although multiplicative updates have contributed significantly to the early
development of machine learning and hold strong theoretical claims, to best of
our knowledge, this is the first work that investigate them in context of DL
training acceleration and robustness. In this work, we propose an optimization
framework that fits to a wide range of optimization algorithms and enables one
to apply alternative update rules. To this end, we propose a novel
multiplicative update rule and we extend their capabilities by combining it
with a traditional additive update term, under a novel hybrid update method. We
claim that the proposed framework accelerates training, while leading to more
robust models in contrast to traditionally used additive update rule and we
experimentally demonstrate their effectiveness in a wide range of task and
optimization methods. Such tasks ranging from convex and non-convex
optimization to difficult image classification benchmarks applying a wide range
of traditionally used optimization methods and Deep Neural Network (DNN)
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirtas_M/0/1/0/all/0/1&quot;&gt;Manos Kirtas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Passalis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1&quot;&gt;Anastasios Tefas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07191">
<title>Benchmarks and Custom Package for Electrical Load Forecasting. (arXiv:2307.07191v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07191</link>
<description rdf:parseType="Literal">&lt;p&gt;Load forecasting is of great significance in the power industry as it can
provide a reference for subsequent tasks such as power grid dispatch, thus
bringing huge economic benefits. However, there are many differences between
load forecasting and traditional time series forecasting. On the one hand, load
forecasting aims to minimize the cost of subsequent tasks such as power grid
dispatch, rather than simply pursuing prediction accuracy. On the other hand,
the load is largely influenced by many external factors, such as temperature or
calendar variables. In addition, the scale of predictions (such as
building-level loads and aggregated-level loads) can also significantly impact
the predicted results. In this paper, we provide a comprehensive load
forecasting archive, which includes load domain-specific feature engineering to
help forecasting models better model load data. In addition, different from the
traditional loss function which only aims for accuracy, we also provide a
method to customize the loss function based on the forecasting error,
integrating it into our forecasting framework. Based on this, we conducted
extensive experiments on load data at different levels, providing a reference
for researchers to compare different load forecasting models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhixian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1&quot;&gt;Qingsong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Liang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krannichfeldt_L/0/1/0/all/0/1&quot;&gt;Leandro Von Krannichfeldt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07195">
<title>Controlling dynamical systems to complex target states using machine learning: next-generation vs. classical reservoir computing. (arXiv:2307.07195v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07195</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling nonlinear dynamical systems using machine learning allows to not
only drive systems into simple behavior like periodicity but also to more
complex arbitrary dynamics. For this, it is crucial that a machine learning
system can be trained to reproduce the target dynamics sufficiently well. On
the example of forcing a chaotic parametrization of the Lorenz system into
intermittent dynamics, we show first that classical reservoir computing excels
at this task. In a next step, we compare those results based on different
amounts of training data to an alternative setup, where next-generation
reservoir computing is used instead. It turns out that while delivering
comparable performance for usual amounts of training data, next-generation RC
significantly outperforms in situations where only very limited data is
available. This opens even further practical control applications in real world
problems where data is restricted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haluszczynski_A/0/1/0/all/0/1&quot;&gt;Alexander Haluszczynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koglmayr_D/0/1/0/all/0/1&quot;&gt;Daniel K&amp;#xf6;glmayr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rath_C/0/1/0/all/0/1&quot;&gt;Christoph R&amp;#xe4;th&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07199">
<title>Ed-Fed: A generic federated learning framework with resource-aware client selection for edge devices. (arXiv:2307.07199v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.07199</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has evolved as a prominent method for edge devices to
cooperatively create a unified prediction model while securing their sensitive
training data local to the device. Despite the existence of numerous research
frameworks for simulating FL algorithms, they do not facilitate comprehensive
deployment for automatic speech recognition tasks on heterogeneous edge
devices. This is where Ed-Fed, a comprehensive and generic FL framework, comes
in as a foundation for future practical FL system research. We also propose a
novel resource-aware client selection algorithm to optimise the waiting time in
the FL settings. We show that our approach can handle the straggler devices and
dynamically set the training time for the selected devices in a round. Our
evaluation has shown that the proposed approach significantly optimises waiting
time in FL compared to conventional random client selection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasindran_Z/0/1/0/all/0/1&quot;&gt;Zitha Sasindran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yelchuri_H/0/1/0/all/0/1&quot;&gt;Harsha Yelchuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakar_T/0/1/0/all/0/1&quot;&gt;T. V. Prabhakar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07246">
<title>Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07246</link>
<description rdf:parseType="Literal">&lt;p&gt;The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Cheng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rongjun Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07250">
<title>Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning. (arXiv:2307.07250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07250</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples derived from deliberately crafted perturbations on
visual inputs can easily harm decision process of deep neural networks. To
prevent potential threats, various adversarial training-based defense methods
have grown rapidly and become a de facto standard approach for robustness.
Despite recent competitive achievements, we observe that adversarial
vulnerability varies across targets and certain vulnerabilities remain
prevalent. Intriguingly, such peculiar phenomenon cannot be relieved even with
deeper architectures and advanced defense methods. To address this issue, in
this paper, we introduce a causal approach called Adversarial Double Machine
Learning (ADML), which allows us to quantify the degree of adversarial
vulnerability for network predictions and capture the effect of treatments on
outcome of interests. ADML can directly estimate causal parameter of
adversarial perturbations per se and mitigate negative effects that can
potentially damage robustness, bridging a causal perspective into the
adversarial vulnerability. Through extensive experiments on various CNN and
Transformer architectures, we corroborate that ADML improves adversarial
robustness with large margins and relieve the empirical observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung-Kwan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07264">
<title>On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07264</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with expert advice and multi-armed bandit are two classic online
decision problems which differ on how the information is observed in each round
of the game. We study a family of problems interpolating the two. For a vector
$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB
indicates that the arms are partitioned into $K$ groups and the $i$-th group
contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same
group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB
and design an optimal PAC algorithm for its pure exploration version,
$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with
as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB
is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number
of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is
$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Both
our upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to a
more general setting, namely the bandit with graph feedback, in terms of the
clique cover and related graph parameters. As consequences, we obtained tight
minimax regret bounds for several families of feedback graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Houshuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuchen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chihao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07269">
<title>Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation. (arXiv:2307.07269v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07269</link>
<description rdf:parseType="Literal">&lt;p&gt;It is imperative to ensure the robustness of deep learning models in critical
applications such as, healthcare. While recent advances in deep learning have
improved the performance of volumetric medical image segmentation models, these
models cannot be deployed for real-world applications immediately due to their
vulnerability to adversarial attacks. We present a 3D frequency domain
adversarial attack for volumetric medical image segmentation models and
demonstrate its advantages over conventional input or voxel domain attacks.
Using our proposed attack, we introduce a novel frequency domain adversarial
training approach for optimizing a robust model against voxel and frequency
domain attacks. Moreover, we propose frequency consistency loss to regulate our
frequency domain adversarial training that achieves a better tradeoff between
model&apos;s performance on clean and adversarial samples. Code is publicly
available at https://github.com/asif-hanif/vafa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hanif_A/0/1/0/all/0/1&quot;&gt;Asif Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07296">
<title>Reinforcement Learning with Frontier-Based Exploration via Autonomous Environment. (arXiv:2307.07296v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07296</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Simultaneous Localisation and Mapping (SLAM) is a critical problem in
autonomous robotics, enabling robots to navigate to new regions while building
an accurate model of their surroundings. Visual SLAM is a popular technique
that uses virtual elements to enhance the experience. However, existing
frontier-based exploration strategies can lead to a non-optimal path in
scenarios where there are multiple frontiers with similar distance. This issue
can impact the efficiency and accuracy of Visual SLAM, which is crucial for a
wide range of robotic applications, such as search and rescue, exploration, and
mapping. To address this issue, this research combines both an existing
Visual-Graph SLAM known as ExploreORB with reinforcement learning. The proposed
algorithm allows the robot to learn and optimize exploration routes through a
reward-based system to create an accurate map of the environment with proper
frontier selection. Frontier-based exploration is used to detect unexplored
areas, while reinforcement learning optimizes the robot&apos;s movement by assigning
rewards for optimal frontier points. Graph SLAM is then used to integrate the
robot&apos;s sensory data and build an accurate map of the environment. The proposed
algorithm aims to improve the efficiency and accuracy of ExploreORB by
optimizing the exploration process of frontiers to build a more accurate map.
To evaluate the effectiveness of the proposed approach, experiments will be
conducted in various virtual environments using Gazebo, a robot simulation
software. Results of these experiments will be compared with existing methods
to demonstrate the potential of the proposed approach as an optimal solution
for SLAM in autonomous robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leong_K/0/1/0/all/0/1&quot;&gt;Kenji Leong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07298">
<title>3D Shape-Based Myocardial Infarction Prediction Using Point Cloud Classification Networks. (arXiv:2307.07298v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07298</link>
<description rdf:parseType="Literal">&lt;p&gt;Myocardial infarction (MI) is one of the most prevalent cardiovascular
diseases with associated clinical decision-making typically based on
single-valued imaging biomarkers. However, such metrics only approximate the
complex 3D structure and physiology of the heart and hence hinder a better
understanding and prediction of MI outcomes. In this work, we investigate the
utility of complete 3D cardiac shapes in the form of point clouds for an
improved detection of MI events. To this end, we propose a fully automatic
multi-step pipeline consisting of a 3D cardiac surface reconstruction step
followed by a point cloud classification network. Our method utilizes recent
advances in geometric deep learning on point clouds to enable direct and
efficient multi-scale learning on high-resolution surface models of the cardiac
anatomy. We evaluate our approach on 1068 UK Biobank subjects for the tasks of
prevalent MI detection and incident MI prediction and find improvements of ~13%
and ~5% respectively over clinical benchmarks. Furthermore, we analyze the role
of each ventricle and cardiac phase for 3D shape-based MI detection and conduct
a visual analysis of the morphological and physiological patterns typically
associated with MI outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beetz_M/0/1/0/all/0/1&quot;&gt;Marcel Beetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yilong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Abhirup Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grau_V/0/1/0/all/0/1&quot;&gt;Vicente Grau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07302">
<title>Solving higher-order Lane-Emden-Fowler type equations using physics-informed neural networks: benchmark tests comparing soft and hard constraints. (arXiv:2307.07302v1 [physics.comp-ph])</title>
<link>http://arxiv.org/abs/2307.07302</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, numerical methods using Physics-Informed Neural Networks
(PINNs) are presented with the aim to solve higher-order ordinary differential
equations (ODEs). Indeed, this deep-learning technique is successfully applied
for solving different classes of singular ODEs, namely the well known
second-order Lane-Emden equations, third order-order Emden-Fowler equations,
and fourth-order Lane-Emden-Fowler equations. Two variants of PINNs technique
are considered and compared. First, a minimization procedure is used to
constrain the total loss function of the neural network, in which the equation
residual is considered with some weight to form a physics-based loss and added
to the training data loss that contains the initial/boundary conditions.
Second, a specific choice of trial solutions ensuring these conditions as hard
constraints is done in order to satisfy the differential equation, contrary to
the first variant based on training data where the constraints appear as soft
ones. Advantages and drawbacks of PINNs variants are highlighted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Baty_H/0/1/0/all/0/1&quot;&gt;Hubert Baty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07313">
<title>HEAL-SWIN: A Vision Transformer On The Sphere. (arXiv:2307.07313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07313</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution wide-angle fisheye images are becoming more and more
important for robotics applications such as autonomous driving. However, using
ordinary convolutional neural networks or vision transformers on this data is
problematic due to projection and distortion losses introduced when projecting
to a rectangular grid on the plane. We introduce the HEAL-SWIN transformer,
which combines the highly uniform Hierarchical Equal Area iso-Latitude
Pixelation (HEALPix) grid used in astrophysics and cosmology with the
Hierarchical Shifted-Window (SWIN) transformer to yield an efficient and
flexible model capable of training on high-resolution, distortion-free
spherical data. In HEAL-SWIN, the nested structure of the HEALPix grid is used
to perform the patching and windowing operations of the SWIN transformer,
resulting in a one-dimensional representation of the spherical data with
minimal computational overhead. We demonstrate the superior performance of our
model for semantic segmentation and depth regression tasks on both synthetic
and real automotive datasets. Our code is available at
https://github.com/JanEGerken/HEAL-SWIN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlsson_O/0/1/0/all/0/1&quot;&gt;Oscar Carlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1&quot;&gt;Jan E. Gerken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1&quot;&gt;Hampus Linander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spiess_H/0/1/0/all/0/1&quot;&gt;Heiner Spie&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohlsson_F/0/1/0/all/0/1&quot;&gt;Fredrik Ohlsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1&quot;&gt;Christoffer Petersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Persson_D/0/1/0/all/0/1&quot;&gt;Daniel Persson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07317">
<title>Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.07317</link>
<description rdf:parseType="Literal">&lt;p&gt;Online news outlets are grappling with the moderation of user-generated
content within their comment section. We present a recommender system based on
ranking class probabilities to support and empower the moderator in choosing
featured posts, a time-consuming task. By combining user and textual content
features we obtain an optimal classification F1-score of 0.44 on the test set.
Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of
validation articles. As an expert evaluation, content moderators assessed the
output of a random selection of articles by choosing comments to feature based
on the recommendations, which resulted in a NDCG score of 0.83. We conclude
that first, adding text features yields the best score and second, while
choosing featured content remains somewhat subjective, content moderators found
suitable comments in all but one evaluated recommendations. We end the paper by
analyzing our best-performing model, a step towards transparency and
explainability in hybrid content moderation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterschoot_C/0/1/0/all/0/1&quot;&gt;Cedric Waterschoot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosch_A/0/1/0/all/0/1&quot;&gt;Antal van den Bosch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07320">
<title>Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])</title>
<link>http://arxiv.org/abs/2307.07320</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential data collection has emerged as a widely adopted technique for
enhancing the efficiency of data gathering processes. Despite its advantages,
such data collection mechanism often introduces complexities to the statistical
inference procedure. For instance, the ordinary least squares (OLS) estimator
in an adaptive linear regression model can exhibit non-normal asymptotic
behavior, posing challenges for accurate inference and interpretation. In this
paper, we propose a general method for constructing debiased estimator which
remedies this issue. It makes use of the idea of adaptive linear estimating
equations, and we establish theoretical guarantees of asymptotic normality,
supplemented by discussions on achieving near-optimal asymptotic variance. A
salient feature of our estimator is that in the context of multi-armed bandits,
our estimator retains the non-asymptotic performance of the least square
estimator while obtaining asymptotic normality property. Consequently, this
work helps connect two fruitful paradigms of adaptive inference: a)
non-asymptotic inference using concentration inequalities and b) asymptotic
inference via asymptotic normality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ying_M/0/1/0/all/0/1&quot;&gt;Mufang Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Khamaru_K/0/1/0/all/0/1&quot;&gt;Koulik Khamaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cun-Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07322">
<title>A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.07322</link>
<description rdf:parseType="Literal">&lt;p&gt;The current cut selection algorithm used in mixed-integer programming solvers
has remained largely unchanged since its creation. In this paper, we propose a
set of new cut scoring measures, cut filtering techniques, and stopping
criteria, extending the current state-of-the-art algorithm and obtaining a 4\%
performance improvement for SCIP over the MIPLIB 2017 benchmark set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Turner_M/0/1/0/all/0/1&quot;&gt;Mark Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Berthold_T/0/1/0/all/0/1&quot;&gt;Timo Berthold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Besancon_M/0/1/0/all/0/1&quot;&gt;Mathieu Besan&amp;#xe7;on&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07325">
<title>Representation Learning With Hidden Unit Clustering For Low Resource Speech Applications. (arXiv:2307.07325v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.07325</link>
<description rdf:parseType="Literal">&lt;p&gt;The representation learning of speech, without textual resources, is an area
of significant interest for many low resource speech applications. In this
paper, we describe an approach to self-supervised representation learning from
raw audio using a hidden unit clustering (HUC) framework. The input to the
model consists of audio samples that are windowed and processed with 1-D
convolutional layers. The learned &quot;time-frequency&quot; representations from the
convolutional neural network (CNN) module are further processed with long short
term memory (LSTM) layers which generate a contextual vector representation for
every windowed segment. The HUC framework, allowing the categorization of the
representations into a small number of phoneme-like units, is used to train the
model for learning semantically rich speech representations. The targets
consist of phoneme-like pseudo labels for each audio segment and these are
generated with an iterative k-means algorithm. We explore techniques that
improve the speaker invariance of the learned representations and illustrate
the effectiveness of the proposed approach on two settings, i) completely
unsupervised speech applications on the sub-tasks described as part of the
ZeroSpeech 2021 challenge and ii) semi-supervised automatic speech recognition
(ASR) applications on the TIMIT dataset and on the GramVaani challenge Hindi
dataset. In these experiments, we achieve state-of-art results for various
ZeroSpeech tasks. Further, on the ASR experiments, the HUC representations are
shown to improve significantly over other established benchmarks based on
Wav2vec, HuBERT and Best-RQ.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krishna_V/0/1/0/all/0/1&quot;&gt;Varun Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sai_T/0/1/0/all/0/1&quot;&gt;Tarun Sai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ganapathy_S/0/1/0/all/0/1&quot;&gt;Sriram Ganapathy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07328">
<title>Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy. (arXiv:2307.07328v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.07328</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-poisoning based backdoor attacks aim to insert backdoor into models by
manipulating training datasets without controlling the training process of the
target model. Existing attack methods mainly focus on designing triggers or
fusion strategies between triggers and benign samples. However, they often
randomly select samples to be poisoned, disregarding the varying importance of
each poisoning sample in terms of backdoor injection. A recent selection
strategy filters a fixed-size poisoning sample pool by recording forgetting
events, but it fails to consider the remaining samples outside the pool from a
global perspective. Moreover, computing forgetting events requires significant
additional computing resources. Therefore, how to efficiently and effectively
select poisoning samples from the entire dataset is an urgent problem in
backdoor attacks.To address it, firstly, we introduce a poisoning mask into the
regular backdoor training loss. We suppose that a backdoored model training
with hard poisoning samples has a more backdoor effect on easy ones, which can
be implemented by hindering the normal training process (\ie, maximizing loss
\wrt mask). To further integrate it with normal training process, we then
propose a learnable poisoning sample selection strategy to learn the mask
together with the model parameters through a min-max optimization.Specifically,
the outer loop aims to achieve the backdoor attack goal by minimizing the loss
based on the selected samples, while the inner loop selects hard poisoning
samples that impede this goal by maximizing the loss. After several rounds of
adversarial training, we finally select effective poisoning samples with high
contribution. Extensive experiments on benchmark datasets demonstrate the
effectiveness and efficiency of our approach in boosting backdoor attack
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shaokui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yanbo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07331">
<title>How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated how to assess the stereotypical bias in
pre-trained English language models. In this work, we extend this branch of
research in multiple different dimensions by systematically investigating (a)
mono- and multilingual models of (b) different underlying architectures with
respect to their bias in (c) multiple different languages. To that end, we make
use of the English StereoSet data set (Nadeem et al., 2021), which we
semi-automatically translate into German, French, Spanish, and Turkish. We find
that it is of major importance to conduct this type of analysis in a
multilingual setting, as our experiments show a much more nuanced picture as
well as notable differences from the English-only analysis. The main takeaways
from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical
behavior across languages, English (monolingual) models exhibit the strongest
bias, and the stereotypes reflected in the data set are least present in
Turkish models. Finally, we release our codebase alongside the translated data
sets and practical guidelines for the semi-automatic translation to encourage a
further extension of our work to other languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozturk_I/0/1/0/all/0/1&quot;&gt;Ibrahim Tolga &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nedelchev_R/0/1/0/all/0/1&quot;&gt;Rostislav Nedelchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1&quot;&gt;Christian Heumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arias_E/0/1/0/all/0/1&quot;&gt;Esteban Garces Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roger_M/0/1/0/all/0/1&quot;&gt;Marius Roger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1&quot;&gt;Matthias A&amp;#xdf;enmacher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07343">
<title>MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model&apos;s Parameters. (arXiv:2307.07343v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07343</link>
<description rdf:parseType="Literal">&lt;p&gt;The selection of model&apos;s parameters plays an important role in the
application of support vector classification (SVC). The commonly used method of
selecting model&apos;s parameters is the k-fold cross validation with grid search
(CV). It is extremely time-consuming because it needs to train a large number
of SVC models. In this paper, a new method is proposed to train SVC with the
selection of model&apos;s parameters. Firstly, training SVC with the selection of
model&apos;s parameters is modeled as a minimax optimization problem
(MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization
problem of finding the closest points between two normal convex hulls
(L2-SVC-NCH) while the maximization problem is an optimization problem of
finding the optimal model&apos;s parameters. A lower time complexity can be expected
in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is
then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a
projected gradient algorithm (PGA) while the maximization problem is solved by
a gradient ascent algorithm with dynamic learning rate. To demonstrate the
advantages of the PGA in solving L2-SVC-NCH, we carry out a comparison of the
PGA and the famous sequential minimal optimization (SMO) algorithm after a SMO
algorithm and some KKT conditions for L2-SVC-NCH are provided. It is revealed
that the SMO algorithm is a special case of the PGA. Thus, the PGA can provide
more flexibility. The comparative experiments between MaxMin-L2-SVC-NCH and the
classical parameter selection models on public datasets show that
MaxMin-L2-SVC-NCH greatly reduces the number of models to be trained and the
test accuracy is not lost to the classical models. It indicates that
MaxMin-L2-SVC-NCH performs better than the other models. We strongly recommend
MaxMin-L2-SVC-NCH as a preferred model for SVC task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Linkai Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiaoling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziyang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07344">
<title>Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural Networks. (arXiv:2307.07344v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07344</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel approach to integrating partial differential
equation (PDE)-based evolution models into neural networks through a new type
of regularization. Specifically, we propose inverse evolution layers (IELs)
based on evolution equations. These layers can achieve specific regularization
objectives and endow neural networks&apos; outputs with corresponding properties of
the evolution models. Moreover, IELs are straightforward to construct and
implement, and can be easily designed for various physical evolutions and
neural networks. Additionally, the design process for these layers can provide
neural networks with intuitive and mathematical interpretability, thus
enhancing the transparency and explainability of the approach. To demonstrate
the effectiveness, efficiency, and simplicity of our approach, we present an
example of endowing semantic segmentation models with the smoothness property
based on the heat diffusion model. To achieve this goal, we design
heat-diffusion IELs and apply them to address the challenge of semantic
segmentation with noisy labels. The experimental results demonstrate that the
heat-diffusion IELs can effectively mitigate the overfitting problem caused by
noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1&quot;&gt;Zhonghua Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07346">
<title>A testing-based approach to assess the clusterability of categorical data. (arXiv:2307.07346v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07346</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of clusterability evaluation is to check whether a clustering
structure exists within the data set. As a crucial yet often-overlooked issue
in cluster analysis, it is essential to conduct such a test before applying any
clustering algorithm. If a data set is unclusterable, any subsequent clustering
analysis would not yield valid results. Despite its importance, the majority of
existing studies focus on numerical data, leaving the clusterability evaluation
issue for categorical data as an open problem. Here we present TestCat, a
testing-based approach to assess the clusterability of categorical data in
terms of an analytical $p$-value. The key idea underlying TestCat is that
clusterable categorical data possess many strongly correlated attribute pairs
and hence the sum of chi-squared statistics of all attribute pairs is employed
as the test statistic for $p$-value calculation. We apply our method to a set
of benchmark categorical data sets, showing that TestCat outperforms those
solutions based on existing clusterability evaluation methods for numeric data.
To the best of our knowledge, our work provides the first way to effectively
recognize the clusterability of categorical data in a statistically sound
manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lianyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junjie Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Mudi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zengyou He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07370">
<title>AIC-AB NET: A Neural Network for Image Captioning with Spatial Attention and Text Attributes. (arXiv:2307.07370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07370</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning is a significant field across computer vision and natural
language processing. We propose and present AIC-AB NET, a novel
Attribute-Information-Combined Attention-Based Network that combines spatial
attention architecture and text attributes in an encoder-decoder. For caption
generation, adaptive spatial attention determines which image region best
represents the image and whether to attend to the visual features or the visual
sentinel. Text attribute information is synchronously fed into the decoder to
help image recognition and reduce uncertainty. We have tested and evaluated our
AICAB NET on the MS COCO dataset and a new proposed Fashion dataset. The
Fashion dataset is employed as a benchmark of single-object images. The results
show the superior performance of the proposed model compared to the
state-of-the-art baseline and ablated models on both the images from MSCOCO and
our single-object images. Our AIC-AB NET outperforms the baseline adaptive
attention network by 0.017 (CIDEr score) on the MS COCO dataset and 0.095
(CIDEr score) on the Fashion dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_G/0/1/0/all/0/1&quot;&gt;Guoyun Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlassov_V/0/1/0/all/0/1&quot;&gt;Vladimir Vlassov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07378">
<title>Defect Classification in Additive Manufacturing Using CNN-Based Vision Processing. (arXiv:2307.07378v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07378</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of computer vision and in-situ monitoring using visual
sensors allows the collection of large datasets from the additive manufacturing
(AM) process. Such datasets could be used with machine learning techniques to
improve the quality of AM. This paper examines two scenarios: first, using
convolutional neural networks (CNNs) to accurately classify defects in an image
dataset from AM and second, applying active learning techniques to the
developed classification model. This allows the construction of a
human-in-the-loop mechanism to reduce the size of the data required to train
and generate training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mileo_A/0/1/0/all/0/1&quot;&gt;Alessandra Mileo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smeaton_A/0/1/0/all/0/1&quot;&gt;Alan F. Smeaton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07380">
<title>Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector representations of natural language are ubiquitous in search
applications. Recently, various methods based on contrastive learning have been
proposed to learn textual representations from unlabelled data; by maximizing
alignment between minimally-perturbed embeddings of the same text, and
encouraging a uniform distribution of embeddings across a broader corpus.
Differently, we propose maximizing alignment between texts and a composition of
their phrasal constituents. We consider several realizations of this objective
and elaborate the impact on representations in each case. Experimental results
on semantic textual similarity tasks show improvements over baselines that are
comparable with state-of-the-art approaches. Moreover, this work is the first
to do so without incurring costs in auxiliary training objectives or additional
network parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanchani_S/0/1/0/all/0/1&quot;&gt;Sachin J. Chanchani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruihong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07383">
<title>Higher-order topological kernels via quantum computation. (arXiv:2307.07383v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.07383</link>
<description rdf:parseType="Literal">&lt;p&gt;Topological data analysis (TDA) has emerged as a powerful tool for extracting
meaningful insights from complex data. TDA enhances the analysis of objects by
embedding them into a simplicial complex and extracting useful global
properties such as the Betti numbers, i.e. the number of multidimensional
holes, which can be used to define kernel methods that are easily integrated
with existing machine-learning algorithms. These kernel methods have found
broad applications, as they rely on powerful mathematical frameworks which
provide theoretical guarantees on their performance. However, the computation
of higher-dimensional Betti numbers can be prohibitively expensive on classical
hardware, while quantum algorithms can approximate them in polynomial time in
the instance size. In this work, we propose a quantum approach to defining
topological kernels, which is based on constructing Betti curves, i.e.
topological fingerprint of filtrations with increasing order. We exhibit a
working prototype of our approach implemented on a noiseless simulator and show
its robustness by means of some empirical results suggesting that topological
approaches may offer an advantage in quantum machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Incudini_M/0/1/0/all/0/1&quot;&gt;Massimiliano Incudini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Martini_F/0/1/0/all/0/1&quot;&gt;Francesco Martini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Pierro_A/0/1/0/all/0/1&quot;&gt;Alessandra Di Pierro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07389">
<title>Learning Sparse Neural Networks with Identity Layers. (arXiv:2307.07389v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07389</link>
<description rdf:parseType="Literal">&lt;p&gt;The sparsity of Deep Neural Networks is well investigated to maximize the
performance and reduce the size of overparameterized networks as possible.
Existing methods focus on pruning parameters in the training process by using
thresholds and metrics. Meanwhile, feature similarity between different layers
has not been discussed sufficiently before, which could be rigorously proved to
be highly correlated to the network sparsity in this paper. Inspired by
interlayer feature similarity in overparameterized models, we investigate the
intrinsic link between network sparsity and interlayer feature similarity.
Specifically, we prove that reducing interlayer feature similarity based on
Centered Kernel Alignment (CKA) improves the sparsity of the network by using
information bottleneck theory. Applying such theory, we propose a plug-and-play
CKA-based Sparsity Regularization for sparse network training, dubbed CKA-SR,
which utilizes CKA to reduce feature similarity between layers and increase
network sparsity. In other words, layers of our sparse network tend to have
their own identity compared to each other. Experimentally, we plug the proposed
CKA-SR into the training process of sparse network training methods and find
that CKA-SR consistently improves the performance of several State-Of-The-Art
sparse training methods, especially at extremely high sparsity. Code is
included in the supplementary materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_M/0/1/0/all/0/1&quot;&gt;Mingjian Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1&quot;&gt;Peixi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07396">
<title>Visualizing Overlapping Biclusterings and Boolean Matrix Factorizations. (arXiv:2307.07396v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07396</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding (bi-)clusters in bipartite graphs is a popular data analysis
approach. Analysts typically want to visualize the clusters, which is simple as
long as the clusters are disjoint. However, many modern algorithms find
overlapping clusters, making visualization more complicated. In this paper, we
study the problem of visualizing \emph{a given clustering} of overlapping
clusters in bipartite graphs and the related problem of visualizing Boolean
Matrix Factorizations. We conceptualize three different objectives that any
good visualization should satisfy: (1) proximity of cluster elements, (2) large
consecutive areas of elements from the same cluster, and (3) large
uninterrupted areas in the visualization, regardless of the cluster membership.
We provide objective functions that capture these goals and algorithms that
optimize these objective functions. Interestingly, in experiments on real-world
datasets, we find that the best trade-off between these competing goals is
achieved by a novel heuristic, which locally aims to place rows and columns
with similar cluster membership next to each other.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marette_T/0/1/0/all/0/1&quot;&gt;Thibault Marette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miettinen_P/0/1/0/all/0/1&quot;&gt;Pauli Miettinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_S/0/1/0/all/0/1&quot;&gt;Stefan Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07397">
<title>Improving Zero-Shot Generalization for CLIP with Synthesized Prompts. (arXiv:2307.07397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07397</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing interest in pretrained vision-language models like CLIP,
recent research has focused on adapting these models to downstream tasks.
Despite achieving promising results, most existing methods require labeled data
for all classes, which may not hold in real-world applications due to the long
tail and Zipf&apos;s law. For example, some classes may lack labeled data entirely,
such as emerging concepts. To address this problem, we propose a plug-and-play
generative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed
\textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods.
Specifically, we follow variational autoencoders to introduce a generator that
reconstructs the visual features by inputting the synthesized prompts and the
corresponding class names to the textual encoder of CLIP. In this manner, we
easily obtain the synthesized features for the remaining label-only classes.
Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeled
and synthesized features. Extensive experiments on base-to-new generalization,
cross-dataset transfer learning, and generalized zero-shot learning demonstrate
the superiority of our approach. The code is available at
\url{https://github.com/mrflogs/SHIP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07405">
<title>Performance of $\ell_1$ Regularization for Sparse Convex Optimization. (arXiv:2307.07405v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07405</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite widespread adoption in practice, guarantees for the LASSO and Group
LASSO are strikingly lacking in settings beyond statistical problems, and these
algorithms are usually considered to be a heuristic in the context of sparse
convex optimization on deterministic inputs. We give the first recovery
guarantees for the Group LASSO for sparse convex optimization with
vector-valued features. We show that if a sufficiently large Group LASSO
regularization is applied when minimizing a strictly convex function $l$, then
the minimizer is a sparse vector supported on vector-valued features with the
largest $\ell_2$ norm of the gradient. Thus, repeating this procedure selects
the same set of features as the Orthogonal Matching Pursuit algorithm, which
admits recovery guarantees for any function $l$ with restricted strong
convexity and smoothness via weak submodularity arguments. This answers open
questions of Tibshirani et al. and Yasuda et al. Our result is the first to
theoretically explain the empirical success of the Group LASSO for convex
functions under general input instances assuming only restricted strong
convexity and smoothness. Our result also generalizes provable guarantees for
the Sequential Attention algorithm, which is a feature selection algorithm
inspired by the attention mechanism proposed by Yasuda et al.
&lt;/p&gt;
&lt;p&gt;As an application of our result, we give new results for the column subset
selection problem, which is well-studied when the loss is the Frobenius norm or
other entrywise matrix losses. We give the first result for general loss
functions for this problem that requires only restricted strong convexity and
smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Axiotis_K/0/1/0/all/0/1&quot;&gt;Kyriakos Axiotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasuda_T/0/1/0/all/0/1&quot;&gt;Taisuke Yasuda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07406">
<title>Improved Convergence Analysis and SNR Control Strategies for Federated Learning in the Presence of Noise. (arXiv:2307.07406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07406</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an improved convergence analysis technique that characterizes the
distributed learning paradigm of federated learning (FL) with imperfect/noisy
uplink and downlink communications. Such imperfect communication scenarios
arise in the practical deployment of FL in emerging communication systems and
protocols. The analysis developed in this paper demonstrates, for the first
time, that there is an asymmetry in the detrimental effects of uplink and
downlink communications in FL. In particular, the adverse effect of the
downlink noise is more severe on the convergence of FL algorithms. Using this
insight, we propose improved Signal-to-Noise (SNR) control strategies that,
discarding the negligible higher-order terms, lead to a similar convergence
rate for FL as in the case of a perfect, noise-free communication channel while
incurring significantly less power resources compared to existing solutions. In
particular, we establish that to maintain the $O(\frac{1}{\sqrt{K}})$ rate of
convergence like in the case of noise-free FL, we need to scale down the uplink
and downlink noise by $\Omega({\sqrt{k}})$ and $\Omega({k})$ respectively,
where $k$ denotes the communication round, $k=1,\dots, K$. Our theoretical
result is further characterized by two major benefits: firstly, it does not
assume the somewhat unrealistic assumption of bounded client dissimilarity, and
secondly, it only requires smooth non-convex loss functions, a function class
better suited for modern machine learning and deep learning models. We also
perform extensive empirical analysis to verify the validity of our theoretical
findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_A/0/1/0/all/0/1&quot;&gt;Antesh Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Hashemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07410">
<title>Implicit regularization in AI meets generalized hardness of approximation in optimization -- Sharp results for diagonal linear networks. (arXiv:2307.07410v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07410</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the implicit regularization imposed by neural network
architectures and gradient based optimization methods is a key challenge in
deep learning and AI. In this work we provide sharp results for the implicit
regularization imposed by the gradient flow of Diagonal Linear Networks (DLNs)
in the over-parameterized regression setting and, potentially surprisingly,
link this to the phenomenon of phase transitions in generalized hardness of
approximation (GHA). GHA generalizes the phenomenon of hardness of
approximation from computer science to, among others, continuous and robust
optimization. It is well-known that the $\ell^1$-norm of the gradient flow of
DLNs with tiny initialization converges to the objective function of basis
pursuit. We improve upon these results by showing that the gradient flow of
DLNs with tiny initialization approximates minimizers of the basis pursuit
optimization problem (as opposed to just the objective function), and we obtain
new and sharp convergence bounds w.r.t.\ the initialization size. Non-sharpness
of our results would imply that the GHA phenomenon would not occur for the
basis pursuit optimization problem -- which is a contradiction -- thus implying
sharpness. Moreover, we characterize $\textit{which}$ $\ell_1$ minimizer of the
basis pursuit problem is chosen by the gradient flow whenever the minimizer is
not unique. Interestingly, this depends on the depth of the DLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wind_J/0/1/0/all/0/1&quot;&gt;Johan S. Wind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antun_V/0/1/0/all/0/1&quot;&gt;Vegard Antun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_A/0/1/0/all/0/1&quot;&gt;Anders C. Hansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07412">
<title>HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07412</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the problem of curriculum discovery and describe a curriculum
learning framework capable of discovering effective curricula in a curriculum
space based on prior knowledge about sample difficulty. Using annotation
entropy and loss as measures of difficulty, we show that (i): the
top-performing discovered curricula for a given model and dataset are often
non-monotonic as opposed to monotonic curricula in existing literature, (ii):
the prevailing easy-to-hard or hard-to-easy transition curricula are often at
the risk of underperforming, and (iii): the curricula discovered for smaller
datasets and models perform well on larger datasets and models respectively.
The proposed framework encompasses some of the existing curriculum learning
approaches and can discover curricula that outperform them across several NLP
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgaar_M/0/1/0/all/0/1&quot;&gt;Mohamed Elgaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1&quot;&gt;Hadi Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07413">
<title>Exploiting Counter-Examples for Active Learning with Partial labels. (arXiv:2307.07413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07413</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a new problem, \emph{active learning with partial labels}
(ALPL). In this setting, an oracle annotates the query samples with partial
labels, relaxing the oracle from the demanding accurate labeling process. To
address ALPL, we first build an intuitive baseline that can be seamlessly
incorporated into existing AL frameworks. Though effective, this baseline is
still susceptible to the \emph{overfitting}, and falls short of the
representative partial-label-based samples during the query process. Drawing
inspiration from human inference in cognitive science, where accurate
inferences can be explicitly derived from \emph{counter-examples} (CEs), our
objective is to leverage this human-like learning pattern to tackle the
\emph{overfitting} while enhancing the process of selecting representative
samples in ALPL. Specifically, we construct CEs by reversing the partial labels
for each instance, and then we propose a simple but effective WorseNet to
directly learn from this complementary pattern. By leveraging the distribution
gap between WorseNet and the predictor, this adversarial evaluation manner
could enhance both the performance of the predictor itself and the sample
selection process, allowing the predictor to capture more accurate patterns in
the data. Experimental results on five real-world datasets and four benchmark
datasets show that our proposed method achieves comprehensive improvements over
ten representative AL frameworks, highlighting the superiority of WorseNet. The
source code will be available at \url{https://github.com/Ferenas/APLL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jieming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalander_M/0/1/0/all/0/1&quot;&gt;Marcus Kalander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07423">
<title>Enhancing ECG Analysis of Implantable Cardiac Monitor Data: An Efficient Pipeline for Multi-Label Classification. (arXiv:2307.07423v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.07423</link>
<description rdf:parseType="Literal">&lt;p&gt;Implantable Cardiac Monitor (ICM) devices are demonstrating as of today, the
fastest-growing market for implantable cardiac devices. As such, they are
becoming increasingly common in patients for measuring heart electrical
activity. ICMs constantly monitor and record a patient&apos;s heart rhythm and when
triggered - send it to a secure server where health care professionals (denote
HCPs from here on) can review it. These devices employ a relatively simplistic
rule-based algorithm (due to energy consumption constraints) to alert for
abnormal heart rhythms. This algorithm is usually parameterized to an
over-sensitive mode in order to not miss a case (resulting in relatively high
false-positive rate) and this, combined with the device&apos;s nature of constantly
monitoring the heart rhythm and its growing popularity, results in HCPs having
to analyze and diagnose an increasingly growing amount of data. In order to
reduce the load on the latter, automated methods for ECG analysis are nowadays
becoming a great tool to assist HCPs in their analysis. While state-of-the-art
algorithms are data-driven rather than rule-based, training data for ICMs often
consist of specific characteristics which make its analysis unique and
particularly challenging. This study presents the challenges and solutions in
automatically analyzing ICM data and introduces a method for its classification
that outperforms existing methods on such data. As such, it could be used in
numerous ways such as aiding HCPs in the analysis of ECGs originating from ICMs
by e.g. suggesting a rhythm type.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bleich_A/0/1/0/all/0/1&quot;&gt;Amnon Bleich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Linnemann_A/0/1/0/all/0/1&quot;&gt;Antje Linnemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaidi_B/0/1/0/all/0/1&quot;&gt;Benjamin Jaidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diem_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn H Diem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conrad_T/0/1/0/all/0/1&quot;&gt;Tim OF Conrad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07426">
<title>Real-time Percussive Technique Recognition and Embedding Learning for the Acoustic Guitar. (arXiv:2307.07426v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.07426</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time music information retrieval (RT-MIR) has much potential to augment
the capabilities of traditional acoustic instruments. We develop RT-MIR
techniques aimed at augmenting percussive fingerstyle, which blends acoustic
guitar playing with guitar body percussion. We formulate several design
objectives for RT-MIR systems for augmented instrument performance: (i) causal
constraint, (ii) perceptually negligible action-to-sound latency, (iii) control
intimacy support, (iv) synthesis control support. We present and evaluate
real-time guitar body percussion recognition and embedding learning techniques
based on convolutional neural networks (CNNs) and CNNs jointly trained with
variational autoencoders (VAEs). We introduce a taxonomy of guitar body
percussion based on hand part and location. We follow a cross-dataset
evaluation approach by collecting three datasets labelled according to the
taxonomy. The embedding quality of the models is assessed using KL-Divergence
across distributions corresponding to different taxonomic classes. Results
indicate that the networks are strong classifiers especially in a simplified
2-class recognition task, and the VAEs yield improved class separation compared
to CNNs as evidenced by increased KL-Divergence across distributions. We argue
that the VAE embedding quality could support control intimacy and rich
interaction when the latent space&apos;s parameters are used to control an external
synthesis engine. Further design challenges around generalisation to different
datasets have been identified.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martelloni_A/0/1/0/all/0/1&quot;&gt;Andrea Martelloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McPherson_A/0/1/0/all/0/1&quot;&gt;Andrew P McPherson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barthet_M/0/1/0/all/0/1&quot;&gt;Mathieu Barthet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07439">
<title>Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07439</link>
<description rdf:parseType="Literal">&lt;p&gt;Age prediction is an important part of medical assessments and research. It
can aid in detecting diseases as well as abnormal ageing by highlighting the
discrepancy between chronological and biological age. To gain a comprehensive
understanding of age-related changes observed in various body parts, we
investigate them on a larger scale by using whole-body images. We utilise the
Grad-CAM interpretability method to determine the body areas most predictive of
a person&apos;s age. We expand our analysis beyond individual subjects by employing
registration techniques to generate population-wide interpretability maps.
Furthermore, we set state-of-the-art whole-body age prediction with a model
that achieves a mean absolute error of 2.76 years. Our findings reveal three
primary areas of interest: the spine, the autochthonous back muscles, and the
cardiac region, which exhibits the highest importance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Starck_S/0/1/0/all/0/1&quot;&gt;Sophie Starck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kini_Y/0/1/0/all/0/1&quot;&gt;Yadunandan Vivekanand Kini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ritter_J/0/1/0/all/0/1&quot;&gt;Jessica Johanna Maria Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07443">
<title>Can Large Language Models Empower Molecular Property Prediction?. (arXiv:2307.07443v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07443</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular property prediction has gained significant attention due to its
transformative potential in multiple scientific disciplines. Conventionally, a
molecule graph can be represented either as a graph-structured data or a SMILES
text. Recently, the rapid development of Large Language Models (LLMs) has
revolutionized the field of NLP. Although it is natural to utilize LLMs to
assist in understanding molecules represented by SMILES, the exploration of how
LLMs will impact molecular property prediction is still in its early stage. In
this work, we advance towards this objective through two perspectives:
zero/few-shot molecular classification, and using the new explanations
generated by LLMs as representations of molecules. To be specific, we first
prompt LLMs to do in-context molecular classification and evaluate their
performance. After that, we employ LLMs to generate semantically enriched
explanations for the original SMILES and then leverage that to fine-tune a
small-scale LM model for multiple downstream tasks. The experimental results
highlight the superiority of text explanations as molecular representations
across multiple benchmark datasets, and confirm the immense potential of LLMs
in molecular property prediction tasks. Codes are available at
\url{https://github.com/ChnQ/LLM4Mol}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huayi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Hong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07445">
<title>TSNet-SAC: Leveraging Transformers for Efficient Task Scheduling. (arXiv:2307.07445v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.07445</link>
<description rdf:parseType="Literal">&lt;p&gt;In future 6G Mobile Edge Computing (MEC), autopilot systems require the
capability of processing multimodal data with strong interdependencies.
However, traditional heuristic algorithms are inadequate for real-time
scheduling due to their requirement for multiple iterations to derive the
optimal scheme. We propose a novel TSNet-SAC based on Transformer, that
utilizes heuristic algorithms solely to guide the training of TSNet.
Additionally, a Sliding Augment Component (SAC) is introduced to enhance the
robustness and resolve algorithm defects. Furthermore, the Extender component
is designed to handle multi-scale training data and provide network
scalability, enabling TSNet to adapt to different access scenarios. Simulation
demonstrates that TSNet-SAC outperforms existing networks in accuracy and
robustness, achieving superior scheduling-making latency compared to heuristic
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1&quot;&gt;Ke Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haohan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Desheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07449">
<title>Differentially Private Clustering in Data Streams. (arXiv:2307.07449v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2307.07449</link>
<description rdf:parseType="Literal">&lt;p&gt;The streaming model is an abstraction of computing over massive data streams,
which is a popular way of dealing with large-scale modern data analysis. In
this model, there is a stream of data points, one after the other. A streaming
algorithm is only allowed one pass over the data stream, and the goal is to
perform some analysis during the stream while using as small space as possible.
&lt;/p&gt;
&lt;p&gt;Clustering problems (such as $k$-means and $k$-median) are fundamental
unsupervised machine learning primitives, and streaming clustering algorithms
have been extensively studied in the past. However, since data privacy becomes
a central concern in many real-world applications, non-private clustering
algorithms are not applicable in many scenarios.
&lt;/p&gt;
&lt;p&gt;In this work, we provide the first differentially private streaming
algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean
data points over a stream with length at most $T$ using $poly(k,d,\log(T))$
space to achieve a {\it constant} multiplicative error and a
$poly(k,d,\log(T))$ additive error. In particular, we present a differentially
private streaming clustering framework which only requires an offline DP
coreset algorithm as a blackbox. By plugging in existing DP coreset results via
Ghazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a
$(1+\gamma)$-multiplicative approximation with
$\tilde{O}_\gamma(poly(k,d,\log(T)))$ space for any $\gamma&amp;gt;0$, and the
additive error is $poly(k,d,\log(T))$ or (2) an $O(1)$-multiplicative
approximation with $\tilde{O}(k \cdot poly(d,\log(T)))$ space and
$poly(k,d,\log(T))$ additive error.
&lt;/p&gt;
&lt;p&gt;In addition, our algorithmic framework is also differentially private under
the continual release setting, i.e., the union of outputs of our algorithms at
every timestamp is always differentially private.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epasto_A/0/1/0/all/0/1&quot;&gt;Alessandro Epasto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1&quot;&gt;Tamalika Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Peilin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07454">
<title>Generative adversarial networks for data-scarce spectral applications. (arXiv:2307.07454v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2307.07454</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) are one of the most robust and
versatile techniques in the field of generative artificial intelligence. In
this work, we report on an application of GANs in the domain of synthetic
spectral data generation, offering a solution to the scarcity of data found in
various scientific contexts. We demonstrate the proposed approach by applying
it to an illustrative problem within the realm of near-field radiative heat
transfer involving a multilayered hyperbolic metamaterial. We find that a
successful generation of spectral data requires two modifications to
conventional GANs: (i) the introduction of Wasserstein GANs (WGANs) to avoid
mode collapse, and, (ii) the conditioning of WGANs to obtain accurate labels
for the generated data. We show that a simple feed-forward neural network
(FFNN), when augmented with data generated by a CWGAN, enhances significantly
its performance under conditions of limited data availability, demonstrating
the intrinsic value of CWGAN data augmentation beyond simply providing larger
datasets. In addition, we show that CWGANs can act as a surrogate model with
improved performance in the low-data regime with respect to simple FFNNs.
Overall, this work highlights the potential of generative machine learning
algorithms in scientific applications beyond image generation and optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Garcia_Esteban_J/0/1/0/all/0/1&quot;&gt;Juan Jos&amp;#xe9; Garc&amp;#xed;a-Esteban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cuevas_J/0/1/0/all/0/1&quot;&gt;Juan Carlos Cuevas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bravo_Abad_J/0/1/0/all/0/1&quot;&gt;Jorge Bravo-Abad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07457">
<title>Structured Pruning of Neural Networks for Constraints Learning. (arXiv:2307.07457v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07457</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the integration of Machine Learning (ML) models with
Operation Research (OR) tools has gained popularity across diverse
applications, including cancer treatment, algorithmic configuration, and
chemical process optimization. In this domain, the combination of ML and OR
often relies on representing the ML model output using Mixed Integer
Programming (MIP) formulations. Numerous studies in the literature have
developed such formulations for many ML predictors, with a particular emphasis
on Artificial Neural Networks (ANNs) due to their significant interest in many
applications. However, ANNs frequently contain a large number of parameters,
resulting in MIP formulations that are impractical to solve, thereby impeding
scalability. In fact, the ML community has already introduced several
techniques to reduce the parameter count of ANNs without compromising their
performance, since the substantial size of modern ANNs presents challenges for
ML applications as it significantly impacts computational efforts during
training and necessitates significant memory resources for storage. In this
paper, we showcase the effectiveness of pruning, one of these techniques, when
applied to ANNs prior to their integration into MIPs. By pruning the ANN, we
achieve significant improvements in the speed of the solution process. We
discuss why pruning is more suitable in this context compared to other ML
compression techniques, and we identify the most appropriate pruning
strategies. To highlight the potential of this approach, we conduct experiments
using feed-forward neural networks with multiple layers to construct
adversarial examples. Our results demonstrate that pruning offers remarkable
reductions in solution times without hindering the quality of the final
decision, enabling the resolution of previously unsolvable instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cacciola_M/0/1/0/all/0/1&quot;&gt;Matteo Cacciola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frangioni_A/0/1/0/all/0/1&quot;&gt;Antonio Frangioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07477">
<title>Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07477</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) combined with differential privacy (DP) offers
machine learning (ML) training with distributed devices and with a formal
privacy guarantee. With a large population of devices, FL with DP produces a
performant model in a timely manner. However, for applications with a smaller
population, not only does the model utility degrade as the DP noise is
inversely proportional to population, but also the training latency increases
since waiting for enough clients to become available from a smaller pool is
slower. In this work, we thus propose expanding the population based on domain
adaptation techniques to speed up the training and improves the final model
quality when training with small populations. We empirically demonstrate that
our techniques can improve the utility by 13% to 30% on real-world language
modeling datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koga_T/0/1/0/all/0/1&quot;&gt;Tatsuki Koga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Congzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelikan_M/0/1/0/all/0/1&quot;&gt;Martin Pelikan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1&quot;&gt;Mona Chitnis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07487">
<title>DreamTeacher: Pretraining Image Backbones with Deep Generative Models. (arXiv:2307.07487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07487</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce a self-supervised feature representation learning
framework DreamTeacher that utilizes generative networks for pre-training
downstream image backbones. We propose to distill knowledge from a trained
generative model into standard image backbones that have been well engineered
for specific perception tasks. We investigate two types of knowledge
distillation: 1) distilling learned generative features onto target image
backbones as an alternative to pretraining these backbones on large labeled
datasets such as ImageNet, and 2) distilling labels obtained from generative
networks with task heads onto logits of target backbones. We perform extensive
analyses on multiple generative models, dense prediction benchmarks, and
several pre-training regimes. We empirically find that our DreamTeacher
significantly outperforms existing self-supervised representation learning
approaches across the board. Unsupervised ImageNet pre-training with
DreamTeacher leads to significant improvements over ImageNet classification
pre-training on downstream datasets, showcasing generative models, and
diffusion generative models specifically, as a promising approach to
representation learning on large, diverse datasets without requiring manual
annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daiqing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Huan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1&quot;&gt;Amlan Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1&quot;&gt;David Acuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1&quot;&gt;Karsten Kreis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07489">
<title>PseudoCal: A Source-Free Approach to Unsupervised Uncertainty Calibration in Domain Adaptation. (arXiv:2307.07489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07489</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) has witnessed remarkable advancements in
improving the accuracy of models for unlabeled target domains. However, the
calibration of predictive uncertainty in the target domain, a crucial aspect of
the safe deployment of UDA models, has received limited attention. The
conventional in-domain calibration method, \textit{temperature scaling}
(TempScal), encounters challenges due to domain distribution shifts and the
absence of labeled target domain data. Recent approaches have employed
importance-weighting techniques to estimate the target-optimal temperature
based on re-weighted labeled source data. Nonetheless, these methods require
source data and suffer from unreliable density estimates under severe domain
shifts, rendering them unsuitable for source-free UDA settings. To overcome
these limitations, we propose PseudoCal, a source-free calibration method that
exclusively relies on unlabeled target data. Unlike previous approaches that
treat UDA calibration as a \textit{covariate shift} problem, we consider it as
an unsupervised calibration problem specific to the target domain. Motivated by
the factorization of the negative log-likelihood (NLL) objective in TempScal,
we generate a labeled pseudo-target set that captures the structure of the real
target. By doing so, we transform the unsupervised calibration problem into a
supervised one, enabling us to effectively address it using widely-used
in-domain methods like TempScal. Finally, we thoroughly evaluate the
calibration performance of PseudoCal by conducting extensive experiments on 10
UDA methods, considering both traditional UDA settings and recent source-free
UDA scenarios. The experimental results consistently demonstrate the superior
performance of PseudoCal, exhibiting significantly reduced calibration error
compared to existing calibration methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dapeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07503">
<title>Brain Tumor Detection using Convolutional Neural Networks with Skip Connections. (arXiv:2307.07503v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07503</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present different architectures of Convolutional Neural
Networks (CNN) to analyze and classify the brain tumors into benign and
malignant types using the Magnetic Resonance Imaging (MRI) technique. Different
CNN architecture optimization techniques such as widening and deepening of the
network and adding skip connections are applied to improve the accuracy of the
network. Results show that a subset of these techniques can judiciously be used
to outperform a baseline CNN model used for the same purpose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hamran_A/0/1/0/all/0/1&quot;&gt;Aupam Hamran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vaeztourshizi_M/0/1/0/all/0/1&quot;&gt;Marzieh Vaeztourshizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Esmaili_A/0/1/0/all/0/1&quot;&gt;Amirhossein Esmaili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07507">
<title>MGit: A Model Versioning and Management System. (arXiv:2307.07507v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07507</link>
<description rdf:parseType="Literal">&lt;p&gt;Models derived from other models are extremely common in machine learning
(ML) today. For example, transfer learning is used to create task-specific
models from &quot;pre-trained&quot; models through finetuning. This has led to an
ecosystem where models are related to each other, sharing structure and often
even parameter values. However, it is hard to manage these model derivatives:
the storage overhead of storing all derived models quickly becomes onerous,
prompting users to get rid of intermediate models that might be useful for
further analysis. Additionally, undesired behaviors in models are hard to track
down (e.g., is a bug inherited from an upstream model?). In this paper, we
propose a model versioning and management system called MGit that makes it
easier to store, test, update, and collaborate on model derivatives. MGit
introduces a lineage graph that records provenance and versioning information
between models, optimizations to efficiently store model parameters, as well as
abstractions over this lineage graph that facilitate relevant testing, updating
and collaboration functionality. MGit is able to reduce the lineage graph&apos;s
storage footprint by up to 7x and automatically update downstream models in
response to updates to upstream models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1&quot;&gt;Wei Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendoza_D/0/1/0/all/0/1&quot;&gt;Daniel Mendoza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1&quot;&gt;Rafael da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1&quot;&gt;Deepak Narayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phanishaye_A/0/1/0/all/0/1&quot;&gt;Amar Phanishaye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07508">
<title>Deep reinforcement learning for the dynamic vehicle dispatching problem: An event-based approach. (arXiv:2307.07508v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07508</link>
<description rdf:parseType="Literal">&lt;p&gt;The dynamic vehicle dispatching problem corresponds to deciding which
vehicles to assign to requests that arise stochastically over time and space.
It emerges in diverse areas, such as in the assignment of trucks to loads to be
transported; in emergency systems; and in ride-hailing services. In this paper,
we model the problem as a semi-Markov decision process, which allows us to
treat time as continuous. In this setting, decision epochs coincide with
discrete events whose time intervals are random. We argue that an event-based
approach substantially reduces the combinatorial complexity of the decision
space and overcomes other limitations of discrete-time models often proposed in
the literature. In order to test our approach, we develop a new discrete-event
simulator and use double deep q-learning to train our decision agents.
Numerical experiments are carried out in realistic scenarios using data from
New York City. We compare the policies obtained through our approach with
heuristic policies often used in practice. Results show that our policies
exhibit better average waiting times, cancellation rates and total service
times, with reduction in average waiting times of up to 50% relative to the
other tested heuristic policies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordeiro_E/0/1/0/all/0/1&quot;&gt;Edyvalberty Alenquer Cordeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pitombeira_Neto_A/0/1/0/all/0/1&quot;&gt;Anselmo Ramalho Pitombeira-Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07512">
<title>Expressive Monotonic Neural Networks. (arXiv:2307.07512v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07512</link>
<description rdf:parseType="Literal">&lt;p&gt;The monotonic dependence of the outputs of a neural network on some of its
inputs is a crucial inductive bias in many scenarios where domain knowledge
dictates such behavior. This is especially important for interpretability and
fairness considerations. In a broader context, scenarios in which monotonicity
is important can be found in finance, medicine, physics, and other disciplines.
It is thus desirable to build neural network architectures that implement this
inductive bias provably. In this work, we propose a weight-constrained
architecture with a single residual connection to achieve exact monotonic
dependence in any subset of the inputs. The weight constraint scheme directly
controls the Lipschitz constant of the neural network and thus provides the
additional benefit of robustness. Compared to currently existing techniques
used for monotonicity, our method is simpler in implementation and in theory
foundations, has negligible computational overhead, is guaranteed to produce
monotonic dependence, and is highly expressive. We show how the algorithm is
used to train powerful, robust, and interpretable discriminators that achieve
competitive performance compared to current state-of-the-art methods across
various benchmarks, from social applications to the classification of the
decays of subatomic particles produced at the CERN Large Hadron Collider.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitouni_O/0/1/0/all/0/1&quot;&gt;Ouail Kitouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolte_N/0/1/0/all/0/1&quot;&gt;Niklas Nolte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Michael Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1810.07287">
<title>Signed iterative random forests to identify enhancer-associated transcription factor binding. (arXiv:1810.07287v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1810.07287</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard ChIP-seq peak calling pipelines seek to differentiate biochemically
reproducible signals of individual genomic elements from background noise.
However, reproducibility alone does not imply functional regulation (e.g.,
enhancer activation, alternative splicing). Here we present a general-purpose,
interpretable machine learning method: signed iterative random forests (siRF),
which we use to infer regulatory interactions among transcription factors and
functional binding signatures surrounding enhancer elements in Drosophila
melanogaster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumbier_K/0/1/0/all/0/1&quot;&gt;Karl Kumbier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sumanta Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frise_E/0/1/0/all/0/1&quot;&gt;Erwin Frise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1&quot;&gt;Susan E. Celniker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Brown_J/0/1/0/all/0/1&quot;&gt;James B. Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Celniker_S/0/1/0/all/0/1&quot;&gt;Susan Celniker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.10113">
<title>Alternating the Population and Control Neural Networks to Solve High-Dimensional Stochastic Mean-Field Games. (arXiv:2002.10113v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2002.10113</link>
<description rdf:parseType="Literal">&lt;p&gt;We present APAC-Net, an alternating population and agent control neural
network for solving stochastic mean field games (MFGs). Our algorithm is geared
toward high-dimensional instances of MFGs that are beyond reach with existing
solution methods. We achieve this in two steps. First, we take advantage of the
underlying variational primal-dual structure that MFGs exhibit and phrase it as
a convex-concave saddle point problem. Second, we parameterize the value and
density functions by two neural networks, respectively. By phrasing the problem
in this manner, solving the MFG can be interpreted as a special case of
training a generative adversarial network (GAN). We show the potential of our
method on up to 100-dimensional MFG problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1&quot;&gt;Alex Tong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1&quot;&gt;Samy Wu Fung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wuchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nurbekyan_L/0/1/0/all/0/1&quot;&gt;Levon Nurbekyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1&quot;&gt;Stanley J. Osher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.08756">
<title>Optimizing Data Augmentation Policy Through Random Unidimensional Search. (arXiv:2106.08756v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.08756</link>
<description rdf:parseType="Literal">&lt;p&gt;It is no secret amongst deep learning researchers that finding the optimal
data augmentation strategy during training can mean the difference between
state-of-the-art performance and a run-of-the-mill result. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve equivalent performance using just 6 trainings with Random
Unidimensional Augmentation. Source code is available at
https://github.com/fastestimator/RUA/tree/v1.0
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1&quot;&gt;Michael Potter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yun-Chan Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1&quot;&gt;V. Ratna Saripalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03443">
<title>Unpacking the Black Box: Regulating Algorithmic Decisions. (arXiv:2110.03443v2 [econ.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03443</link>
<description rdf:parseType="Literal">&lt;p&gt;We show how to optimally regulate prediction algorithms in a world where an
agent uses complex &apos;black-box&apos; prediction functions to make decisions such as
lending, medical testing, or hiring, and where a principal is limited in how
much she can learn about the agent&apos;s black-box model. We show that limiting
agents to prediction functions that are simple enough to be fully transparent
is inefficient as long as the misalignment is limited and first-best prediction
functions are sufficiently complex. Algorithmic audits can improve welfare, but
the gains depend on the design of the audit tools. Tools that focus on
minimizing overall information loss, the focus of many explainer tools, will
generally be inefficient since they focus on explaining the average behavior of
the prediction function. Targeted tools that focus on the source of incentive
misalignment, e.g., excess false positives or racial disparities, can provide
second-best solutions. We provide empirical support for our theoretical
findings using an application in consumer lending, where we document that
complex models regulated based on context-specific explanation tools outperform
simple, fully transparent models. This gain from complex models represents a
Pareto improvement across our empirical applications that are preferred both by
the lender and from the perspective of the financial regulator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Blattner_L/0/1/0/all/0/1&quot;&gt;Laura Blattner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Nelson_S/0/1/0/all/0/1&quot;&gt;Scott Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Spiess_J/0/1/0/all/0/1&quot;&gt;Jann Spiess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.12319">
<title>Privacy-preserving machine learning with tensor networks. (arXiv:2202.12319v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2202.12319</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor networks, widely used for providing efficient representations of
low-energy states of local quantum many-body systems, have been recently
proposed as machine learning architectures which could present advantages with
respect to traditional ones. In this work we show that tensor network
architectures have especially prospective properties for privacy-preserving
machine learning, which is important in tasks such as the processing of medical
records. First, we describe a new privacy vulnerability that is present in
feedforward neural networks, illustrating it in synthetic and real-world
datasets. Then, we develop well-defined conditions to guarantee robustness to
such vulnerability, which involve the characterization of models equivalent
under gauge symmetry. We rigorously prove that such conditions are satisfied by
tensor-network architectures. In doing so, we define a novel canonical form for
matrix product states, which has a high degree of regularity and fixes the
residual gauge that is left in the canonical forms based on singular value
decompositions. We supplement the analytical findings with practical examples
where matrix product states are trained on datasets of medical records, which
show large reductions on the probability of an attacker extracting information
about the training dataset from the model&apos;s parameters. Given the growing
expertise in training tensor-network architectures, these results imply that
one may not have to be forced to make a choice between accuracy in prediction
and ensuring the privacy of the information processed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozas_Kerstjens_A/0/1/0/all/0/1&quot;&gt;Alejandro Pozas-Kerstjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Santana_S/0/1/0/all/0/1&quot;&gt;Senaida Hern&amp;#xe1;ndez-Santana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monturiol_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Ram&amp;#xf3;n Pareja Monturiol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1&quot;&gt;Marco Castrill&amp;#xf3;n L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarpa_G/0/1/0/all/0/1&quot;&gt;Giannicola Scarpa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Guillen_C/0/1/0/all/0/1&quot;&gt;Carlos E. Gonz&amp;#xe1;lez-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_D/0/1/0/all/0/1&quot;&gt;David P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.08768">
<title>Rank-based Decomposable Losses in Machine Learning: A Survey. (arXiv:2207.08768v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.08768</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works have revealed an essential paradigm in designing loss functions
that differentiate individual losses vs. aggregate losses. The individual loss
measures the quality of the model on a sample, while the aggregate loss
combines individual losses/scores over each training sample. Both have a common
procedure that aggregates a set of individual values to a single numerical
value. The ranking order reflects the most fundamental relation among
individual values in designing losses. In addition, decomposability, in which a
loss can be decomposed into an ensemble of individual terms, becomes a
significant property of organizing losses/scores. This survey provides a
systematic and comprehensive review of rank-based decomposable losses in
machine learning. Specifically, we provide a new taxonomy of loss functions
that follows the perspectives of aggregate loss and individual loss. We
identify the aggregator to form such losses, which are examples of set
functions. We organize the rank-based decomposable losses into eight
categories. Following these categories, we review the literature on rank-based
aggregate losses and rank-based individual losses. We describe general formulas
for these losses and connect them with existing research topics. We also
suggest future research directions spanning unexplored, remaining, and emerging
issues in rank-based decomposable losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.09874">
<title>Stream-based active learning with linear models. (arXiv:2207.09874v5 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2207.09874</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of automated data collection schemes and the advances in
sensorics are increasing the amount of data we are able to monitor in
real-time. However, given the high annotation costs and the time required by
quality inspections, data is often available in an unlabeled form. This is
fostering the use of active learning for the development of soft sensors and
predictive models. In production, instead of performing random inspections to
obtain product information, labels are collected by evaluating the information
content of the unlabeled data. Several query strategy frameworks for regression
have been proposed in the literature but most of the focus has been dedicated
to the static pool-based scenario. In this work, we propose a new strategy for
the stream-based scenario, where instances are sequentially offered to the
learner, which must instantaneously decide whether to perform the quality check
to obtain the label or discard the instance. The approach is inspired by the
optimal experimental design theory and the iterative aspect of the
decision-making process is tackled by setting a threshold on the
informativeness of the unlabeled data points. The proposed approach is
evaluated using numerical simulations and the Tennessee Eastman Process
simulator. The results confirm that selecting the examples suggested by the
proposed algorithm allows for a faster reduction in the prediction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cacciarelli_D/0/1/0/all/0/1&quot;&gt;Davide Cacciarelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kulahci_M/0/1/0/all/0/1&quot;&gt;Murat Kulahci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tyssedal_J/0/1/0/all/0/1&quot;&gt;John S&amp;#xf8;lve Tyssedal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04856">
<title>Fully probabilistic deep models for forward and inverse problems in parametric PDEs. (arXiv:2208.04856v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04856</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a physics-driven deep latent variable model (PDDLVM) to learn
simultaneously parameter-to-solution (forward) and solution-to-parameter
(inverse) maps of parametric partial differential equations (PDEs). Our
formulation leverages conventional PDE discretization techniques, deep neural
networks, probabilistic modelling, and variational inference to assemble a
fully probabilistic coherent framework. In the posited probabilistic model,
both the forward and inverse maps are approximated as Gaussian distributions
with a mean and covariance parameterized by deep neural networks. The PDE
residual is assumed to be an observed random vector of value zero, hence we
model it as a random vector with a zero mean and a user-prescribed covariance.
The model is trained by maximizing the probability, that is the evidence or
marginal likelihood, of observing a residual of zero by maximizing the evidence
lower bound (ELBO). Consequently, the proposed methodology does not require any
independent PDE solves and is physics-informed at training time, allowing the
real-time solution of PDE forward and inverse problems after training. The
proposed framework can be easily extended to seamlessly integrate observed data
to solve inverse problems and to build generative models. We demonstrate the
efficiency and robustness of our method on finite element discretized
parametric PDE problems such as linear and nonlinear Poisson problems, elastic
shells with complex 3D geometries, and time-dependent nonlinear and
inhomogeneous PDEs using a physics-informed neural network (PINN)
discretization. We achieve up to three orders of magnitude speed-up after
training compared to traditional finite element method (FEM), while outputting
coherent uncertainty estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vadeboncoeur_A/0/1/0/all/0/1&quot;&gt;Arnaud Vadeboncoeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;mer Deniz Akyildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kazlauskaite_I/0/1/0/all/0/1&quot;&gt;Ieva Kazlauskaite&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cirak_F/0/1/0/all/0/1&quot;&gt;Fehmi Cirak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04188">
<title>Differentially Private Stochastic Gradient Descent with Low-Noise. (arXiv:2209.04188v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04188</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern machine learning algorithms aim to extract fine-grained information
from data to provide accurate predictions, which often conflicts with the goal
of privacy protection. This paper addresses the practical and theoretical
importance of developing privacy-preserving machine learning algorithms that
ensure good performance while preserving privacy. In this paper, we focus on
the privacy and utility (measured by excess risk bounds) performances of
differentially private stochastic gradient descent (SGD) algorithms in the
setting of stochastic convex optimization. Specifically, we examine the
pointwise problem in the low-noise setting for which we derive sharper excess
risk bounds for the differentially private SGD algorithm. In the pairwise
learning setting, we propose a simple differentially private SGD algorithm
based on gradient perturbation. Furthermore, we develop novel utility bounds
for the proposed algorithm, proving that it achieves optimal excess risk rates
even for non-smooth losses. Notably, we establish fast learning rates for
privacy-preserving pairwise learning under the low-noise condition, which is
the first of its kind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Puyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yunwen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1&quot;&gt;Yiming Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Ding-Xuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15609">
<title>$\Phi$-DVAE: Physics-Informed Dynamical Variational Autoencoders for Unstructured Data Assimilation. (arXiv:2209.15609v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15609</link>
<description rdf:parseType="Literal">&lt;p&gt;Incorporating unstructured data into physical models is a challenging problem
that is emerging in data assimilation. Traditional approaches focus on
well-defined observation operators whose functional forms are typically assumed
to be known. This prevents these methods from achieving a consistent model-data
synthesis in configurations where the mapping from data-space to model-space is
unknown. To address these shortcomings, in this paper we develop a
physics-informed dynamical variational autoencoder ($\Phi$-DVAE) to embed
diverse data streams into time-evolving physical systems described by
differential equations. Our approach combines a standard, possibly nonlinear,
filter for the latent state-space model and a VAE, to assimilate the
unstructured data into the latent dynamical system. Unstructured data, in our
example systems, comes in the form of video data and velocity field
measurements, however the methodology is suitably generic to allow for
arbitrary unknown observation operators. A variational Bayesian framework is
used for the joint estimation of the encoding, latent states, and unknown
system parameters. To demonstrate the method, we provide case studies with the
Lorenz-63 ordinary differential equation, and the advection and Korteweg-de
Vries partial differential equations. Our results, with synthetic data, show
that $\Phi$-DVAE provides a data efficient dynamics encoding methodology which
is competitive with standard approaches. Unknown parameters are recovered with
uncertainty quantification, and unseen data are accurately predicted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Glyn_Davies_A/0/1/0/all/0/1&quot;&gt;Alex Glyn-Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duffin_C/0/1/0/all/0/1&quot;&gt;Connor Duffin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Akyildiz_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;. Deniz Akyildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12271">
<title>Global $k$-means$++$: an effective relaxation of the global $k$-means clustering algorithm. (arXiv:2211.12271v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12271</link>
<description rdf:parseType="Literal">&lt;p&gt;The $k$-means algorithm is a prevalent clustering method due to its
simplicity, effectiveness, and speed. However, its main disadvantage is its
high sensitivity to the initial positions of the cluster centers. The global
$k$-means is a deterministic algorithm proposed to tackle the random
initialization problem of k-means but its well-known that requires high
computational cost. It partitions the data to $K$ clusters by solving all
$k$-means sub-problems incrementally for all $k=1,\ldots, K$. For each $k$
cluster problem, the method executes the $k$-means algorithm $N$ times, where
$N$ is the number of datapoints. In this paper, we propose the \emph{global
$k$-means\texttt{++}} clustering algorithm, which is an effective way of
acquiring quality clustering solutions akin to those of global $k$-means with a
reduced computational load. This is achieved by exploiting the center selection
probability that is effectively used in the $k$-means\texttt{++} algorithm. The
proposed method has been tested and compared in various benchmark datasets
yielding very satisfactory results in terms of clustering quality and execution
speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardakas_G/0/1/0/all/0/1&quot;&gt;Georgios Vardakas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Likas_A/0/1/0/all/0/1&quot;&gt;Aristidis Likas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11267">
<title>Online Convex Optimization with Stochastic Constraints: Zero Constraint Violation and Bandit Feedback. (arXiv:2301.11267v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies online convex optimization with stochastic constraints. We
propose a variant of the drift-plus-penalty algorithm that guarantees
$O(\sqrt{T})$ expected regret and zero constraint violation, after a fixed
number of iterations, which improves the vanilla drift-plus-penalty method with
$O(\sqrt{T})$ constraint violation. Our algorithm is oblivious to the length of
the time horizon $T$, in contrast to the vanilla drift-plus-penalty method.
This is based on our novel drift lemma that provides time-varying bounds on the
virtual queue drift and, as a result, leads to time-varying bounds on the
expected virtual queue length. Moreover, we extend our framework to
stochastic-constrained online convex optimization under two-point bandit
feedback. We show that by adapting our algorithmic framework to the bandit
feedback setting, we may still achieve $O(\sqrt{T})$ expected regret and zero
constraint violation, improving upon the previous work for the case of
identical constraint functions. Numerical results demonstrate our theoretical
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yeongjong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dabeen Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00543">
<title>DoCoFL: Downlink Compression for Cross-Device Federated Learning. (arXiv:2302.00543v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;Many compression techniques have been proposed to reduce the communication
overhead of Federated Learning training procedures. However, these are
typically designed for compressing model updates, which are expected to decay
throughout training. As a result, such methods are inapplicable to downlink
(i.e., from the parameter server to clients) compression in the cross-device
setting, where heterogeneous clients $\textit{may appear only once}$ during
training and thus must download the model parameters. Accordingly, we propose
$\textsf{DoCoFL}$ -- a new framework for downlink compression in the
cross-device setting. Importantly, $\textsf{DoCoFL}$ can be seamlessly combined
with many uplink compression schemes, rendering it suitable for bi-directional
compression. Through extensive evaluation, we show that $\textsf{DoCoFL}$
offers significant bi-directional bandwidth reduction while achieving
competitive accuracy to that of a baseline without any compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorfman_R/0/1/0/all/0/1&quot;&gt;Ron Dorfman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1&quot;&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1&quot;&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Y. Levy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00736">
<title>Approximating the Shapley Value without Marginal Contributions. (arXiv:2302.00736v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00736</link>
<description rdf:parseType="Literal">&lt;p&gt;The Shapley value is arguably the most popular approach for assigning a
meaningful contribution value to players in a cooperative game, which has
recently been used intensively in explainable artificial intelligence. The
meaningfulness is due to axiomatic properties that only the Shapley value
satisfies, which, however, comes at the expense of an exact computation growing
exponentially with the number of agents. Accordingly, a number of works are
devoted to the efficient approximation of the Shapley values, most of them
revolve around the notion of an agent&apos;s marginal contribution. In this paper,
we propose with SVARM and Stratified SVARM two parameter-free and
domain-independent approximation algorithms based on a representation of the
Shapley value detached from the notion of marginal contributions. We prove
unmatched theoretical guarantees regarding their approximation quality and
provide empirical results including synthetic games as well as common
explainability use cases comparing ourselves with state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolpaczki_P/0/1/0/all/0/1&quot;&gt;Patrick Kolpaczki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengs_V/0/1/0/all/0/1&quot;&gt;Viktor Bengs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muschalik_M/0/1/0/all/0/1&quot;&gt;Maximilian Muschalik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00864">
<title>CLIPood: Generalizing CLIP to Out-of-Distributions. (arXiv:2302.00864v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) generalization, where the model needs to handle
distribution shifts from training, is a major challenge of machine learning.
Contrastive language-image pre-training (CLIP) models have shown impressive
zero-shot ability, but the further adaptation of CLIP on downstream tasks
undesirably degrades OOD performances. This paper aims at generalizing CLIP to
out-of-distribution test data on downstream tasks. We propose CLIPood, a
fine-tuning method that can adapt CLIP models to OOD situations where both
domain shifts and open classes may occur on the unseen test data. To exploit
the semantic relations between classes from the text modality, CLIPood
introduces a new training objective, margin metric softmax (MMS), with class
adaptive margins for fine-tuning. To incorporate both pre-trained zero-shot
model and fine-tuned task-adaptive model, CLIPood leverages a new optimization
strategy, Beta moving average (BMA), to maintain a temporal ensemble weighted
by Beta distribution. Experiments on diverse datasets with different OOD
scenarios show that CLIPood consistently outperforms existing generalization
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1&quot;&gt;Yang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xingzhuo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jialong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Ximei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianmin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1&quot;&gt;Mingsheng Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04391">
<title>The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.07014">
<title>A Data Mining Approach for Detecting Collusion in Unproctored Online Exams. (arXiv:2302.07014v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2302.07014</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the precautionary measures during the COVID-19 pandemic many
universities offered unproctored take-home exams. We propose methods to detect
potential collusion between students and apply our approach on event log data
from take-home exams during the pandemic. We find groups of students with
suspiciously similar exams. In addition, we compare our findings to a proctored
control group. By this, we establish a rule of thumb for evaluating which cases
are &quot;outstandingly similar&quot;, i.e., suspicious cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langerbein_J/0/1/0/all/0/1&quot;&gt;Janine Langerbein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Massing_T/0/1/0/all/0/1&quot;&gt;Till Massing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klenke_J/0/1/0/all/0/1&quot;&gt;Jens Klenke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reckmann_N/0/1/0/all/0/1&quot;&gt;Natalie Reckmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Striewe_M/0/1/0/all/0/1&quot;&gt;Michael Striewe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goedicke_M/0/1/0/all/0/1&quot;&gt;Michael Goedicke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanck_C/0/1/0/all/0/1&quot;&gt;Christoph Hanck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14460">
<title>Interpretable and Intervenable Ultrasonography-based Machine Learning Models for Pediatric Appendicitis. (arXiv:2302.14460v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14460</link>
<description rdf:parseType="Literal">&lt;p&gt;Appendicitis is among the most frequent reasons for pediatric abdominal
surgeries. With recent advances in machine learning, data-driven decision
support could help clinicians diagnose and manage patients while reducing the
number of non-critical surgeries. Previous decision support systems for
appendicitis focused on clinical, laboratory, scoring and computed tomography
data, mainly ignoring abdominal ultrasound, a noninvasive and readily available
diagnostic modality. To this end, we developed and validated interpretable
machine learning models for predicting the diagnosis, management and severity
of suspected appendicitis using ultrasound images. Our models were trained on a
dataset comprising 579 pediatric patients with 1709 ultrasound images
accompanied by clinical and laboratory data. Our methodological contribution is
the generalization of concept bottleneck models to prediction problems with
multiple views and incomplete concept sets. Notably, such models lend
themselves to interpretation and interaction via high-level concepts
understandable to clinicians without sacrificing performance or requiring
time-consuming image annotation when deployed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcinkevics_R/0/1/0/all/0/1&quot;&gt;Ri&amp;#x10d;ards Marcinkevi&amp;#x10d;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolfertstetter_P/0/1/0/all/0/1&quot;&gt;Patricia Reis Wolfertstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimiene_U/0/1/0/all/0/1&quot;&gt;Ugne Klimiene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_Cheong_K/0/1/0/all/0/1&quot;&gt;Kieran Chin-Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschke_A/0/1/0/all/0/1&quot;&gt;Alyssia Paschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerres_J/0/1/0/all/0/1&quot;&gt;Julia Zerres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denzinger_M/0/1/0/all/0/1&quot;&gt;Markus Denzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niederberger_D/0/1/0/all/0/1&quot;&gt;David Niederberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wellmann_S/0/1/0/all/0/1&quot;&gt;Sven Wellmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozkan_E/0/1/0/all/0/1&quot;&gt;Ece Ozkan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knorr_C/0/1/0/all/0/1&quot;&gt;Christian Knorr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogt_J/0/1/0/all/0/1&quot;&gt;Julia E. Vogt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09032">
<title>Conditionally Optimistic Exploration for Cooperative Deep Multi-Agent Reinforcement Learning. (arXiv:2303.09032v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09032</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient exploration is critical in cooperative deep Multi-Agent
Reinforcement Learning (MARL). In this work, we propose an exploration method
that effectively encourages cooperative exploration based on the idea of
sequential action-computation scheme. The high-level intuition is that to
perform optimism-based exploration, agents would explore cooperative strategies
if each agent&apos;s optimism estimate captures a structured dependency relationship
with other agents. Assuming agents compute actions following a sequential order
at \textit{each environment timestep}, we provide a perspective to view MARL as
tree search iterations by considering agents as nodes at different depths of
the search tree. Inspired by the theoretically justified tree search algorithm
UCT (Upper Confidence bounds applied to Trees), we develop a method called
Conditionally Optimistic Exploration (COE). COE augments each agent&apos;s
state-action value estimate with an action-conditioned optimistic bonus derived
from the visitation count of the global state and joint actions of preceding
agents. COE is performed during training and disabled at deployment, making it
compatible with any value decomposition method for centralized training with
decentralized execution. Experiments across various cooperative MARL benchmarks
show that COE outperforms current state-of-the-art exploration methods on
hard-exploration tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xutong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yangchen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chenjun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1&quot;&gt;Sarath Chandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1&quot;&gt;Janarthanan Rajendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14863">
<title>DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion. (arXiv:2303.14863v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14863</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new formulation of temporal action detection (TAD) with
denoising diffusion, DiffTAD in short. Taking as input random temporal
proposals, it can yield action proposals accurately given an untrimmed long
video. This presents a generative modeling perspective, against previous
discriminative learning manners. This capability is achieved by first diffusing
the ground-truth proposals to random ones (i.e., the forward/noising process)
and then learning to reverse the noising process (i.e., the backward/denoising
process). Concretely, we establish the denoising process in the Transformer
decoder (e.g., DETR) by introducing a temporal location query design with
faster convergence in training. We further propose a cross-step selective
conditioning algorithm for inference acceleration. Extensive evaluations on
ActivityNet and THUMOS show that our DiffTAD achieves top performance compared
to previous art alternatives. The code will be made available at
https://github.com/sauradip/DiffusionTAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiankang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16464">
<title>Lipschitzness Effect of a Loss Function on Generalization Performance of Deep Neural Networks Trained by Adam and AdamW Optimizers. (arXiv:2303.16464v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16464</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalization performance of deep neural networks with regard to the
optimization algorithm is one of the major concerns in machine learning. This
performance can be affected by various factors. In this paper, we theoretically
prove that the Lipschitz constant of a loss function is an important factor to
diminish the generalization error of the output model obtained by Adam or
AdamW. The results can be used as a guideline for choosing the loss function
when the optimization algorithm is Adam or AdamW. In addition, to evaluate the
theoretical bound in a practical setting, we choose the human age estimation
problem in computer vision. For assessing the generalization better, the
training and test datasets are drawn from different distributions. Our
experimental evaluation shows that the loss function with a lower Lipschitz
constant and maximum value improves the generalization of the model trained by
Adam or AdamW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lashkari_M/0/1/0/all/0/1&quot;&gt;Mohammad Lashkari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gheibi_A/0/1/0/all/0/1&quot;&gt;Amin Gheibi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08349">
<title>Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach. (arXiv:2304.08349v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08349</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite numerous successes in Deep Reinforcement Learning (DRL), the learned
policies are not interpretable. Moreover, since DRL does not exploit symbolic
relational representations, it has difficulties in coping with structural
changes in its environment (such as increasing the number of objects).
Relational Reinforcement Learning, on the other hand, inherits the relational
representations from symbolic planning to learn reusable policies. However, it
has so far been unable to scale up and exploit the power of deep neural
networks. We propose Deep Explainable Relational Reinforcement Learning
(DERRL), a framework that exploits the best of both -- neural and symbolic
worlds. By resorting to a neuro-symbolic approach, DERRL combines relational
representations and constraints from symbolic planning with deep learning to
extract interpretable policies. These policies are in the form of logical rules
that explain how each decision (or action) is arrived at. Through several
experiments, in setups like the Countdown Game, Blocks World, Gridworld, and
Traffic, we show that the policies learned by DERRL can be applied to different
configurations and contexts, hence generalizing to environmental modifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazra_R/0/1/0/all/0/1&quot;&gt;Rishi Hazra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raedt_L/0/1/0/all/0/1&quot;&gt;Luc De Raedt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17375">
<title>Attention Schema in Neural Agents. (arXiv:2305.17375v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17375</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention has become a common ingredient in deep learning architectures. It
adds a dynamical selection of information on top of the static selection of
information supported by weights. In the same way, we can imagine a
higher-order informational filter built on top of attention: an Attention
Schema (AS), namely, a descriptive and predictive model of attention. In
cognitive neuroscience, Attention Schema Theory (AST) supports this idea of
distinguishing attention from AS. A strong prediction of this theory is that an
agent can use its own AS to also infer the states of other agents&apos; attention
and consequently enhance coordination with other agents. As such, multi-agent
reinforcement learning would be an ideal setting to experimentally test the
validity of AST. We explore different ways in which attention and AS interact
with each other. Our preliminary results indicate that agents that implement
the AS as a recurrent internal control achieve the best performance. In
general, these exploratory experiments suggest that equipping artificial agents
with a model of attention can enhance their social intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dianbo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolotta_S/0/1/0/all/0/1&quot;&gt;Samuele Bolotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;He Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1&quot;&gt;Guillaume Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18405">
<title>Dink-Net: Neural Clustering on Large Graphs. (arXiv:2305.18405v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18405</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep graph clustering, which aims to group the nodes of a graph into disjoint
clusters with deep neural networks, has achieved promising progress in recent
years. However, the existing methods fail to scale to the large graph with
million nodes. To solve this problem, a scalable deep graph clustering method
(Dink-Net) is proposed with the idea of dilation and shrink. Firstly, by
discriminating nodes, whether being corrupted by augmentations, representations
are learned in a self-supervised manner. Meanwhile, the cluster centres are
initialized as learnable neural parameters. Subsequently, the clustering
distribution is optimized by minimizing the proposed cluster dilation loss and
cluster shrink loss in an adversarial manner. By these settings, we unify the
two-step clustering, i.e., representation learning and clustering optimization,
into an end-to-end framework, guiding the network to learn clustering-friendly
features. Besides, Dink-Net scales well to large graphs since the designed loss
functions adopt the mini-batch data to optimize the clustering distribution
even without performance drops. Both experimental results and theoretical
analyses demonstrate the superiority of our method. Compared to the runner-up,
Dink-Net achieves 9.62% NMI improvement on the ogbn-papers100M dataset with 111
million nodes and 1.6 billion edges. The source code is released at
https://github.com/yueliu1999/Dink-Net. Besides, a collection (papers, codes,
and datasets) of deep graph clustering is shared at
https://github.com/yueliu1999/Awesome-Deep-Graph-Clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19694">
<title>Hypothesis Transfer Learning with Surrogate Classification Losses: Generalization Bounds through Algorithmic Stability. (arXiv:2305.19694v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19694</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypothesis transfer learning (HTL) contrasts domain adaptation by allowing
for a previous task leverage, named the source, into a new one, the target,
without requiring access to the source data. Indeed, HTL relies only on a
hypothesis learnt from such source data, relieving the hurdle of expansive data
storage and providing great practical benefits. Hence, HTL is highly beneficial
for real-world applications relying on big data. The analysis of such a method
from a theoretical perspective faces multiple challenges, particularly in
classification tasks. This paper deals with this problem by studying the
learning theory of HTL through algorithmic stability, an attractive theoretical
framework for machine learning algorithms analysis. In particular, we are
interested in the statistical behaviour of the regularized empirical risk
minimizers in the case of binary classification. Our stability analysis
provides learning guarantees under mild assumptions. Consequently, we derive
several complexity-free generalization bounds for essential statistical
quantities like the training error, the excess risk and cross-validation
estimates. These refined bounds allow understanding the benefits of transfer
learning and comparing the behaviour of standard losses in different scenarios,
leading to valuable insights for practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aghbalou_A/0/1/0/all/0/1&quot;&gt;Anass Aghbalou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1&quot;&gt;Guillaume Staerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06283">
<title>14 Examples of How LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language Model Hackathon. (arXiv:2306.06283v4 [cond-mat.mtrl-sci] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06283</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-language models (LLMs) such as GPT-4 caught the interest of many
scientists. Recent studies suggested that these models could be useful in
chemistry and materials science. To explore these possibilities, we organized a
hackathon.
&lt;/p&gt;
&lt;p&gt;This article chronicles the projects built as part of this hackathon.
Participants employed LLMs for various applications, including predicting
properties of molecules and materials, designing novel interfaces for tools,
extracting knowledge from unstructured data, and developing new educational
applications.
&lt;/p&gt;
&lt;p&gt;The diverse topics and the fact that working prototypes could be generated in
less than two days highlight that LLMs will profoundly impact the future of our
fields. The rich collection of ideas and projects also indicates that the
applications of LLMs are not limited to materials science and chemistry but
offer potential benefits to a wide range of scientific disciplines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Jablonka_K/0/1/0/all/0/1&quot;&gt;Kevin Maik Jablonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ai_Q/0/1/0/all/0/1&quot;&gt;Qianxiang Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Al_Feghali_A/0/1/0/all/0/1&quot;&gt;Alexander Al-Feghali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Badhwar_S/0/1/0/all/0/1&quot;&gt;Shruti Badhwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bocarsly_J/0/1/0/all/0/1&quot;&gt;Joshua D. Bocarsly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bran_A/0/1/0/all/0/1&quot;&gt;Andres M Bran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Bringuier_S/0/1/0/all/0/1&quot;&gt;Stefan Bringuier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Brinson_L/0/1/0/all/0/1&quot;&gt;L. Catherine Brinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Choudhary_K/0/1/0/all/0/1&quot;&gt;Kamal Choudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Circi_D/0/1/0/all/0/1&quot;&gt;Defne Circi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Cox_S/0/1/0/all/0/1&quot;&gt;Sam Cox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Jong_W/0/1/0/all/0/1&quot;&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Evans_M/0/1/0/all/0/1&quot;&gt;Matthew L. Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gastellu_N/0/1/0/all/0/1&quot;&gt;Nicolas Gastellu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Genzling_J/0/1/0/all/0/1&quot;&gt;Jerome Genzling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gil_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Victoria Gil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Ankur K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Imran_A/0/1/0/all/0/1&quot;&gt;Alishba Imran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kruschwitz_S/0/1/0/all/0/1&quot;&gt;Sabine Kruschwitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Labarre_A/0/1/0/all/0/1&quot;&gt;Anne Labarre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lala_J/0/1/0/all/0/1&quot;&gt;Jakub L&amp;#xe1;la&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Steven Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Majumdar_S/0/1/0/all/0/1&quot;&gt;Sauradeep Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Merz_G/0/1/0/all/0/1&quot;&gt;Garrett W. Merz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Moitessier_N/0/1/0/all/0/1&quot;&gt;Nicolas Moitessier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Moubarak_E/0/1/0/all/0/1&quot;&gt;Elias Moubarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Mourino_B/0/1/0/all/0/1&quot;&gt;Beatriz Mouri&amp;#xf1;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Pelkie_B/0/1/0/all/0/1&quot;&gt;Brenden Pelkie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Pieler_M/0/1/0/all/0/1&quot;&gt;Michael Pieler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ramos_M/0/1/0/all/0/1&quot;&gt;Mayk Caldas Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Rankovic_B/0/1/0/all/0/1&quot;&gt;Bojana Rankovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Rodriques_S/0/1/0/all/0/1&quot;&gt;Samuel G. Rodriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sanders_J/0/1/0/all/0/1&quot;&gt;Jacob N. Sanders&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Schwaller_P/0/1/0/all/0/1&quot;&gt;Philippe Schwaller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Schwarting_M/0/1/0/all/0/1&quot;&gt;Marcus Schwarting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiale Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Smit_B/0/1/0/all/0/1&quot;&gt;Berend Smit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Smith_B/0/1/0/all/0/1&quot;&gt;Ben E. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Herck_J/0/1/0/all/0/1&quot;&gt;Joren Van Herck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Volker_C/0/1/0/all/0/1&quot;&gt;Christoph V&amp;#xf6;lker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ward_L/0/1/0/all/0/1&quot;&gt;Logan Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Warren_S/0/1/0/all/0/1&quot;&gt;Sean Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Weiser_B/0/1/0/all/0/1&quot;&gt;Benjamin Weiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sylvester Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zia_G/0/1/0/all/0/1&quot;&gt;Ghezal Ahmad Zia&lt;/a&gt;, et al. (5 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14369">
<title>Few-Shot Continual Learning via Flat-to-Wide Approaches. (arXiv:2306.14369v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14369</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing approaches on continual learning call for a lot of samples in their
training processes. Such approaches are impractical for many real-world
problems having limited samples because of the overfitting problem. This paper
proposes a few-shot continual learning approach, termed FLat-tO-WidE AppRoach
(FLOWER), where a flat-to-wide learning process finding the flat-wide minima is
proposed to address the catastrophic forgetting problem. The issue of data
scarcity is overcome with a data augmentation approach making use of a ball
generator concept to restrict the sampling space into the smallest enclosing
ball. Our numerical studies demonstrate the advantage of FLOWER achieving
significantly improved performances over prior arts notably in the small base
tasks. For further study, source codes of FLOWER, competitor algorithms and
experimental logs are shared publicly in
\url{https://github.com/anwarmaxsum/FLOWER}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masum_M/0/1/0/all/0/1&quot;&gt;Muhammad Anwar Ma&amp;#x27;sum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1&quot;&gt;Mahardhika Pratama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1&quot;&gt;Edwin Lughofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habibullah/0/1/0/all/0/1&quot;&gt;Habibullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalczyk_R/0/1/0/all/0/1&quot;&gt;Ryszard Kowalczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00828">
<title>Model-Assisted Probabilistic Safe Adaptive Control With Meta-Bayesian Learning. (arXiv:2307.00828v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00828</link>
<description rdf:parseType="Literal">&lt;p&gt;Breaking safety constraints in control systems can lead to potential risks,
resulting in unexpected costs or catastrophic damage. Nevertheless, uncertainty
is ubiquitous, even among similar tasks. In this paper, we develop a novel
adaptive safe control framework that integrates meta learning, Bayesian models,
and control barrier function (CBF) method. Specifically, with the help of CBF
method, we learn the inherent and external uncertainties by a unified adaptive
Bayesian linear regression (ABLR) model, which consists of a forward neural
network (NN) and a Bayesian output layer. Meta learning techniques are
leveraged to pre-train the NN weights and priors of the ABLR model using data
collected from historical similar tasks. For a new control task, we refine the
meta-learned models using a few samples, and introduce pessimistic confidence
bounds into CBF constraints to ensure safe control. Moreover, we provide
theoretical criteria to guarantee probabilistic safety during the control
processes. To validate our approach, we conduct comparative experiments in
various obstacle avoidance scenarios. The results demonstrate that our
algorithm significantly improves the Bayesian model-based CBF method, and is
capable for efficient safe exploration even with multiple uncertain
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuting Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tingwen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Shiping Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01946">
<title>A Synthetic Electrocardiogram (ECG) Image Generation Toolbox to Facilitate Deep Learning-Based Scanned ECG Digitization. (arXiv:2307.01946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01946</link>
<description rdf:parseType="Literal">&lt;p&gt;The electrocardiogram (ECG) is an accurate and widely available tool for
diagnosing cardiovascular diseases. ECGs have been recorded in printed formats
for decades and their digitization holds great potential for training machine
learning (ML) models in algorithmic ECG diagnosis. Physical ECG archives are at
risk of deterioration and scanning printed ECGs alone is insufficient, as ML
models require ECG time-series data. Therefore, the digitization and conversion
of paper ECG archives into time-series data is of utmost importance. Deep
learning models for image processing show promise in this regard. However, the
scarcity of ECG archives with reference time-series is a challenge. Data
augmentation techniques utilizing \textit{digital twins} present a potential
solution.
&lt;/p&gt;
&lt;p&gt;We introduce a novel method for generating synthetic ECG images on standard
paper-like ECG backgrounds with realistic artifacts. Distortions including
handwritten text artifacts, wrinkles, creases and perspective transforms are
applied to the generated images, without personally identifiable information.
As a use case, we generated an ECG image dataset of 21,801 records from the
12-lead PhysioNet PTB-XL ECG time-series dataset. A deep ECG image digitization
model was built and trained on the synthetic dataset, and was employed to
convert the synthetic images to time-series data for evaluation. The
signal-to-noise ratio (SNR) was calculated to assess the image digitization
quality vs the ground truth ECG time-series. The results show an average signal
recovery SNR of 27$\pm$2.8\,dB, demonstrating the significance of the proposed
synthetic ECG image dataset for training deep learning models. The codebase is
available as an open-access toolbox for ECG research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivashankara_K/0/1/0/all/0/1&quot;&gt;Kshama Kodthalu Shivashankara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shervedani_A/0/1/0/all/0/1&quot;&gt;Afagh Mehri Shervedani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sameni_R/0/1/0/all/0/1&quot;&gt;Reza Sameni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03380">
<title>On Formal Feature Attribution and Its Approximation. (arXiv:2307.03380v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03380</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed the widespread use of artificial intelligence
(AI) algorithms and machine learning (ML) models. Despite their tremendous
success, a number of vital problems like ML model brittleness, their fairness,
and the lack of interpretability warrant the need for the active developments
in explainable artificial intelligence (XAI) and formal ML model verification.
The two major lines of work in XAI include feature selection methods, e.g.
Anchors, and feature attribution techniques, e.g. LIME and SHAP. Despite their
promise, most of the existing feature selection and attribution approaches are
susceptible to a range of critical issues, including explanation unsoundness
and out-of-distribution sampling. A recent formal approach to XAI (FXAI)
although serving as an alternative to the above and free of these issues
suffers from a few other limitations. For instance and besides the scalability
limitation, the formal approach is unable to tackle the feature attribution
problem. Additionally, a formal explanation despite being formally sound is
typically quite large, which hampers its applicability in practical settings.
Motivated by the above, this paper proposes a way to apply the apparatus of
formal XAI to the case of feature attribution based on formal explanation
enumeration. Formal feature attribution (FFA) is argued to be advantageous over
the existing methods, both formal and non-formal. Given the practical
complexity of the problem, the paper then proposes an efficient technique for
approximating exact FFA. Finally, it offers experimental evidence of the
effectiveness of the proposed approximate FFA in comparison to the existing
feature attribution algorithms not only in terms of feature importance and but
also in terms of their relative order.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jinqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1&quot;&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuckey_P/0/1/0/all/0/1&quot;&gt;Peter J. Stuckey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03500">
<title>DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03500</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient sparsification is a widely adopted solution for reducing the
excessive communication traffic in distributed deep learning. However, most
existing gradient sparsifiers have relatively poor scalability because of
considerable computational cost of gradient selection and/or increased
communication traffic owing to gradient build-up. To address these challenges,
we propose a novel gradient sparsification scheme, DEFT, that partitions the
gradient selection task into sub tasks and distributes them to workers. DEFT
differs from existing sparsifiers, wherein every worker selects gradients among
all gradients. Consequently, the computational cost can be reduced as the
number of workers increases. Moreover, gradient build-up can be eliminated
because DEFT allows workers to select gradients in partitions that are
non-intersecting (between workers). Therefore, even if the number of workers
increases, the communication traffic can be maintained as per user requirement.
&lt;/p&gt;
&lt;p&gt;To avoid the loss of significance of gradient selection, DEFT selects more
gradients in the layers that have a larger gradient norm than the other layers.
Because every layer has a different computational load, DEFT allocates layers
to workers using a bin-packing algorithm to maintain a balanced load of
gradient selection between workers. In our empirical evaluation, DEFT shows a
significant improvement in training performance in terms of speed in gradient
selection over existing sparsifiers while achieving high convergence
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1&quot;&gt;Daegun Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sangyoon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03716">
<title>SAR: Generalization of Physiological Agility and Dexterity via Synergistic Action Representation. (arXiv:2307.03716v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03716</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning effective continuous control policies in high-dimensional systems,
including musculoskeletal agents, remains a significant challenge. Over the
course of biological evolution, organisms have developed robust mechanisms for
overcoming this complexity to learn highly sophisticated strategies for motor
control. What accounts for this robust behavioral flexibility? Modular control
via muscle synergies, i.e. coordinated muscle co-contractions, is considered to
be one putative mechanism that enables organisms to learn muscle control in a
simplified and generalizable action space. Drawing inspiration from this
evolved motor control strategy, we use physiologically accurate human hand and
leg models as a testbed for determining the extent to which a Synergistic
Action Representation (SAR) acquired from simpler tasks facilitates learning
more complex tasks. We find in both cases that SAR-exploiting policies
significantly outperform end-to-end reinforcement learning. Policies trained
with SAR were able to achieve robust locomotion on a wide set of terrains with
high sample efficiency, while baseline approaches failed to learn meaningful
behaviors. Additionally, policies trained with SAR on a multiobject
manipulation task significantly outperformed (&amp;gt;70% success) baseline approaches
(&amp;lt;20% success). Both of these SAR-exploiting policies were also found to
generalize zero-shot to out-of-domain environmental conditions, while policies
that did not adopt SAR failed to generalize. Finally, we establish the
generality of SAR on broader high-dimensional control problems using a robotic
manipulation task set and a full-body humanoid locomotion task. To the best of
our knowledge, this investigation is the first of its kind to present an
end-to-end pipeline for discovering synergies and using this representation to
learn high-dimensional continuous control across a wide diversity of tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berg_C/0/1/0/all/0/1&quot;&gt;Cameron Berg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caggiano_V/0/1/0/all/0/1&quot;&gt;Vittorio Caggiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04675">
<title>LINFA: a Python library for variational inference with normalizing flow and annealing. (arXiv:2307.04675v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04675</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference is an increasingly popular method in statistics and
machine learning for approximating probability distributions. We developed
LINFA (Library for Inference with Normalizing Flow and Annealing), a Python
library for variational inference to accommodate computationally expensive
models and difficult-to-sample distributions with dependent parameters. We
discuss the theoretical background, capabilities, and performance of LINFA in
various benchmarks. LINFA is publicly available on GitHub at
https://github.com/desResLab/LINFA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobian_E/0/1/0/all/0/1&quot;&gt;Emma R. Cobian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jubilee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauenstein_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Hauenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiavazzi_D/0/1/0/all/0/1&quot;&gt;Daniele E. Schiavazzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04962">
<title>Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04962</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by the visited nodes in the environment. We use these
proposed features as rewards for graph neural-network-based reinforcement
learning. On multiple classes of synthetically generated graphs, we find that
trained agents generalize to larger environments and to longer exploratory
walks than are seen during training. Our method computes more efficiently than
the greedy evaluation of the relevant topological properties. The proposed
intrinsic motivations bear particular relevance for recommender systems. We
demonstrate that curiosity-based recommendations are more predictive of human
behavior than PageRank centrality for several real-world graph datasets,
including MovieLens, Amazon Books, and Wikispeedia.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shubhankar P. Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1&quot;&gt;Mathieu Ouellet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1&quot;&gt;Juan Cervino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kieran A. Murphy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1&quot;&gt;Dani S. Bassett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05187">
<title>Decorrelation using Optimal Transport. (arXiv:2307.05187v2 [hep-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05187</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to decorrelate a feature space from protected attributes is an
area of active research and study in ethics, fairness, and also natural
sciences. We introduce a novel decorrelation method using Convex Neural Optimal
Transport Solvers (Cnots) that is able to decorrelate a continuous feature
space against protected attributes with optimal transport. We demonstrate how
well it performs in the context of jet classification in high energy physics,
where classifier scores are desired to be decorrelated from the mass of a jet.
The decorrelation achieved in binary classification approaches the levels
achieved by the state-of-the-art using conditional normalising flows. When
moving to multiclass outputs the optimal transport approach performs
significantly better than the state-of-the-art, suggesting substantial gains at
decorrelating multidimensional feature spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Algren_M/0/1/0/all/0/1&quot;&gt;Malte Algren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05189">
<title>Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05189</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a simple linear regression based approach for learning the weights
and biases of a neural network, as an alternative to standard gradient based
backpropagation. The present work is exploratory in nature, and we restrict the
description and experiments to (i) simple feedforward neural networks, (ii)
scalar (single output) regression problems, and (iii) invertible activation
functions. However, the approach is intended to be extensible to larger, more
complex architectures. The key idea is the observation that the input to every
neuron in a neural network is a linear combination of the activations of
neurons in the previous layer, as well as the parameters (weights and biases)
of the layer. If we are able to compute the ideal total input values to every
neuron by working backwards from the output, we can formulate the learning
problem as a linear least squares problem which iterates between updating the
parameters and the activation values. We present an explicit algorithm that
implements this idea, and we show that (at least for small problems) the
approach is more stable and faster than gradient-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1&quot;&gt;Harshad Khadilkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05695">
<title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1&quot;&gt;Vladislav Lialin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1&quot;&gt;Namrata Shivagunde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1&quot;&gt;Sherin Muckatira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1&quot;&gt;Anna Rumshisky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06250">
<title>Identifiability Guarantees for Causal Disentanglement from Soft Interventions. (arXiv:2307.06250v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06250</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal disentanglement aims to uncover a representation of data using latent
variables that are interrelated through a causal model. Such a representation
is identifiable if the latent model that explains the data is unique. In this
paper, we focus on the scenario where unpaired observational and interventional
data are available, with each intervention changing the mechanism of a latent
variable. When the causal variables are fully observed, statistically
consistent algorithms have been developed to identify the causal model under
faithfulness assumptions. We here show that identifiability can still be
achieved with unobserved causal variables, given a generalized notion of
faithfulness. Our results guarantee that we can recover the latent causal model
up to an equivalence class and predict the effect of unseen combinations of
interventions, in the limit of infinite data. We implement our causal
disentanglement framework by developing an autoencoding variational Bayes
algorithm and apply it to the problem of predicting combinatorial perturbation
effects in genomics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Squires_C/0/1/0/all/0/1&quot;&gt;Chandler Squires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Greenewald_K/0/1/0/all/0/1&quot;&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Uhler_C/0/1/0/all/0/1&quot;&gt;Caroline Uhler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06457">
<title>Tackling Combinatorial Distribution Shift: A Matrix Completion Perspective. (arXiv:2307.06457v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06457</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining rigorous statistical guarantees for generalization under
distribution shift remains an open and active research area. We study a setting
we call combinatorial distribution shift, where (a) under the test- and
training-distributions, the labels $z$ are determined by pairs of features
$(x,y)$, (b) the training distribution has coverage of certain marginal
distributions over $x$ and $y$ separately, but (c) the test distribution
involves examples from a product distribution over $(x,y)$ that is {not}
covered by the training distribution. Focusing on the special case where the
labels are given by bilinear embeddings into a Hilbert space $H$: $\mathbb{E}[z
\mid x,y ]=\langle f_{\star}(x),g_{\star}(y)\rangle_{{H}}$, we aim to
extrapolate to a test distribution domain that is $not$ covered in training,
i.e., achieving bilinear combinatorial extrapolation.
&lt;/p&gt;
&lt;p&gt;Our setting generalizes a special case of matrix completion from
missing-not-at-random data, for which all existing results require the
ground-truth matrices to be either exactly low-rank, or to exhibit very sharp
spectral cutoffs. In this work, we develop a series of theoretical results that
enable bilinear combinatorial extrapolation under gradual spectral decay as
observed in typical high-dimensional data, including novel algorithms,
generalization guarantees, and linear-algebraic results. A key tool is a novel
perturbation bound for the rank-$k$ singular value decomposition approximations
between two matrices that depends on the relative spectral gap rather than the
absolute spectral gap, a result that may be of broader independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1&quot;&gt;Max Simchowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaiqing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06501">
<title>Hybrid Control Policy for Artificial Pancreas via Ensemble Deep Reinforcement Learning. (arXiv:2307.06501v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06501</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: The artificial pancreas (AP) has shown promising potential in
achieving closed-loop glucose control for individuals with type 1 diabetes
mellitus (T1DM). However, designing an effective control policy for the AP
remains challenging due to the complex physiological processes, delayed insulin
response, and inaccurate glucose measurements. While model predictive control
(MPC) offers safety and stability through the dynamic model and safety
constraints, it lacks individualization and is adversely affected by
unannounced meals. Conversely, deep reinforcement learning (DRL) provides
personalized and adaptive strategies but faces challenges with distribution
shifts and substantial data requirements. Methods: We propose a hybrid control
policy for the artificial pancreas (HyCPAP) to address the above challenges.
HyCPAP combines an MPC policy with an ensemble DRL policy, leveraging the
strengths of both policies while compensating for their respective limitations.
To facilitate faster deployment of AP systems in real-world settings, we
further incorporate meta-learning techniques into HyCPAP, leveraging previous
experience and patient-shared knowledge to enable fast adaptation to new
patients with limited available data. Results: We conduct extensive experiments
using the FDA-accepted UVA/Padova T1DM simulator across three scenarios. Our
approaches achieve the highest percentage of time spent in the desired
euglycemic range and the lowest occurrences of hypoglycemia. Conclusion: The
results clearly demonstrate the superiority of our methods for closed-loop
glucose management in individuals with T1DM. Significance: The study presents
novel control policies for AP systems, affirming the great potential of
proposed methods for efficient closed-loop glucose control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_W/0/1/0/all/0/1&quot;&gt;Wenzhou Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Luolin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_F/0/1/0/all/0/1&quot;&gt;Feng Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06836">
<title>PC-Droid: Faster diffusion and improved quality for particle cloud generation. (arXiv:2307.06836v2 [hep-ex] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06836</link>
<description rdf:parseType="Literal">&lt;p&gt;Building on the success of PC-JeDi we introduce PC-Droid, a substantially
improved diffusion model for the generation of jet particle clouds. By
leveraging a new diffusion formulation, studying more recent integration
solvers, and training on all jet types simultaneously, we are able to achieve
state-of-the-art performance for all types of jets across all evaluation
metrics. We study the trade-off between generation speed and quality by
comparing two attention based architectures, as well as the potential of
consistency distillation to reduce the number of diffusion steps. Both the
faster architecture and consistency models demonstrate performance surpassing
many competing models, with generation time up to two orders of magnitude
faster than PC-JeDi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Leigh_M/0/1/0/all/0/1&quot;&gt;Matthew Leigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Sengupta_D/0/1/0/all/0/1&quot;&gt;Debajyoti Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Quetant_G/0/1/0/all/0/1&quot;&gt;Guillaume Qu&amp;#xe9;tant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Golling_T/0/1/0/all/0/1&quot;&gt;Tobias Golling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06913">
<title>Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the inner workings of deep learning models is crucial for
establishing trust and ensuring model safety. Concept-based explanations have
emerged as a superior approach that is more interpretable than feature
attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the user&apos;s
expectations on the concepts. To address this, we propose a novel post-hoc
unsupervised method that automatically uncovers the concepts learned by deep
models during training. By decomposing the latent space of a layer in singular
vectors and refining them by unsupervised clustering, we uncover concept
vectors aligned with directions of high variance that are relevant to the model
prediction, and that point to semantically distinct concepts. Our extensive
experiments reveal that the majority of our concepts are readily understandable
to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where
our concept vectors successfully identify outlier training samples affected by
various confounding factors. This novel exploration technique has remarkable
versatility to data types and model architectures and it will facilitate the
identification of biases and the discovery of sources of error within training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1&quot;&gt;Mara Graziani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahony_L/0/1/0/all/0/1&quot;&gt;Laura O&amp;#x27; Mahony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An-Phi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>