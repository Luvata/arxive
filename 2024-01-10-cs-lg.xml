<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-08T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03302" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.05839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.00676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.08440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.00846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.02338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.03482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.15462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.05631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.04370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.14566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14260" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16228" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02740" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.02961">
<title>A Surrogate-Assisted Extended Generative Adversarial Network for Parameter Optimization in Free-Form Metasurface Design. (arXiv:2401.02961v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.02961</link>
<description rdf:parseType="Literal">&lt;p&gt;Metasurfaces have widespread applications in fifth-generation (5G) microwave
communication. Among the metasurface family, free-form metasurfaces excel in
achieving intricate spectral responses compared to regular-shape counterparts.
However, conventional numerical methods for free-form metasurfaces are
time-consuming and demand specialized expertise. Alternatively, recent studies
demonstrate that deep learning has great potential to accelerate and refine
metasurface designs. Here, we present XGAN, an extended generative adversarial
network (GAN) with a surrogate for high-quality free-form metasurface designs.
The proposed surrogate provides a physical constraint to XGAN so that XGAN can
accurately generate metasurfaces monolithically from input spectral responses.
In comparative experiments involving 20000 free-form metasurface designs, XGAN
achieves 0.9734 average accuracy and is 500 times faster than the conventional
methodology. This method facilitates the metasurface library building for
specific spectral responses and can be extended to various inverse design
problems, including optical metamaterials, nanophotonic devices, and drug
discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Manna Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattoraj_J/0/1/0/all/0/1&quot;&gt;Joyjit Chattoraj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingzhi Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinxing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weijiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1&quot;&gt;My Ha Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02989">
<title>On the selection and effectiveness of pseudo-absences for species distribution modeling with deep learning. (arXiv:2401.02989v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.02989</link>
<description rdf:parseType="Literal">&lt;p&gt;Species distribution modeling is a highly versatile tool for understanding
the intricate relationship between environmental conditions and species
occurrences. However, the available data often lacks information on confirmed
species absence and is limited to opportunistically sampled, presence-only
observations. To overcome this limitation, a common approach is to employ
pseudo-absences, which are specific geographic locations designated as negative
samples. While pseudo-absences are well-established for single-species
distribution models, their application in the context of multi-species neural
networks remains underexplored. Notably, the significant class imbalance
between species presences and pseudo-absences is often left unaddressed.
Moreover, the existence of different types of pseudo-absences (e.g., random and
target-group background points) adds complexity to the selection process.
Determining the optimal combination of pseudo-absences types is difficult and
depends on the characteristics of the data, particularly considering that
certain types of pseudo-absences can be used to mitigate geographic biases. In
this paper, we demonstrate that these challenges can be effectively tackled by
integrating pseudo-absences in the training of multi-species neural networks
through modifications to the loss function. This adjustment involves assigning
different weights to the distinct terms of the loss function, thereby
addressing both the class imbalance and the choice of pseudo-absence types.
Additionally, we propose a strategy to set these loss weights using spatial
block cross-validation with presence-only data. We evaluate our approach using
a benchmark dataset containing independent presence-absence data from six
different regions and report improved results when compared to competing
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zbinden_R/0/1/0/all/0/1&quot;&gt;Robin Zbinden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tiel_N/0/1/0/all/0/1&quot;&gt;Nina van Tiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kellenberger_B/0/1/0/all/0/1&quot;&gt;Benjamin Kellenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hughes_L/0/1/0/all/0/1&quot;&gt;Lloyd Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tuia_D/0/1/0/all/0/1&quot;&gt;Devis Tuia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02991">
<title>GLIDE-RL: Grounded Language Instruction through DEmonstration in RL. (arXiv:2401.02991v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.02991</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the final frontiers in the development of complex human - AI
collaborative systems is the ability of AI agents to comprehend the natural
language and perform tasks accordingly. However, training efficient
Reinforcement Learning (RL) agents grounded in natural language has been a
long-standing challenge due to the complexity and ambiguity of the language and
sparsity of the rewards, among other factors. Several advances in reinforcement
learning, curriculum learning, continual learning, language models have
independently contributed to effective training of grounded agents in various
environments. Leveraging these developments, we present a novel algorithm,
Grounded Language Instruction through DEmonstration in RL (GLIDE-RL) that
introduces a teacher-instructor-student curriculum learning framework for
training an RL agent capable of following natural language instructions that
can generalize to previously unseen language instructions. In this multi-agent
framework, the teacher and the student agents learn simultaneously based on the
student&apos;s current skill level. We further demonstrate the necessity for
training the student agent with not just one, but multiple teacher agents.
Experiments on a complex sparse reward environment validates the effectiveness
of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kharyal_C/0/1/0/all/0/1&quot;&gt;Chaitanya Kharyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottipati_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Gottipati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_T/0/1/0/all/0/1&quot;&gt;Tanmay Kumar Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Srijita Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02996">
<title>An AI-enabled Bias-Free Respiratory Disease Diagnosis Model using Cough Audio: A Case Study for COVID-19. (arXiv:2401.02996v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.02996</link>
<description rdf:parseType="Literal">&lt;p&gt;Cough-based diagnosis for Respiratory Diseases (RDs) using Artificial
Intelligence (AI) has attracted considerable attention, yet many existing
studies overlook confounding variables in their predictive models. These
variables can distort the relationship between cough recordings (input data)
and RD status (output variable), leading to biased associations and unrealistic
model performance. To address this gap, we propose the Bias Free Network
(RBFNet), an end to end solution that effectively mitigates the impact of
confounders in the training data distribution. RBFNet ensures accurate and
unbiased RD diagnosis features, emphasizing its relevance by incorporating a
COVID19 dataset in this study. This approach aims to enhance the reliability of
AI based RD diagnosis models by navigating the challenges posed by confounding
variables. A hybrid of a Convolutional Neural Networks (CNN) and Long-Short
Term Memory (LSTM) networks is proposed for the feature encoder module of
RBFNet. An additional bias predictor is incorporated in the classification
scheme to formulate a conditional Generative Adversarial Network (cGAN) which
helps in decorrelating the impact of confounding variables from RD prediction.
The merit of RBFNet is demonstrated by comparing classification performance
with State of The Art (SoTA) Deep Learning (DL) model (CNN LSTM) after training
on different unbalanced COVID-19 data sets, created by using a large scale
proprietary cough data set. RBF-Net proved its robustness against extremely
biased training scenarios by achieving test set accuracies of 84.1%, 84.6%, and
80.5% for the following confounding variables gender, age, and smoking status,
respectively. RBF-Net outperforms the CNN-LSTM model test set accuracies by
5.5%, 7.7%, and 8.2%, respectively
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeed_T/0/1/0/all/0/1&quot;&gt;Tabish Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ijaz_A/0/1/0/all/0/1&quot;&gt;Aneeqa Ijaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadiq_I/0/1/0/all/0/1&quot;&gt;Ismail Sadiq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_H/0/1/0/all/0/1&quot;&gt;Haneya N. Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizwan_A/0/1/0/all/0/1&quot;&gt;Ali Rizwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imran_A/0/1/0/all/0/1&quot;&gt;Ali Imran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03000">
<title>Bridging Modalities: Knowledge Distillation and Masked Training for Translating Multi-Modal Emotion Recognition to Uni-Modal, Speech-Only Emotion Recognition. (arXiv:2401.03000v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.03000</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an innovative approach to address the challenges of
translating multi-modal emotion recognition models to a more practical and
resource-efficient uni-modal counterpart, specifically focusing on speech-only
emotion recognition. Recognizing emotions from speech signals is a critical
task with applications in human-computer interaction, affective computing, and
mental health assessment. However, existing state-of-the-art models often rely
on multi-modal inputs, incorporating information from multiple sources such as
facial expressions and gestures, which may not be readily available or feasible
in real-world scenarios. To tackle this issue, we propose a novel framework
that leverages knowledge distillation and masked training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Muaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paull_N/0/1/0/all/0/1&quot;&gt;Nathan Paull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malagavalli_J/0/1/0/all/0/1&quot;&gt;Jahnavi Malagavalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03001">
<title>UnetTSF: A Better Performance Linear Complexity Time Series Prediction Model. (arXiv:2401.03001v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03001</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Transformer-base models have made significant progress in the field
of time series prediction which have achieved good results and become baseline
models beyond Dlinear. The paper proposes an U-Net time series prediction model
(UnetTSF) with linear complexity, which adopts the U-Net architecture. We are
the first to use FPN technology to extract features from time series data,
replacing the method of decomposing time series data into trend and seasonal
terms, while designing a fusion structure suitable for time series data. After
testing on 8 open-source datasets, compared to the best linear model DLiner.
Out of 32 testing projects, 31 achieved the best results. The average decrease
in mse is 10.1%, while the average decrease in mae is 9.1%. Compared with the
complex transformer-base PatchTST, UnetTSF obtained 9 optimal results for mse
and 15 optimal results for mae in 32 testing projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+chu_L/0/1/0/all/0/1&quot;&gt;Li chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+bingjia_X/0/1/0/all/0/1&quot;&gt;Xiao bingjia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+qiping_Y/0/1/0/all/0/1&quot;&gt;Yuan qiping&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03003">
<title>AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have made significant advancements in
code-related tasks, yet many LLMs treat code as simple sequences, neglecting
its structured nature. We introduce AST-T5, a novel pretraining paradigm that
leverages the Abstract Syntax Tree (AST) for enhanced code generation,
transpilation, and understanding. Using dynamic programming, our AST-Aware
Segmentation retains code structure, while our AST-Aware Span Corruption
objective equips the model to reconstruct various code structures. Unlike other
models, AST-T5 avoids intricate program analyses or architectural changes, so
it integrates seamlessly with any encoder-decoder Transformer. Evaluations show
that AST-T5 consistently outperforms similar-sized LMs across various
code-related tasks. Structure-awareness makes AST-T5 particularly powerful in
code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the
Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in
CodeXGLUE. Our code and model are publicly available at
https://github.com/gonglinyuan/ast_t5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Linyuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1&quot;&gt;Mostafa Elhoushi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_A/0/1/0/all/0/1&quot;&gt;Alvin Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03006">
<title>The Rise of Diffusion Models in Time-Series Forecasting. (arXiv:2401.03006v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey delves into the application of diffusion models in time-series
forecasting. Diffusion models are demonstrating state-of-the-art results in
various fields of generative AI. The paper includes comprehensive background
information on diffusion models, detailing their conditioning methods and
reviewing their use in time-series forecasting. The analysis covers 11 specific
time-series implementations, the intuition and theory behind them, the
effectiveness on different datasets, and a comparison among each other. Key
contributions of this work are the thorough exploration of diffusion models&apos;
applications in time-series forecasting and a chronologically ordered overview
of these models. Additionally, the paper offers an insightful discussion on the
current state-of-the-art in this domain and outlines potential future research
directions. This serves as a valuable resource for researchers in AI and
time-series analysis, offering a clear view of the latest advancements and
future potential of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meijer_C/0/1/0/all/0/1&quot;&gt;Caspar Meijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lydia Y. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03040">
<title>AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis. (arXiv:2401.03040v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03040</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accident analysis is pivotal for enhancing public safety and
developing road regulations. Traditional approaches, although widely used, are
often constrained by manual analysis processes, subjective decisions, uni-modal
outputs, as well as privacy issues related to sensitive data. This paper
introduces the idea of AccidentGPT, a foundation model of traffic accident
analysis, which incorporates multi-modal input data to automatically
reconstruct the accident process video with dynamics details, and furthermore
provide multi-task analysis with multi-modal outputs. The design of the
AccidentGPT is empowered with a multi-modality prompt with feedback for
task-oriented adaptability, a hybrid training schema to leverage labelled and
unlabelled data, and a edge-cloud split configuration for data privacy. To
fully realize the functionalities of this model, we proposes several research
opportunities. This paper serves as the stepping stone to fill the gaps in
traditional approaches of traffic accident analysis and attract the research
community attention for automatic, objective, and privacy-preserving traffic
accident analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kebin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03051">
<title>On the Convergence of Semi Unsupervised Calibration through Prior Adaptation Algorithm. (arXiv:2401.03051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03051</link>
<description rdf:parseType="Literal">&lt;p&gt;Calibration is an essential key in machine leaning. Semi Unsupervised
Calibration through Prior Adaptation (SUCPA) is a calibration algorithm used in
(but not limited to) large-scale language models defined by a {system of
first-order difference equation. The map derived by this system} has the
peculiarity of being non-hyperbolic {with a non-bounded set of non-isolated
fixed points}. In this work, we prove several convergence properties of this
algorithm from the perspective of dynamical systems. For a binary
classification problem, it can be shown that the algorithm always converges,
{more precisely, the map is globally asymptotically stable, and the orbits
converge} to a single line of fixed points. Finally, we perform numerical
experiments on real-world application to support the presented results.
Experiment codes are available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estienne_L/0/1/0/all/0/1&quot;&gt;Lautaro Estienne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_R/0/1/0/all/0/1&quot;&gt;Roberta Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_M/0/1/0/all/0/1&quot;&gt;Matias Vera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1&quot;&gt;Luciana Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1&quot;&gt;Pablo Piantanida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03058">
<title>Krylov Cubic Regularized Newton: A Subspace Second-Order Method with Dimension-Free Convergence Rate. (arXiv:2401.03058v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.03058</link>
<description rdf:parseType="Literal">&lt;p&gt;Second-order optimization methods, such as cubic regularized Newton methods,
are known for their rapid convergence rates; nevertheless, they become
impractical in high-dimensional problems due to their substantial memory
requirements and computational costs. One promising approach is to execute
second-order updates within a lower-dimensional subspace, giving rise to
subspace second-order methods. However, the majority of existing subspace
second-order methods randomly select subspaces, consequently resulting in
slower convergence rates depending on the problem&apos;s dimension $d$. In this
paper, we introduce a novel subspace cubic regularized Newton method that
achieves a dimension-independent global convergence rate of
${O}\left(\frac{1}{mk}+\frac{1}{k^2}\right)$ for solving convex optimization
problems. Here, $m$ represents the subspace dimension, which can be
significantly smaller than $d$. Instead of adopting a random subspace, our
primary innovation involves performing the cubic regularized Newton update
within the Krylov subspace associated with the Hessian and the gradient of the
objective function. This result marks the first instance of a
dimension-independent convergence rate for a subspace second-order method.
Furthermore, when specific spectral conditions of the Hessian are met, our
method recovers the convergence rate of a full-dimensional cubic regularized
Newton method. Numerical experiments show our method converges faster than
existing random subspace methods, especially for high-dimensional problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruichen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sabach_S/0/1/0/all/0/1&quot;&gt;Shoham Sabach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1&quot;&gt;Aryan Mokhtari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cevher_V/0/1/0/all/0/1&quot;&gt;Volkan Cevher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03059">
<title>Reliability-Optimized User Admission Control for URLLC Traffic: A Neural Contextual Bandit Approach. (arXiv:2401.03059v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03059</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultra-reliable low-latency communication (URLLC) is the cornerstone for a
broad range of emerging services in next-generation wireless networks. URLLC
fundamentally relies on the network&apos;s ability to proactively determine whether
sufficient resources are available to support the URLLC traffic, and thus,
prevent so-called cell overloads. Nonetheless, achieving accurate
quality-of-service (QoS) predictions for URLLC user equipment (UEs) and
preventing cell overloads are very challenging tasks. This is due to dependency
of the QoS metrics (latency and reliability) on traffic and channel statistics,
users&apos; mobility, and interdependent performance across UEs. In this paper, a
new QoS-aware UE admission control approach is developed to proactively
estimate QoS for URLLC UEs, prior to associating them with a cell, and
accordingly, admit only a subset of UEs that do not lead to a cell overload. To
this end, an optimization problem is formulated to find an efficient UE
admission control policy, cognizant of UEs&apos; QoS requirements and cell-level
load dynamics. To solve this problem, a new machine learning based method is
proposed that builds on (deep) neural contextual bandits, a suitable framework
for dealing with nonlinear bandit problems. In fact, the UE admission
controller is treated as a bandit agent that observes a set of network
measurements (context) and makes admission control decisions based on
context-dependent QoS (reward) predictions. The simulation results show that
the proposed scheme can achieve near-optimal performance and yield substantial
gains in terms of cell-level service reliability and efficient resource
utilization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semiari_O/0/1/0/all/0/1&quot;&gt;Omid Semiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikopour_H/0/1/0/all/0/1&quot;&gt;Hosein Nikopour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talwar_S/0/1/0/all/0/1&quot;&gt;Shilpa Talwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03065">
<title>CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution. (arXiv:2401.03065v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.03065</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CRUXEval (Code Reasoning, Understanding, and eXecution
Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each
function comes with an input-output pair, leading to two natural tasks: input
prediction and output prediction. First, we propose a generic recipe for
generating our execution benchmark which can be used to create future variation
of the benchmark. Second, we evaluate twenty code models on our benchmark and
discover that many recent high-scoring models on HumanEval do not show the same
improvements on our benchmark. Third, we show that simple CoT and fine-tuning
schemes can improve performance on our benchmark but remain far from solving
it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%
and 81% on input and output prediction, respectively. In contrast, Code Llama
34B achieves a pass@1 of 50% and 46% on input and output prediction,
highlighting the gap between open and closed source models. As no model is
close to acing CRUXEval, we provide examples of consistent GPT-4 failures on
simple programs as a lens into its code reasoning capabilities and areas for
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1&quot;&gt;Alex Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roziere_B/0/1/0/all/0/1&quot;&gt;Baptiste Rozi&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leather_H/0/1/0/all/0/1&quot;&gt;Hugh Leather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solar_Lezama_A/0/1/0/all/0/1&quot;&gt;Armando Solar-Lezama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1&quot;&gt;Gabriel Synnaeve&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sida I. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03069">
<title>Towards Enhancing the Reproducibility of Deep Learning Bugs: An Empirical Study. (arXiv:2401.03069v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.03069</link>
<description rdf:parseType="Literal">&lt;p&gt;Context: Deep learning has achieved remarkable progress in various domains.
However, like traditional software systems, deep learning systems contain bugs,
which can have severe impacts, as evidenced by crashes involving autonomous
vehicles. Despite substantial advancements in deep learning techniques, little
research has focused on reproducing deep learning bugs, which hinders resolving
them. Existing literature suggests that only 3% of deep learning bugs are
reproducible, underscoring the need for further research.
&lt;/p&gt;
&lt;p&gt;Objective: This paper examines the reproducibility of deep learning bugs. We
identify edit actions and useful information that could improve deep learning
bug reproducibility.
&lt;/p&gt;
&lt;p&gt;Method: First, we construct a dataset of 668 deep learning bugs from Stack
Overflow and Defects4ML across 3 frameworks and 22 architectures. Second, we
select 102 bugs using stratified sampling and try to determine their
reproducibility. While reproducing these bugs, we identify edit actions and
useful information necessary for their reproduction. Third, we used the Apriori
algorithm to identify useful information and edit actions required to reproduce
specific bug types. Finally, we conduct a user study with 22 developers to
assess the effectiveness of our findings in real-life settings.
&lt;/p&gt;
&lt;p&gt;Results: We successfully reproduced 85 bugs and identified ten edit actions
and five useful information categories that can help us reproduce deep learning
bugs. Our findings improved bug reproducibility by 22.92% and reduced
reproduction time by 24.35% based on our user study.
&lt;/p&gt;
&lt;p&gt;Conclusions: Our research addresses the critical issue of deep learning bug
reproducibility. Practitioners and researchers can leverage our findings to
improve deep learning bug reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mehil B. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mohammad Masudur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1&quot;&gt;Foutse Khomh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03077">
<title>A Topology-aware Graph Coarsening Framework for Continual Graph Learning. (arXiv:2401.03077v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03077</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning on graphs tackles the problem of training a graph neural
network (GNN) where graph data arrive in a streaming fashion and the model
tends to forget knowledge from previous tasks when updating with new data.
Traditional continual learning strategies such as Experience Replay can be
adapted to streaming graphs, however, these methods often face challenges such
as inefficiency in preserving graph topology and incapability of capturing the
correlation between old and new tasks. To address these challenges, we propose
TA$\mathbb{CO}$, a (t)opology-(a)ware graph (co)arsening and (co)ntinual
learning framework that stores information from previous tasks as a reduced
graph. At each time period, this reduced graph expands by combining with a new
graph and aligning shared nodes, and then it undergoes a &quot;zoom out&quot; process by
reduction to maintain a stable size. We design a graph coarsening algorithm
based on node representation proximities to efficiently reduce a graph and
preserve topological information. We empirically demonstrate the learning
process on the reduced graph can approximate that of the original graph. Our
experiments validate the effectiveness of the proposed framework on three
real-world datasets using different backbone GNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoxue Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhuo Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1&quot;&gt;Yue Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03078">
<title>StreamVC: Real-Time Low-Latency Voice Conversion. (arXiv:2401.03078v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.03078</link>
<description rdf:parseType="Literal">&lt;p&gt;We present StreamVC, a streaming voice conversion solution that preserves the
content and prosody of any source speech while matching the voice timbre from
any target speech. Unlike previous approaches, StreamVC produces the resulting
waveform at low latency from the input signal even on a mobile platform, making
it applicable to real-time communication scenarios like calls and video
conferencing, and addressing use cases such as voice anonymization in these
scenarios. Our design leverages the architecture and training strategy of the
SoundStream neural audio codec for lightweight high-quality speech synthesis.
We demonstrate the feasibility of learning soft speech units causally, as well
as the effectiveness of supplying whitened fundamental frequency information to
improve pitch stability without leaking the source timbre information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kartynnik_Y/0/1/0/all/0/1&quot;&gt;Yury Kartynnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiuqiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sung_G/0/1/0/all/0/1&quot;&gt;George Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grundmann_M/0/1/0/all/0/1&quot;&gt;Matthias Grundmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03083">
<title>Energy-efficient Decentralized Learning via Graph Sparsification. (arXiv:2401.03083v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03083</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims at improving the energy efficiency of decentralized learning
by optimizing the mixing matrix, which controls the communication demands
during the learning process. Through rigorous analysis based on a
state-of-the-art decentralized learning algorithm, the problem is formulated as
a bi-level optimization, with the lower level solved by graph sparsification. A
solution with guaranteed performance is proposed for the special case of
fully-connected base topology and a greedy heuristic is proposed for the
general case. Simulations based on real topology and dataset show that the
proposed solution can lower the energy consumption at the busiest node by
54%-76% while maintaining the quality of the trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xusheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Cho-Chun Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Ting He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03085">
<title>Consensus-Threshold Criterion for Offline Signature Verification using Convolutional Neural Network Learned Representations. (arXiv:2401.03085v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03085</link>
<description rdf:parseType="Literal">&lt;p&gt;A genuine signer&apos;s signature is naturally unstable even at short
time-intervals whereas, expert forgers always try to perfectly mimic a genuine
signer&apos;s signature. This presents a challenge which puts a genuine signer at
risk of being denied access, while a forge signer is granted access. The
implication is a high false acceptance rate (FAR) which is the percentage of
forge signature classified as belonging to a genuine class. Existing work have
only scratched the surface of signature verification because the
misclassification error remains high. In this paper, a consensus-threshold
distance-based classifier criterion is proposed for offline writer-dependent
signature verification. Using features extracted from SigNet and SigNet-F deep
convolutional neural network models, the proposed classifier minimizes FAR.
This is demonstrated via experiments on four datasets: GPDS-300, MCYT, CEDAR
and Brazilian PUC-PR datasets. On GPDS-300, the consensus threshold classifier
improves the state-of-the-art performance by achieving a 1.27% FAR compared to
8.73% and 17.31% recorded in literature. This performance is consistent across
other datasets and guarantees that the risk of imposters gaining access to
sensitive documents or transactions is minimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brimoh_P/0/1/0/all/0/1&quot;&gt;Paul Brimoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olisah_C/0/1/0/all/0/1&quot;&gt;Chollette C. Olisah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03097">
<title>Adaptive Boosting with Fairness-aware Reweighting Technique for Fair Classification. (arXiv:2401.03097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03097</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning methods based on AdaBoost have been widely applied to
various classification problems across many mission-critical applications
including healthcare, law and finance. However, there is a growing concern
about the unfairness and discrimination of data-driven classification models,
which is inevitable for classical algorithms including AdaBoost. In order to
achieve fair classification, a novel fair AdaBoost (FAB) approach is proposed
that is an interpretable fairness-improving variant of AdaBoost. We mainly
investigate binary classification problems and focus on the fairness of three
different indicators (i.e., accuracy, false positive rate and false negative
rate). By utilizing a fairness-aware reweighting technique for base
classifiers, the proposed FAB approach can achieve fair classification while
maintaining the advantage of AdaBoost with negligible sacrifice of predictive
performance. In addition, a hyperparameter is introduced in FAB to show
preferences for the fairness-accuracy trade-off. An upper bound for the target
loss function that quantifies error rate and unfairness is theoretically
derived for FAB, which provides a strict theoretical support for the
fairness-improving methods designed for AdaBoost. The effectiveness of the
proposed method is demonstrated on three real-world datasets (i.e., Adult,
COMPAS and HSLS) with respect to the three fairness indicators. The results are
accordant with theoretic analyses, and show that (i) FAB significantly improves
classification fairness at a small cost of accuracy compared with AdaBoost; and
(ii) FAB outperforms state-of-the-art fair classification methods including
equalized odds method, exponentiated gradient method, and disparate
mistreatment method in terms of the fairness-accuracy trade-off.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaobin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Benben Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03104">
<title>When To Grow? A Fitting Risk-Aware Policy for Layer Growing in Deep Neural Networks. (arXiv:2401.03104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03104</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural growth is the process of growing a small neural network to a large
network and has been utilized to accelerate the training of deep neural
networks. One crucial aspect of neural growth is determining the optimal growth
timing. However, few studies investigate this systematically. Our study reveals
that neural growth inherently exhibits a regularization effect, whose intensity
is influenced by the chosen policy for growth timing. While this regularization
effect may mitigate the overfitting risk of the model, it may lead to a notable
accuracy drop when the model underfits. Yet, current approaches have not
addressed this issue due to their lack of consideration of the regularization
effect from neural growth. Motivated by these findings, we propose an
under/over fitting risk-aware growth timing policy, which automatically adjusts
the growth timing informed by the level of potential under/overfitting risks to
address both risks. Comprehensive experiments conducted using CIFAR-10/100 and
ImageNet datasets show that the proposed policy achieves accuracy improvements
of up to 1.3% in models prone to underfitting while achieving similar
accuracies in models suffering from overfitting compared to the existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haihang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malepathirana_T/0/1/0/all/0/1&quot;&gt;Tamasha Malepathirana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senanayake_D/0/1/0/all/0/1&quot;&gt;Damith Senanayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oetomo_D/0/1/0/all/0/1&quot;&gt;Denny Oetomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halgamuge_S/0/1/0/all/0/1&quot;&gt;Saman Halgamuge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03114">
<title>GLISP: A Scalable GNN Learning System by Exploiting Inherent Structural Properties of Graphs. (arXiv:2401.03114v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03114</link>
<description rdf:parseType="Literal">&lt;p&gt;As a powerful tool for modeling graph data, Graph Neural Networks (GNNs) have
received increasing attention in both academia and industry. Nevertheless, it
is notoriously difficult to deploy GNNs on industrial scale graphs, due to
their huge data size and complex topological structures. In this paper, we
propose GLISP, a sampling based GNN learning system for industrial scale
graphs. By exploiting the inherent structural properties of graphs, such as
power law distribution and data locality, GLISP addresses the scalability and
performance issues that arise at different stages of the graph learning
process. GLISP consists of three core components: graph partitioner, graph
sampling service and graph inference engine. The graph partitioner adopts the
proposed vertex-cut graph partitioning algorithm AdaDNE to produce balanced
partitioning for power law graphs, which is essential for sampling based GNN
systems. The graph sampling service employs a load balancing design that allows
the one hop sampling request of high degree vertices to be handled by multiple
servers. In conjunction with the memory efficient data structure, the
efficiency and scalability are effectively improved. The graph inference engine
splits the $K$-layer GNN into $K$ slices and caches the vertex embeddings
produced by each slice in the data locality aware hybrid caching system for
reuse, thus completely eliminating redundant computation caused by the data
dependency of graph. Extensive experiments show that GLISP achieves up to
$6.53\times$ and $70.77\times$ speedups over existing GNN systems for training
and inference tasks, respectively, and can scale to the graph with over 10
billion vertices and 40 billion edges with limited resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhongshu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Bin Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiaopei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhou_J/0/1/0/all/0/1&quot;&gt;Jun zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03116">
<title>Advancing DDoS Attack Detection: A Synergistic Approach Using Deep Residual Neural Networks and Synthetic Oversampling. (arXiv:2401.03116v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.03116</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed Denial of Service (DDoS) attacks pose a significant threat to the
stability and reliability of online systems. Effective and early detection of
such attacks is pivotal for safeguarding the integrity of networks. In this
work, we introduce an enhanced approach for DDoS attack detection by leveraging
the capabilities of Deep Residual Neural Networks (ResNets) coupled with
synthetic oversampling techniques. Because of the inherent class imbalance in
many cyber-security datasets, conventional methods often struggle with false
negatives, misclassifying subtle DDoS patterns as benign. By applying the
Synthetic Minority Over-sampling Technique (SMOTE) to the CICIDS dataset, we
balance the representation of benign and malicious data points, enabling the
model to better discern intricate patterns indicative of an attack. Our deep
residual network, tailored for this specific task, further refines the
detection process. Experimental results on a real-world dataset demonstrate
that our approach achieves an accuracy of 99.98%, significantly outperforming
traditional methods. This work underscores the potential of combining advanced
data augmentation techniques with deep learning models to bolster
cyber-security defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfatemi_A/0/1/0/all/0/1&quot;&gt;Ali Alfatemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahouti_M/0/1/0/all/0/1&quot;&gt;Mohamed Rahouti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_R/0/1/0/all/0/1&quot;&gt;Ruhul Amin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ALJamal_S/0/1/0/all/0/1&quot;&gt;Sarah ALJamal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_K/0/1/0/all/0/1&quot;&gt;Kaiqi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yufeng Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03123">
<title>A least distance estimator for a multivariate regression model using deep neural networks. (arXiv:2401.03123v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2401.03123</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a deep neural network (DNN) based least distance (LD) estimator
(DNN-LD) for a multivariate regression problem, addressing the limitations of
the conventional methods. Due to the flexibility of a DNN structure, both
linear and nonlinear conditional mean functions can be easily modeled, and a
multivariate regression model can be realized by simply adding extra nodes at
the output layer. The proposed method is more efficient in capturing the
dependency structure among responses than the least squares loss, and robust to
outliers. In addition, we consider $L_1$-type penalization for variable
selection, crucial in analyzing high-dimensional data. Namely, we propose what
we call (A)GDNN-LD estimator that enjoys variable selection and model
estimation simultaneously, by applying the (adaptive) group Lasso penalty to
weight parameters in the DNN structure. For the computation, we propose a
quadratic smoothing approximation method to facilitate optimizing the
non-smooth objective function based on the least distance loss. The simulation
studies and a real data analysis demonstrate the promising performance of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jungmin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seung Jun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bang_S/0/1/0/all/0/1&quot;&gt;Sungwan Bang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03131">
<title>A Physics-guided Generative AI Toolkit for Geophysical Monitoring. (arXiv:2401.03131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03131</link>
<description rdf:parseType="Literal">&lt;p&gt;Full-waveform inversion (FWI) plays a vital role in geoscience to explore the
subsurface. It utilizes the seismic wave to image the subsurface velocity map.
As the machine learning (ML) technique evolves, the data-driven approaches
using ML for FWI tasks have emerged, offering enhanced accuracy and reduced
computational cost compared to traditional physics-based methods. However, a
common challenge in geoscience, the unprivileged data, severely limits ML
effectiveness. The issue becomes even worse during model pruning, a step
essential in geoscience due to environmental complexities. To tackle this, we
introduce the EdGeo toolkit, which employs a diffusion-based model guided by
physics principles to generate high-fidelity velocity maps. The toolkit uses
the acoustic wave equation to generate corresponding seismic waveform data,
facilitating the fine-tuning of pruned ML models. Our results demonstrate
significant improvements in SSIM scores and reduction in both MAE and MSE
across various pruning ratios. Notably, the ML model fine-tuned using data
generated by EdGeo yields superior quality of velocity maps, especially in
representing unprivileged features, outperforming other existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Junhuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Yi Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youzuo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03132">
<title>Vision Transformers and Bi-LSTM for Alzheimer&apos;s Disease Diagnosis from 3D MRI. (arXiv:2401.03132v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03132</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s is a brain disease that gets worse over time and affects memory,
thinking, and behavior. Alzheimer&apos;s disease (AD) can be treated and managed if
it is diagnosed early, which can slow the progression of symptoms and improve
quality of life. In this study, we suggested using the Visual Transformer (ViT)
and bi-LSTM to process MRI images for diagnosing Alzheimer&apos;s disease. We used
ViT to extract features from the MRI and then map them to a feature sequence.
Then, we used Bi-LSTM sequence modeling to keep the interdependencies between
related features. In addition, we evaluated the performance of the proposed
model for the binary classification of AD patients using data from the
Alzheimer&apos;s Disease Neuroimaging Initiative (ADNI). Finally, we evaluated our
method against other deep learning models in the literature. The proposed
method performs well in terms of accuracy, precision, F-score, and recall for
the diagnosis of AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akan_T/0/1/0/all/0/1&quot;&gt;Taymaz Akan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alp_S/0/1/0/all/0/1&quot;&gt;Sait Alp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhuiyanb_M/0/1/0/all/0/1&quot;&gt;Mohammad A. N Bhuiyanb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03134">
<title>TimeGraphs: Graph-based Temporal Reasoning. (arXiv:2401.03134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03134</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world systems exhibit temporal, dynamic behaviors, which are
captured as time series of complex agent interactions. To perform temporal
reasoning, current methods primarily encode temporal dynamics through simple
sequence-based models. However, in general these models fail to efficiently
capture the full spectrum of rich dynamics in the input, since the dynamics is
not uniformly distributed. In particular, relevant information might be harder
to extract and computing power is wasted for processing all individual
timesteps, even if they contain no significant changes or no new information.
Here we propose TimeGraphs, a novel approach that characterizes dynamic
interactions as a hierarchical temporal graph, diverging from traditional
sequential representations. Our approach models the interactions using a
compact graph-based representation, enabling adaptive reasoning across diverse
time scales. Adopting a self-supervised method, TimeGraphs constructs a
multi-level event hierarchy from a temporal input, which is then used to
efficiently reason about the unevenly distributed dynamics. This construction
process is scalable and incremental to accommodate streaming data. We evaluate
TimeGraphs on multiple datasets with complex, dynamic agent interactions,
including a football simulator, the Resistance game, and the MOMA human
activity dataset. The results demonstrate both robustness and efficiency of
TimeGraphs on a range of temporal reasoning tasks. Our approach obtains
state-of-the-art performance and leads to a performance increase of up to 12.2%
on event prediction and recognition tasks over current approaches. Our
experiments further demonstrate a wide array of capabilities including
zero-shot generalization, robustness in case of data sparsity, and adaptability
to streaming data flow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1&quot;&gt;Paridhi Maheshwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongyu Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sosic_R/0/1/0/all/0/1&quot;&gt;Rok Sosic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03137">
<title>SPQR: Controlling Q-ensemble Independence with Spiked Random Model for Reinforcement Learning. (arXiv:2401.03137v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03137</link>
<description rdf:parseType="Literal">&lt;p&gt;Alleviating overestimation bias is a critical challenge for deep
reinforcement learning to achieve successful performance on more complex tasks
or offline datasets containing out-of-distribution data. In order to overcome
overestimation bias, ensemble methods for Q-learning have been investigated to
exploit the diversity of multiple Q-functions. Since network initialization has
been the predominant approach to promote diversity in Q-functions,
heuristically designed diversity injection methods have been studied in the
literature. However, previous studies have not attempted to approach guaranteed
independence over an ensemble from a theoretical perspective. By introducing a
novel regularization loss for Q-ensemble independence based on random matrix
theory, we propose spiked Wishart Q-ensemble independence regularization (SPQR)
for reinforcement learning. Specifically, we modify the intractable hypothesis
testing criterion for the Q-ensemble independence into a tractable KL
divergence between the spectral distribution of the Q-ensemble and the target
Wigner&apos;s semicircle distribution. We implement SPQR in several online and
offline ensemble Q-learning algorithms. In the experiments, SPQR outperforms
the baseline algorithms in both online and offline RL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dohyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Seungyub Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_T/0/1/0/all/0/1&quot;&gt;Taehyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwoo Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03138">
<title>TelTrans: Applying Multi-Type Telecom Data to Transportation Evaluation and Prediction via Multifaceted Graph Modeling. (arXiv:2401.03138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03138</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the limitations of traffic prediction from location-bound
detectors, we present Geographical Cellular Traffic (GCT) flow, a novel data
source that leverages the extensive coverage of cellular traffic to capture
mobility patterns. Our extensive analysis validates its potential for
transportation. Focusing on vehicle-related GCT flow prediction, we propose a
graph neural network that integrates multivariate, temporal, and spatial facets
for improved accuracy. Experiments reveal our model&apos;s superiority over
baselines, especially in long-term predictions. We also highlight the potential
for GCT flow integration into transportation systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;ChungYi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_S/0/1/0/all/0/1&quot;&gt;Shen-Lung Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hung-Ting Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;Winston H. Hsu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03140">
<title>Fair Sampling in Diffusion Models through Switching Mechanism. (arXiv:2401.03140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03140</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown their effectiveness in generation tasks by
well-approximating the underlying probability distribution. However, diffusion
models are known to suffer from an amplified inherent bias from the training
data in terms of fairness. While the sampling process of diffusion models can
be controlled by conditional guidance, previous works have attempted to find
empirical guidance to achieve quantitative fairness. To address this
limitation, we propose a fairness-aware sampling method called
\textit{attribute switching} mechanism for diffusion models. Without additional
training, the proposed sampling can obfuscate sensitive attributes in generated
data without relying on classifiers. We mathematically prove and experimentally
demonstrate the effectiveness of the proposed method on two key aspects: (i)
the generation of fair data and (ii) the preservation of the utility of the
generated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yujin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinseong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hoki Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Saeroom Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03151">
<title>Semi-supervised learning via DQN for log anomaly detection. (arXiv:2401.03151v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.03151</link>
<description rdf:parseType="Literal">&lt;p&gt;Log anomaly detection plays a critical role in ensuring the security and
maintenance of modern software systems. At present, the primary approach for
detecting anomalies in log data is through supervised anomaly detection.
Nonetheless, existing supervised methods heavily rely on labeled data, which
can be frequently limited in real-world scenarios. In this paper, we propose a
semi-supervised log anomaly detection method that combines the DQN algorithm
from deep reinforcement learning, which is called DQNLog. DQNLog leverages a
small amount of labeled data and a large-scale unlabeled dataset, effectively
addressing the challenges of imbalanced data and limited labeling. This
approach not only learns known anomalies by interacting with an environment
biased towards anomalies but also discovers unknown anomalies by actively
exploring the unlabeled dataset. Additionally, DQNLog incorporates a
cross-entropy loss term to prevent model overestimation during Deep
Reinforcement Learning (DRL). Our evaluation on three widely-used datasets
demonstrates that DQNLog significantly improves recall rate and F1-score while
maintaining precision, validating its practicality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yingying He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1&quot;&gt;Xiaobing Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lihong Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03152">
<title>Controllable Image Synthesis of Industrial Data Using Stable Diffusion. (arXiv:2401.03152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03152</link>
<description rdf:parseType="Literal">&lt;p&gt;Training supervised deep neural networks that perform defect detection and
segmentation requires large-scale fully-annotated datasets, which can be hard
or even impossible to obtain in industrial environments. Generative AI offers
opportunities to enlarge small industrial datasets artificially, thus enabling
the usage of state-of-the-art supervised approaches in the industry.
Unfortunately, also good generative models need a lot of data to train, while
industrial datasets are often tiny. Here, we propose a new approach for reusing
general-purpose pre-trained generative models on industrial data, ultimately
allowing the generation of self-labelled defective images. First, we let the
model learn the new concept, entailing the novel data distribution. Then, we
force it to learn to condition the generative process, producing industrial
images that satisfy well-defined topological characteristics and show defects
with a given geometry and location. To highlight the advantage of our approach,
we use the synthetic dataset to optimise a crack segmentor for a real
industrial use case. When the available data is small, we observe considerable
performance increase under several metrics, showing the method&apos;s potential in
production environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1&quot;&gt;Gabriele Valvano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostino_A/0/1/0/all/0/1&quot;&gt;Antonino Agostino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magistris_G/0/1/0/all/0/1&quot;&gt;Giovanni De Magistris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziano_A/0/1/0/all/0/1&quot;&gt;Antonino Graziano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veneri_G/0/1/0/all/0/1&quot;&gt;Giacomo Veneri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03154">
<title>Decentralized Multi-Agent Active Search and Tracking when Targets Outnumber Agents. (arXiv:2401.03154v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03154</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent multi-target tracking has a wide range of applications, including
wildlife patrolling, security surveillance or environment monitoring. Such
algorithms often make restrictive assumptions: the number of targets and/or
their initial locations may be assumed known, or agents may be pre-assigned to
monitor disjoint partitions of the environment, reducing the burden of
exploration. This also limits applicability when there are fewer agents than
targets, since agents are unable to continuously follow the targets in their
fields of view. Multi-agent tracking algorithms additionally assume inter-agent
synchronization of observations, or the presence of a central controller to
coordinate joint actions. Instead, we focus on the setting of decentralized
multi-agent, multi-target, simultaneous active search-and-tracking with
asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a
sequential monte carlo implementation of the probability hypothesis density
filter for posterior inference combined with Thompson sampling for
decentralized multi-agent decision making. We compare different action
selection policies, focusing on scenarios where targets outnumber agents. In
simulation, we demonstrate that DecSTER is robust to unreliable inter-agent
communication and outperforms information-greedy baselines in terms of the
Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets
and varying teamsizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Arundhati Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jeff Schneider&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03156">
<title>Data-Dependent Stability Analysis of Adversarial Training. (arXiv:2401.03156v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03156</link>
<description rdf:parseType="Literal">&lt;p&gt;Stability analysis is an essential aspect of studying the generalization
ability of deep learning, as it involves deriving generalization bounds for
stochastic gradient descent-based training algorithms. Adversarial training is
the most widely used defense against adversarial example attacks. However,
previous generalization bounds for adversarial training have not included
information regarding the data distribution. In this paper, we fill this gap by
providing generalization bounds for stochastic gradient descent-based
adversarial training that incorporate data distribution information. We utilize
the concepts of on-average stability and high-order approximate Lipschitz
conditions to examine how changes in data distribution and adversarial budget
can affect robust generalization gaps. Our derived generalization bounds for
both convex and non-convex losses are at least as good as the uniform
stability-based counterparts which do not include data distribution
information. Furthermore, our findings demonstrate how distribution shifts from
data poisoning attacks can impact robust generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiao-Shan Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03159">
<title>Distributed client selection with multi-objective in federated learning assisted Internet of Vehicles. (arXiv:2401.03159v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03159</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is an emerging distributed machine learning framework in
the Internet of Vehicles (IoV). In IoV, millions of vehicles are willing to
train the model to share their knowledge. Maintaining an active state means the
participants must update their state to the FL server in a fixed interval and
participate to next round. However, the cost by maintaining an active state is
very large when there are a huge number of participating vehicles. In this
paper, we proposed a distributed client selection scheme to reduce the cost of
maintaining the active state for all participants. The clients with the highest
evaluation are elected among the neighbours. In the evaluator, four variables
are considered including sample quantity, throughput available, computational
capability and the quality of the local dataset. We adopted fuzzy logic as the
evaluator since the closed-form solution over four variables does not exist.
Extensive simulation results show our proposal approximates the centralized
client selection in terms of accuracy and can significantly reduce the
communication overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cha_N/0/1/0/all/0/1&quot;&gt;Narisu Cha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1&quot;&gt;Long Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03160">
<title>Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning for Safe and Efficient Autonomous Driving. (arXiv:2401.03160v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03160</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant progress in autonomous vehicles (AVs), the development of
driving policies that ensure both the safety of AVs and traffic flow efficiency
has not yet been fully explored. In this paper, we propose an enhanced
human-in-the-loop reinforcement learning method, termed the Human as AI
mentor-based deep reinforcement learning (HAIM-DRL) framework, which
facilitates safe and efficient autonomous driving in mixed traffic platoon.
Drawing inspiration from the human learning process, we first introduce an
innovative learning paradigm that effectively injects human intelligence into
AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves
as a mentor to the AI agent. While allowing the agent to sufficiently explore
uncertain environments, the human expert can take control in dangerous
situations and demonstrate correct actions to avoid potential accidents. On the
other hand, the agent could be guided to minimize traffic flow disturbance,
thereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data
collected from free exploration and partial human demonstrations as its two
training sources. Remarkably, we circumvent the intricate process of manually
designing reward functions; instead, we directly derive proxy state-action
values from partial human demonstrations to guide the agents&apos; policy learning.
Additionally, we employ a minimal intervention technique to reduce the human
mentor&apos;s cognitive load. Comparative results show that HAIM-DRL outperforms
traditional methods in driving safety, sampling efficiency, mitigation of
traffic flow disturbance, and generalizability to unseen traffic scenarios. The
code and demo videos for this paper can be accessed at:
https://zilin-huang.github.io/HAIM-DRL-website/}{https://zilin-huang.github.io/HAIM-DRL-website/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zilin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zihao Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chengyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sikai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03162">
<title>QoS-Aware Graph Contrastive Learning for Web Service Recommendation. (arXiv:2401.03162v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.03162</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid growth of cloud services driven by advancements in web service
technology, selecting a high-quality service from a wide range of options has
become a complex task. This study aims to address the challenges of data
sparsity and the cold-start problem in web service recommendation using Quality
of Service (QoS). We propose a novel approach called QoS-aware graph
contrastive learning (QAGCL) for web service recommendation. Our model
harnesses the power of graph contrastive learning to handle cold-start problems
and improve recommendation accuracy effectively. By constructing contextually
augmented graphs with geolocation information and randomness, our model
provides diverse views. Through the use of graph convolutional networks and
graph contrastive learning techniques, we learn user and service embeddings
from these augmented graphs. The learned embeddings are then utilized to
seamlessly integrate QoS considerations into the recommendation process.
Experimental results demonstrate the superiority of our QAGCL model over
several existing models, highlighting its effectiveness in addressing data
sparsity and the cold-start problem in QoS-aware service recommendations. Our
research contributes to the potential for more accurate recommendations in
real-world scenarios, even with limited user-service interaction data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1&quot;&gt;Duksan Ryu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03163">
<title>An Empirical Investigation of Value-Based Multi-objective Reinforcement Learning for Stochastic Environments. (arXiv:2401.03163v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03163</link>
<description rdf:parseType="Literal">&lt;p&gt;One common approach to solve multi-objective reinforcement learning (MORL)
problems is to extend conventional Q-learning by using vector Q-values in
combination with a utility function. However issues can arise with this
approach in the context of stochastic environments, particularly when
optimising for the Scalarised Expected Reward (SER) criterion. This paper
extends prior research, providing a detailed examination of the factors
influencing the frequency with which value-based MORL Q-learning algorithms
learn the SER-optimal policy for an environment with stochastic state
transitions. We empirically examine several variations of the core
multi-objective Q-learning algorithm as well as reward engineering approaches,
and demonstrate the limitations of these methods. In particular, we highlight
the critical impact of the noisy Q-value estimates issue on the stability and
convergence of these algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Kewen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vamplew_P/0/1/0/all/0/1&quot;&gt;Peter Vamplew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foale_C/0/1/0/all/0/1&quot;&gt;Cameron Foale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dazeley_R/0/1/0/all/0/1&quot;&gt;Richard Dazeley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03170">
<title>Preserving Silent Features for Domain Generalization. (arXiv:2401.03170v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03170</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) aims to improve the generalization ability of the
model trained on several known training domains over unseen test domains.
Previous work has shown that self-supervised contrastive pre-training improves
the robustness of the model on downstream tasks. However, in this paper, we
find that self-supervised models do not exhibit better generalization
performance than supervised models pre-trained on the same dataset in the DG
setting. We argue that this is owing to the fact that the richer intra-class
discriminative features extracted by self-supervised contrastive learning,
which we term silent features, are suppressed during supervised fine-tuning.
These silent features are likely to contain features that are more
generalizable on the test domain. In this work, we model and analyze this
feature suppression phenomenon and theoretically prove that preserving silent
features can achieve lower expected test domain risk under certain conditions.
In light of this, we propose a simple yet effective method termed STEP (Silent
Feature Preservation) to improve the generalization performance of the
self-supervised contrastive learning pre-trained model by alleviating the
suppression of silent features during the supervised fine-tuning process.
Experimental results show that STEP exhibits state-of-the-art performance on
standard DG benchmarks with significant distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chujie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03171">
<title>Exploration of Adolescent Depression Risk Prediction Based on Census Surveys and General Life Issues. (arXiv:2401.03171v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03171</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary society, the escalating pressures of life and work have
propelled psychological disorders to the forefront of modern health concerns,
an issue that has been further accentuated by the COVID-19 pandemic. The
prevalence of depression among adolescents is steadily increasing, and
traditional diagnostic methods, which rely on scales or interviews, prove
particularly inadequate for detecting depression in young people. Addressing
these challenges, numerous AI-based methods for assisting in the diagnosis of
mental health issues have emerged. However, most of these methods center around
fundamental issues with scales or use multimodal approaches like facial
expression recognition. Diagnosis of depression risk based on everyday habits
and behaviors has been limited to small-scale qualitative studies. Our research
leverages adolescent census data to predict depression risk, focusing on
children&apos;s experiences with depression and their daily life situations. We
introduced a method for managing severely imbalanced high-dimensional data and
an adaptive predictive approach tailored to data structure characteristics.
Furthermore, we proposed a cloud-based architecture for automatic online
learning and data updates. This study utilized publicly available NSCH youth
census data from 2020 to 2022, encompassing nearly 150,000 data entries. We
conducted basic data analyses and predictive experiments, demonstrating
significant performance improvements over standard machine learning and deep
learning algorithms. This affirmed our data processing method&apos;s broad
applicability in handling imbalanced medical data. Diverging from typical
predictive method research, our study presents a comprehensive architectural
solution, considering a wider array of user needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yufeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hefeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03173">
<title>UGGNet: Bridging U-Net and VGG for Advanced Breast Cancer Diagnosis. (arXiv:2401.03173v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03173</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, breast ultrasound has emerged as a crucial
diagnostic tool for early detection of breast cancer. However, the accuracy of
diagnosing the location of the affected area and the extent of the disease
depends on the experience of the physician. In this paper, we propose a novel
model called UGGNet, combining the power of the U-Net and VGG architectures to
enhance the performance of breast ultrasound image analysis. The U-Net
component of the model helps accurately segment the lesions, while the VGG
component utilizes deep convolutional layers to extract features. The fusion of
these two architectures in UGGNet aims to optimize both segmentation and
feature representation, providing a comprehensive solution for accurate
diagnosis in breast ultrasound images. Experimental results have demonstrated
that the UGGNet model achieves a notable accuracy of 78.2% on the &quot;Breast
Ultrasound Images Dataset.&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Minh_T/0/1/0/all/0/1&quot;&gt;Tran Cao Minh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Quoc_N/0/1/0/all/0/1&quot;&gt;Nguyen Kim Quoc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vinh_P/0/1/0/all/0/1&quot;&gt;Phan Cong Vinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Phu_D/0/1/0/all/0/1&quot;&gt;Dang Nhu Phu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_V/0/1/0/all/0/1&quot;&gt;Vuong Xuan Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Ha Minh Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03175">
<title>Part-of-Speech Tagger for Bodo Language using Deep Learning approach. (arXiv:2401.03175v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03175</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Processing systems such as Part-of-speech tagging, Named entity
recognition, Machine translation, Speech recognition, and Language modeling
(LM) are well-studied in high-resource languages. Nevertheless, research on
these systems for several low-resource languages, including Bodo, Mizo,
Nagamese, and others, is either yet to commence or is in its nascent stages.
Language model plays a vital role in the downstream tasks of modern NLP.
Extensive studies are carried out on LMs for high-resource languages.
Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack
coverage. In this study, we first present BodoBERT, a language model for the
Bodo language. To the best of our knowledge, this work is the first such effort
to develop a language model for Bodo. Secondly, we present an ensemble DL-based
POS tagging model for Bodo. The POS tagging model is based on combinations of
BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We
cover several language models in the experiment to see how well they work in
POS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A
comparative experiment was also conducted on Assamese POS taggers, considering
that the language is spoken in the same region as Bodo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Dhrubajyoti Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narzary_S/0/1/0/all/0/1&quot;&gt;Sanjib Narzary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandi_S/0/1/0/all/0/1&quot;&gt;Sukumar Nandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Som_B/0/1/0/all/0/1&quot;&gt;Bidisha Som&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03192">
<title>On the Convergence of Hermitian Dynamic Mode Decomposition. (arXiv:2401.03192v1 [math.NA])</title>
<link>http://arxiv.org/abs/2401.03192</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the convergence of Hermitian Dynamic Mode
Decomposition (DMD) to the spectral properties of self-adjoint Koopman
operators. Hermitian DMD is a data-driven method for approximating the Koopman
operator associated with an unknown nonlinear dynamical system from
discrete-time snapshots, while preserving the self-adjointness of the operator
on its finite-dimensional approximations. We show that, under suitable
conditions, the eigenvalues and eigenfunctions of HDMD converge to the spectral
properties of the underlying Koopman operator. Along the way, we establish a
general theorem on the convergence of spectral measures, and demonstrate our
results numerically on the two-dimensional Schr\&quot;odinger equation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Boulle_N/0/1/0/all/0/1&quot;&gt;Nicolas Boull&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Colbrook_M/0/1/0/all/0/1&quot;&gt;Matthew J. Colbrook&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03194">
<title>Learning Persistent Community Structures in Dynamic Networks via Topological Data Analysis. (arXiv:2401.03194v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03194</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic community detection methods often lack effective mechanisms to ensure
temporal consistency, hindering the analysis of network evolution. In this
paper, we propose a novel deep graph clustering framework with temporal
consistency regularization on inter-community structures, inspired by the
concept of minimal network topological changes within short intervals.
Specifically, to address the representation collapse problem, we first
introduce MFC, a matrix factorization-based deep graph clustering algorithm
that preserves node embedding. Based on static clustering results, we construct
probabilistic community networks and compute their persistence homology, a
robust topological measure, to assess structural similarity between them.
Moreover, a novel neural network regularization TopoReg is introduced to ensure
the preservation of topological similarity between inter-community structures
over time intervals. Our approach enhances temporal consistency and clustering
accuracy on real-world datasets with both fixed and varying numbers of
communities. It is also a pioneer application of TDA in temporally persistent
community detection, offering an insightful contribution to field of network
analysis. Code and data are available at the public git repository:
https://github.com/kundtx/MFC_TopoReg
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_D/0/1/0/all/0/1&quot;&gt;Dexu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Anping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03195">
<title>Efficient Bitrate Ladder Construction using Transfer Learning and Spatio-Temporal Features. (arXiv:2401.03195v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.03195</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing high-quality video with efficient bitrate is a main challenge in
video industry. The traditional one-size-fits-all scheme for bitrate ladders is
inefficient and reaching the best content-aware decision computationally
impractical due to extensive encodings required. To mitigate this, we propose a
bitrate and complexity efficient bitrate ladder prediction method using
transfer learning and spatio-temporal features. We propose: (1) using feature
maps from well-known pre-trained DNNs to predict rate-quality behavior with
limited training data; and (2) improving highest quality rung efficiency by
predicting minimum bitrate for top quality and using it for the top rung. The
method tested on 102 video scenes demonstrates 94.1% reduction in complexity
versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning
was thoroughly studied through four networks and ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falahati_A/0/1/0/all/0/1&quot;&gt;Ali Falahati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safavi_M/0/1/0/all/0/1&quot;&gt;Mohammad Karim Safavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_A/0/1/0/all/0/1&quot;&gt;Ardavan Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakdaman_F/0/1/0/all/0/1&quot;&gt;Farhad Pakdaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03197">
<title>Decision Making in Non-Stationary Environments with Policy-Augmented Search. (arXiv:2401.03197v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.03197</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential decision-making under uncertainty is present in many important
problems. Two popular approaches for tackling such problems are reinforcement
learning and online search (e.g., Monte Carlo tree search). While the former
learns a policy by interacting with the environment (typically done before
execution), the latter uses a generative model of the environment to sample
promising action trajectories at decision time. Decision-making is particularly
challenging in non-stationary environments, where the environment in which an
agent operates can change over time. Both approaches have shortcomings in such
settings -- on the one hand, policies learned before execution become stale
when the environment changes and relearning takes both time and computational
effort. Online search, on the other hand, can return sub-optimal actions when
there are limitations on allowed runtime. In this paper, we introduce
\textit{Policy-Augmented Monte Carlo tree search} (PA-MCTS), which combines
action-value estimates from an out-of-date policy with an online search using
an up-to-date model of the environment. We prove theoretical results showing
conditions under which PA-MCTS selects the one-step optimal action and also
bound the error accrued while following PA-MCTS as a policy. We compare and
contrast our approach with AlphaZero, another hybrid planning approach, and
Deep Q Learning on several OpenAI Gym environments. Through extensive
experiments, we show that under non-stationary settings with limited time
constraints, PA-MCTS outperforms these baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pettet_A/0/1/0/all/0/1&quot;&gt;Ava Pettet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Baiting Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wray_K/0/1/0/all/0/1&quot;&gt;Kyle Wray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baier_H/0/1/0/all/0/1&quot;&gt;Hendrik Baier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszka_A/0/1/0/all/0/1&quot;&gt;Aron Laszka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhishek Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukhopadhyay_A/0/1/0/all/0/1&quot;&gt;Ayan Mukhopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03198">
<title>Learning-Augmented K-Means Clustering Using Dimensional Reduction. (arXiv:2401.03198v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03198</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning augmented is a machine learning concept built to improve the
performance of a method or model, such as enhancing its ability to predict and
generalize data or features, or testing the reliability of the method by
introducing noise and other factors. On the other hand, clustering is a
fundamental aspect of data analysis and has long been used to understand the
structure of large datasets. Despite its long history, the k-means algorithm
still faces challenges. One approach, as suggested by Ergun et al,is to use a
predictor to minimize the sum of squared distances between each data point and
a specified centroid. However, it is known that the computational cost of this
algorithm increases with the value of k, and it often gets stuck in local
minima. In response to these challenges, we propose a solution to reduce the
dimensionality of the dataset using Principal Component Analysis (PCA). It is
worth noting that when using k values of 10 and 25, the proposed algorithm
yields lower cost results compared to running it without PCA. &quot;Principal
component analysis (PCA) is the problem of fitting a low-dimensional affine
subspace to a set of data points in a high-dimensional space. PCA is
well-established in the literature and has become one of the most useful tools
for data modeling, compression, and visualization.&quot;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabari_I/0/1/0/all/0/1&quot;&gt;Issam K.O Jabari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shofiyah/0/1/0/all/0/1&quot;&gt;Shofiyah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_P/0/1/0/all/0/1&quot;&gt;Pradiptya Kahvi S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putriwijaya_N/0/1/0/all/0/1&quot;&gt;Novi Nur Putriwijaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03206">
<title>A Robbins--Monro Sequence That Can Exploit Prior Information For Faster Convergence. (arXiv:2401.03206v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03206</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new method to improve the convergence speed of the Robbins-Monro
algorithm by introducing prior information about the target point into the
Robbins-Monro iteration. We achieve the incorporation of prior information
without the need of a -- potentially wrong -- regression model, which would
also entail additional constraints. We show that this prior-information
Robbins-Monro sequence is convergent for a wide range of prior distributions,
even wrong ones, such as Gaussian, weighted sum of Gaussians, e.g., in a kernel
density estimate, as well as bounded arbitrary distribution functions greater
than zero. We furthermore analyse the sequence numerically to understand its
performance and the influence of parameters. The results demonstrate that the
prior-information Robbins-Monro sequence converges faster than the standard
one, especially during the first steps, which are particularly important for
applications where the number of function measurements is limited, and when the
noise of observing the underlying function is large. We finally propose a rule
to select the parameters of the sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Ke Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goetz_S/0/1/0/all/0/1&quot;&gt;Stephan M. Goetz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03214">
<title>Understanding Representation Learnability of Nonlinear Self-Supervised Learning. (arXiv:2401.03214v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03214</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has empirically shown its data representation
learnability in many downstream tasks. There are only a few theoretical works
on data representation learnability, and many of those focus on final data
representation, treating the nonlinear neural network as a ``black box&quot;.
However, the accurate learning results of neural networks are crucial for
describing the data distribution features learned by SSL models. Our paper is
the first to analyze the learning results of the nonlinear SSL model
accurately. We consider a toy data distribution that contains two features: the
label-related feature and the hidden feature. Unlike previous linear setting
work that depends on closed-form solutions, we use the gradient descent
algorithm to train a 1-layer nonlinear SSL model with a certain initialization
region and prove that the model converges to a local minimum. Furthermore,
different from the complex iterative analysis, we propose a new analysis
process which uses the exact version of Inverse Function Theorem to accurately
describe the features learned by the local minimum. With this local minimum, we
prove that the nonlinear SSL model can capture the label-related feature and
hidden feature at the same time. In contrast, the nonlinear supervised learning
(SL) model can only learn the label-related feature. We also present the
learning processes and results of the nonlinear SSL and SL model via simulation
experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03215">
<title>End-to-End Anti-Backdoor Learning on Images and Time Series. (arXiv:2401.03215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03215</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attacks present a substantial security concern for deep learning
models, especially those utilized in applications critical to safety and
security. These attacks manipulate model behavior by embedding a hidden trigger
during the training phase, allowing unauthorized control over the model&apos;s
output during inference time. Although numerous defenses exist for image
classification models, there is a conspicuous absence of defenses tailored for
time series data, as well as an end-to-end solution capable of training clean
models on poisoned data. To address this gap, this paper builds upon
Anti-Backdoor Learning (ABL) and introduces an innovative method, End-to-End
Anti-Backdoor Learning (E2ABL), for robust training against backdoor attacks.
Unlike the original ABL, which employs a two-stage training procedure, E2ABL
accomplishes end-to-end training through an additional classification head
linked to the shallow layers of a Deep Neural Network (DNN). This secondary
head actively identifies potential backdoor triggers, allowing the model to
dynamically cleanse these samples and their corresponding labels during
training. Our experiments reveal that E2ABL significantly improves on existing
defenses and is effective against a broad range of backdoor attacks in both
image and time series domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yujing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Monazam Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yige Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03228">
<title>Reflected Schr\&quot;odinger Bridge for Constrained Generative Modeling. (arXiv:2401.03228v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.03228</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have become the go-to method for large-scale generative
models in real-world applications. These applications often involve data
distributions confined within bounded domains, typically requiring ad-hoc
thresholding techniques for boundary enforcement. Reflected diffusion models
(Lou23) aim to enhance generalizability by generating the data distribution
through a backward process governed by reflected Brownian motion. However,
reflected diffusion models may not easily adapt to diverse domains without the
derivation of proper diffeomorphic mappings and do not guarantee optimal
transport properties. To overcome these limitations, we introduce the Reflected
Schrodinger Bridge algorithm: an entropy-regularized optimal transport approach
tailored for generating data within diverse bounded domains. We derive elegant
reflected forward-backward stochastic differential equations with Neumann and
Robin boundary conditions, extend divergence-based likelihood training to
bounded domains, and explore natural connections to entropic optimal transport
for the study of approximate linear convergence - a valuable insight for
practical training. Our algorithm yields robust generative modeling in diverse
domains, and its scalability is demonstrated in real-world constrained
generative modeling through standard image benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nicole Tianjiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hengrong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ricky T. Q. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03230">
<title>FedTGP: Trainable Global Prototypes with Adaptive-Margin-Enhanced Contrastive Learning for Data and Model Heterogeneity in Federated Learning. (arXiv:2401.03230v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03230</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Heterogeneous Federated Learning (HtFL) has attracted attention due
to its ability to support heterogeneous models and data. To reduce the high
communication cost of transmitting model parameters, a major challenge in HtFL,
prototype-based HtFL methods are proposed to solely share class
representatives, a.k.a, prototypes, among heterogeneous clients while
maintaining the privacy of clients&apos; models. However, these prototypes are
naively aggregated into global prototypes on the server using weighted
averaging, resulting in suboptimal global knowledge which negatively impacts
the performance of clients. To overcome this challenge, we introduce a novel
HtFL approach called FedTGP, which leverages our Adaptive-margin-enhanced
Contrastive Learning (ACL) to learn Trainable Global Prototypes (TGP) on the
server. By incorporating ACL, our approach enhances prototype separability
while preserving semantic meaning. Extensive experiments with twelve
heterogeneous models demonstrate that our FedTGP surpasses state-of-the-art
methods by up to 9.08% in accuracy while maintaining the communication and
privacy advantages of prototype-based HtFL. Our code is available at
https://github.com/TsingZ0/FedTGP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jian Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03233">
<title>Convergence Rate Maximization for Split Learning-based Control of EMG Prosthetic Devices. (arXiv:2401.03233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03233</link>
<description rdf:parseType="Literal">&lt;p&gt;Split Learning (SL) is a promising Distributed Learning approach in
electromyography (EMG) based prosthetic control, due to its applicability
within resource-constrained environments. Other learning approaches, such as
Deep Learning and Federated Learning (FL), provide suboptimal solutions, since
prosthetic devices are extremely limited in terms of processing power and
battery life. The viability of implementing SL in such scenarios is caused by
its inherent model partitioning, with clients executing the smaller model
segment. However, selecting an inadequate cut layer hinders the training
process in SL systems. This paper presents an algorithm for optimal cut layer
selection in terms of maximizing the convergence rate of the model. The
performance evaluation demonstrates that the proposed algorithm substantially
accelerates the convergence in an EMG pattern recognition task for improving
prosthetic device control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinova_M/0/1/0/all/0/1&quot;&gt;Matea Marinova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denkovski_D/0/1/0/all/0/1&quot;&gt;Daniel Denkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gjoreski_H/0/1/0/all/0/1&quot;&gt;Hristijan Gjoreski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadzi_Velkov_Z/0/1/0/all/0/1&quot;&gt;Zoran Hadzi-Velkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakovic_V/0/1/0/all/0/1&quot;&gt;Valentin Rakovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03240">
<title>Interpreting Adaptive Gradient Methods by Parameter Scaling for Learning-Rate-Free Optimization. (arXiv:2401.03240v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03240</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the challenge of estimating the learning rate for adaptive
gradient methods used in training deep neural networks. While several
learning-rate-free approaches have been proposed, they are typically tailored
for steepest descent. However, although steepest descent methods offer an
intuitive approach to finding minima, many deep learning applications require
adaptive gradient methods to achieve faster convergence. In this paper, we
interpret adaptive gradient methods as steepest descent applied on
parameter-scaled networks, proposing learning-rate-free adaptive gradient
methods. Experimental results verify the effectiveness of this approach,
demonstrating comparable performance to hand-tuned learning rates across
various scenarios. This work extends the applicability of learning-rate-free
methods, enhancing training with adaptive gradient methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_M/0/1/0/all/0/1&quot;&gt;Min-Kook Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seung-Woo Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03246">
<title>SeqNAS: Neural Architecture Search for Event Sequence Classification. (arXiv:2401.03246v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03246</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) methods are widely used in various
industries to obtain high quality taskspecific solutions with minimal human
intervention. Event Sequences find widespread use in various industrial
applications including churn prediction customer segmentation fraud detection
and fault diagnosis among others. Such data consist of categorical and
real-valued components with irregular timestamps. Despite the usefulness of NAS
methods previous approaches only have been applied to other domains images
texts or time series. Our work addresses this limitation by introducing a novel
NAS algorithm SeqNAS specifically designed for event sequence classification.
We develop a simple yet expressive search space that leverages commonly used
building blocks for event sequence classification including multihead self
attention convolutions and recurrent cells. To perform the search we adopt
sequential Bayesian Optimization and utilize previously trained models as an
ensemble of teachers to augment knowledge distillation. As a result of our work
we demonstrate that our method surpasses state of the art NAS methods and
popular architectures suitable for sequence classification and holds great
potential for various industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1&quot;&gt;Igor Udovichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shvetsov_E/0/1/0/all/0/1&quot;&gt;Egor Shvetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divitsky_D/0/1/0/all/0/1&quot;&gt;Denis Divitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osin_D/0/1/0/all/0/1&quot;&gt;Dmitry Osin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trofimov_I/0/1/0/all/0/1&quot;&gt;Ilya Trofimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glushenko_A/0/1/0/all/0/1&quot;&gt;Anatoly Glushenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukharev_I/0/1/0/all/0/1&quot;&gt;Ivan Sukharev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berestenev_D/0/1/0/all/0/1&quot;&gt;Dmitry Berestenev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03251">
<title>TeLeS: Temporal Lexeme Similarity Score to Estimate Confidence in End-to-End ASR. (arXiv:2401.03251v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Confidence estimation of predictions from an End-to-End (E2E) Automatic
Speech Recognition (ASR) model benefits ASR&apos;s downstream and upstream tasks.
Class-probability-based confidence scores do not accurately represent the
quality of overconfident ASR predictions. An ancillary Confidence Estimation
Model (CEM) calibrates the predictions. State-of-the-art (SOTA) solutions use
binary target scores for CEM training. However, the binary labels do not reveal
the granular information of predicted words, such as temporal alignment between
reference and hypothesis and whether the predicted word is entirely incorrect
or contains spelling errors. Addressing this issue, we propose a novel
Temporal-Lexeme Similarity (TeLeS) confidence score to train CEM. To address
the data imbalance of target scores while training CEM, we use shrinkage loss
to focus on hard-to-learn data points and minimise the impact of easily learned
data points. We conduct experiments with ASR models trained in three languages,
namely Hindi, Tamil, and Kannada, with varying training data sizes. Experiments
show that TeLeS generalises well across domains. To demonstrate the
applicability of the proposed method, we formulate a TeLeS-based Acquisition
(TeLeS-A) function for sampling uncertainty in active learning. We observe a
significant reduction in the Word Error Rate (WER) as compared to SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravi_N/0/1/0/all/0/1&quot;&gt;Nagarathna Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+T_T/0/1/0/all/0/1&quot;&gt;Thishyan Raj T&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arora_V/0/1/0/all/0/1&quot;&gt;Vipul Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03253">
<title>Large Language Models as Visual Cross-Domain Learners. (arXiv:2401.03253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03253</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To address the above issues, cross-domain
learning aims at extracting domain-invariant knowledge to reduce the domain
shift between training and testing data. However, in visual cross-domain
learning, traditional methods concentrate solely on the image modality,
neglecting the use of the text modality to alleviate the domain shift. In this
work, we propose Large Language models as Visual cross-dOmain learners (LLaVO).
LLaVO uses vision-language models to convert images into detailed textual
descriptions. A large language model is then finetuned on textual descriptions
of the source/target domain generated by a designed instruction template.
Extensive experimental results on various cross-domain tasks under the domain
generalization and unsupervised domain adaptation settings have demonstrated
the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Weisen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiangang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03267">
<title>Autonomous Navigation in Complex Environments. (arXiv:2401.03267v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.03267</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerstenslager_A/0/1/0/all/0/1&quot;&gt;Andrew Gerstenslager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;Jomol Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKenna_L/0/1/0/all/0/1&quot;&gt;Liam McKenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_P/0/1/0/all/0/1&quot;&gt;Poorva Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03301">
<title>On Sample-Efficient Offline Reinforcement Learning: Data Diversity, Posterior Sampling, and Beyond. (arXiv:2401.03301v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03301</link>
<description rdf:parseType="Literal">&lt;p&gt;We seek to understand what facilitates sample-efficient learning from
historical datasets for sequential decision-making, a problem that is popularly
known as offline reinforcement learning (RL). Further, we are interested in
algorithms that enjoy sample efficiency while leveraging (value) function
approximation. In this paper, we address these fundamental questions by (i)
proposing a notion of data diversity that subsumes the previous notions of
coverage measures in offline RL and (ii) using this notion to {unify} three
distinct classes of offline RL algorithms based on version spaces (VS),
regularized optimization (RO), and posterior sampling (PS). We establish that
VS-based, RO-based, and PS-based algorithms, under standard assumptions,
achieve \emph{comparable} sample efficiency, which recovers the
state-of-the-art sub-optimality bounds for finite and linear model classes with
the standard assumptions. This result is surprising, given that the prior work
suggested an unfavorable sample complexity of the RO-based algorithm compared
to the VS-based algorithm, whereas posterior sampling is rarely considered in
offline RL due to its explorative nature. Notably, our proposed model-free
PS-based algorithm for offline RL is {novel}, with sub-optimality bounds that
are {frequentist} (i.e., worst-case) in nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Tang_T/0/1/0/all/0/1&quot;&gt;Thanh Nguyen-Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1&quot;&gt;Raman Arora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03302">
<title>Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical Images Using YOLOv8 and DeiT. (arXiv:2401.03302v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.03302</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hashemi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Hossein Hashemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Safari_L/0/1/0/all/0/1&quot;&gt;Leila Safari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taromi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Dadashzade Taromi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03306">
<title>MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot Learning. (arXiv:2401.03306v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03306</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of offline pre-training and online fine-tuning for
reinforcement learning from high-dimensional observations in the context of
realistic robot tasks. Recent offline model-free approaches successfully use
online fine-tuning to either improve the performance of the agent over the data
collection policy or adapt to novel tasks. At the same time, model-based RL
algorithms have achieved significant progress in sample efficiency and the
complexity of the tasks they can solve, yet remain under-utilized in the
fine-tuning setting. In this work, we argue that existing model-based offline
RL methods are not suitable for offline-to-online fine-tuning in
high-dimensional domains due to issues with distribution shifts, off-dynamics
data, and non-stationary rewards. We propose an on-policy model-based method
that can efficiently reuse prior data through model-based value expansion and
policy regularization, while preventing model exploitation by controlling
epistemic uncertainty. We find that our approach successfully solves tasks from
the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation
environment completely from images. To the best of our knowledge, MOTO is the
first method to solve this environment from pixels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1&quot;&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1&quot;&gt;Kyle Hatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolev_V/0/1/0/all/0/1&quot;&gt;Victor Kolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1&quot;&gt;John D. Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phielipp_M/0/1/0/all/0/1&quot;&gt;Mariano Phielipp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03314">
<title>Enhancing Context Through Contrast. (arXiv:2401.03314v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.03314</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural machine translation benefits from semantically rich representations.
Considerable progress in learning such representations has been achieved by
language modelling and mutual information maximization objectives using
contrastive learning. The language-dependent nature of language modelling
introduces a trade-off between the universality of the learned representations
and the model&apos;s performance on the language modelling tasks. Although
contrastive learning improves performance, its success cannot be attributed to
mutual information alone. We propose a novel Context Enhancement step to
improve performance on neural machine translation by maximizing mutual
information using the Barlow Twins loss. Unlike other approaches, we do not
explicitly augment the data but view languages as implicit augmentations,
eradicating the risk of disrupting semantic information. Further, our method
does not learn embeddings from scratch and can be generalised to any set of
pre-trained embeddings. Finally, we evaluate the language-agnosticism of our
embeddings through language classification and use them for neural machine
translation to compare with state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambilduke_K/0/1/0/all/0/1&quot;&gt;Kshitij Ambilduke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetye_A/0/1/0/all/0/1&quot;&gt;Aneesh Shetye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagade_D/0/1/0/all/0/1&quot;&gt;Diksha Bagade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitter_K/0/1/0/all/0/1&quot;&gt;Khurshed Fitter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vagdargi_P/0/1/0/all/0/1&quot;&gt;Prasad Vagdargi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiddarwar_S/0/1/0/all/0/1&quot;&gt;Shital Chiddarwar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03319">
<title>Comparison of Microservice Call Rate Predictions for Replication in the Cloud. (arXiv:2401.03319v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.03319</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, many users deploy their microservice-based applications with various
interconnections on a cluster of Cloud machines, subject to stochastic changes
due to dynamic user requirements. To address this problem, we compare three
machine learning (ML) models for predicting the microservice call rates based
on the microservice times and aiming at estimating the scalability
requirements. We apply the linear regression (LR), multilayer perception (MLP),
and gradient boosting regression (GBR) models on the Alibaba microservice
traces. The prediction results reveal that the LR model reaches a lower
training time than the GBR and MLP models. However, the GBR reduces the mean
absolute error and the mean absolute percentage error compared to LR and MLP
models. Moreover, the prediction results show that the required number of
replicas for each microservice by the gradient boosting model is close to the
actual test data without any prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehran_N/0/1/0/all/0/1&quot;&gt;Narges Mehran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighi_A/0/1/0/all/0/1&quot;&gt;Arman Haghighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aminharati_P/0/1/0/all/0/1&quot;&gt;Pedram Aminharati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolov_N/0/1/0/all/0/1&quot;&gt;Nikolay Nikolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soylu_A/0/1/0/all/0/1&quot;&gt;Ahmet Soylu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roman_D/0/1/0/all/0/1&quot;&gt;Dumitru Roman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prodan_R/0/1/0/all/0/1&quot;&gt;Radu Prodan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03322">
<title>Attention and Autoencoder Hybrid Model for Unsupervised Online Anomaly Detection. (arXiv:2401.03322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03322</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a hybrid attention and autoencoder (AE) model for
unsupervised online anomaly detection in time series. The autoencoder captures
local structural patterns in short embeddings, while the attention model learns
long-term features, facilitating parallel computing with positional encoding.
Unique in its approach, our proposed hybrid model combines attention and
autoencoder for the first time in time series anomaly detection. It employs an
attention-based mechanism, akin to the deep transformer model, with key
architectural modifications for predicting the next time step window in the
autoencoder&apos;s latent space. The model utilizes a threshold from the validation
dataset for anomaly detection and introduces an alternative method based on
analyzing the first statistical moment of error, improving accuracy without
dependence on a validation dataset. Evaluation on diverse real-world benchmark
datasets and comparing with other well-established models, confirms the
effectiveness of our proposed model in anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafi_S/0/1/0/all/0/1&quot;&gt;Seyed Amirhossein Najafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asemani_M/0/1/0/all/0/1&quot;&gt;Mohammad Hassan Asemani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setoodeh_P/0/1/0/all/0/1&quot;&gt;Peyman Setoodeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03331">
<title>Walnut Detection Through Deep Learning Enhanced by Multispectral Synthetic Images. (arXiv:2401.03331v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03331</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate identification of walnuts within orchards brings forth a
plethora of advantages, profoundly amplifying the efficiency and productivity
of walnut orchard management. Nevertheless, the unique characteristics of
walnut trees, characterized by their closely resembling shapes, colors, and
textures between the walnuts and leaves, present a formidable challenge in
precisely distinguishing between them during the annotation process. In this
study, we present a novel approach to improve walnut detection efficiency,
utilizing YOLOv5 trained on an enriched image set that incorporates both real
and synthetic RGB and NIR images. Our analysis comparing results from our
original and augmented datasets shows clear improvements in detection when
using the synthetic images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kaiming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1&quot;&gt;Tong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halubok_M/0/1/0/all/0/1&quot;&gt;Maryia Halubok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_B/0/1/0/all/0/1&quot;&gt;Brian N. Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03336">
<title>A deep learning framework for jointly extracting spectra and source-count distributions in astronomy. (arXiv:2401.03336v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2401.03336</link>
<description rdf:parseType="Literal">&lt;p&gt;Astronomical observations typically provide three-dimensional maps, encoding
the distribution of the observed flux in (1) the two angles of the celestial
sphere and (2) energy/frequency. An important task regarding such maps is to
statistically characterize populations of point sources too dim to be
individually detected. As the properties of a single dim source will be poorly
constrained, instead one commonly studies the population as a whole, inferring
a source-count distribution (SCD) that describes the number density of sources
as a function of their brightness. Statistical and machine learning methods for
recovering SCDs exist; however, they typically entirely neglect spectral
information associated with the energy distribution of the flux. We present a
deep learning framework able to jointly reconstruct the spectra of different
emission components and the SCD of point-source populations. In a
proof-of-concept example, we show that our method accurately extracts even
complex-shaped spectra and SCDs from simulated maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wolf_F/0/1/0/all/0/1&quot;&gt;Florian Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+List_F/0/1/0/all/0/1&quot;&gt;Florian List&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Rodd_N/0/1/0/all/0/1&quot;&gt;Nicholas L. Rodd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Hahn_O/0/1/0/all/0/1&quot;&gt;Oliver Hahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03341">
<title>Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection. (arXiv:2401.03341v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to their unsupervised training and uncertainty estimation, deep
Variational Autoencoders (VAEs) have become powerful tools for
reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based
TSAD methods, either statistical or deep, tune meta-priors to estimate the
likelihood probability for effectively capturing spatiotemporal dependencies in
the data. However, these methods confront the challenge of inherent data
scarcity, which is often the case in anomaly detection tasks. Such scarcity
easily leads to latent holes, discontinuous regions in latent space, resulting
in non-robust reconstructions on these discontinuous spaces. We propose a novel
generative framework that combines VAEs with self-supervised learning (SSL) to
address this issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhangkai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Longbing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junxian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03346">
<title>An Investigation of Large Language Models for Real-World Hate Speech Detection. (arXiv:2401.03346v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.03346</link>
<description rdf:parseType="Literal">&lt;p&gt;Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing
methods are still significantly limited in effectively detecting hate speech
online. A major limitation of existing methods is that hate speech detection is
a highly contextual problem, and these methods cannot fully capture the context
of hate speech to make accurate predictions. Recently, large language models
(LLMs) have demonstrated state-of-the-art performance in several natural
language tasks. LLMs have undergone extensive training using vast amounts of
natural language data, enabling them to grasp intricate contextual details.
Hence, they could be used as knowledge bases for context-aware hate speech
detection. However, a fundamental problem with using LLMs to detect hate speech
is that there are no studies on effectively prompting LLMs for context-aware
hate speech detection. In this study, we conduct a large-scale study of hate
speech detection, employing five established hate speech datasets. We discover
that LLMs not only match but often surpass the performance of current benchmark
machine learning models in identifying hate speech. By proposing four diverse
prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively
capture the context of hate speech by fully utilizing the knowledge base in
LLMs, significantly outperforming existing techniques. Furthermore, although
LLMs can provide a rich knowledge base for the contextual detection of hate
speech, suitable prompting strategies play a crucial role in effectively
leveraging this knowledge base for efficient detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Keyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1&quot;&gt;Alexander Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Jaden Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Ziheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwamitra_N/0/1/0/all/0/1&quot;&gt;Nishant Vishwamitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hongxin Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03349">
<title>Image Inpainting via Tractable Steering of Diffusion Models. (arXiv:2401.03349v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.03349</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are the current state of the art for generating
photorealistic images. Controlling the sampling process for constrained image
generation tasks such as inpainting, however, remains challenging since exact
conditioning on such constraints is intractable. While existing methods use
various techniques to approximate the constrained posterior, this paper
proposes to exploit the ability of Tractable Probabilistic Models (TPMs) to
exactly and efficiently compute the constrained posterior, and to leverage this
signal to steer the denoising process of diffusion models. Specifically, this
paper adopts a class of expressive TPMs termed Probabilistic Circuits (PCs).
Building upon prior advances, we further scale up PCs and make them capable of
guiding the image generation process of diffusion models. Empirical results
suggest that our approach can consistently improve the overall quality and
semantic coherence of inpainted images across three natural image datasets
(i.e., CelebA-HQ, ImageNet, and LSUN) with only ~10% additional computational
overhead brought by the TPM. Further, with the help of an image encoder and
decoder, our method can readily accept semantic constraints on specific regions
of the image, which opens up the potential for more controlled image generation
tasks. In addition to proposing a new framework for constrained image
generation, this paper highlights the benefit of more tractable models and
motivates the development of expressive TPMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1&quot;&gt;Guy Van den Broeck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03350">
<title>Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.03350</link>
<description rdf:parseType="Literal">&lt;p&gt;While graph neural networks (GNNs) are widely used for node and graph
representation learning tasks, the reliability of GNN uncertainty estimates
under distribution shifts remains relatively under-explored. Indeed, while
post-hoc calibration strategies can be used to improve in-distribution
calibration, they need not also improve calibration under distribution shift.
However, techniques which produce GNNs with better intrinsic uncertainty
estimates are particularly valuable, as they can always be combined with
post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a
novel training framework designed to improve intrinsic GNN uncertainty
estimates. Our framework adapts the principle of stochastic data centering to
graph data through novel graph anchoring strategies, and is able to support
partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic
networks are necessary to obtain reliable estimates, we find that the
functional diversity induced by our anchoring strategies when sampling
hypotheses renders this unnecessary and allows us to support G-$\Delta$UQ on
pretrained models. Indeed, through extensive evaluation under covariate,
concept and graph size shifts, we show that G-$\Delta$UQ leads to better
calibrated GNNs for node and graph classification. Further, it also improves
performance on the uncertainty-based tasks of out-of-distribution detection and
generalization gap estimation. Overall, our work provides insights into
uncertainty estimation for GNNs, and demonstrates the utility of G-$\Delta$UQ
in obtaining reliable estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1&quot;&gt;Puja Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heimann_M/0/1/0/all/0/1&quot;&gt;Mark Heimann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1&quot;&gt;Danai Koutra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.05839">
<title>A Theory of the Risk for Optimization with Relaxation and its Application to Support Vector Machines. (arXiv:2004.05839v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2004.05839</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we consider optimization with relaxation, an ample paradigm to
make data-driven designs. This approach was previously considered by the same
authors of this work in Garatti and Campi (2019), a study that revealed a
deep-seated connection between two concepts: risk (probability of not
satisfying a new, out-of-sample, constraint) and complexity (according to a
definition introduced in paper Garatti and Campi (2019)). This connection was
shown to have profound implications in applications because it implied that the
risk can be estimated from the complexity, a quantity that can be measured from
the data without any knowledge of the data-generation mechanism. In the present
work we establish new results. First, we expand the scope of Garatti and Campi
(2019) so as to embrace a more general setup that covers various algorithms in
machine learning. Then, we study classical support vector methods - including
SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support
Vector Data Description) - and derive new results for the ability of these
methods to generalize. All results are valid for any finite size of the data
set. When the sample size tends to infinity, we establish the unprecedented
result that the risk approaches the ratio between the complexity and the
cardinality of the data sample, regardless of the value of the complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campi_M/0/1/0/all/0/1&quot;&gt;Marco C. Campi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garatti_S/0/1/0/all/0/1&quot;&gt;Simone Garatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.00676">
<title>Token-Modification Adversarial Attacks for Natural Language Processing: A Survey. (arXiv:2103.00676v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2103.00676</link>
<description rdf:parseType="Literal">&lt;p&gt;Many adversarial attacks target natural language processing systems, most of
which succeed through modifying the individual tokens of a document. Despite
the apparent uniqueness of each of these attacks, fundamentally they are simply
a distinct configuration of four components: a goal function, allowable
transformations, a search method, and constraints. In this survey, we
systematically present the different components used throughout the literature,
using an attack-independent framework which allows for easy comparison and
categorisation of components. Our work aims to serve as a comprehensive guide
for newcomers to the field and to spark targeted research into refining the
individual attack components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_T/0/1/0/all/0/1&quot;&gt;Tom Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yansong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1&quot;&gt;Alsharif Abuadbba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nepal_S/0/1/0/all/0/1&quot;&gt;Surya Nepal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03452">
<title>Location Leakage in Federated Signal Maps. (arXiv:2112.03452v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03452</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of predicting cellular network performance (signal
maps) from measurements collected by several mobile devices. We formulate the
problem within the online federated learning framework: (i) federated learning
(FL) enables users to collaboratively train a model, while keeping their
training data on their devices; (ii) measurements are collected as users move
around over time and are used for local training in an online fashion. We
consider an honest-but-curious server, who observes the updates from target
users participating in FL and infers their location using a deep leakage from
gradients (DLG) type of attack, originally developed to reconstruct training
data of DNN image classifiers. We make the key observation that a DLG attack,
applied to our setting, infers the average location of a batch of local data,
and can thus be used to reconstruct the target users&apos; trajectory at a coarse
granularity. We build on this observation to protect location privacy, in our
setting, by revisiting and designing mechanisms within the federated learning
framework including: tuning the FL parameters for averaging, curating local
batches so as to mislead the DLG attacker, and aggregating across multiple
users with different trajectories. We evaluate the performance of our
algorithms through both analysis and simulation based on real-world mobile
datasets, and we show that they achieve a good privacy-utility tradeoff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakopoulou_E/0/1/0/all/0/1&quot;&gt;Evita Bakopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Psounis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Psounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markopoulou_A/0/1/0/all/0/1&quot;&gt;Athina Markopoulou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.08440">
<title>Climate-Invariant Machine Learning. (arXiv:2112.08440v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.08440</link>
<description rdf:parseType="Literal">&lt;p&gt;Projecting climate change is a generalization problem: we extrapolate the
recent past using physical models across past, present, and future climates.
Current climate models require representations of processes that occur at
scales smaller than model grid size, which have been the main source of model
projection uncertainty. Recent machine learning (ML) algorithms hold promise to
improve such process representations, but tend to extrapolate poorly to climate
regimes they were not trained on. To get the best of the physical and
statistical worlds, we propose a new framework - termed &quot;climate-invariant&quot; ML
- incorporating knowledge of climate processes into ML algorithms, and show
that it can maintain high offline accuracy across a wide range of climate
conditions and configurations in three distinct atmospheric models. Our results
suggest that explicitly incorporating physical knowledge into data-driven
models of Earth system processes can improve their consistency, data
efficiency, and generalizability across climate regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beucler_T/0/1/0/all/0/1&quot;&gt;Tom Beucler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gentine_P/0/1/0/all/0/1&quot;&gt;Pierre Gentine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuval_J/0/1/0/all/0/1&quot;&gt;Janni Yuval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Ankitesh Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Liran Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jerry Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sungduk Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasp_S/0/1/0/all/0/1&quot;&gt;Stephan Rasp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1&quot;&gt;Fiaz Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OGorman_P/0/1/0/all/0/1&quot;&gt;Paul A. O&amp;#x27;Gorman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neelin_J/0/1/0/all/0/1&quot;&gt;J. David Neelin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutsko_N/0/1/0/all/0/1&quot;&gt;Nicholas J. Lutsko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritchard_M/0/1/0/all/0/1&quot;&gt;Michael Pritchard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03618">
<title>On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error. (arXiv:2202.03618v4 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03618</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the Unbalanced Optimal Transport (UOT) between two measures of
possibly different masses with at most $n$ components, where the marginal
constraints of standard Optimal Transport (OT) are relaxed via Kullback-Leibler
divergence with regularization factor $\tau$. Although only Sinkhorn-based UOT
solvers have been analyzed in the literature with the iteration complexity of
${O}\big(\tfrac{\tau \log(n)}{\varepsilon}
\log\big(\tfrac{\log(n)}{{\varepsilon}}\big)\big)$ and per-iteration cost of
$O(n^2)$ for achieving the desired error $\varepsilon$, their positively dense
output transportation plans strongly hinder the practicality. On the other
hand, while being vastly used as heuristics for computing UOT in modern deep
learning applications and having shown success in sparse OT problem, gradient
methods applied to UOT have not been formally studied. In this paper, we
propose a novel algorithm based on Gradient Extrapolation Method (GEM-UOT) to
find an $\varepsilon$-approximate solution to the UOT problem in $O\big( \kappa
\log\big(\frac{\tau n}{\varepsilon}\big) \big)$ iterations with
$\widetilde{O}(n^2)$ per-iteration cost, where $\kappa$ is the condition number
depending on only the two input measures. Our proof technique is based on a
novel dual formulation of the squared $\ell_2$-norm UOT objective, which fills
the lack of sparse UOT literature and also leads to a new characterization of
approximation error between UOT and OT. To this end, we further present a novel
approach of OT retrieval from UOT, which is based on GEM-UOT with fine tuned
$\tau$ and a post-process projection step. Extensive experiments on synthetic
and real datasets validate our theories and demonstrate the favorable
performance of our methods in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang Minh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoang H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Lam M. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00031">
<title>The complexity of quantum support vector machines. (arXiv:2203.00031v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00031</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum support vector machines employ quantum circuits to define the kernel
function. It has been shown that this approach offers a provable exponential
speedup compared to any known classical algorithm for certain data sets. The
training of such models corresponds to solving a convex optimization problem
either via its primal or dual formulation. Due to the probabilistic nature of
quantum mechanics, the training algorithms are affected by statistical
uncertainty, which has a major impact on their complexity. We show that the
dual problem can be solved in $O(M^{4.67}/\varepsilon^2)$ quantum circuit
evaluations, where $M$ denotes the size of the data set and $\varepsilon$ the
solution accuracy compared to the ideal result from exact expectation values,
which is only obtainable in theory. We prove under an empirically motivated
assumption that the kernelized primal problem can alternatively be solved in
$O(\min \{ M^2/\varepsilon^6, \, 1/\varepsilon^{10} \})$ evaluations by
employing a generalization of a known classical algorithm called Pegasos.
Accompanying empirical results demonstrate these analytical complexities to be
essentially tight. In addition, we investigate a variational approximation to
quantum support vector machines and show that their heuristic training achieves
considerably better scaling in our experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gentinetta_G/0/1/0/all/0/1&quot;&gt;Gian Gentinetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Thomsen_A/0/1/0/all/0/1&quot;&gt;Arne Thomsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sutter_D/0/1/0/all/0/1&quot;&gt;David Sutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Woerner_S/0/1/0/all/0/1&quot;&gt;Stefan Woerner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.00846">
<title>Chordal Sparsity for Lipschitz Constant Estimation of Deep Neural Networks. (arXiv:2204.00846v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.00846</link>
<description rdf:parseType="Literal">&lt;p&gt;Lipschitz constants of neural networks allow for guarantees of robustness in
image classification, safety in controller design, and generalizability beyond
the training data. As calculating Lipschitz constants is NP-hard, techniques
for estimating Lipschitz constants must navigate the trade-off between
scalability and accuracy. In this work, we significantly push the scalability
frontier of a semidefinite programming technique known as LipSDP while
achieving zero accuracy loss. We first show that LipSDP has chordal sparsity,
which allows us to derive a chordally sparse formulation that we call
Chordal-LipSDP. The key benefit is that the main computational bottleneck of
LipSDP, a large semidefinite constraint, is now decomposed into an equivalent
collection of smaller ones: allowing Chordal-LipSDP to outperform LipSDP
particularly as the network depth grows. Moreover, our formulation uses a
tunable sparsity parameter that enables one to gain tighter estimates without
incurring a significant computational cost. We illustrate the scalability of
our approach through extensive numerical experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_A/0/1/0/all/0/1&quot;&gt;Anton Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindemann_L/0/1/0/all/0/1&quot;&gt;Lars Lindemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1&quot;&gt;Alexander Robey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1&quot;&gt;Rajeev Alur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.02338">
<title>MGDCF: Distance Learning via Markov Graph Diffusion for Neural Collaborative Filtering. (arXiv:2204.02338v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2204.02338</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have recently been utilized to build
Collaborative Filtering (CF) models to predict user preferences based on
historical user-item interactions. However, there is relatively little
understanding of how GNN-based CF models relate to some traditional Network
Representation Learning (NRL) approaches. In this paper, we show the
equivalence between some state-of-the-art GNN-based CF models and a traditional
1-layer NRL model based on context encoding. Based on a Markov process that
trades off two types of distances, we present Markov Graph Diffusion
Collaborative Filtering (MGDCF) to generalize some state-of-the-art GNN-based
CF models. Instead of considering the GNN as a trainable black box that
propagates learnable user/item vertex embeddings, we treat GNNs as an
untrainable Markov process that can construct constant context features of
vertices for a traditional NRL model that encodes context features with a
fully-connected layer. Such simplification can help us to better understand how
GNNs benefit CF models. Especially, it helps us realize that ranking losses
play crucial roles in GNN-based CF tasks. With our proposed simple yet powerful
ranking loss InfoBPR, the NRL model can still perform well without the context
features constructed by GNNs. We conduct experiments to perform detailed
analysis on MGDCF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengsheng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Quan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11677">
<title>Semi-Supervised Clustering of Sparse Graphs: Crossing the Information-Theoretic Threshold. (arXiv:2205.11677v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11677</link>
<description rdf:parseType="Literal">&lt;p&gt;The stochastic block model is a canonical random graph model for clustering
and community detection on network-structured data. Decades of extensive study
on the problem have established many profound results, among which the phase
transition at the Kesten-Stigum threshold is particularly interesting both from
a mathematical and an applied standpoint. It states that no estimator based on
the network topology can perform substantially better than chance on sparse
graphs if the model parameter is below certain threshold. Nevertheless, if we
slightly extend the horizon to the ubiquitous semi-supervised setting, such a
fundamental limitation will disappear completely. We prove that with arbitrary
fraction of the labels revealed, the detection problem is feasible throughout
the parameter domain. Moreover, we introduce two efficient algorithms, one
combinatorial and one based on optimization, to integrate label information
with graph structures. Our work brings a new perspective to stochastic model of
networks and semidefinite program research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sheng_J/0/1/0/all/0/1&quot;&gt;Junda Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strohmer_T/0/1/0/all/0/1&quot;&gt;Thomas Strohmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03019">
<title>The Survival Bandit Problem. (arXiv:2206.03019v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03019</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and study a new variant of the multi-armed bandit problem (MAB),
called the survival bandit problem (S-MAB). While in both problems, the
objective is to maximize the so-called cumulative reward, in this new variant,
the procedure is interrupted if the cumulative reward falls below a preset
threshold. This simple yet unexplored extension of the MAB follows from many
practical applications. For example, when testing two medicines against each
other on voluntary patients, people&apos;s health are at stake, and it is necessary
to be able to interrupt experiments if serious side effects occur or if the
disease syndromes are not dissipated by the treatment. From a theoretical
perspective, the S-MAB is the first variant of the MAB where the procedure may
or may not be interrupted. We start by formalizing the S-MAB and we define its
objective as the minimization of the so-called survival regret, which naturally
generalizes the regret of the MAB. Then, we show that the objective of the
S-MAB is considerably more difficult than the MAB, in the sense that contrary
to the MAB, no policy can achieve a reasonably small (i.e., sublinear) survival
regret. Instead, we minimize the survival regret in the sense of Pareto, i.e.,
we seek a policy whose cumulative reward cannot be improved for some problem
instance without being sacrificed for another one. For that purpose, we
identify two key components in the survival regret: the regret given no ruin
(which corresponds to the regret in the MAB), and the probability that the
procedure is interrupted, called the probability of ruin. We derive a lower
bound on the probability of ruin, as well as policies whose probability of ruin
matches the lower bound. Finally, based on a doubling trick on those policies,
we derive a policy which minimizes the survival regret in the sense of Pareto,
giving an answer to an open problem by Perotto et al. (COLT 2019).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riou_C/0/1/0/all/0/1&quot;&gt;Charles Riou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honda_J/0/1/0/all/0/1&quot;&gt;Junya Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1&quot;&gt;Masashi Sugiyama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03208">
<title>From Attribution Maps to Human-Understandable Explanations through Concept Relevance Propagation. (arXiv:2206.03208v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03208</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of eXplainable Artificial Intelligence (XAI) aims to bring
transparency to today&apos;s powerful but opaque deep learning models. While local
XAI methods explain individual predictions in form of attribution maps, thereby
identifying where important features occur (but not providing information about
what they represent), global explanation techniques visualize what concepts a
model has generally learned to encode. Both types of methods thus only provide
partial insights and leave the burden of interpreting the model&apos;s reasoning to
the user. In this work we introduce the Concept Relevance Propagation (CRP)
approach, which combines the local and global perspectives and thus allows
answering both the &quot;where&quot; and &quot;what&quot; questions for individual predictions. We
demonstrate the capability of our method in various settings, showcasing that
CRP leads to more human interpretable explanations and provides deep insights
into the model&apos;s representation and reasoning through concept atlases, concept
composition analyses, and quantitative investigations of concept subspaces and
their role in fine-grained decision making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achtibat_R/0/1/0/all/0/1&quot;&gt;Reduan Achtibat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1&quot;&gt;Maximilian Dreyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisenbraun_I/0/1/0/all/0/1&quot;&gt;Ilona Eisenbraun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosse_S/0/1/0/all/0/1&quot;&gt;Sebastian Bosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiegand_T/0/1/0/all/0/1&quot;&gt;Thomas Wiegand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.03482">
<title>Chordal Sparsity for SDP-based Neural Network Verification. (arXiv:2206.03482v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.03482</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are central to many emerging technologies, but verifying
their correctness remains a major challenge. It is known that network outputs
can be sensitive and fragile to even small input perturbations, thereby
increasing the risk of unpredictable and undesirable behavior. Fast and
accurate verification of neural networks is therefore critical to their
widespread adoption, and in recent years, various methods have been developed
as a response to this problem. In this paper, we focus on improving
semidefinite programming (SDP) based techniques for neural network
verification. Such techniques offer the power of expressing complex geometric
constraints while retaining a convex problem formulation, but scalability
remains a major issue in practice. Our starting point is the DeepSDP framework
proposed by Fazlyab et al., which uses quadratic constraints to abstract the
verification problem into a large-scale SDP. However, solving this SDP quickly
becomes intractable when the network grows. Our key observation is that by
leveraging chordal sparsity, we can decompose the primary computational
bottleneck of DeepSDP -- a large linear matrix inequality (LMI) -- into an
equivalent collection of smaller LMIs. We call our chordally sparse
optimization program Chordal-DeepSDP and prove that its construction is
identically expressive as that of DeepSDP. Moreover, we show that additional
analysis of Chordal-DeepSDP allows us to further rewrite its collection of LMIs
in a second level of decomposition that we call Chordal-DeepSDP-2 -- which
results in another significant computational gain. Finally, we provide
numerical experiments on real networks of learned cart-pole dynamics,
showcasing the computational advantage of Chordal-DeepSDP and Chordal-DeepSDP-2
over DeepSDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_A/0/1/0/all/0/1&quot;&gt;Anton Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindemann_L/0/1/0/all/0/1&quot;&gt;Lars Lindemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1&quot;&gt;Rajeev Alur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.15462">
<title>Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations. (arXiv:2206.15462v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.15462</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a margin-based loss for tuning joint vision-language models so
that their gradient-based explanations are consistent with region-level
annotations provided by humans for relatively smaller grounding datasets. We
refer to this objective as Attention Mask Consistency (AMC) and demonstrate
that it produces superior visual grounding results than previous methods that
rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language
modeling objectives obtains a state-of-the-art accuracy of 86.49% in the
Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when
compared to the best previous model trained under the same level of
supervision. Our approach also performs exceedingly well on established
benchmarks for referring expression comprehension where it obtains 80.34%
accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC
is effective, easy to implement, and is general as it can be adopted by any
vision-language model, and can use any type of region annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1&quot;&gt;Kushal Kafle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ordonez_V/0/1/0/all/0/1&quot;&gt;Vicente Ordonez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.05631">
<title>DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization. (arXiv:2207.05631v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.05631</link>
<description rdf:parseType="Literal">&lt;p&gt;Most reinforcement learning algorithms seek a single optimal strategy that
solves a given task. However, it can often be valuable to learn a diverse set
of solutions, for instance, to make an agent&apos;s interaction with users more
engaging, or improve the robustness of a policy to an unexpected perturbance.
We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm
that discovers multiple strategies for solving a given task. Unlike prior work,
it achieves this with a shared policy network trained over a single run.
Specifically, we design an intrinsic reward based on an information-theoretic
diversity objective. Our final objective alternately constraints on the
diversity of the strategies and on the extrinsic reward. We solve the
constrained optimization problem by casting it as a probabilistic inference
task and use policy iteration to maximize the derived lower bound. Experimental
results show that our method efficiently discovers diverse strategies in a wide
variety of reinforcement learning tasks. Compared to baseline methods, DGPO
achieves comparable rewards, while discovering more diverse strategies, and
often with better sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentse Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1&quot;&gt;Yuan Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_T/0/1/0/all/0/1&quot;&gt;Tim Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1&quot;&gt;Wei-Wei Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14624">
<title>Is Complexity Required for Neural Network Pruning? A Case Study on Global Magnitude Pruning. (arXiv:2209.14624v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14624</link>
<description rdf:parseType="Literal">&lt;p&gt;Pruning neural networks has become popular in the last decade when it was
shown that a large number of weights can be safely removed from modern neural
networks without compromising accuracy. Numerous pruning methods have been
proposed since, each claiming to be better than prior art, however, at the cost
of increasingly complex pruning methodologies. These methodologies include
utilizing importance scores, getting feedback through back-propagation or
having heuristics-based pruning rules amongst others. In this work, we question
whether this pattern of introducing complexity is really necessary to achieve
better pruning results. We benchmark these SOTA techniques against a simple
pruning baseline, namely, Global Magnitude Pruning (Global MP), that ranks
weights in order of their magnitudes and prunes the smallest ones.
Surprisingly, we find that vanilla Global MP performs very well against the
SOTA techniques. When considering sparsity-accuracy trade-off, Global MP
performs better than all SOTA techniques at all sparsity ratios. When
considering FLOPs-accuracy trade-off, some SOTA techniques outperform Global MP
at lower sparsity ratios, however, Global MP starts performing well at high
sparsity ratios and performs very well at extremely high sparsity ratios.
Moreover, we find that a common issue that many pruning algorithms run into at
high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP.
We explore why layer collapse occurs in networks and how it can be mitigated in
Global MP by utilizing a technique called Minimum Threshold. We showcase the
above findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1
and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is
available at https://github.com/manasgupta-1/GlobalMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Manas Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camci_E/0/1/0/all/0/1&quot;&gt;Efe Camci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keneta_V/0/1/0/all/0/1&quot;&gt;Vishandi Rudy Keneta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaidyanathan_A/0/1/0/all/0/1&quot;&gt;Abhishek Vaidyanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanodia_R/0/1/0/all/0/1&quot;&gt;Ritwik Kanodia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1&quot;&gt;Wu Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jie_L/0/1/0/all/0/1&quot;&gt;Lin Jie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02339">
<title>Particle clustering in turbulence: Prediction of spatial and statistical properties with deep learning. (arXiv:2210.02339v2 [astro-ph.EP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02339</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the utility of deep learning for modeling the clustering of
particles that are aerodynamically coupled to turbulent fluids. Using a
Lagrangian particle module within the Athena++ hydrodynamics code, we simulate
the dynamics of particles in the Epstein drag regime within a periodic domain
of isotropic forced hydrodynamic turbulence. This setup is an idealized model
relevant to the collisional growth of micron to mm-sized dust particles in
early stage planet formation. The simulation data are used to train a U-Net
deep learning model to predict gridded three-dimensional representations of the
particle density and velocity fields, given as input the corresponding fluid
fields. The trained model qualitatively captures the filamentary structure of
clustered particles in a highly non-linear regime. We assess model fidelity by
calculating metrics of the density field (the radial distribution function) and
of the velocity field (the relative velocity and the relative radial velocity
between particles). Although trained only on the spatial fields, the model
predicts these statistical quantities with errors that are typically &amp;lt;10%. Our
results suggest that, given appropriately expanded training data, deep learning
could complement direct numerical simulations in predicting particle clustering
within turbulent flows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Chan_Y/0/1/0/all/0/1&quot;&gt;Yan-Mong Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Manger_N/0/1/0/all/0/1&quot;&gt;Natascha Manger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Chin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhaohuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Armitage_P/0/1/0/all/0/1&quot;&gt;Philip J. Armitage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1&quot;&gt;Shirley Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13690">
<title>Highly Efficient Real-Time Streaming and Fully On-Device Speaker Diarization with Multi-Stage Clustering. (arXiv:2210.13690v4 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13690</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent research advances in speaker diarization mostly focus on
improving the quality of diarization results, there is also an increasing
interest in improving the efficiency of diarization systems. In this paper, we
demonstrate that a multi-stage clustering strategy that uses different
clustering algorithms for input of different lengths can address multi-faceted
challenges of on-device speaker diarization applications. Specifically, a
fallback clusterer is used to handle short-form inputs; a main clusterer is
used to handle medium-length inputs; and a pre-clusterer is used to compress
long-form inputs before they are processed by the main clusterer. Both the main
clusterer and the pre-clusterer can be configured with an upper bound of the
computational complexity to adapt to devices with different resource
constraints. This multi-stage clustering strategy is critical for streaming
on-device speaker diarization systems, where the budgets of CPU, memory and
battery are tight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Han Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guanlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moreno_I/0/1/0/all/0/1&quot;&gt;Ignacio Lopez Moreno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15469">
<title>Learning Failure-Inducing Models for Testing Software-Defined Networks. (arXiv:2210.15469v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15469</link>
<description rdf:parseType="Literal">&lt;p&gt;Software-defined networks (SDN) enable flexible and effective communication
systems that are managed by centralized software controllers. However, such a
controller can undermine the underlying communication network of an SDN-based
system and thus must be carefully tested. When an SDN-based system fails, in
order to address such a failure, engineers need to precisely understand the
conditions under which it occurs. In this article, we introduce a machine
learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating
effective test data leading to failures in SDN-based systems and (2) learning
accurate failure-inducing models that characterize conditions under which such
system fails. To our knowledge, no existing work simultaneously addresses these
two objectives for SDNs. We evaluate FuzzSDN by applying it to systems
controlled by two open-source SDN controllers. Further, we compare FuzzSDN with
two state-of-the-art methods for fuzzing SDNs and two baselines for learning
failure-inducing models. Our results show that (1) compared to the
state-of-the-art methods, FuzzSDN generates at least 12 times more failures,
within the same time budget, with a controller that is fairly robust to fuzzing
and (2) our failure-inducing models have, on average, a precision of 98% and a
recall of 86%, significantly outperforming the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ollando_R/0/1/0/all/0/1&quot;&gt;Rapha&amp;#xeb;l Ollando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seung Yeob Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1&quot;&gt;Lionel C. Briand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.04370">
<title>NESTER: An Adaptive Neurosymbolic Method for Causal Effect Estimation. (arXiv:2211.04370v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.04370</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal effect estimation from observational data is a central problem in
causal inference. Methods based on potential outcomes framework solve this
problem by exploiting inductive biases and heuristics from causal inference.
Each of these methods addresses a specific aspect of causal effect estimation,
such as controlling propensity score, enforcing randomization, etc., by
designing neural network (NN) architectures and regularizers. In this paper, we
propose an adaptive method called Neurosymbolic Causal Effect Estimator
(NESTER), a generalized method for causal effect estimation. NESTER integrates
the ideas used in existing methods based on multi-head NNs for causal effect
estimation into one framework. We design a Domain Specific Language (DSL)
tailored for causal effect estimation based on causal inductive biases used in
literature. We conduct a theoretical analysis to investigate NESTER&apos;s efficacy
in estimating causal effects. Our comprehensive empirical results show that
NESTER performs better than state-of-the-art methods on benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Abbavaram Gowtham Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09782">
<title>Assessing Neural Network Robustness via Adversarial Pivotal Tuning. (arXiv:2211.09782v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09782</link>
<description rdf:parseType="Literal">&lt;p&gt;The robustness of image classifiers is essential to their deployment in the
real world. The ability to assess this resilience to manipulations or
deviations from the training data is thus crucial. These modifications have
traditionally consisted of minimal changes that still manage to fool
classifiers, and modern approaches are increasingly robust to them. Semantic
manipulations that modify elements of an image in meaningful ways have thus
gained traction for this purpose. However, they have primarily been limited to
style, color, or attribute changes. While expressive, these manipulations do
not make use of the full capabilities of a pretrained generative model. In this
work, we aim to bridge this gap. We show how a pretrained image generator can
be used to semantically manipulate images in a detailed, diverse, and
photorealistic way while still preserving the class of the original image.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a
pivot latent space input that reconstructs the image using a pretrained
generator. It then adjusts the generator&apos;s weights to create small yet semantic
manipulations in order to fool a pretrained classifier. APT preserves the full
expressive editing capabilities of the generative model. We demonstrate that
APT is capable of a wide range of class-preserving semantic image manipulations
that fool a variety of pretrained classifiers. Finally, we show that
classifiers that are robust to other benchmarks are not robust to APT
manipulations and suggest a method to improve them. Code available at:
https://captaine.github.io/apt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_P/0/1/0/all/0/1&quot;&gt;Peter Ebert Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snaebjarnarson_V/0/1/0/all/0/1&quot;&gt;V&amp;#xe9;steinn Sn&amp;#xe6;bjarnarson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1&quot;&gt;Andrea Dittadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1&quot;&gt;Serge Belongie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1&quot;&gt;Sagie Benaim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09940">
<title>Entry Dependent Expert Selection in Distributed Gaussian Processes Using Multilabel Classification. (arXiv:2211.09940v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09940</link>
<description rdf:parseType="Literal">&lt;p&gt;By distributing the training process, local approximation reduces the cost of
the standard Gaussian Process. An ensemble technique combines local predictions
from Gaussian experts trained on different partitions of the data. Ensemble
methods aggregate models&apos; predictions by assuming a perfect diversity of local
predictors. Although it keeps the aggregation tractable, this assumption is
often violated in practice. Even though ensemble methods provide consistent
results by assuming dependencies between experts, they have a high
computational cost, which is cubic in the number of experts involved. By
implementing an expert selection strategy, the final aggregation step uses
fewer experts and is more efficient. However, a selection approach that assigns
a fixed set of experts to each new data point cannot encode the specific
properties of each unique data point. This paper proposes a flexible expert
selection approach based on the characteristics of entry data points. To this
end, we investigate the selection task as a multi-label classification problem
where the experts define labels, and each entry point is assigned to some
experts. The proposed solution&apos;s prediction quality, efficiency, and asymptotic
properties are discussed in detail. We demonstrate the efficacy of our method
through extensive numerical experiments using synthetic and real-world data
sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalali_H/0/1/0/all/0/1&quot;&gt;Hamed Jalali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1&quot;&gt;Gjergji Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11183">
<title>Causal Fairness Assessment of Treatment Allocation with Electronic Health Records. (arXiv:2211.11183v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11183</link>
<description rdf:parseType="Literal">&lt;p&gt;Healthcare continues to grapple with the persistent issue of treatment
disparities, sparking concerns regarding the equitable allocation of treatments
in clinical practice. While various fairness metrics have emerged to assess
fairness in decision-making processes, a growing focus has been on
causality-based fairness concepts due to their capacity to mitigate confounding
effects and reason about bias. However, the application of causal fairness
notions in evaluating the fairness of clinical decision-making with electronic
health record (EHR) data remains an understudied domain. This study aims to
address the methodological gap in assessing causal fairness of treatment
allocation with electronic health records data. We propose a causal fairness
algorithm to assess fairness in clinical decision-making. Our algorithm
accounts for the heterogeneity of patient populations and identifies potential
unfairness in treatment allocation by conditioning on patients who have the
same likelihood to benefit from the treatment. We apply this framework to a
patient cohort with coronary artery disease derived from an EHR database to
evaluate the fairness of treatment decisions. In addition, we investigate the
impact of social determinants of health on the assessment of causal fairness of
treatment allocation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richter_L/0/1/0/all/0/1&quot;&gt;Lauren R. Richter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostropolets_A/0/1/0/all/0/1&quot;&gt;Anna Ostropolets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhadad_N/0/1/0/all/0/1&quot;&gt;Noemie Elhadad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David M. Blei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hripcsak_G/0/1/0/all/0/1&quot;&gt;George Hripcsak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11403">
<title>General time-reversal equivariant neural network potential for magnetic materials. (arXiv:2211.11403v3 [cond-mat.mtrl-sci] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11403</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces time-reversal E(3)-equivariant neural network and
SpinGNN++ framework for constructing a comprehensive interatomic potential for
magnetic systems, encompassing spin-orbit coupling and noncollinear magnetic
moments. SpinGNN++ integrates multitask spin equivariant neural network with
explicit spin-lattice terms, including Heisenberg, Dzyaloshinskii-Moriya,
Kitaev, single-ion anisotropy, and biquadratic interactions, and employs
time-reversal equivariant neural network to learn high-order spin-lattice
interactions using time-reversal E(3)-equivariant convolutions. To validate
SpinGNN++, a complex magnetic model dataset is introduced as a benchmark and
employed to demonstrate its capabilities. SpinGNN++ provides accurate
descriptions of the complex spin-lattice coupling in monolayer CrI$_3$ and
CrTe$_2$, achieving sub-meV errors. Importantly, it facilitates large-scale
parallel spin-lattice dynamics, thereby enabling the exploration of associated
properties, including the magnetic ground state and phase transition.
Remarkably, SpinGNN++ identifies a new ferrimagnetic state as the ground
magnetic state for monolayer CrTe2, thereby enriching its phase diagram and
providing deeper insights into the distinct magnetic signals observed in
various experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongyu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Boyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Liangliang Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Junyi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xingao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xiang_H/0/1/0/all/0/1&quot;&gt;Hongjun Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15120">
<title>Improved Representation of Asymmetrical Distances with Interval Quasimetric Embeddings. (arXiv:2211.15120v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15120</link>
<description rdf:parseType="Literal">&lt;p&gt;Asymmetrical distance structures (quasimetrics) are ubiquitous in our lives
and are gaining more attention in machine learning applications. Imposing such
quasimetric structures in model representations has been shown to improve many
tasks, including reinforcement learning (RL) and causal relation learning. In
this work, we present four desirable properties in such quasimetric models, and
show how prior works fail at them. We propose Interval Quasimetric Embedding
(IQE), which is designed to satisfy all four criteria. On three quasimetric
learning experiments, IQEs show strong approximation and generalization
abilities, leading to better performance and improved efficiency over prior
methods.
&lt;/p&gt;
&lt;p&gt;Project Page: https://www.tongzhouwang.info/interval_quasimetric_embedding
&lt;/p&gt;
&lt;p&gt;Quasimetric Learning Code Package:
https://www.github.com/quasimetric-learning/torch-quasimetric
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tongzhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02649">
<title>Thales: Formulating and Estimating Architectural Vulnerability Factors for DNN Accelerators. (arXiv:2212.02649v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02649</link>
<description rdf:parseType="Literal">&lt;p&gt;As Deep Neural Networks (DNNs) are increasingly deployed in safety critical
and privacy sensitive applications such as autonomous driving and biometric
authentication, it is critical to understand the fault-tolerance nature of
DNNs. Prior work primarily focuses on metrics such as Failures In Time (FIT)
rate and the Silent Data Corruption (SDC) rate, which quantify how often a
device fails. Instead, this paper focuses on quantifying the DNN accuracy given
that a transient error has occurred, which tells us how well a network behaves
when a transient error occurs. We call this metric Resiliency Accuracy (RA). We
show that existing RA formulation is fundamentally inaccurate, because it
incorrectly assumes that software variables (model weights/activations) have
equal faulty probability under hardware transient faults. We present an
algorithm that captures the faulty probabilities of DNN variables under
transient faults and, thus, provides correct RA estimations validated by
hardware. To accelerate RA estimation, we reformulate RA calculation as a Monte
Carlo integration problem, and solve it using importance sampling driven by DNN
specific heuristics. Using our lightweight RA estimation method, we show that
transient faults lead to far greater accuracy degradation than what todays DNN
resiliency tools estimate. We show how our RA estimation tool can help design
more resilient DNNs by integrating it with a Network Architecture Search
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_A/0/1/0/all/0/1&quot;&gt;Abhishek Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1&quot;&gt;Yiming Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaoshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whatmough_P/0/1/0/all/0/1&quot;&gt;Paul Whatmough&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11870">
<title>Impossibility Theorems for Feature Attribution. (arXiv:2212.11870v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11870</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite a sea of interpretability methods that can produce plausible
explanations, the field has also empirically seen many failure cases of such
methods. In light of these results, it remains unclear for practitioners how to
use these methods and choose between them in a principled way. In this paper,
we show that for moderately rich model classes (easily satisfied by neural
networks), any feature attribution method that is complete and linear -- for
example, Integrated Gradients and SHAP -- can provably fail to improve on
random guessing for inferring model behaviour. Our results apply to common
end-tasks such as characterizing local model behaviour, identifying spurious
features, and algorithmic recourse. One takeaway from our work is the
importance of concretely defining end-tasks: once such an end-task is defined,
a simple and direct approach of repeated model evaluations can outperform many
other complex feature attribution methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilodeau_B/0/1/0/all/0/1&quot;&gt;Blair Bilodeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaques_N/0/1/0/all/0/1&quot;&gt;Natasha Jaques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Been Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.14566">
<title>Pontryagin Optimal Control via Neural Networks. (arXiv:2212.14566v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2212.14566</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving real-world optimal control problems are challenging tasks, as the
complex, high-dimensional system dynamics are usually unrevealed to the
decision maker. It is thus hard to find the optimal control actions
numerically. To deal with such modeling and computation challenges, in this
paper, we integrate Neural Networks with the Pontryagin&apos;s Maximum Principle
(PMP), and propose a sample efficient framework NN-PMP-Gradient. The resulting
controller can be implemented for systems with unknown and complex dynamics. By
taking an iterative approach, the proposed framework not only utilizes the
accurate surrogate models parameterized by neural networks, it also efficiently
recovers the optimality conditions along with the optimal action sequences via
PMP conditions. Numerical simulations on Linear Quadratic Regulator, energy
arbitrage of grid-connected lossy battery, control of single pendulum, and two
MuJoCo locomotion tasks demonstrate our proposed NN-PMP-Gradient is a general
and versatile computation tool for finding optimal solutions. And compared with
the widely applied model-free and model-based reinforcement learning (RL)
algorithms, our NN-PMP-Gradient achieves higher sample-efficiency and
performance in terms of control objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_C/0/1/0/all/0/1&quot;&gt;Chengyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yize Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12767">
<title>Compression, Generalization and Learning. (arXiv:2301.12767v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12767</link>
<description rdf:parseType="Literal">&lt;p&gt;A compression function is a map that slims down an observational set into a
subset of reduced size, while preserving its informational content. In multiple
applications, the condition that one new observation makes the compressed set
change is interpreted that this observation brings in extra information and, in
learning theory, this corresponds to misclassification, or misprediction. In
this paper, we lay the foundations of a new theory that allows one to keep
control on the probability of change of compression (which maps into the
statistical &quot;risk&quot; in learning applications). Under suitable conditions, the
cardinality of the compressed set is shown to be a consistent estimator of the
probability of change of compression (without any upper limit on the size of
the compressed set); moreover, unprecedentedly tight finite-sample bounds to
evaluate the probability of change of compression are obtained under a
generally applicable condition of preference. All results are usable in a fully
agnostic setup, i.e., without requiring any a priori knowledge on the
probability distribution of the observations. Not only these results offer a
valid support to develop trust in observation-driven methodologies, they also
play a fundamental role in learning techniques as a tool for hyper-parameter
tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campi_M/0/1/0/all/0/1&quot;&gt;Marco C. Campi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garatti_S/0/1/0/all/0/1&quot;&gt;Simone Garatti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13126">
<title>LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain. (arXiv:2301.13126v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13126</link>
<description rdf:parseType="Literal">&lt;p&gt;Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niklaus_J/0/1/0/all/0/1&quot;&gt;Joel Niklaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matoshi_V/0/1/0/all/0/1&quot;&gt;Veton Matoshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1&quot;&gt;Pooja Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1&quot;&gt;Andrea Galassi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sturmer_M/0/1/0/all/0/1&quot;&gt;Matthias St&amp;#xfc;rmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1&quot;&gt;Ilias Chalkidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13128">
<title>Standardized CycleGAN training for unsupervised stain adaptation in invasive carcinoma classification for breast histopathology. (arXiv:2301.13128v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13128</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalization is one of the main challenges of computational pathology.
Slide preparation heterogeneity and the diversity of scanners lead to poor
model performance when used on data from medical centers not seen during
training. In order to achieve stain invariance in breast invasive carcinoma
patch classification, we implement a stain translation strategy using cycleGANs
for unsupervised image-to-image translation. We compare three cycleGAN-based
approaches to a baseline classification model obtained without any stain
invariance strategy. Two of the proposed approaches use cycleGAN&apos;s translations
at inference or training in order to build stain-specific classification
models. The last method uses them for stain data augmentation during training.
This constrains the classification model to learn stain-invariant features.
Baseline metrics are set by training and testing the baseline classification
model on a reference stain. We assessed performances using three medical
centers with H&amp;amp;E and H&amp;amp;E&amp;amp;S staining. Every approach tested in this study
improves baseline metrics without needing labels on target stains. The stain
augmentation-based approach produced the best results on every stain. Each
method&apos;s pros and cons are studied and discussed in this paper. However,
training highly performing cycleGANs models in itself represents a challenge.
In this work, we introduce a systematical method for optimizing cycleGAN
training by setting a novel stopping criterion. This method has the benefit of
not requiring any visual inspection of cycleGAN results and proves superiority
to methods using a predefined number of training epochs. In addition, we also
study the minimal amount of data required for cycleGAN training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nerrienet_N/0/1/0/all/0/1&quot;&gt;Nicolas Nerrienet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peyret_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Peyret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sockeel_M/0/1/0/all/0/1&quot;&gt;Marie Sockeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sockeel_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Sockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13340">
<title>Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning. (arXiv:2301.13340v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13340</link>
<description rdf:parseType="Literal">&lt;p&gt;Hard negative mining has shown effective in enhancing self-supervised
contrastive learning (CL) on diverse data types, including graph CL (GCL). The
existing hardness-aware CL methods typically treat negative instances that are
most similar to the anchor instance as hard negatives, which helps improve the
CL performance, especially on image data. However, this approach often fails to
identify the hard negatives but leads to many false negatives on graph data.
This is mainly due to that the learned graph representations are not
sufficiently discriminative due to oversmooth representations and/or
non-independent and identically distributed (non-i.i.d.) issues in graph data.
To tackle this problem, this article proposes a novel approach that builds a
discriminative model on collective affinity information (i.e., two sets of
pairwise affinities between the negative instances and the anchor instance) to
mine hard negatives in GCL. In particular, the proposed approach evaluates how
confident/uncertain the discriminative model is about the affinity of each
negative instance to an anchor instance to determine its hardness weight
relative to the anchor instance. This uncertainty information is then
incorporated into the existing GCL loss functions via a weighting term to
enhance their performance. The enhanced GCL is theoretically grounded that the
resulting GCL loss is equivalent to a triplet loss with an adaptive margin
being exponentially proportional to the learned uncertainty of each negative
instance. Extensive experiments on ten graph datasets show that our approach
does the following: 1) consistently enhances different state-of-the-art (SOTA)
GCL methods in both graph and node classification tasks and 2) significantly
improves their robustness against adversarial attacks. Code is available at
https://github.com/mala-lab/AUGCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1&quot;&gt;Chaoxi Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00919">
<title>QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models. (arXiv:2302.00919v4 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00919</link>
<description rdf:parseType="Literal">&lt;p&gt;In practical compressed sensing (CS), the obtained measurements typically
necessitate quantization to a limited number of bits prior to transmission or
storage. This nonlinear quantization process poses significant recovery
challenges, particularly with extreme coarse quantization such as 1-bit.
Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS
(QCS) which utilizes score-based generative models (SGM) as an implicit prior.
Due to the adeptness of SGM in capturing the intricate structures of natural
signals, QCS-SGM substantially outperforms previous QCS methods. However,
QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices as
the computation of the likelihood score becomes intractable otherwise. To
address this limitation, we introduce an advanced variant of QCS-SGM, termed
QCS-SGM+, capable of handling general matrices effectively. The key idea is a
Bayesian inference perspective on the likelihood score computation, wherein
expectation propagation is employed for its approximate computation. Extensive
experiments are conducted, demonstrating the substantial superiority of
QCS-SGM+ over QCS-SGM for general sensing matrices beyond mere
row-orthogonality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangming Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kabashima_Y/0/1/0/all/0/1&quot;&gt;Yoshiyuki Kabashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00997">
<title>Constrained Online Two-stage Stochastic Optimization: Near Optimal Algorithms via Adversarial Learning. (arXiv:2302.00997v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00997</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider an online two-stage stochastic optimization with long-term
constraints over a finite horizon of $T$ periods. At each period, we take the
first-stage action, observe a model parameter realization and then take the
second-stage action from a feasible set that depends both on the first-stage
decision and the model parameter. We aim to minimize the cumulative objective
value while guaranteeing that the long-term average second-stage decision
belongs to a set. We develop online algorithms for the online two-stage problem
from adversarial learning algorithms. Also, the regret bound of our algorithm
cam be reduced to the regret bound of embedded adversarial learning algorithms.
Based on our framework, we obtain new results under various settings. When the
model parameter at each period is drawn from identical distributions, we derive
\textit{state-of-art} $O(\sqrt{T})$ regret that improves previous bounds under
special cases. Our algorithm is also robust to adversarial corruptions of model
parameter realizations. When the model parameters are drawn from unknown
non-stationary distributions and we are given machine-learned predictions of
the distributions, we develop a new algorithm from our framework with a regret
$O(W_T+\sqrt{T})$, where $W_T$ measures the total inaccuracy of the
machine-learned predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiashuo Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03068">
<title>Evaluating Self-Supervised Learning via Risk Decomposition. (arXiv:2302.03068v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03068</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) pipelines differ in many design choices such
as the architecture, augmentations, or pretraining data. Yet SSL is typically
evaluated using a single metric: linear probing on ImageNet. This does not
provide much insight into why or when a model is better, now how to improve it.
To address this, we propose an SSL risk decomposition, which generalizes the
classical supervised approximation-estimation decomposition by considering
errors arising from the representation learning step. Our decomposition
consists of four error components: approximation, representation usability,
probe generalization, and encoder generalization. We provide efficient
estimators for each component and use them to analyze the effect of 30 design
choices on 169 SSL vision models evaluated on ImageNet. Our analysis gives
valuable insights for designing and using SSL models. For example, it
highlights the main sources of error and shows how to improve SSL in specific
settings (full- vs few-shot) by trading off error components. All results and
pretrained models are at https://github.com/YannDubs/SSL-Risk-Decomposition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13850">
<title>Towards Learning and Explaining Indirect Causal Effects in Neural Networks. (arXiv:2303.13850v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13850</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been a growing interest in learning and explaining causal
effects within Neural Network (NN) models. By virtue of NN architectures,
previous approaches consider only direct and total causal effects assuming
independence among input variables. We view an NN as a structural causal model
(SCM) and extend our focus to include indirect causal effects by introducing
feedforward connections among input neurons. We propose an ante-hoc method that
captures and maintains direct, indirect, and total causal effects during NN
model training. We also propose an algorithm for quantifying learned causal
effects in an NN model and efficient approximation strategies for quantifying
causal effects in high-dimensional data. Extensive experiments conducted on
synthetic and real-world datasets demonstrate that the causal effects learned
by our ante-hoc method better approximate the ground truth effects compared to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Abbavaram Gowtham Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachu_S/0/1/0/all/0/1&quot;&gt;Saketh Bachu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_H/0/1/0/all/0/1&quot;&gt;Harsharaj Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Godfrey_B/0/1/0/all/0/1&quot;&gt;Benin L Godfrey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N. Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1&quot;&gt;Varshaneya V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kar_S/0/1/0/all/0/1&quot;&gt;Satya Narayanan Kar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16113">
<title>Graph Neural Networks for Power Allocation in Wireless Networks with Full Duplex Nodes. (arXiv:2303.16113v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16113</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to mutual interference between users, power allocation problems in
wireless networks are often non-convex and computationally challenging. Graph
neural networks (GNNs) have recently emerged as a promising approach to
tackling these problems and an approach that exploits the underlying topology
of wireless networks. In this paper, we propose a novel graph representation
method for wireless networks that include full-duplex (FD) nodes. We then
design a corresponding FD Graph Neural Network (F-GNN) with the aim of
allocating transmit powers to maximise the network throughput. Our results show
that our F-GNN achieves state-of-art performance with significantly less
computation time. Besides, F-GNN offers an excellent trade-off between
performance and complexity compared to classical approaches. We further refine
this trade-off by introducing a distance-based threshold for inclusion or
exclusion of edges in the network. We show that an appropriately chosen
threshold reduces required training time by roughly 20% with a relatively minor
loss in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingge Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_J/0/1/0/all/0/1&quot;&gt;Jamie Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08242">
<title>The Deep Latent Position Topic Model for Clustering and Representation of Networks with Textual Edges. (arXiv:2304.08242v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08242</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerical interactions leading to users sharing textual content published by
others are naturally represented by a network where the individuals are
associated with the nodes and the exchanged texts with the edges. To understand
those heterogeneous and complex data structures, clustering nodes into
homogeneous groups as well as rendering a comprehensible visualisation of the
data is mandatory. To address both issues, we introduce Deep-LPTM, a
model-based clustering strategy relying on a variational graph auto-encoder
approach as well as a probabilistic model to characterise the topics of
discussion. Deep-LPTM allows to build a joint representation of the nodes and
of the edges in two embeddings spaces. The parameters are inferred using a
variational inference algorithm. We also introduce IC2L, a model selection
criterion specifically designed to choose models with relevant clustering and
visualisation properties. An extensive benchmark study on synthetic data is
provided. In particular, we find that Deep-LPTM better recovers the partitions
of the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails
of the Enron company are analysed and visualisations of the results are
presented, with meaningful highlights of the graph structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutin_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Boutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latouche_P/0/1/0/all/0/1&quot;&gt;Pierre Latouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouveyron_C/0/1/0/all/0/1&quot;&gt;Charles Bouveyron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02375">
<title>MaskSearch: Querying Image Masks at Scale. (arXiv:2305.02375v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02375</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning tasks over image databases often generate masks that
annotate image content (e.g., saliency maps, segmentation maps, depth maps) and
enable a variety of applications (e.g., determine if a model is learning
spurious correlations or if an image was maliciously modified to mislead a
model). While queries that retrieve examples based on mask properties are
valuable to practitioners, existing systems do not support them efficiently. In
this paper, we formalize the problem and propose MaskSearch, a system that
focuses on accelerating queries over databases of image masks while
guaranteeing the correctness of query results. MaskSearch leverages a novel
indexing technique and an efficient filter-verification query execution
framework. Experiments with our prototype show that MaskSearch, using indexes
approximately 5% of the compressed data size, accelerates individual queries by
up to two orders of magnitude and consistently outperforms existing methods on
various multi-query workloads that simulate dataset exploration and analysis
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Dong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daum_M/0/1/0/all/0/1&quot;&gt;Maureen Daum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balazinska_M/0/1/0/all/0/1&quot;&gt;Magdalena Balazinska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02657">
<title>On the Eigenvalue Decay Rates of a Class of Neural-Network Related Kernel Functions Defined on General Domains. (arXiv:2305.02657v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02657</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide a strategy to determine the eigenvalue decay rate
(EDR) of a large class of kernel functions defined on a general domain rather
than $\mathbb S^{d}$. This class of kernel functions include but are not
limited to the neural tangent kernel associated with neural networks with
different depths and various activation functions. After proving that the
dynamics of training the wide neural networks uniformly approximated that of
the neural tangent kernel regression on general domains, we can further
illustrate the minimax optimality of the wide neural network provided that the
underground truth function $f\in [\mathcal H_{\mathrm{NTK}}]^{s}$, an
interpolation space associated with the RKHS $\mathcal{H}_{\mathrm{NTK}}$ of
NTK. We also showed that the overfitted neural network can not generalize well.
We believe our approach for determining the EDR of kernels might be also of
independent interests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zixiong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qian Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03711">
<title>Medical records condensation: a roadmap towards healthcare data democratisation. (arXiv:2305.03711v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03711</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of artificial intelligence (AI) has envisioned an era of
healthcare democratisation that promises every stakeholder a new and better way
of life. However, the advancement of clinical AI research is significantly
hurdled by the dearth of data democratisation in healthcare. To truly
democratise data for AI studies, challenges are two-fold: 1. the sensitive
information in clinical data should be anonymised appropriately, and 2.
AI-oriented clinical knowledge should flow freely across organisations. This
paper considers a recent deep-learning advent, dataset condensation (DC), as a
stone that kills two birds in democratising healthcare data. The condensed data
after DC, which can be viewed as statistical metadata, abstracts original
clinical records and irreversibly conceals sensitive information at individual
levels; nevertheless, it still preserves adequate knowledge for learning deep
neural networks (DNNs). More favourably, the compressed volumes and the
accelerated model learnings of condensed data portray a more efficient clinical
knowledge sharing and flowing system, as necessitated by data democratisation.
We underline DC&apos;s prospects for democratising clinical data, specifically
electrical healthcare records (EHRs), for AI research through experimental
results and analysis across three healthcare datasets of varying data types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yujiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1&quot;&gt;Anshul Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mingzhi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1&quot;&gt;Stavros Petridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Li Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tingting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1&quot;&gt;David A. Clifton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05126">
<title>Comparing Foundation Models using Data Kernels. (arXiv:2305.05126v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05126</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in self-supervised learning and neural network scaling have
enabled the creation of large models, known as foundation models, which can be
easily adapted to a wide range of downstream tasks. The current paradigm for
comparing foundation models involves evaluating them with aggregate metrics on
various benchmark datasets. This method of model comparison is heavily
dependent on the chosen evaluation metric, which makes it unsuitable for
situations where the ideal metric is either not obvious or unavailable. In this
work, we present a methodology for directly comparing the embedding space
geometry of foundation models, which facilitates model comparison without the
need for an explicit evaluation metric. Our methodology is grounded in random
graph theory and enables valid hypothesis testing of embedding similarity on a
per-datum basis. Further, we demonstrate how our methodology can be extended to
facilitate population level model comparison. In particular, we show how our
framework can induce a manifold of models equipped with a distance function
that correlates strongly with several downstream metrics. We remark on the
utility of this population level model comparison as a first step towards a
taxonomic science of foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duderstadt_B/0/1/0/all/0/1&quot;&gt;Brandon Duderstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helm_H/0/1/0/all/0/1&quot;&gt;Hayden S. Helm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05465">
<title>The emergence of clusters in self-attention dynamics. (arXiv:2305.05465v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05465</link>
<description rdf:parseType="Literal">&lt;p&gt;Viewing Transformers as interacting particle systems, we describe the
geometry of learned representations when the weights are not time dependent. We
show that particles, representing tokens, tend to cluster toward particular
limiting objects as time tends to infinity. Cluster locations are determined by
the initial tokens, confirming context-awareness of representations learned by
Transformers. Using techniques from dynamical systems and partial differential
equations, we show that the type of limiting object that emerges depends on the
spectrum of the value matrix. Additionally, in the one-dimensional case we
prove that the self-attention matrix converges to a low-rank Boolean matrix.
The combination of these results mathematically confirms the empirical
observation made by Vaswani et al. [VSP&apos;17] that leaders appear in a sequence
of tokens when processed by Transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geshkovski_B/0/1/0/all/0/1&quot;&gt;Borjan Geshkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letrouit_C/0/1/0/all/0/1&quot;&gt;Cyril Letrouit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyanskiy_Y/0/1/0/all/0/1&quot;&gt;Yury Polyanskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigollet_P/0/1/0/all/0/1&quot;&gt;Philippe Rigollet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14387">
<title>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their strong instruction-following abilities. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following requires
tackling three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 50x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1&quot;&gt;Ishaan Gulrajani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01951">
<title>GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction. (arXiv:2306.01951v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01951</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes
within graphs, finding applications in network security, fraud detection,
social media spam detection, and various other domains. A common method for GAD
is Graph Auto-Encoders (GAEs), which encode graph data into node
representations and identify anomalies by assessing the reconstruction quality
of the graphs based on these representations. However, existing GAE models are
primarily optimized for direct link reconstruction, resulting in nodes
connected in the graph being clustered in the latent space. As a result, they
excel at detecting cluster-type structural anomalies but struggle with more
complex structural anomalies that do not conform to clusters. To address this
limitation, we propose a novel solution called GAD-NR, a new variant of GAE
that incorporates neighborhood reconstruction for graph anomaly detection.
GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the
local structure, self-attributes, and neighbor attributes, based on the
corresponding node representation. By comparing the neighborhood reconstruction
loss between anomalous nodes and normal nodes, GAD-NR can effectively detect
any anomalies. Extensive experimentation conducted on six real-world datasets
validates the effectiveness of GAD-NR, showcasing significant improvements (by
up to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NR
is openly available. Importantly, the comparative analysis reveals that the
existing methods perform well only in detecting one or two types of anomalies
out of the three types studied. In contrast, GAD-NR excels at detecting all
three types of anomalies across the datasets, demonstrating its comprehensive
anomaly detection capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Amit Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1&quot;&gt;Juan Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Carl Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elshocht_O/0/1/0/all/0/1&quot;&gt;Olivier Elshocht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smeets_J/0/1/0/all/0/1&quot;&gt;Jeroen Smeets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10592">
<title>Conditional expectation using compactification operators. (arXiv:2306.10592v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10592</link>
<description rdf:parseType="Literal">&lt;p&gt;The separate tasks of denoising, least squares expectation, and manifold
learning can often be posed in a common setting of finding the conditional
expectations arising from a product of two random variables. This paper focuses
on this more general problem and describes an operator theoretic approach to
estimating the conditional expectation. Kernel integral operators are used as a
compactification tool, to set up the estimation problem as a linear inverse
problem in a reproducing kernel Hilbert space. This equation is shown to have
solutions that allow numerical approximation, thus guaranteeing the convergence
of data-driven implementations. The overall technique is easy to implement, and
their successful application to some real-world problems are also shown.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Suddhasattwa Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10694">
<title>On the Model-Misspecification in Reinforcement Learning. (arXiv:2306.10694v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10694</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of reinforcement learning (RL) crucially depends on effective
function approximation when dealing with complex ground-truth models. Existing
sample-efficient RL algorithms primarily employ three approaches to function
approximation: policy-based, value-based, and model-based methods. However, in
the face of model misspecification (a disparity between the ground-truth and
optimal function approximators), it is shown that policy-based approaches can
be robust even when the policy function approximation is under a large
locally-bounded misspecification error, with which the function class may
exhibit a $\Omega(1)$ approximation error in specific states and actions, but
remains small on average within a policy-induced state distribution. Yet it
remains an open question whether similar robustness can be achieved with
value-based and model-based approaches, especially with general function
approximation.
&lt;/p&gt;
&lt;p&gt;To bridge this gap, in this paper we present a unified theoretical framework
for addressing model misspecification in RL. We demonstrate that, through
meticulous algorithm design and sophisticated analysis, value-based and
model-based methods employing general function approximation can achieve
robustness under local misspecification error bounds. In particular, they can
attain a regret bound of $\widetilde{O}\left(\text{poly}(d H)(\sqrt{K} +
K\zeta) \right)$, where $d$ represents the complexity of the function class,
$H$ is the episode length, $K$ is the total number of episodes, and $\zeta$
denotes the local bound for misspecification error. Furthermore, we propose an
algorithmic framework that can achieve the same order of regret bound without
prior knowledge of $\zeta$, thereby enhancing its practical applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunfan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11335">
<title>DamWorld: Progressive Reasoning with World Models for Robotic Manipulation. (arXiv:2306.11335v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11335</link>
<description rdf:parseType="Literal">&lt;p&gt;The research on embodied AI has greatly promoted the development of robot
manipulation. However, it still faces significant challenges in various aspects
such as benchmark construction, multi-modal perception and decision-making, and
physical execution. Previous robot manipulation simulators were primarily
designed to enrich manipulation types and types of objects while neglecting the
balance between physical manipulation and language instruction complexity in
multi-modal environments. This paper proposes a new robot manipulation
simulator and builds a comprehensive and systematic robot manipulation
benchmark with progressive reasoning tasks called SeaWave (i.e., a progressive
reasoning benchmark). It provides a standard test platform for embedded AI
agents in a multi-modal environment, which can evaluate and execute four levels
of human natural language instructions at the same time.
&lt;/p&gt;
&lt;p&gt;Previous world model-based robot manipulation work lacked research on the
perception and decision-making of complex instructions in multi-modal
environments. To this end, we propose a new world model tailored for
cross-modal robot manipulation called DamWorld. Specifically, DamWorld takes
the current visual scene and predicted execution actions based on natural
language instructions as input, and uses the next action frame to supervise the
output of the world model to force the model to learn robot manipulation
consistent with world knowledge. Compared with the renowned baselines (e.g.,
RT-1), our DamWorld improves the manipulation success rate by 5.6% on average
on four levels of progressive reasoning tasks. It is worth noting that on the
most challenging level 4 manipulation task, DamWorld still improved by 9.0%
compared to prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengzhen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaidong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hetao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengda Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Mas Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15909">
<title>RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15909</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as
promising approaches for learning data-efficient RL algorithms tailored to a
given task distribution. However, these RL algorithms struggle with
long-horizon tasks and out-of-distribution tasks since they rely on recurrent
neural networks to process the sequence of experiences instead of summarizing
them into general RL components such as value functions. Moreover, even
transformers have a practical limit to the length of histories they can
efficiently reason about before training and inference costs become
prohibitive. In contrast, traditional RL algorithms are data-inefficient since
they do not leverage domain knowledge, but they do converge to an optimal
policy as more data becomes available. In this paper, we propose RL$^3$, a
principled hybrid approach that combines traditional RL and meta-RL by
incorporating task-specific action-values learned through traditional RL as an
input to the meta-RL neural network. We show that RL$^3$ earns greater
cumulative reward on long-horizon and out-of-distribution tasks compared to
RL$^2$, while maintaining the efficiency of the latter in the short term.
Experiments are conducted on both custom and benchmark discrete domains from
the meta-RL literature that exhibit a range of short-term, long-term, and
complex dependencies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_A/0/1/0/all/0/1&quot;&gt;Abhinav Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nashed_S/0/1/0/all/0/1&quot;&gt;Samer B. Nashed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilberstein_S/0/1/0/all/0/1&quot;&gt;Shlomo Zilberstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07449">
<title>Differentially Private Clustering in Data Streams. (arXiv:2307.07449v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07449</link>
<description rdf:parseType="Literal">&lt;p&gt;The streaming model is an abstraction of computing over massive data streams,
which is a popular way of dealing with large-scale modern data analysis. In
this model, there is a stream of data points, one after the other. A streaming
algorithm is only allowed one pass over the data stream, and the goal is to
perform some analysis during the stream while using as small space as possible.
Clustering problems (such as $k$-means and $k$-median) are fundamental
unsupervised machine learning primitives, and streaming clustering algorithms
have been extensively studied in the past. However, since data privacy becomes
a central concern in many real-world applications, non-private clustering
algorithms are not applicable in many scenarios.
&lt;/p&gt;
&lt;p&gt;In this work, we provide the first differentially private streaming
algorithms for $k$-means and $k$-median clustering of $d$-dimensional Euclidean
data points over a stream with length at most $T$ using $poly(k,d,\log(T))$
space to achieve a constant multiplicative error and a $poly(k,d,\log(T))$
additive error. In particular, we present a differentially private streaming
clustering framework which only requires an offline DP coreset or clustering
algorithm as a blackbox. By plugging in existing results from DP clustering
Ghazi, Kumar, Manurangsi 2020 and Kaplan, Stemmer 2018, we achieve (1) a
$(1+\gamma)$-multiplicative approximation with
$\tilde{O}_\gamma(poly(k,d,\log(T)))$ space for any $\gamma&amp;gt;0$, and the
additive error is $poly(k,d,\log(T))$ or (2) an $O(1)$-multiplicative
approximation with $\tilde{O}(k^{1.5} \cdot poly(d,\log(T)))$ space and
$poly(k,d,\log(T))$ additive error. In addition, our algorithmic framework is
also differentially private under the continual release setting, i.e., the
union of outputs of our algorithms at every timestamp is always differentially
private.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epasto_A/0/1/0/all/0/1&quot;&gt;Alessandro Epasto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1&quot;&gt;Tamalika Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Peilin Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08919">
<title>Systematic comparison of semi-supervised and self-supervised learning for medical image classification. (arXiv:2307.08919v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;In many medical image classification problems, labeled data is scarce while
unlabeled data is more available. Semi-supervised learning and self-supervised
learning are two different research directions that can improve accuracy by
learning from extra unlabeled data. Recent methods from both directions have
reported significant gains on traditional benchmarks. Yet past benchmarks do
not focus on medical tasks and rarely compare self- and semi- methods together
on equal footing. Furthermore, past benchmarks often handle hyperparameter
tuning suboptimally. First, they may not tune hyperparameters at all, leading
to underfitting. Second, when tuning does occur, it often unrealistically uses
a labeled validation set much larger than the train set. Both cases make
previously published rankings of methods difficult to translate to practical
settings. This study contributes a systematic evaluation of self- and semi-
methods with a unified experimental protocol intended to guide a practitioner
with scarce overall labeled data and a limited compute budget. We answer two
key questions: Can hyperparameter tuning be effective with realistic-sized
validation sets? If so, when all methods are tuned well, which self- or
semi-supervised methods reach the best accuracy? Our study compares 13
representative semi- and self-supervised methods to strong labeled-set-only
baselines on 4 medical datasets. From 20000+ total GPU hours of computation, we
provide valuable best practices to resource-constrained, results-focused
practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Ruijie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aeron_S/0/1/0/all/0/1&quot;&gt;Shuchin Aeron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Michael C. Hughes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09312">
<title>Multi-Modal Discussion Transformer: Integrating Text, Images and Graph Transformers to Detect Hate Speech on Social Media. (arXiv:2307.09312v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09312</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_L/0/1/0/all/0/1&quot;&gt;Liam Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1&quot;&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivas_N/0/1/0/all/0/1&quot;&gt;Nanda Kishore Sreenivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1&quot;&gt;Lukasz Golab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_R/0/1/0/all/0/1&quot;&gt;Robin Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16424">
<title>MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning. (arXiv:2307.16424v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16424</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipping a deep model the abaility of few-shot learning, i.e., learning
quickly from only few examples, is a core challenge for artificial
intelligence. Gradient-based meta-learning approaches effectively address the
challenge by learning how to learn novel tasks. Its key idea is learning a deep
model in a bi-level optimization manner, where the outer-loop process learns a
shared gradient descent algorithm (i.e., its hyperparameters), while the
inner-loop process leverage it to optimize a task-specific model by using only
few labeled data. Although these existing methods have shown superior
performance, the outer-loop process requires calculating second-order
derivatives along the inner optimization path, which imposes considerable
memory burdens and the risk of vanishing gradients. Drawing inspiration from
recent progress of diffusion models, we find that the inner-loop gradient
descent process can be actually viewed as a reverse process (i.e., denoising)
of diffusion where the target of denoising is model weights but the origin
data. Based on this fact, in this paper, we propose to model the gradient
descent optimizer as a diffusion model and then present a novel
task-conditional diffusion-based meta-learning, called MetaDiff, that
effectively models the optimization process of model weights from Gaussion
noises to target weights in a denoising manner. Thanks to the training
efficiency of diffusion models, our MetaDiff do not need to differentiate
through the inner-loop path such that the memory burdens and the risk of
vanishing gradients can be effectvely alleviated. Experiment results show that
our MetaDiff outperforms the state-of-the-art gradient-based meta-learning
family in few-shot learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chuyao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Demin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Huiwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xutao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunming Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01562">
<title>Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity. (arXiv:2308.01562v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01562</link>
<description rdf:parseType="Literal">&lt;p&gt;While a practical wireless network has many tiers where end users do not
directly communicate with the central server, the users&apos; devices have limited
computation and battery powers, and the serving base station (BS) has a fixed
bandwidth. Owing to these practical constraints and system models, this paper
leverages model pruning and proposes a pruning-enabled hierarchical federated
learning (PHFL) in heterogeneous networks (HetNets). We first derive an upper
bound of the convergence rate that clearly demonstrates the impact of the model
pruning and wireless communications between the clients and the associated BS.
Then we jointly optimize the model pruning ratio, central processing unit (CPU)
frequency and transmission power of the clients in order to minimize the
controllable terms of the convergence bound under strict delay and energy
constraints. However, since the original problem is not convex, we perform
successive convex approximation (SCA) and jointly optimize the parameters for
the relaxed convex problem. Through extensive simulation, we validate the
effectiveness of our proposed PHFL algorithm in terms of test accuracy, wall
clock time, energy consumption and bandwidth requirement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pervej_M/0/1/0/all/0/1&quot;&gt;Md Ferdous Pervej&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Richeng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huaiyu Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03735">
<title>Randomized algorithms for precise measurement of differentially-private, personalized recommendations. (arXiv:2308.03735v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03735</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalized recommendations form an important part of today&apos;s internet
ecosystem, helping artists and creators to reach interested users, and helping
users to discover new and engaging content. However, many users today are
skeptical of platforms that personalize recommendations, in part due to
historically careless treatment of personal data and data privacy. Now,
businesses that rely on personalized recommendations are entering a new
paradigm, where many of their systems must be overhauled to be privacy-first.
In this article, we propose an algorithm for personalized recommendations that
facilitates both precise and differentially-private measurement. We consider
advertising as an example application, and conduct offline experiments to
quantify how the proposed privacy-preserving algorithm affects key metrics
related to user experience, advertiser value, and platform revenue compared to
the extremes of both (private) non-personalized and non-private, personalized
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laro_A/0/1/0/all/0/1&quot;&gt;Allegra Laro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aghazadeh_B/0/1/0/all/0/1&quot;&gt;Babak Aghazadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11700">
<title>SuperCalo: Calorimeter shower super-resolution. (arXiv:2308.11700v2 [physics.ins-det] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11700</link>
<description rdf:parseType="Literal">&lt;p&gt;Calorimeter shower simulation is a major bottleneck in the Large Hadron
Collider computational pipeline. There have been recent efforts to employ
deep-generative surrogate models to overcome this challenge. However, many of
best performing models have training and generation times that do not scale
well to high-dimensional calorimeter showers. In this work, we introduce
SuperCalo, a flow-based super-resolution model, and demonstrate that
high-dimensional fine-grained calorimeter showers can be quickly upsampled from
coarse-grained showers. This novel approach presents a way to reduce
computational cost, memory requirements and generation time associated with
fast calorimeter simulation models. Additionally, we show that the showers
upsampled by SuperCalo possess a high degree of variation. This allows a large
number of high-dimensional calorimeter showers to be upsampled from much fewer
coarse showers with high-fidelity, which results in additional reduction in
generation time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pang_I/0/1/0/all/0/1&quot;&gt;Ian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Raine_J/0/1/0/all/0/1&quot;&gt;John Andrew Raine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shih_D/0/1/0/all/0/1&quot;&gt;David Shih&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13537">
<title>STEM: Unleashing the Power of Embeddings for Multi-task Recommendation. (arXiv:2308.13537v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13537</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) has gained significant popularity in recommender
systems as it enables simultaneous optimization of multiple objectives. A key
challenge in MTL is negative transfer, but existing studies explored negative
transfer on all samples, overlooking the inherent complexities within them. We
split the samples according to the relative amount of positive feedback among
tasks. Surprisingly, negative transfer still occurs in existing MTL methods on
samples that receive comparable feedback across tasks. Existing work commonly
employs a shared-embedding paradigm, limiting the ability of modeling diverse
user preferences on different tasks. In this paper, we introduce a novel Shared
and Task-specific EMbeddings (STEM) paradigm that aims to incorporate both
shared and task-specific embeddings to effectively capture task-specific user
preferences. Under this paradigm, we propose a simple model STEM-Net, which is
equipped with an All Forward Task-specific Backward gating network to
facilitate the learning of task-specific embeddings and direct knowledge
transfer across tasks. Remarkably, STEM-Net demonstrates exceptional
performance on comparable samples, achieving positive transfer. Comprehensive
evaluation on three public MTL recommendation datasets demonstrates that
STEM-Net outperforms state-of-the-art models by a substantial margin. Our code
is released at https://github.com/LiangcaiSu/STEM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1&quot;&gt;Liangcai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Junwei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Ximei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_S/0/1/0/all/0/1&quot;&gt;Shijie Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xihua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jie Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13819">
<title>Guaranteed Stable Quadratic Models and their applications in SINDy and Operator Inference. (arXiv:2308.13819v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13819</link>
<description rdf:parseType="Literal">&lt;p&gt;Scientific machine learning for inferring dynamical systems combines
data-driven modeling, physics-based modeling, and empirical knowledge. It plays
an essential role in engineering design and digital twinning. In this work, we
primarily focus on an operator inference methodology that builds dynamical
models, preferably in low-dimension, with a prior hypothesis on the model
structure, often determined by known physics or given by experts. Then, for
inference, we aim to learn the operators of a model by setting up an
appropriate optimization problem. One of the critical properties of dynamical
systems is stability. However, this property is not guaranteed by the inferred
models. In this work, we propose inference formulations to learn quadratic
models, which are stable by design. Precisely, we discuss the parameterization
of quadratic systems that are locally and globally stable. Moreover, for
quadratic systems with no stable point yet bounded (e.g., chaotic Lorenz
model), we discuss how to parameterize such bounded behaviors in the learning
process. Using those parameterizations, we set up inference problems, which are
then solved using a gradient-based optimization method. Furthermore, to avoid
numerical derivatives and still learn continuous systems, we make use of an
integral form of differential equations. We present several numerical examples,
illustrating the preservation of stability and discussing its comparison with
the existing state-of-the-art approach to infer operators. By means of
numerical examples, we also demonstrate how the proposed methods are employed
to discover governing equations and energy-preserving models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Pawan Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duff_I/0/1/0/all/0/1&quot;&gt;Igor Pontes Duff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benner_P/0/1/0/all/0/1&quot;&gt;Peter Benner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15107">
<title>Stochastic Graph Bandit Learning with Side-Observations. (arXiv:2308.15107v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15107</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the stochastic contextual bandit with general
function space and graph feedback. We propose an algorithm that addresses this
problem by adapting to both the underlying graph structures and reward gaps. To
the best of our knowledge, our algorithm is the first to provide a
gap-dependent upper bound in this stochastic setting, bridging the research gap
left by the work in [35]. In comparison to [31,33,35], our method offers
improved regret upper bounds and does not require knowledge of graphical
quantities. We conduct numerical experiments to demonstrate the computational
efficiency and effectiveness of our approach in terms of regret upper bounds.
These findings highlight the significance of our algorithm in advancing the
field of stochastic contextual bandits with graph feedback, opening up avenues
for practical applications in various domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xueping Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02045">
<title>Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies. (arXiv:2309.02045v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02045</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have made significant strides in both scientific
research and practical applications. Existing studies have demonstrated the
state-of-the-art (SOTA) performance of LLMs in various natural language
processing tasks. However, the question of how to further enhance LLMs&apos;
performance in specific task using prompting strategies remains a pivotal
concern. This paper explores the enhancement of LLMs&apos; performance in sentiment
analysis through the application of prompting strategies. We formulate the
process of prompting for sentiment analysis tasks and introduce two novel
strategies tailored for sentiment analysis: RolePlaying (RP) prompting and
Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT
prompting strategy which is a combination of RP prompting and CoT prompting. We
conduct comparative experiments on three distinct domain datasets to evaluate
the effectiveness of the proposed sentiment analysis strategies. The results
demonstrate that the adoption of the proposed prompting strategies leads to a
increasing enhancement in sentiment analysis accuracy. Further, the CoT
prompting strategy exhibits a notable impact on implicit sentiment analysis,
with the RP-CoT prompting strategy delivering the most superior performance
among all strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yajing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zongwei Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03447">
<title>Broadband Ground Motion Synthesis via Generative Adversarial Neural Operators: Development and Validation. (arXiv:2309.03447v2 [physics.geo-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03447</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a data-driven model for ground-motion synthesis using a Generative
Adversarial Neural Operator (GANO) that combines recent advancements in machine
learning and open access strong motion data sets to generate three-component
acceleration time histories conditioned on moment magnitude ($M$), rupture
distance ($R_{rup}$), time-average shear-wave velocity at the top $30m$
($V_{S30}$), and tectonic environment or style of faulting. We use Neural
Operators, a resolution invariant architecture that guarantees that the model
training is independent of the data sampling frequency. We first present the
conditional ground-motion synthesis algorithm (referred to heretofore as
cGM-GANO) and discuss its advantages compared to previous work. Next, we verify
the cGM-GANO framework using simulated ground motions generated with the
Southern California Earthquake Center (SCEC) Broadband Platform (BBP). We
lastly train cGM-GANO on a KiK-net dataset from Japan, showing that the
framework can recover the magnitude, distance, and $V_{S30}$ scaling of Fourier
amplitude and pseudo-spectral accelerations. We evaluate cGM-GANO through
residual analysis with the empirical dataset as well as by comparison with
conventional Ground Motion Models (GMMs) for selected ground motion scenarios.
Results show that cGM-GANO produces consistent median scaling with the GMMs for
the corresponding tectonic environments. The largest misfit is observed at
short distances due to the scarcity of training data. With the exception of
short distances, the aleatory variability of the response spectral ordinates is
also well captured, especially for subduction events due to the adequacy of
training data. Applications of the presented framework include generation of
risk-targeted ground motions for site-specific engineering applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yaozhong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lavrentiadis_G/0/1/0/all/0/1&quot;&gt;Grigorios Lavrentiadis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Asimaki_D/0/1/0/all/0/1&quot;&gt;Domniki Asimaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ross_Z/0/1/0/all/0/1&quot;&gt;Zachary E. Ross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Azizzadenesheli_K/0/1/0/all/0/1&quot;&gt;Kamyar Azizzadenesheli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04339">
<title>Online Submodular Maximization via Online Convex Optimization. (arXiv:2309.04339v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;We study monotone submodular maximization under general matroid constraints
in the online setting. We prove that online optimization of a large class of
submodular functions, namely, weighted threshold potential functions, reduces
to online convex optimization (OCO). This is precisely because functions in
this class admit a concave relaxation; as a result, OCO policies, coupled with
an appropriate rounding scheme, can be used to achieve sublinear regret in the
combinatorial setting. We show that our reduction extends to many different
versions of the online learning problem, including the dynamic regret, bandit,
and optimistic-learning settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1&quot;&gt;Tareq Si Salem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozcan_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde &amp;#xd6;zcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaou_I/0/1/0/all/0/1&quot;&gt;Iasonas Nikolaou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzi_E/0/1/0/all/0/1&quot;&gt;Evimaria Terzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1&quot;&gt;Stratis Ioannidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08023">
<title>USM-SCD: Multilingual Speaker Change Detection Based on Large Pretrained Foundation Models. (arXiv:2309.08023v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08023</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a multilingual speaker change detection model (USM-SCD) that can
simultaneously detect speaker turns and perform ASR for 96 languages. This
model is adapted from a speech foundation model trained on a large quantity of
supervised and unsupervised data, demonstrating the utility of fine-tuning from
a large generic foundation model for a downstream task. We analyze the
performance of this multilingual speaker change detection model through a
series of ablation studies. We show that the USM-SCD model can achieve more
than 75% average speaker change detection F1 score across a test set that
consists of data from 96 languages. On American English, the USM-SCD model can
achieve an 85.8% speaker change detection F1 score across various public and
internal test sets, beating the previous monolingual baseline model by 21%
relative. We also show that we only need to fine-tune one-quarter of the
trainable model parameters to achieve the best model performance. The USM-SCD
model exhibits state-of-the-art ASR quality compared with a strong public ASR
baseline, making it suitable to handle both tasks with negligible additional
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guanlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pelecanos_J/0/1/0/all/0/1&quot;&gt;Jason Pelecanos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hank Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiling Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Han Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00806">
<title>Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00806</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a general theory to optimize the frequentist regret for sequential
learning problems, where efficient bandit and reinforcement learning algorithms
can be derived from unified Bayesian principles. We propose a novel
optimization approach to generate &quot;algorithmic beliefs&quot; at each round, and use
Bayesian posteriors to make decisions. The optimization objective to create
&quot;algorithmic beliefs,&quot; which we term &quot;Algorithmic Information Ratio,&quot;
represents an intrinsic complexity measure that effectively characterizes the
frequentist regret of any algorithm. To the best of our knowledge, this is the
first systematical approach to make Bayesian-type algorithms prior-free and
applicable to adversarial settings, in a generic and optimal manner. Moreover,
the algorithms are simple and often efficient to implement. As a major
application, we present a novel algorithm for multi-armed bandits that achieves
the &quot;best-of-all-worlds&quot; empirical performance in the stochastic, adversarial,
and non-stationary environments. And we illustrate how these principles can be
used in linear bandits, bandit convex optimization, and reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yunbei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1&quot;&gt;Assaf Zeevi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00829">
<title>Online Sensitivity Optimization in Differentially Private Learning. (arXiv:2310.00829v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00829</link>
<description rdf:parseType="Literal">&lt;p&gt;Training differentially private machine learning models requires constraining
an individual&apos;s contribution to the optimization process. This is achieved by
clipping the $2$-norm of their gradient at a predetermined threshold prior to
averaging and batch sanitization. This selection adversely influences
optimization in two opposing ways: it either exacerbates the bias due to
excessive clipping at lower values, or augments sanitization noise at higher
values. The choice significantly hinges on factors such as the dataset, model
architecture, and even varies within the same optimization, demanding
meticulous tuning usually accomplished through a grid search. In order to
circumvent the privacy expenses incurred in hyperparameter tuning, we present a
novel approach to dynamically optimize the clipping threshold. We treat this
threshold as an additional learnable parameter, establishing a clean
relationship between the threshold and the cost function. This allows us to
optimize the former with gradient descent, with minimal repercussions on the
overall privacy analysis. Our method is thoroughly assessed against alternative
fixed and adaptive strategies across diverse datasets, tasks, model dimensions,
and privacy levels. Our results indicate that it performs comparably or better
in the evaluated scenarios, given the same privacy requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galli_F/0/1/0/all/0/1&quot;&gt;Filippo Galli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1&quot;&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucinotta_T/0/1/0/all/0/1&quot;&gt;Tommaso Cucinotta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01119">
<title>Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models. (arXiv:2310.01119v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study synthetic data generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model&apos;s performance, occasionally
necessitating only a minor fraction of the original training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1&quot;&gt;Jean Kaddour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02489">
<title>ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for Transformer Layers. (arXiv:2310.02489v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02489</link>
<description rdf:parseType="Literal">&lt;p&gt;Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04477">
<title>Higher-Order DeepTrails: Unified Approach to *Trails. (arXiv:2310.04477v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04477</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing, understanding, and describing human behavior is advantageous in
different settings, such as web browsing or traffic navigation. Understanding
human behavior naturally helps to improve and optimize the underlying
infrastructure or user interfaces. Typically, human navigation is represented
by sequences of transitions between states. Previous work suggests to use
hypotheses, representing different intuitions about the navigation to analyze
these transitions. To mathematically grasp this setting, first-order Markov
chains are used to capture the behavior, consequently allowing to apply
different kinds of graph comparisons, but comes with the inherent drawback of
losing information about higher-order dependencies within the sequences. To
this end, we propose to analyze entire sequences using autoregressive language
models, as they are traditionally used to model higher-order dependencies in
sequences. We show that our approach can be easily adapted to model different
settings introduced in previous work, namely HypTrails, MixedTrails and even
SubTrails, while at the same time bringing unique advantages: 1. Modeling
higher-order dependencies between state transitions, while 2. being able to
identify short comings in proposed hypotheses, and 3. naturally introducing a
unified approach to model all settings. To show the expressiveness of our
approach, we evaluate our approach on different synthetic datasets and conclude
with an exemplary analysis of a real-world dataset, examining the behavior of
users who interact with voice assistants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koopmann_T/0/1/0/all/0/1&quot;&gt;Tobias Koopmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_J/0/1/0/all/0/1&quot;&gt;Jan Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markus_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Markus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carolus_A/0/1/0/all/0/1&quot;&gt;Astrid Carolus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wienrich_C/0/1/0/all/0/1&quot;&gt;Carolin Wienrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotho_A/0/1/0/all/0/1&quot;&gt;Andreas Hotho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07535">
<title>Fairness under Covariate Shift: Improving Fairness-Accuracy tradeoff with few Unlabeled Test Samples. (arXiv:2310.07535v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07535</link>
<description rdf:parseType="Literal">&lt;p&gt;Covariate shift in the test data is a common practical phenomena that can
significantly downgrade both the accuracy and the fairness performance of the
model. Ensuring fairness across different sensitive groups under covariate
shift is of paramount importance due to societal implications like criminal
justice. We operate in the unsupervised regime where only a small set of
unlabeled test samples along with a labeled training set is available. Towards
improving fairness under this highly challenging yet realistic scenario, we
make three contributions. First is a novel composite weighted entropy based
objective for prediction accuracy which is optimized along with a
representation matching loss for fairness. We experimentally verify that
optimizing with our loss formulation outperforms a number of state-of-the-art
baselines in the pareto sense with respect to the fairness-accuracy tradeoff on
several standard datasets. Our second contribution is a new setting we term
Asymmetric Covariate Shift that, to the best of our knowledge, has not been
studied before. Asymmetric covariate shift occurs when distribution of
covariates of one group shifts significantly compared to the other groups and
this happens when a dominant group is over-represented. While this setting is
extremely challenging for current baselines, We show that our proposed method
significantly outperforms them. Our third contribution is theoretical, where we
show that our weighted entropy term along with prediction loss on the training
set approximates test loss under covariate shift. Empirically and through
formal sample complexity bounds, we show that this approximation to the unseen
test loss does not depend on importance sampling variance which affects many
other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreyas Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1&quot;&gt;Jatin Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandy_J/0/1/0/all/0/1&quot;&gt;Jay Nandy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1&quot;&gt;Aravindan Raghuveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09866">
<title>Federated Multi-Objective Learning. (arXiv:2310.09866v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09866</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, multi-objective optimization (MOO) emerges as a foundational
problem underpinning many multi-agent multi-task learning applications.
However, existing algorithms in MOO literature remain limited to centralized
learning settings, which do not satisfy the distributed nature and data privacy
needs of such multi-agent multi-task learning applications. This motivates us
to propose a new federated multi-objective learning (FMOL) framework with
multiple clients distributively and collaboratively solving an MOO problem
while keeping their training data private. Notably, our FMOL framework allows a
different set of objective functions across different clients to support a wide
range of applications, which advances and generalizes the MOO formulation to
the federated learning paradigm for the first time. For this FMOL framework, we
propose two new federated multi-objective optimization (FMOO) algorithms called
federated multi-gradient descent averaging (FMGDA) and federated stochastic
multi-gradient descent averaging (FSMGDA). Both algorithms allow local updates
to significantly reduce communication costs, while achieving the {\em same}
convergence rates as those of their algorithmic counterparts in the
single-objective federated learning. Our extensive experiments also corroborate
the efficacy of our proposed FMOO algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chaosheng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momma_M/0/1/0/all/0/1&quot;&gt;Michinari Momma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11571">
<title>Pragmatic Evaluation of Clarifying Questions with Fact-Level Masking. (arXiv:2310.11571v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11571</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to derive useful information by asking clarifying questions (ACQ)
is an important element of real life collaboration on reasoning tasks, such as
question answering (QA). Existing natural language ACQ challenges, however,
evaluate generations based on word overlap rather than the value of the
information itself. Word overlap is often an inappropriate metric for question
generation since many different questions could be useful in a given situation,
and a single question can be phrased many different ways. Instead, we propose
evaluating questions pragmatically based on the value of the information they
retrieve. Here we present a definition and framework for natural language
pragmatic asking of clarifying questions (PACQ), the problem of generating
questions that result in answers useful for a reasoning task. We also present
fact-level masking (FLM), a procedure for converting natural language datasets
into self-supervised PACQ datasets by omitting particular critical facts.
Finally, we generate a PACQ dataset from the HotpotQA dataset using FLM and
evaluate several zero-shot language models on it. Our experiments show that
current zero-shot models struggle to ask questions that retrieve useful
information, as compared to human annotators. These results demonstrate an
opportunity to use FLM datasets and the PACQ framework to objectively evaluate
and improve question generation and other language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toles_M/0/1/0/all/0/1&quot;&gt;Matthew Toles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yukun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gravano_L/0/1/0/all/0/1&quot;&gt;Luis Gravano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13807">
<title>Learning to (Learn at Test Time). (arXiv:2310.13807v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13807</link>
<description rdf:parseType="Literal">&lt;p&gt;We reformulate the problem of supervised learning as learning to learn with
two nested loops (i.e. learning problems). The inner loop learns on each
individual instance with self-supervision before final prediction. The outer
loop learns the self-supervised task used by the inner loop, such that its
final prediction improves. Our inner loop turns out to be equivalent to linear
attention when the inner-loop learner is only a linear model, and to
self-attention when it is a kernel estimator. For practical comparison with
linear or self-attention layers, we replace each of them in a transformer with
an inner loop, so our outer loop is equivalent to training the architecture.
When each inner-loop learner is a neural network, our approach vastly
outperforms transformers with linear attention on ImageNet from 224 x 224 raw
pixels in both accuracy and FLOPs, while (regular) transformers cannot run.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_K/0/1/0/all/0/1&quot;&gt;Karan Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chloe Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16781">
<title>Kiki or Bouba? Sound Symbolism in Vision-and-Language Models. (arXiv:2310.16781v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16781</link>
<description rdf:parseType="Literal">&lt;p&gt;Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1&quot;&gt;Morris Alper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1&quot;&gt;Hadar Averbuch-Elor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17468">
<title>Cross-modal Active Complementary Learning with Self-refining Correspondence. (arXiv:2310.17468v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17468</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17848">
<title>Boosting Data Analytics With Synthetic Volume Expansion. (arXiv:2310.17848v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17848</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data generation, a cornerstone of Generative Artificial
Intelligence (GAI), signifies a paradigm shift in data science by addressing
data scarcity and privacy while enabling unprecedented performance. As
synthetic data gains prominence, questions arise concerning the accuracy of
statistical methods when applied to synthetic data compared to raw data. This
article introduces the Synthetic Data Generation for Analytics (Syn) framework.
This framework employs statistical methods on high-fidelity synthetic data
generated by advanced models such as tabular diffusion and Generative
Pre-trained Transformer (GPT) models. These models, trained on raw data, are
further enhanced with insights from pertinent studies through knowledge
transfer. A significant discovery within this framework is the generational
effect: the error of a statistical method on synthetic data initially
diminishes with additional synthetic data but may eventually increase or
plateau. This phenomenon, rooted in the complexities of replicating raw data
distributions, highlights a &quot;reflection point&quot; - an optimal threshold in the
size of synthetic data determined by specific error metrics. Through three case
studies - sentiment analysis of texts, predictive modeling of structured data,
and inference in tabular data - we demonstrate the effectiveness of this
framework over traditional ones. We underline its potential to amplify various
statistical methods, including gradient boosting for prediction and hypothesis
testing, thereby underscoring the transformative potential of synthetic data
generation in data science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xiaotong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_R/0/1/0/all/0/1&quot;&gt;Rex Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19043">
<title>Differentially Private Permutation Tests: Applications to Kernel Methods. (arXiv:2310.19043v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19043</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed growing concerns about the privacy of sensitive
data. In response to these concerns, differential privacy has emerged as a
rigorous framework for privacy protection, gaining widespread recognition in
both academic and industrial circles. While substantial progress has been made
in private data analysis, existing methods often suffer from impracticality or
a significant loss of statistical efficiency. This paper aims to alleviate
these concerns in the context of hypothesis testing by introducing
differentially private permutation tests. The proposed framework extends
classical non-private permutation tests to private settings, maintaining both
finite-sample validity and differential privacy in a rigorous manner. The power
of the proposed test depends on the choice of a test statistic, and we
establish general conditions for consistency and non-asymptotic uniform power.
To demonstrate the utility and practicality of our framework, we focus on
reproducing kernel-based test statistics and introduce differentially private
kernel tests for two-sample and independence testing: dpMMD and dpHSIC. The
proposed kernel tests are straightforward to implement, applicable to various
types of data, and attain minimax optimal power across different privacy
regimes. Our empirical evaluations further highlight their competitive power
under various synthetic and real-world scenarios, emphasizing their practical
value. The code is publicly available to facilitate the implementation of our
framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Ilmun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Schrab_A/0/1/0/all/0/1&quot;&gt;Antonin Schrab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19802">
<title>Stochastic Thermodynamics of Learning Parametric Probabilistic Models. (arXiv:2310.19802v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19802</link>
<description rdf:parseType="Literal">&lt;p&gt;We have formulated a family of machine learning problems as the time
evolution of Parametric Probabilistic Models (PPMs), inherently rendering a
thermodynamic process. Our primary motivation is to leverage the rich toolbox
of thermodynamics of information to assess the information-theoretic content of
learning a probabilistic model. We first introduce two information-theoretic
metrics: Memorized-information (M-info) and Learned-information (L-info), which
trace the flow of information during the learning process of PPMs. Then, we
demonstrate that the accumulation of L-info during the learning process is
associated with entropy production, and parameters serve as a heat reservoir in
this process, capturing learned information in the form of M-info.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parsi_S/0/1/0/all/0/1&quot;&gt;Shervin Sadat Parsi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03489">
<title>Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v4 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03489</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meech_J/0/1/0/all/0/1&quot;&gt;James T. Meech&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03496">
<title>Asynchronous Local Computations in Distributed Bayesian Learning. (arXiv:2311.03496v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03496</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the expanding scope of machine learning (ML) to the fields of sensor
networking, cooperative robotics and many other multi-agent systems,
distributed deployment of inference algorithms has received a lot of attention.
These algorithms involve collaboratively learning unknown parameters from
dispersed data collected by multiple agents. There are two competing aspects in
such algorithms, namely, intra-agent computation and inter-agent communication.
Traditionally, algorithms are designed to perform both synchronously. However,
certain circumstances need frugal use of communication channels as they are
either unreliable, time-consuming, or resource-expensive. In this paper, we
propose gossip-based asynchronous communication to leverage fast computations
and reduce communication overhead simultaneously. We analyze the effects of
multiple (local) intra-agent computations by the active agents between
successive inter-agent communications. For local computations, Bayesian
sampling via unadjusted Langevin algorithm (ULA) MCMC is utilized. The
communication is assumed to be over a connected graph (e.g., as in
decentralized learning), however, the results can be extended to coordinated
communication where there is a central server (e.g., federated learning). We
theoretically quantify the convergence rates in the process. To demonstrate the
efficacy of the proposed algorithm, we present simulations on a toy problem as
well as on real world data sets to train ML models to perform classification
tasks. We observe faster initial convergence and improved performance accuracy,
especially in the low data range. We achieve on average 78% and over 90%
classification accuracy respectively on the Gamma Telescope and mHealth data
sets from the UCI ML repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhar_K/0/1/0/all/0/1&quot;&gt;Kinjal Bhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;He Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_J/0/1/0/all/0/1&quot;&gt;Jemin George&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busart_C/0/1/0/all/0/1&quot;&gt;Carl Busart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11249">
<title>Open Set Dandelion Network for IoT Intrusion Detection. (arXiv:2311.11249v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11249</link>
<description rdf:parseType="Literal">&lt;p&gt;As IoT devices become widely, it is crucial to protect them from malicious
intrusions. However, the data scarcity of IoT limits the applicability of
traditional intrusion detection methods, which are highly data-dependent. To
address this, in this paper we propose the Open-Set Dandelion Network (OSDN)
based on unsupervised heterogeneous domain adaptation in an open-set manner.
The OSDN model performs intrusion knowledge transfer from the knowledge-rich
source network intrusion domain to facilitate more accurate intrusion detection
for the data-scarce target IoT intrusion domain. Under the open-set setting, it
can also detect newly-emerged target domain intrusions that are not observed in
the source domain. To achieve this, the OSDN model forms the source domain into
a dandelion-like feature space in which each intrusion category is compactly
grouped and different intrusion categories are separated, i.e., simultaneously
emphasising inter-category separability and intra-category compactness. The
dandelion-based target membership mechanism then forms the target dandelion.
Then, the dandelion angular separation mechanism achieves better inter-category
separability, and the dandelion embedding alignment mechanism further aligns
both dandelions in a finer manner. To promote intra-category compactness, the
discriminating sampled dandelion mechanism is used. Assisted by the intrusion
classifier trained using both known and generated unknown intrusion knowledge,
a semantic dandelion correction mechanism emphasises easily-confused categories
and guides better inter-category separability. Holistically, these mechanisms
form the OSDN model that effectively performs intrusion knowledge transfer to
benefit IoT intrusion detection. Comprehensive experiments on several intrusion
datasets verify the effectiveness of the OSDN model, outperforming three
state-of-the-art baseline methods by 16.9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiashu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kent_K/0/1/0/all/0/1&quot;&gt;Kenneth B. Kent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1&quot;&gt;Jerome Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chengzhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15051">
<title>Large Catapults in Momentum Gradient Descent with Warmup: An Empirical Study. (arXiv:2311.15051v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15051</link>
<description rdf:parseType="Literal">&lt;p&gt;Although gradient descent with momentum is widely used in modern deep
learning, a concrete understanding of its effects on the training trajectory
still remains elusive. In this work, we empirically show that momentum gradient
descent with a large learning rate and learning rate warmup displays large
catapults, driving the iterates towards flatter minima than those found by
gradient descent. We then provide empirical evidence and theoretical intuition
that the large catapult is caused by momentum &quot;amplifying&quot; the
self-stabilization effect (Damian et al., 2023).B.1
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phunyaphibarn_P/0/1/0/all/0/1&quot;&gt;Prin Phunyaphibarn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_C/0/1/0/all/0/1&quot;&gt;Chulhee Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16167">
<title>Moving Sampling Physics-informed Neural Networks induced by Moving Mesh PDE. (arXiv:2311.16167v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16167</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose an end-to-end adaptive sampling neural network
(MMPDE-Net) based on the moving mesh method, which can adaptively generate new
sampling points by solving the moving mesh PDE. This model focuses on improving
the quality of sampling points generation. Moreover, we develop an iterative
algorithm based on MMPDE-Net, which makes the sampling points more precise and
controllable. Since MMPDE-Net is a framework independent of the deep learning
solver, we combine it with physics-informed neural networks (PINN) to propose
moving sampling PINN (MS-PINN) and demonstrate its effectiveness by error
analysis under some assumptions. Finally, we demonstrate the performance
improvement of MS-PINN compared to PINN through numerical experiments of four
typical examples, which numerically verify the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qihong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yangtao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiaolin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17552">
<title>An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17552</link>
<description rdf:parseType="Literal">&lt;p&gt;Tiger conservation necessitates the strategic deployment of multifaceted
initiatives encompassing the preservation of ecological habitats, anti-poaching
measures, and community involvement for sustainable growth in the tiger
population. With the advent of artificial intelligence, tiger surveillance can
be automated using object detection. In this paper, an accurate illumination
invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger
detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without
illumination enhancement. The illumination enhancement improves the mAP by
0.7%. The approaches elevate the state-of-the-art performance on the ATRW
dataset by approximately 6% to 7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pendharkar_G/0/1/0/all/0/1&quot;&gt;Gaurav Pendharkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheal_A/0/1/0/all/0/1&quot;&gt;A.Ancy Micheal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misquitta_J/0/1/0/all/0/1&quot;&gt;Jason Misquitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaippada_R/0/1/0/all/0/1&quot;&gt;Ranjeesh Kaippada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18520">
<title>Calibration-free online test-time adaptation for electroencephalography motor imagery decoding. (arXiv:2311.18520v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18520</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing a promising pathway to link the human brain with external devices,
Brain-Computer Interfaces (BCIs) have seen notable advancements in decoding
capabilities, primarily driven by increasingly sophisticated techniques,
especially deep learning. However, achieving high accuracy in real-world
scenarios remains a challenge due to the distribution shift between sessions
and subjects. In this paper we will explore the concept of online test-time
adaptation (OTTA) to continuously adapt the model in an unsupervised fashion
during inference time. Our approach guarantees the preservation of privacy by
eliminating the requirement to access the source data during the adaptation
process. Additionally, OTTA achieves calibration-free operation by not
requiring any session- or subject-specific data. We will investigate the task
of electroencephalography (EEG) motor imagery decoding using a lightweight
architecture together with different OTTA techniques like alignment, adaptive
batch normalization, and entropy minimization. We examine two datasets and
three distinct data settings for a comprehensive analysis. Our adaptation
methods produce state-of-the-art results, potentially instigating a shift in
transfer learning for BCI decoding towards online adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wimpff_M/0/1/0/all/0/1&quot;&gt;Martin Wimpff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dobler_M/0/1/0/all/0/1&quot;&gt;Mario D&amp;#xf6;bler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02380">
<title>FaultFormer: Pretraining Transformers for Adaptable Bearing Fault Classification. (arXiv:2312.02380v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02380</link>
<description rdf:parseType="Literal">&lt;p&gt;The growth of global consumption has motivated important applications of deep
learning to smart manufacturing and machine health monitoring. In particular,
vibration data offers a rich and reliable source to provide meaningful insights
into machine health and predictive maintenance. In this work, we present
pretraining and fine-tuning frameworks for identifying bearing faults based on
transformer models. In particular, we investigate different tokenization and
data augmentation strategies to improve performance and reach state of the art
accuracies. Furthermore, we demonstrate masked self-supervised pretraining for
vibration signals and its application to low-data regimes, task adaptation, and
dataset adaptation. Pretraining is able to improve performance on 10-way
bearing classification on scarce, unseen training samples. Transformer models
also benefit from pretraining when fine-tuning on fault classes outside of the
pretraining distribution. Lastly, pretrained transformers are shown to be able
to generalize to a different dataset in a few-shot manner. This introduces a
new paradigm where models can be pretrained across different bearings, faults,
and machinery and quickly deployed to new, data-scarce applications to suit
specific manufacturing needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Anthony Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farimani_A/0/1/0/all/0/1&quot;&gt;Amir Barati Farimani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03020">
<title>Enhanced Breast Cancer Tumor Classification using MobileNetV2: A Detailed Exploration on Image Intensity, Error Mitigation, and Streamlit-driven Real-time Deployment. (arXiv:2312.03020v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03020</link>
<description rdf:parseType="Literal">&lt;p&gt;This research introduces a sophisticated transfer learning model based on
Google&apos;s MobileNetV2 for breast cancer tumor classification into normal,
benign, and malignant categories, utilizing a dataset of 1576 ultrasound images
(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of
0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and
MCC of 0.74. It examines image intensity distributions and misclassification
errors, offering improvements for future applications. Addressing dataset
imbalances, the study ensures a generalizable model. This work, using a dataset
from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,
emphasizes MobileNetV2&apos;s potential in medical imaging, aiming to improve
diagnostic precision in oncology. Additionally, the paper explores
Streamlit-based deployment for real-time tumor classification, demonstrating
MobileNetV2&apos;s applicability in medical imaging and setting a benchmark for
future research in oncology diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Surya_A/0/1/0/all/0/1&quot;&gt;Aaditya Surya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Aditya Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kabore_J/0/1/0/all/0/1&quot;&gt;Jarnell Kabore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sasikumar_S/0/1/0/all/0/1&quot;&gt;Subash Sasikumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05910">
<title>Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field and Online Inference. (arXiv:2312.05910v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05910</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process state-space models (GPSSMs) are a versatile and principled
family of nonlinear dynamical system models. However, existing variational
learning and inference methods for GPSSMs often necessitate optimizing a
substantial number of variational parameters, leading to inadequate performance
and efficiency. To overcome this issue, we propose incorporating the ensemble
Kalman filter (EnKF), a well-established model-based filtering technique, into
the variational inference framework to approximate the posterior distribution
of latent states. This utilization of EnKF can effectively exploit the
dependencies between latent states and GP dynamics, while eliminating the need
for parameterizing the variational distribution, thereby significantly reducing
the number of variational parameters. Moreover, we show that our proposed
algorithm allows straightforward evaluation of an approximated evidence lower
bound (ELBO) in variational inference via simply summating multiple terms with
readily available closed-form solutions. Leveraging automatic differentiation
tools, we hence can maximize the ELBO and train the GPSSM efficiently. We also
extend the proposed algorithm to accommodate an online setting and provide
detailed algorithmic analyses and insights. Extensive evaluation on diverse
real and synthetic datasets demonstrates the superiority of our EnKF-aided
variational inference algorithms in terms of learning and inference performance
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhidi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiyong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Feng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiery_A/0/1/0/all/0/1&quot;&gt;Alexandre Hoang Thi&amp;#xe9;ry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06454">
<title>Point Transformer with Federated Learning for Predicting Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images. (arXiv:2312.06454v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06454</link>
<description rdf:parseType="Literal">&lt;p&gt;Directly predicting human epidermal growth factor receptor 2 (HER2) status
from widely available hematoxylin and eosin (HE)-stained whole slide images
(WSIs) can reduce technical costs and expedite treatment selection. Accurately
predicting HER2 requires large collections of multi-site WSIs. Federated
learning enables collaborative training of these WSIs without gigabyte-size
WSIs transportation and data privacy concerns. However, federated learning
encounters challenges in addressing label imbalance in multi-site WSIs from the
real world. Moreover, existing WSI classification methods cannot simultaneously
exploit local context information and long-range dependencies in the site-end
feature representation of federated learning. To address these issues, we
present a point transformer with federated learning for multi-site HER2 status
prediction from HE-stained WSIs. Our approach incorporates two novel designs.
We propose a dynamic label distribution strategy and an auxiliary classifier,
which helps to establish a well-initialized model and mitigate label
distribution variations across sites. Additionally, we propose a farthest
cosine sampling based on cosine distance. It can sample the most distinctive
features and capture the long-range dependencies. Extensive experiments and
analysis show that our method achieves state-of-the-art performance at four
sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can
generalize to two unseen sites with 229 WSIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lizhi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bensheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bu_H/0/1/0/all/0/1&quot;&gt;Hong Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jie Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06710">
<title>Class-Prototype Conditional Diffusion Model for Continual Learning with Generative Replay. (arXiv:2312.06710v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06710</link>
<description rdf:parseType="Literal">&lt;p&gt;Mitigating catastrophic forgetting is a key hurdle in continual learning.
Deep Generative Replay (GR) provides techniques focused on generating samples
from prior tasks to enhance the model&apos;s memory capabilities. With the
progression in generative AI, generative models have advanced from Generative
Adversarial Networks (GANs) to the more recent Diffusion Models (DMs). A major
issue is the deterioration in the quality of generated data compared to the
original, as the generator continuously self-learns from its outputs. This
degradation can lead to the potential risk of catastrophic forgetting occurring
in the classifier. To address this, we propose the Class-Prototype Conditional
Diffusion Model (CPDM), a GR-based approach for continual learning that
enhances image quality in generators and thus reduces catastrophic forgetting
in classifiers. The cornerstone of CPDM is a learnable class-prototype that
captures the core characteristics of images in a given class. This prototype,
integrated into the diffusion model&apos;s denoising process, ensures the generation
of high-quality images. It maintains its effectiveness for old tasks even when
new tasks are introduced, preserving image generation quality and reducing the
risk of catastrophic forgetting in classifiers. Our empirical studies on
diverse datasets demonstrate that our proposed method significantly outperforms
existing state-of-the-art models, highlighting its exceptional ability to
preserve image quality and enhance the model&apos;s memory retention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_K/0/1/0/all/0/1&quot;&gt;Khanh Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1&quot;&gt;Quyen Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1&quot;&gt;Dinh Phung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06786">
<title>Mixture-of-Linear-Experts for Long-term Time Series Forecasting. (arXiv:2312.06786v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06786</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-term time series forecasting (LTSF) aims to predict future values of a
time series given the past values. The current state-of-the-art (SOTA) on this
problem is attained in some cases by linear-centric models, which primarily
feature a linear mapping layer. However, due to their inherent simplicity, they
are not able to adapt their prediction rules to periodic changes in time series
patterns. To address this challenge, we propose a Mixture-of-Experts-style
augmentation for linear-centric models and propose Mixture-of-Linear-Experts
(MoLE). Instead of training a single model, MoLE trains multiple linear-centric
models (i.e., experts) and a router model that weighs and mixes their outputs.
While the entire framework is trained end-to-end, each expert learns to
specialize in a specific temporal pattern, and the router model learns to
compose the experts adaptively. Experiments show that MoLE reduces forecasting
error of linear-centric models, including DLinear, RLinear, and RMLP, in over
78% of the datasets and settings we evaluated. By using MoLE existing
linear-centric models can achieve SOTA LTSF results in 68% of the experiments
that PatchTST reports and we compare to, whereas existing single-head
linear-centric models achieve SOTA results in only 25% of cases. Additionally,
MoLE models achieve SOTA in all settings for the newly released Weather2K
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_R/0/1/0/all/0/1&quot;&gt;Ronghao Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanti_G/0/1/0/all/0/1&quot;&gt;Giulia Fanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07281">
<title>Safe Multi-Task Bayesian Optimization. (arXiv:2312.07281v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07281</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization has become a powerful tool for safe online optimization
of systems, due to its high sample efficiency and noise robustness. For further
speed-up reduced physical models of the system can be incorporated into the
optimization to accelerate the process, since the models are able to offer an
approximation of the actual system, and sampling from them is significantly
cheaper. The similarity between model and reality is represented by additional
hyperparameters and learned within the optimization process. Safety is an
important criteria for online optimization methods like Bayesian optimization,
which has been addressed by recent literature, which provide safety guarantees
under the assumption of known hyperparameters. However, in practice this is not
applicable. Therefore, we extend the robust Gaussian process uniform error
bounds to meet the multi-task setting, which involves the calculation of a
confidence region from the hyperparameter posterior distribution utilizing
Markov chain Monte Carlo methods. Then, using the robust safety bounds,
Bayesian optimization is applied to safely optimize the system while
incorporating measurements of the models. Simulations show that the
optimization can be significantly accelerated compared to other
state-of-the-art safe Bayesian optimization methods depending on the fidelity
of the models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lubsen_J/0/1/0/all/0/1&quot;&gt;Jannis O. L&amp;#xfc;bsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hespe_C/0/1/0/all/0/1&quot;&gt;Christian Hespe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichler_A/0/1/0/all/0/1&quot;&gt;Annika Eichler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08055">
<title>Breaking the Silence: the Threats of Using LLMs in Software Engineering. (arXiv:2312.08055v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08055</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have gained considerable traction within the
Software Engineering (SE) community, impacting various SE tasks from code
completion to test generation, from program repair to code summarization.
Despite their promise, researchers must still be careful as numerous intricate
factors can influence the outcomes of experiments involving LLMs. This paper
initiates an open discussion on potential threats to the validity of LLM-based
research including issues such as closed-source models, possible data leakage
between LLM training data and research evaluation, and the reproducibility of
LLM-based findings. In response, this paper proposes a set of guidelines
tailored for SE researchers and Language Model (LM) providers to mitigate these
concerns. The implications of the guidelines are illustrated using existing
good practices followed by LLM providers and a practical example for SE
researchers in the context of test case generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sallou_J/0/1/0/all/0/1&quot;&gt;June Sallou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durieux_T/0/1/0/all/0/1&quot;&gt;Thomas Durieux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panichella_A/0/1/0/all/0/1&quot;&gt;Annibale Panichella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09433">
<title>Point-of-Care Real-Time Signal Quality for Fetal Doppler Ultrasound Using a Deep Learning Approach. (arXiv:2312.09433v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09433</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a deep learning framework designed to integrate
with our previously developed system that facilitates large-scale 1D fetal
Doppler data collection, aiming to enhance data quality. This system, tailored
for traditional Indigenous midwives in low-resource communities, leverages a
cost-effective Android phone to improve the quality of recorded signals. We
have shown that the Doppler data can be used to identify fetal growth
restriction, hypertension, and other concerning issues during pregnancy.
However, the quality of the signal is dependent on many factors, including
radio frequency interference, position of the fetus, maternal body habitus, and
usage of the Doppler by the birth attendants. In order to provide instant
feedback to allow correction of the data at source, a signal quality metric is
required that can run in real-time on the mobile phone.
&lt;/p&gt;
&lt;p&gt;In this study, 191 DUS signals with durations mainly in the range between 5
to 10 minutes were evaluated for quality and classified into five categories:
Good, Poor, (Radiofrequency) Interference, Talking, and Silent, at a resolution
of 3.75 seconds. A deep neural network was trained on each 3.75-second segment
from these recordings and validated using five-fold cross-validation.
&lt;/p&gt;
&lt;p&gt;An average micro F1 = 97.4\% and macro F1 = 94.2\% were achieved, with F1 =
99.2\% for `Good&apos; quality data. These results indicate that the algorithm,
which will now be implemented in the midwives&apos; app, should allow a significant
increase in the quality of data at the time of capture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Motie_Shirazi_M/0/1/0/all/0/1&quot;&gt;Mohsen Motie-Shirazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1&quot;&gt;Reza Sameni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rohloff_P/0/1/0/all/0/1&quot;&gt;Peter Rohloff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Katebi_N/0/1/0/all/0/1&quot;&gt;Nasim Katebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clifford_G/0/1/0/all/0/1&quot;&gt;Gari D. Clifford&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10494">
<title>Do Bayesian Neural Networks Improve Weapon System Predictive Maintenance?. (arXiv:2312.10494v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10494</link>
<description rdf:parseType="Literal">&lt;p&gt;We implement a Bayesian inference process for Neural Networks to model the
time to failure of highly reliable weapon systems with interval-censored data
and time-varying covariates. We analyze and benchmark our approach, LaplaceNN,
on synthetic and real datasets with standard classification metrics such as
Receiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall
(PR) AUC, and reliability curve visualizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1&quot;&gt;Michael Potter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jun_M/0/1/0/all/0/1&quot;&gt;Miru Jun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10578">
<title>SAME: Sample Reconstruction against Model Extraction Attacks. (arXiv:2312.10578v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10578</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning models have shown significant performance across various
domains, their deployment needs extensive resources and advanced computing
infrastructure. As a solution, Machine Learning as a Service (MLaaS) has
emerged, lowering the barriers for users to release or productize their deep
learning models. However, previous studies have highlighted potential privacy
and security concerns associated with MLaaS, and one primary threat is model
extraction attacks. To address this, there are many defense solutions but they
suffer from unrealistic assumptions and generalization issues, making them less
practical for reliable protection. Driven by these limitations, we introduce a
novel defense mechanism, SAME, based on the concept of sample reconstruction.
This strategy imposes minimal prerequisites on the defender&apos;s capabilities,
eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user
query history, white-box model access, and additional intervention during model
training. It is compatible with existing active defense methods. Our extensive
experiments corroborate the superior efficacy of SAME over state-of-the-art
solutions. Our code is available at https://github.com/xythink/SAME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiqian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11509">
<title>Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11509</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a Reinforcement-Learning-based system that would automatically
prescribe a hypothetical patient medication that may help the patient with
their mental-health-related speech disfluency, and adjust the medication and
the dosages in response to zero-cost frequent measurement of the fluency of the
patient. We demonstrate the components of the system: a module that detects and
evaluates speech disfluency on a large dataset we built, and a Reinforcement
Learning algorithm that automatically finds good combinations of medications.
To support the two modules, we collect data on the effect of psychiatric
medications for speech disfluency from the literature, and build a plausible
patient simulation system. We demonstrate that the Reinforcement Learning
system is, under some circumstances, able to converge to a good medication
regime. We collect and label a dataset of people with possible speech
disfluency and demonstrate our methods using that dataset. Our work is a proof
of concept: we show that there is promise in the idea of using automatic data
collection to address disfluency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constas_P/0/1/0/all/0/1&quot;&gt;Pavlos Constas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawal_V/0/1/0/all/0/1&quot;&gt;Vikram Rawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1&quot;&gt;Matthew Honorio Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constas_A/0/1/0/all/0/1&quot;&gt;Andreas Constas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aditya Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_K/0/1/0/all/0/1&quot;&gt;Kaison Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultani_N/0/1/0/all/0/1&quot;&gt;Najma Sultani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Carrie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altomare_M/0/1/0/all/0/1&quot;&gt;Micol Altomare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akzam_M/0/1/0/all/0/1&quot;&gt;Michael Akzam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_V/0/1/0/all/0/1&quot;&gt;Vhea He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altomare_L/0/1/0/all/0/1&quot;&gt;Lauren Altomare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murqi_H/0/1/0/all/0/1&quot;&gt;Heraa Murqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanshali_N/0/1/0/all/0/1&quot;&gt;Nimit Amikumar Bhanshali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachad_Y/0/1/0/all/0/1&quot;&gt;Youssef Rachad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerzhoy_M/0/1/0/all/0/1&quot;&gt;Michael Guerzhoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11973">
<title>Continual Learning: Forget-free Winning Subnetworks for Video Representations. (arXiv:2312.11973v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11973</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO&apos;s effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Haeyong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13314">
<title>Unlocking Pre-trained Image Backbones for Semantic Image Synthesis. (arXiv:2312.13314v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13314</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13650">
<title>Distributed Quantum Neural Networks via Partitioned Features Encoding. (arXiv:2312.13650v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13650</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum neural networks are expected to be a promising application in
near-term quantum computing, but face challenges such as vanishing gradients
during optimization and limited expressibility by a limited number of qubits
and shallow circuits. To mitigate these challenges, an approach using
distributed quantum neural networks has been proposed to make a prediction by
approximating outputs of a large circuit using multiple small circuits.
However, the approximation of a large circuit requires an exponential number of
small circuit evaluations. Here, we instead propose to distribute partitioned
features over multiple small quantum neural networks and use the ensemble of
their expectation values to generate predictions. To verify our distributed
approach, we demonstrate ten class classification of the Semeion and MNIST
handwritten digit datasets. The results of the Semeion dataset imply that while
our distributed approach may outperform a single quantum neural network in
classification performance, excessive partitioning reduces performance.
Nevertheless, for the MNIST dataset, we succeeded in ten class classification
with exceeding 96\% accuracy. Our proposed method not only achieved highly
accurate predictions for a large dataset but also reduced the hardware
requirements for each quantum neural network compared to a large single quantum
neural network. Our results highlight distributed quantum neural networks as a
promising direction for practical quantum machine learning algorithms
compatible with near-term quantum devices. We hope that our approach is useful
for exploring quantum machine learning applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kawase_Y/0/1/0/all/0/1&quot;&gt;Yoshiaki Kawase&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14260">
<title>Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience. (arXiv:2312.14260v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14260</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are being used in an increasing number of critical
applications; thus, securing their integrity and ownership is critical. Recent
studies observed that adversarial training and watermarking have a conflicting
interaction. This work introduces a novel framework to integrate adversarial
training with watermarking techniques to fortify against evasion attacks and
provide confident model verification in case of intellectual property theft. We
use adversarial training together with adversarial watermarks to train a robust
watermarked model. The key intuition is to use a higher perturbation budget to
generate adversarial watermarks compared to the budget used for adversarial
training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets
to evaluate our proposed technique on various model stealing attacks. The
results obtained consistently outperform the existing baseline in terms of
robustness performance and further prove the resilience of this defense against
pruning and fine-tuning removal attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakkar_J/0/1/0/all/0/1&quot;&gt;Janvi Thakkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zizzo_G/0/1/0/all/0/1&quot;&gt;Giulio Zizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maffeis_S/0/1/0/all/0/1&quot;&gt;Sergio Maffeis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15186">
<title>Efficient Asynchronous Federated Learning with Sparsification and Quantization. (arXiv:2312.15186v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15186</link>
<description rdf:parseType="Literal">&lt;p&gt;While data is distributed in multiple edge devices, Federated Learning (FL)
is attracting more and more attention to collaboratively train a machine
learning model without transferring raw data. FL generally exploits a parameter
server and a large number of edge devices during the whole process of the model
training, while several devices are selected in each round. However, straggler
devices may slow down the training process or even make the system crash during
training. Meanwhile, other idle edge devices remain unused. As the bandwidth
between the devices and the server is relatively low, the communication of
intermediate data becomes a bottleneck. In this paper, we propose
Time-Efficient Asynchronous federated learning with Sparsification and
Quantization, i.e., TEASQ-Fed. TEASQ-Fed can fully exploit edge devices to
asynchronously participate in the training process by actively applying for
tasks. We utilize control parameters to choose an appropriate number of
parallel edge devices, which simultaneously execute the training tasks. In
addition, we introduce a caching mechanism and weighted averaging with respect
to model staleness to further improve the accuracy. Furthermore, we propose a
sparsification and quantitation approach to compress the intermediate data to
accelerate the training. The experimental results reveal that TEASQ-Fed
improves the accuracy (up to 16.67% higher) while accelerating the convergence
of model training (up to twice faster).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Juncheng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chendi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mianxiong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1&quot;&gt;Dejing Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15195">
<title>Mutual Information as Intrinsic Reward of Reinforcement Learning Agents for On-demand Ride Pooling. (arXiv:2312.15195v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15195</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of on-demand ride pooling services allows each vehicle to serve
multiple passengers at a time, thus increasing drivers&apos; income and enabling
passengers to travel at lower prices than taxi/car on-demand services (only one
passenger can be assigned to a car at a time like UberX and Lyft). Although
on-demand ride pooling services can bring so many benefits, ride pooling
services need a well-defined matching strategy to maximize the benefits for all
parties (passengers, drivers, aggregation companies and environment), in which
the regional dispatching of vehicles has a significant impact on the matching
and revenue. Existing algorithms often only consider revenue maximization,
which makes it difficult for requests with unusual distribution to get a ride.
How to increase revenue while ensuring a reasonable assignment of requests
brings a challenge to ride pooling service companies (aggregation companies).
In this paper, we propose a framework for vehicle dispatching for ride pooling
tasks, which splits the city into discrete dispatching regions and uses the
reinforcement learning (RL) algorithm to dispatch vehicles in these regions. We
also consider the mutual information (MI) between vehicle and order
distribution as the intrinsic reward of the RL algorithm to improve the
correlation between their distributions, thus ensuring the possibility of
getting a ride for unusually distributed requests. In experimental results on a
real-world taxi dataset, we demonstrate that our framework can significantly
increase revenue up to an average of 3\% over the existing best on-demand ride
pooling method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xianjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiahao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yifei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16228">
<title>Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16228</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have achieved promising results on a variety of tasks. However,
the quadratic complexity in self-attention computation has limited the
applications, especially in low-resource settings and mobile or edge devices.
Existing works have proposed to exploit hand-crafted attention patterns to
reduce computation complexity. However, such hand-crafted patterns are
data-agnostic and may not be optimal. Hence, it is likely that relevant keys or
values are being reduced, while less important ones are still preserved. Based
on this key insight, we propose a novel deformable audio Transformer for audio
recognition, named DATAR, where a deformable attention equipping with a pyramid
transformer backbone is constructed and learnable. Such an architecture has
been proven effective in prediction tasks,~\textit{e.g.}, event classification.
Moreover, we identify that the deformable attention map computation may
over-simplify the input feature, which can be further enhanced. Hence, we
introduce a learnable input adaptor to alleviate this issue, and DATAR achieves
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16242">
<title>Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16242</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation transfers knowledge from large models into small
models, and has recently made remarkable achievements. However, few studies has
investigated the mechanism of knowledge distillation against distribution
shift. Distribution shift refers to the data distribution drifts between
training and testing phases. In this paper, we reconsider the paradigm of
knowledge distillation by reformulating the objective function in shift
situations. Under the real scenarios, we propose a unified and systematic
framework to benchmark knowledge distillation against two general
distributional shifts including diversity and correlation shift. The evaluation
benchmark covers more than 30 methods from algorithmic, data-driven, and
optimization perspectives for five benchmark datasets. Overall, we conduct
extensive experiments on the student model. We reveal intriguing observations
of poor teaching performance under distribution shifts; in particular, complex
algorithms and data augmentation offer limited gains in many cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Ziyu Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16430">
<title>Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16430</link>
<description rdf:parseType="Literal">&lt;p&gt;Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference is deterministic. IPO uses a root-finding MSE loss to solve the
ignoring KL-regularization problem. In this paper, we&apos;ll figure out, although
IPO fix the problem when preference is deterministic, but both DPO and IPO
fails the KL-regularization term because the support of preference distribution
not equal to reference distribution. Then, we design a simple and intuitive
off-policy preference optimization algorithm from an importance sampling view,
which we call Maximum Preference Optimization (MPO), and add off-policy
KL-regularization terms which makes KL-regularization truly effective. The
objective of MPO bears resemblance to RLHF&apos;s objective, and likes IPO, MPO is
off-policy. So, MPO attains the best of both worlds. To simplify the learning
process and save memory usage, MPO eliminates the needs for both reward model
and reference policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zaifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17100">
<title>TSPP: A Unified Benchmarking Tool for Time-series Forecasting. (arXiv:2312.17100v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17100</link>
<description rdf:parseType="Literal">&lt;p&gt;While machine learning has witnessed significant advancements, the emphasis
has largely been on data acquisition and model creation. However, achieving a
comprehensive assessment of machine learning solutions in real-world settings
necessitates standardization throughout the entire pipeline. This need is
particularly acute in time series forecasting, where diverse settings impede
meaningful comparisons between various methods. To bridge this gap, we propose
a unified benchmarking framework that exposes the crucial modelling and machine
learning decisions involved in developing time series forecasting models. This
framework fosters seamless integration of models and datasets, aiding both
practitioners and researchers in their development efforts. We benchmark
recently proposed models within this framework, demonstrating that carefully
implemented deep learning models with minimal effort can rival
gradient-boosting decision trees requiring extensive feature engineering and
expert knowledge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baczek_J/0/1/0/all/0/1&quot;&gt;Jan B&amp;#x105;czek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhylko_D/0/1/0/all/0/1&quot;&gt;Dmytro Zhylko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Titericz_G/0/1/0/all/0/1&quot;&gt;Gilberto Titericz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darabi_S/0/1/0/all/0/1&quot;&gt;Sajad Darabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puget_J/0/1/0/all/0/1&quot;&gt;Jean-Francois Puget&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Putterman_I/0/1/0/all/0/1&quot;&gt;Izzy Putterman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majchrowski_D/0/1/0/all/0/1&quot;&gt;Dawid Majchrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Anmol Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kranen_K/0/1/0/all/0/1&quot;&gt;Kyle Kranen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morkisz_P/0/1/0/all/0/1&quot;&gt;Pawel Morkisz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00422">
<title>Interpreting the Curse of Dimensionality from Distance Concentration and Manifold Effect. (arXiv:2401.00422v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00422</link>
<description rdf:parseType="Literal">&lt;p&gt;The characteristics of data like distribution and heterogeneity, become more
complex and counterintuitive as the dimensionality increases. This phenomenon
is known as curse of dimensionality, where common patterns and relationships
(e.g., internal and boundary pattern) that hold in low-dimensional space may be
invalid in higher-dimensional space. It leads to a decreasing performance for
the regression, classification or clustering models or algorithms. Curse of
dimensionality can be attributed to many causes. In this paper, we first
summarize five challenges associated with manipulating high-dimensional data,
and explains the potential causes for the failure of regression, classification
or clustering tasks. Subsequently, we delve into two major causes of the curse
of dimensionality, distance concentration and manifold effect, by performing
theoretical and empirical analyses. The results demonstrate that nearest
neighbor search (NNS) using three typical distance measurements, Minkowski
distance, Chebyshev distance, and cosine distance, becomes meaningless as the
dimensionality increases. Meanwhile, the data incorporates more redundant
features, and the variance contribution of principal component analysis (PCA)
is skewed towards a few dimensions. By interpreting the causes of the curse of
dimensionality, we can better understand the limitations of current models and
algorithms, and drive to improve the performance of data analysis and machine
learning tasks in high-dimensional space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dehua Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Huayi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00793">
<title>SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00793</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and difficult to circumvent or optimize effectively. To
address this concern, we introduce an advanced optimization framework called
SecFormer, to achieve fast and accurate PPI for Transformer models. By
implementing model design optimization, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials, Fourier series and Goldschmidt&apos;s
method to handle other complex nonlinear functions within PPI, such as GeLU,
LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer
outperforms MPCFormer in performance, showing improvements of $5.6\%$ and
$24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In
terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness
and speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jinglong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yehong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1&quot;&gt;Xin Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yue Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01306">
<title>Learning solutions to some toy constrained optimization problems in infinite dimensional Hilbert spaces. (arXiv:2401.01306v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01306</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we present deep learning implementations of two popular
theoretical constrained optimization algorithms in infinite dimensional Hilbert
spaces, namely, the penalty and the augmented Lagrangian methods. We test these
algorithms on some toy problems originating in either calculus of variations or
physics. We demonstrate that both methods are able to produce decent
approximations for the test problems and are comparable in terms of different
errors produced. Leveraging the common occurrence of the Lagrange multiplier
update rule being computationally less expensive than solving subproblems in
the penalty method, we achieve significant speedups in cases when the output of
the constraint function is itself a function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mandal_P/0/1/0/all/0/1&quot;&gt;Pinak Mandal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01383">
<title>Predicting Infant Brain Connectivity with Federated Multi-Trajectory GNNs using Scarce Data. (arXiv:2401.01383v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01383</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital&apos;s local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pistos_M/0/1/0/all/0/1&quot;&gt;Michalis Pistos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weili Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rekik_I/0/1/0/all/0/1&quot;&gt;Islem Rekik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01393">
<title>Backtracking New Q-Newton&apos;s method, Newton&apos;s flow, Voronoi&apos;s diagram and Stochastic root finding. (arXiv:2401.01393v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01393</link>
<description rdf:parseType="Literal">&lt;p&gt;A new variant of Newton&apos;s method - named Backtracking New Q-Newton&apos;s method
(BNQN) - which has strong theoretical guarantee, is easy to implement, and has
good experimental performance, was recently introduced by the third author.
&lt;/p&gt;
&lt;p&gt;Experiments performed previously showed some remarkable properties of the
basins of attractions for finding roots of polynomials and meromorphic
functions, with BNQN. In general, they look more smooth than that of Newton&apos;s
method.
&lt;/p&gt;
&lt;p&gt;In this paper, we continue to experimentally explore in depth this remarkable
phenomenon, and connect BNQN to Newton&apos;s flow and Voronoi&apos;s diagram. This link
poses a couple of challenging puzzles to be explained. Experiments also
indicate that BNQN is more robust against random perturbations than Newton&apos;s
method and Random Relaxed Newton&apos;s method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Fornaess_J/0/1/0/all/0/1&quot;&gt;John Erik Fornaess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Mi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Tuyen Trung Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Takayuki Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01404">
<title>Scalable network reconstruction in subquadratic time. (arXiv:2401.01404v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01404</link>
<description rdf:parseType="Literal">&lt;p&gt;Network reconstruction consists in determining the unobserved pairwise
couplings between $N$ nodes given only observational data on the resulting
behavior that is conditioned on those couplings -- typically a time-series or
independent samples from a graphical model. A major obstacle to the scalability
of algorithms proposed for this problem is a seemingly unavoidable quadratic
complexity of $O(N^2)$, corresponding to the requirement of each possible
pairwise coupling being contemplated at least once, despite the fact that most
networks of interest are sparse, with a number of non-zero couplings that is
only $O(N)$. Here we present a general algorithm applicable to a broad range of
reconstruction problems that achieves its result in subquadratic time, with a
data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with
a more typical log-linear complexity of $O(N\log^2N)$. Our algorithm relies on
a stochastic second neighbor search that produces the best edge candidates with
high probability, thus bypassing an exhaustive quadratic search. In practice,
our algorithm achieves a performance that is many orders of magnitude faster
than the quadratic baseline, allows for easy parallelization, and thus enables
the reconstruction of networks with hundreds of thousands and even millions of
nodes and edges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peixoto_T/0/1/0/all/0/1&quot;&gt;Tiago P. Peixoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01470">
<title>TPC-ViT: Token Propagation Controller for Efficient Vision Transformer. (arXiv:2401.01470v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01519">
<title>Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review. (arXiv:2401.01519v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01519</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the frontiers of large language models (LLMs) in
psychology applications. Psychology has undergone several theoretical changes,
and the current use of Artificial Intelligence (AI) and Machine Learning,
particularly LLMs, promises to open up new research directions. We provide a
detailed exploration of how LLMs like ChatGPT are transforming psychological
research. It discusses the impact of LLMs across various branches of
psychology, including cognitive and behavioral, clinical and counseling,
educational and developmental, and social and cultural psychology, highlighting
their potential to simulate aspects of human cognition and behavior. The paper
delves into the capabilities of these models to emulate human-like text
generation, offering innovative tools for literature review, hypothesis
generation, experimental design, experimental subjects, data analysis, academic
writing, and peer review in psychology. While LLMs are essential in advancing
research methodologies in psychology, the paper also cautions about their
technical and ethical challenges. There are issues like data privacy, the
ethical implications of using LLMs in psychological research, and the need for
a deeper understanding of these models&apos; limitations. Researchers should
responsibly use LLMs in psychological studies, adhering to ethical standards
and considering the potential consequences of deploying these technologies in
sensitive areas. Overall, the article provides a comprehensive overview of the
current state of LLMs in psychology, exploring potential benefits and
challenges. It serves as a call to action for researchers to leverage LLMs&apos;
advantages responsibly while addressing associated risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1&quot;&gt;Luoma Ke&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Song Tong&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1&quot;&gt;Kaiping Peng&lt;/a&gt; (1) ((1) Department of Psychology, Tsinghua University, (2) School of Social Science, Tsinghua University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01625">
<title>SCALA: Sparsification-based Contrastive Learning for Anomaly Detection on Attributed Networks. (arXiv:2401.01625v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01625</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection on attributed networks aims to find the nodes whose
behaviors are significantly different from other majority nodes. Generally,
network data contains information about relationships between entities, and the
anomaly is usually embodied in these relationships. Therefore, how to
comprehensively model complex interaction patterns in networks is still a major
focus. It can be observed that anomalies in networks violate the homophily
assumption. However, most existing studies only considered this phenomenon
obliquely rather than explicitly. Besides, the node representation of normal
entities can be perturbed easily by the noise relationships introduced by
anomalous nodes. To address the above issues, we present a novel contrastive
learning framework for anomaly detection on attributed networks,
\textbf{SCALA}, aiming to improve the embedding quality of the network and
provide a new measurement of qualifying the anomaly score for each node by
introducing sparsification into the conventional method. Extensive experiments
are conducted on five benchmark real-world datasets and the results show that
SCALA consistently outperforms all baseline methods significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_E/0/1/0/all/0/1&quot;&gt;Enbo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yitong Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guisheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01843">
<title>Investigating Semi-Supervised Learning Algorithms in Text Datasets. (arXiv:2401.01843v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01843</link>
<description rdf:parseType="Literal">&lt;p&gt;Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesgin_H/0/1/0/all/0/1&quot;&gt;Himmet Toprak Kesgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amasyali_M/0/1/0/all/0/1&quot;&gt;Mehmet Fatih Amasyali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01923">
<title>IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01923</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipped with sensing, networking, and computing capabilities, Internet of
Things (IoT) such as smartphones, wearables, smart speakers, and household
robots have been seamlessly weaved into our daily lives. Recent advancements in
Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold
immense promise to push IoT to the next level. In this article, we share our
vision and views on the benefits that Generative AI brings to IoT, and discuss
some of the most important applications of Generative AI in IoT-related
domains. Fully harnessing Generative AI in IoT is a complex challenge. We
identify some of the most critical challenges including high resource demands
of the Generative AI models, prompt engineering, on-device inference,
offloading, on-device fine-tuning, federated learning, security, as well as
development tools and benchmarks, and discuss current gaps as well as promising
opportunities on enabling Generative AI for IoT. We hope this article can
inspire new research on IoT in the era of Generative AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hekmati_A/0/1/0/all/0/1&quot;&gt;Arvin Hekmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_M/0/1/0/all/0/1&quot;&gt;Mingyu Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Samiul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamachari_B/0/1/0/all/0/1&quot;&gt;Bhaskar Krishnamachari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02086">
<title>View-based Explanations for Graph Neural Networks. (arXiv:2401.02086v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02086</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating explanations for graph neural networks (GNNs) has been studied to
understand their behavior in analytical tasks such as graph classification.
Existing approaches aim to understand the overall results of GNNs rather than
providing explanations for specific class labels of interest, and may return
explanation structures that are hard to access, nor directly queryable.We
propose GVEX, a novel paradigm that generates Graph Views for EXplanation. (1)
We design a two-tier explanation structure called explanation views. An
explanation view consists of a set of graph patterns and a set of induced
explanation subgraphs. Given a database G of multiple graphs and a specific
class label l assigned by a GNN-based classifier M, it concisely describes the
fraction of G that best explains why l is assigned by M. (2) We propose quality
measures and formulate an optimization problem to compute optimal explanation
views for GNN explanation. We show that the problem is $\Sigma^2_P$-hard. (3)
We present two algorithms. The first one follows an explain-and-summarize
strategy that first generates high-quality explanation subgraphs which best
explain GNNs in terms of feature influence maximization, and then performs a
summarization step to generate patterns. We show that this strategy provides an
approximation ratio of 1/2. Our second algorithm performs a single-pass to an
input node stream in batches to incrementally maintain explanation views,
having an anytime quality guarantee of 1/4 approximation. Using real-world
benchmark data, we experimentally demonstrate the effectiveness, efficiency,
and scalability of GVEX. Through case studies, we showcase the practical
applications of GVEX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tingyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_D/0/1/0/all/0/1&quot;&gt;Dazhuo Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yinghui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Arijit Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_X/0/1/0/all/0/1&quot;&gt;Xiangyu Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02325">
<title>A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning. (arXiv:2401.02325v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02325</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributional Reinforcement Learning (RL) estimates return distribution
mainly by learning quantile values via minimizing the quantile Huber loss
function, entailing a threshold parameter often selected heuristically or via
hyperparameter search, which may not generalize well and can be suboptimal.
This paper introduces a generalized quantile Huber loss function derived from
Wasserstein distance (WD) calculation between Gaussian distributions, capturing
noise in predicted (current) and target (Bellman-updated) quantile values.
Compared to the classical quantile Huber loss, this innovative loss function
enhances robustness against outliers. Notably, the classical Huber loss
function can be seen as an approximation of our proposed loss, enabling
parameter adjustment by approximating the amount of noise in the data during
the learning process. Empirical tests on Atari games, a common application in
distributional RL, and a recent hedging strategy using distributional RL,
validate the effectiveness of our proposed loss function and its potential for
parameter adjustments in distributional RL. The implementation of the proposed
loss function is available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1&quot;&gt;Parvin Malekzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulos_Z/0/1/0/all/0/1&quot;&gt;Zissis Poulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02740">
<title>Fairness-Aware Job Scheduling for Multi-Job Federated Learning. (arXiv:2401.02740v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02740</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) enables multiple data owners (a.k.a. FL clients) to
collaboratively train machine learning models without disclosing sensitive
private data. Existing FL research mostly focuses on the monopoly scenario in
which a single FL server selects a subset of FL clients to update their local
models in each round of training. In practice, there can be multiple FL servers
simultaneously trying to select clients from the same pool. In this paper, we
propose a first-of-its-kind Fairness-aware Federated Job Scheduling (FairFedJS)
approach to bridge this gap. Based on Lyapunov optimization, it ensures fair
allocation of high-demand FL client datasets to FL jobs in need of them, by
jointly considering the current demand and the job payment bids, in order to
prevent prolonged waiting. Extensive experiments comparing FairFedJS against
four state-of-the-art approaches on two datasets demonstrate its significant
advantages. It outperforms the best baseline by 31.9% and 1.0% on average in
terms of scheduling fairness and convergence time, respectively, while
achieving comparable test accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Han Yu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>