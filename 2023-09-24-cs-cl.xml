<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11896" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.05232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.11489" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.11506">
<title>Matching Table Metadata with Business Glossaries Using Large Language Models. (arXiv:2309.11506v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2309.11506</link>
<description rdf:parseType="Literal">&lt;p&gt;Enterprises often own large collections of structured data in the form of
large databases or an enterprise data lake. Such data collections come with
limited metadata and strict access policies that could limit access to the data
contents and, therefore, limit the application of classic retrieval and
analysis solutions. As a result, there is a need for solutions that can
effectively utilize the available metadata. In this paper, we study the problem
of matching table metadata to a business glossary containing data labels and
descriptions. The resulting matching enables the use of an available or curated
business glossary for retrieval and analysis without or before requesting
access to the data contents. One solution to this problem is to use
manually-defined rules or similarity measures on column names and glossary
descriptions (or their vector embeddings) to find the closest match. However,
such approaches need to be tuned through manual labeling and cannot handle many
business glossaries that contain a combination of simple as well as complex and
long descriptions. In this work, we leverage the power of large language models
(LLMs) to design generic matching methods that do not require manual tuning and
can identify complex relations between column names and glossaries. We propose
methods that utilize LLMs in two ways: a) by generating additional context for
column names that can aid with matching b) by using LLMs to directly infer if
there is a relation between column names and glossary descriptions. Our
preliminary experimental results show the effectiveness of our proposed
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_E/0/1/0/all/0/1&quot;&gt;Elita Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassanzadeh_O/0/1/0/all/0/1&quot;&gt;Oktie Hassanzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1&quot;&gt;Nhan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1&quot;&gt;Nandana Mihindukulasooriya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_D/0/1/0/all/0/1&quot;&gt;Dharmashankar Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samulowitz_H/0/1/0/all/0/1&quot;&gt;Horst Samulowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11508">
<title>Towards LLM-based Autograding for Short Textual Answers. (arXiv:2309.11508v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11508</link>
<description rdf:parseType="Literal">&lt;p&gt;Grading of exams is an important, labor intensive, subjective, repetitive and
frequently challenging task. The feasibility of autograding textual responses
has greatly increased thanks to the availability of large language models
(LLMs) such as ChatGPT and because of the substantial influx of data brought
about by digitalization. However, entrusting AI models with decision-making
roles raises ethical considerations, mainly stemming from potential biases and
issues related to generating false information. Thus, in this manuscript we
provide an evaluation of a large language model for the purpose of autograding,
while also highlighting how LLMs can support educators in validating their
grading procedures. Our evaluation is targeted towards automatic short textual
answers grading (ASAG), spanning various languages and examinations from two
distinct courses. Our findings suggest that while &quot;out-of-the-box&quot; LLMs provide
a valuable tool to provide a complementary perspective, their readiness for
independent automated grading remains a work in progress, necessitating human
oversight.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Johannes Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schenk_B/0/1/0/all/0/1&quot;&gt;Bernd Schenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niklaus_C/0/1/0/all/0/1&quot;&gt;Christina Niklaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlachos_M/0/1/0/all/0/1&quot;&gt;Michaelis Vlachos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11564">
<title>Hierarchical reinforcement learning with natural language subgoals. (arXiv:2309.11564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.11564</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical reinforcement learning has been a compelling approach for
achieving goal directed behavior over long sequences of actions. However, it
has been challenging to implement in realistic or open-ended environments. A
main challenge has been to find the right space of sub-goals over which to
instantiate a hierarchy. We present a novel approach where we use data from
humans solving these tasks to softly supervise the goal space for a set of long
range tasks in a 3D embodied environment. In particular, we use unconstrained
natural language to parameterize this space. This has two advantages: first, it
is easy to generate this data from naive human participants; second, it is
flexible enough to represent a vast range of sub-goals in human-relevant tasks.
Our approach outperforms agents that clone expert behavior on these tasks, as
well as HRL from scratch without this supervised sub-goal space. Our work
presents a novel approach to combining human expert supervision with the
benefits and flexibility of reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_A/0/1/0/all/0/1&quot;&gt;Arun Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopparapu_K/0/1/0/all/0/1&quot;&gt;Kavya Kopparapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fergus_R/0/1/0/all/0/1&quot;&gt;Rob Fergus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1&quot;&gt;Ishita Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11566">
<title>SignBank+: Multilingual Sign Language Translation Dataset. (arXiv:2309.11566v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11566</link>
<description rdf:parseType="Literal">&lt;p&gt;This work advances the field of sign language machine translation by focusing
on dataset quality and simplification of the translation system. We introduce
SignBank+, a clean version of the SignBank dataset, optimized for machine
translation. Contrary to previous works that employ complex factorization
techniques for translation, we advocate for a simplified text-to-text
translation approach. Our evaluation shows that models trained on SignBank+
surpass those on the original dataset, establishing a new benchmark and
providing an open resource for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moryossef_A/0/1/0/all/0/1&quot;&gt;Amit Moryossef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zifan Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11568">
<title>BTLM-3B-8K: 7B Parameter Performance in a 3B Parameter Model. (arXiv:2309.11568v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.11568</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Bittensor Language Model, called &quot;BTLM-3B-8K&quot;, a new
state-of-the-art 3 billion parameter open-source language model. BTLM-3B-8K was
trained on 627B tokens from the SlimPajama dataset with a mixture of 2,048 and
8,192 context lengths. BTLM-3B-8K outperforms all existing 3B parameter models
by 2-5.5% across downstream tasks. BTLM-3B-8K is even competitive with some 7B
parameter models. Additionally, BTLM-3B-8K provides excellent long context
performance, outperforming MPT-7B-8K and XGen-7B-8K on tasks up to 8,192
context length. We trained the model on a cleaned and deduplicated SlimPajama
dataset; aggressively tuned the \textmu P hyperparameters and schedule; used
ALiBi position embeddings; and adopted the SwiGLU nonlinearity.
&lt;/p&gt;
&lt;p&gt;On Hugging Face, the most popular models have 7B parameters, indicating that
users prefer the quality-size ratio of 7B models. Compacting the 7B parameter
model to one with 3B parameters, with little performance impact, is an
important milestone. BTLM-3B-8K needs only 3GB of memory with 4-bit precision
and takes 2.5x less inference compute than 7B models, helping to open up access
to a powerful language model on mobile and edge devices. BTLM-3B-8K is
available under an Apache 2.0 license on Hugging Face:
https://huggingface.co/cerebras/btlm-3b-8k-base.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1&quot;&gt;Nolan Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soboleva_D/0/1/0/all/0/1&quot;&gt;Daria Soboleva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Khateeb_F/0/1/0/all/0/1&quot;&gt;Faisal Al-Khateeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bowen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathria_R/0/1/0/all/0/1&quot;&gt;Ribhu Pathria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khachane_H/0/1/0/all/0/1&quot;&gt;Hemant Khachane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1&quot;&gt;Shaheer Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhiming/0/1/0/all/0/1&quot;&gt;Zhiming&lt;/a&gt; (Charles) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1&quot;&gt;Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_R/0/1/0/all/0/1&quot;&gt;Robert Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeves_J/0/1/0/all/0/1&quot;&gt;Jacob Robert Steeves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vassilieva_N/0/1/0/all/0/1&quot;&gt;Natalia Vassilieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1&quot;&gt;Marvin Tom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hestness_J/0/1/0/all/0/1&quot;&gt;Joel Hestness&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11576">
<title>Examining the Limitations of Computational Rumor Detection Models Trained on Static Datasets. (arXiv:2309.11576v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11576</link>
<description rdf:parseType="Literal">&lt;p&gt;A crucial aspect of a rumor detection model is its ability to generalize,
particularly its ability to detect emerging, previously unknown rumors. Past
research has indicated that content-based (i.e., using solely source posts as
input) rumor detection models tend to perform less effectively on unseen
rumors. At the same time, the potential of context-based models remains largely
untapped. The main contribution of this paper is in the in-depth evaluation of
the performance gap between content and context-based models specifically on
detecting new, unseen rumors. Our empirical findings demonstrate that
context-based models are still overly dependent on the information derived from
the rumors&apos; source post and tend to overlook the significant role that
contextual information can play. We also study the effect of data split
strategies on classifier performance. Based on our experimental results, the
paper also offers practical suggestions on how to minimize the effects of
temporal concept drift in static datasets during the training of rumor
detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yida Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingyi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1&quot;&gt;Kalina Bontcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1&quot;&gt;Nikolaos Aletras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11582">
<title>Incorporating Singletons and Mention-based Features in Coreference Resolution via Multi-task Learning for Better Generalization. (arXiv:2309.11582v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11582</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous attempts to incorporate a mention detection step into end-to-end
neural coreference resolution for English have been hampered by the lack of
singleton mention span data as well as other entity information. This paper
presents a coreference model that learns singletons as well as features such as
entity type and information status via a multi-task learning-based approach.
This approach achieves new state-of-the-art scores on the OntoGUM benchmark
(+2.7 points) and increases robustness on multiple out-of-domain datasets (+2.3
points on average), likely due to greater generalizability for mention
detection and utilization of more data from singletons when compared to only
coreferent mention pair matching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yilun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Siyao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1&quot;&gt;Sameer Pradhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1&quot;&gt;Amir Zeldes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11585">
<title>SpeechAlign: a Framework for Speech Translation Alignment Evaluation. (arXiv:2309.11585v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11585</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-to-Speech and Speech-to-Text translation are currently dynamic areas
of research. To contribute to these fields, we present SpeechAlign, a framework
to evaluate the underexplored field of source-target alignment in speech
models. Our framework has two core components. First, to tackle the absence of
suitable evaluation datasets, we introduce the Speech Gold Alignment dataset,
built upon a English-German text translation gold alignment dataset. Secondly,
we introduce two novel metrics, Speech Alignment Error Rate (SAER) and
Time-weighted Speech Alignment Error Rate (TW-SAER), to evaluate alignment
quality in speech models. By publishing SpeechAlign we provide an accessible
evaluation framework for model assessment, and we employ it to benchmark
open-source Speech Translation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1&quot;&gt;Belen Alastruey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sant_A/0/1/0/all/0/1&quot;&gt;Aleix Sant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1&quot;&gt;Gerard I. G&amp;#xe1;llego&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dale_D/0/1/0/all/0/1&quot;&gt;David Dale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1&quot;&gt;Marta R. Costa-juss&amp;#xe0;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11611">
<title>Hate speech detection in algerian dialect using deep learning. (arXiv:2309.11611v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11611</link>
<description rdf:parseType="Literal">&lt;p&gt;With the proliferation of hate speech on social networks under different
formats, such as abusive language, cyberbullying, and violence, etc., people
have experienced a significant increase in violence, putting them in
uncomfortable situations and threats. Plenty of efforts have been dedicated in
the last few years to overcome this phenomenon to detect hate speech in
different structured languages like English, French, Arabic, and others.
However, a reduced number of works deal with Arabic dialects like Tunisian,
Egyptian, and Gulf, mainly the Algerian ones. To fill in the gap, we propose in
this work a complete approach for detecting hate speech on online Algerian
messages. Many deep learning architectures have been evaluated on the corpus we
created from some Algerian social networks (Facebook, YouTube, and Twitter).
This corpus contains more than 13.5K documents in Algerian dialect written in
Arabic, labeled as hateful or non-hateful. Promising results are obtained,
which show the efficiency of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanasri_D/0/1/0/all/0/1&quot;&gt;Dihia Lanasri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olano_J/0/1/0/all/0/1&quot;&gt;Juan Olano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klioui_S/0/1/0/all/0/1&quot;&gt;Sifal Klioui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sin Liang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sekkai_L/0/1/0/all/0/1&quot;&gt;Lamia Sekkai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11668">
<title>Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11668</link>
<description rdf:parseType="Literal">&lt;p&gt;Resolving semantic ambiguity has long been recognised as a central challenge
in the field of machine translation. Recent work on benchmarking translation
performance on ambiguous sentences has exposed the limitations of conventional
Neural Machine Translation (NMT) systems, which fail to capture many of these
cases. Large language models (LLMs) have emerged as a promising alternative,
demonstrating comparable performance to traditional NMT models while
introducing new paradigms for controlling the target outputs. In this paper, we
study the capabilities of LLMs to translate ambiguous sentences containing
polysemous words and rare word senses. We also propose two ways to improve the
handling of such ambiguity through in-context learning and fine-tuning on
carefully curated ambiguous datasets. Experiments show that our methods can
match or outperform state-of-the-art systems such as DeepL and NLLB in four out
of five language directions. Our research provides valuable insights into
effectively adapting LLMs for disambiguation during machine translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1&quot;&gt;Vivek Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pinzhen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1&quot;&gt;Alexandra Birch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11669">
<title>Construction of Paired Knowledge Graph-Text Datasets Informed by Cyclic Evaluation. (arXiv:2309.11669v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11669</link>
<description rdf:parseType="Literal">&lt;p&gt;Datasets that pair Knowledge Graphs (KG) and text together (KG-T) can be used
to train forward and reverse neural models that generate text from KG and vice
versa. However models trained on datasets where KG and text pairs are not
equivalent can suffer from more hallucination and poorer recall. In this paper,
we verify this empirically by generating datasets with different levels of
noise and find that noisier datasets do indeed lead to more hallucination. We
argue that the ability of forward and reverse models trained on a dataset to
cyclically regenerate source KG or text is a proxy for the equivalence between
the KG and the text in the dataset. Using cyclic evaluation we find that
manually created WebNLG is much better than automatically created TeKGen and
T-REx. Guided by these observations, we construct a new, improved dataset
called LAGRANGE using heuristics meant to improve equivalence between KG and
text and show the impact of each of the heuristics on cyclic evaluation. We
also construct two synthetic datasets using large language models (LLMs), and
observe that these are conducive to models that perform significantly well on
cyclic generation of text, but less so on cyclic generation of KGs, probably
because of a lack of a consistent underlying ontology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_A/0/1/0/all/0/1&quot;&gt;Ali Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1&quot;&gt;Xin Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;He Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1&quot;&gt;Peng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1&quot;&gt;Theo Rekatsinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Benjamin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pound_J/0/1/0/all/0/1&quot;&gt;Jeff Pound&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Josh Susskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schluter_N/0/1/0/all/0/1&quot;&gt;Natalie Schluter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1&quot;&gt;Ihab Ilyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaitly_N/0/1/0/all/0/1&quot;&gt;Navdeep Jaitly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11674">
<title>A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11674</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Large Language Models (LLMs) have achieved remarkable advancements
in various NLP tasks. However, these advances have not been reflected in the
translation task, especially those with moderate model sizes (i.e., 7B or 13B
parameters), which still lag behind conventional supervised encoder-decoder
translation models. Previous studies have attempted to improve the translation
capabilities of these moderate LLMs, but their gains have been limited. In this
study, we propose a novel fine-tuning approach for LLMs that is specifically
designed for the translation task, eliminating the need for the abundant
parallel data that traditional translation models usually depend on. Our
approach consists of two fine-tuning stages: initial fine-tuning on monolingual
data followed by subsequent fine-tuning on a small set of high-quality parallel
data. We introduce the LLM developed through this strategy as Advanced Language
Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying model, our
results show that the model can achieve an average improvement of more than 12
BLEU and 12 COMET over its zero-shot performance across 10 translation
directions from the WMT&apos;21 (2 directions) and WMT&apos;22 (8 directions) test
datasets. The performance is significantly better than all prior work and even
superior to the NLLB-54B model and GPT-3.5-text-davinci-003, with only 7B or
13B parameters. This method establishes the foundation for a novel training
paradigm in machine translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1&quot;&gt;Amr Sharaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_H/0/1/0/all/0/1&quot;&gt;Hany Hassan Awadalla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11688">
<title>LLM Guided Inductive Inference for Solving Compositional Problems. (arXiv:2309.11688v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11688</link>
<description rdf:parseType="Literal">&lt;p&gt;While large language models (LLMs) have demonstrated impressive performance
in question-answering tasks, their performance is limited when the questions
require knowledge that is not included in the model&apos;s training data and can
only be acquired through direct observation or interaction with the real world.
Existing methods decompose reasoning tasks through the use of modules invoked
sequentially, limiting their ability to answer deep reasoning tasks. We
introduce a method, Recursion based extensible LLM (REBEL), which handles
open-world, deep reasoning tasks by employing automated reasoning techniques
like dynamic planning and forward-chaining strategies. REBEL allows LLMs to
reason via recursive problem decomposition and utilization of external tools.
The tools that REBEL uses are specified only by natural language description.
We further demonstrate REBEL capabilities on a set of problems that require a
deeply nested use of external tools in a compositional and conversational
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sodani_A/0/1/0/all/0/1&quot;&gt;Abhigya Sodani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moos_L/0/1/0/all/0/1&quot;&gt;Lauren Moos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirman_M/0/1/0/all/0/1&quot;&gt;Matthew Mirman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11692">
<title>Semi-supervised News Discourse Profiling with Contrastive Learning. (arXiv:2309.11692v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11692</link>
<description rdf:parseType="Literal">&lt;p&gt;News Discourse Profiling seeks to scrutinize the event-related role of each
sentence in a news article and has been proven useful across various downstream
applications. Specifically, within the context of a given news discourse, each
sentence is assigned to a pre-defined category contingent upon its depiction of
the news event structure. However, existing approaches suffer from an
inadequacy of available human-annotated data, due to the laborious and
time-intensive nature of generating discourse-level annotations. In this paper,
we present a novel approach, denoted as Intra-document Contrastive Learning
with Distillation (ICLD), for addressing the news discourse profiling task,
capitalizing on its unique structural characteristics. Notably, we are the
first to apply a semi-supervised methodology within this task paradigm, and
evaluation demonstrates the effectiveness of the presented approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruihong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11696">
<title>Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11696</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable
proficiency in comprehending and generating natural language. However, their
unpersonalized generation paradigm may result in suboptimal user-specific
outcomes. Typically, users converse differently based on their knowledge and
preferences. This necessitates the task of enhancing user-oriented LLM which
remains unexplored. While one can fully train an LLM for this objective, the
resource consumption is unaffordable. Prior research has explored memory-based
methods to store and retrieve knowledge to enhance generation without
retraining for new queries. However, we contend that a mere memory module is
inadequate to comprehend a user&apos;s preference, and fully training an LLM can be
excessively costly. In this study, we propose a novel computational bionic
memory mechanism, equipped with a parameter-efficient fine-tuning schema, to
personalize LLMs. Our extensive experimental results demonstrate the
effectiveness and superiority of the proposed approach. To encourage further
research into this area, we are releasing a new conversation dataset generated
entirely by LLM based on an open-source medical corpus, as well as our
implementation code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fubang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11710">
<title>ContextRef: Evaluating Referenceless Metrics For Image Description Generation. (arXiv:2309.11710v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11710</link>
<description rdf:parseType="Literal">&lt;p&gt;Referenceless metrics (e.g., CLIPScore) use pretrained vision--language
models to assess image descriptions directly without costly ground-truth
reference texts. Such methods can facilitate rapid progress, but only if they
truly align with human preference judgments. In this paper, we introduce
ContextRef, a benchmark for assessing referenceless metrics for such alignment.
ContextRef has two components: human ratings along a variety of established
quality dimensions, and ten diverse robustness checks designed to uncover
fundamental weaknesses. A crucial aspect of ContextRef is that images and
descriptions are presented in context, reflecting prior work showing that
context is important for description quality. Using ContextRef, we assess a
variety of pretrained models, scoring functions, and techniques for
incorporating context. None of the methods is successful with ContextRef, but
we show that careful fine-tuning yields substantial improvements. ContextRef
remains a challenging benchmark though, in large part due to the challenge of
context dependence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreiss_E/0/1/0/all/0/1&quot;&gt;Elisa Kreiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zelikman_E/0/1/0/all/0/1&quot;&gt;Eric Zelikman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1&quot;&gt;Nick Haber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11791">
<title>SLHCat: Mapping Wikipedia Categories and Lists to DBpedia by Leveraging Semantic, Lexical, and Hierarchical Features. (arXiv:2309.11791v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2309.11791</link>
<description rdf:parseType="Literal">&lt;p&gt;Wikipedia articles are hierarchically organized through categories and lists,
providing one of the most comprehensive and universal taxonomy, but its open
creation is causing redundancies and inconsistencies. Assigning DBPedia classes
to Wikipedia categories and lists can alleviate the problem, realizing a large
knowledge graph which is essential for categorizing digital contents through
entity linking and typing. However, the existing approach of CaLiGraph is
producing incomplete and non-fine grained mappings. In this paper, we tackle
the problem as ontology alignment, where structural information of knowledge
graphs and lexical and semantic features of ontology class names are utilized
to discover confident mappings, which are in turn utilized for finetuing
pretrained language models in a distant supervision fashion. Our method SLHCat
consists of two main parts: 1) Automatically generating training data by
leveraging knowledge graph structure, semantic similarities, and named entity
typing. 2) Finetuning and prompt-tuning of the pre-trained language model BERT
are carried out over the training data, to capture semantic and syntactic
properties of class names. Our model SLHCat is evaluated over a benchmark
dataset constructed by annotating 3000 fine-grained CaLiGraph-DBpedia mapping
pairs. SLHCat is outperforming the baseline model by a large margin of 25% in
accuracy, offering a practical solution for large-scale ontology mapping.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jiaxin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwaihara_M/0/1/0/all/0/1&quot;&gt;Mizuho Iwaihara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11824">
<title>Word Embedding with Neural Probabilistic Prior. (arXiv:2309.11824v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11824</link>
<description rdf:parseType="Literal">&lt;p&gt;To improve word representation learning, we propose a probabilistic prior
which can be seamlessly integrated with word embedding models. Different from
previous methods, word embedding is taken as a probabilistic generative model,
and it enables us to impose a prior regularizing word representation learning.
The proposed prior not only enhances the representation of embedding vectors
but also improves the model&apos;s robustness and stability. The structure of the
proposed prior is simple and effective, and it can be easily implemented and
flexibly plugged in most existing word embedding models. Extensive experiments
show the proposed method improves word representation on various tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shaogang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dingcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11830">
<title>A Chinese Prompt Attack Dataset for LLMs with Evil Content. (arXiv:2309.11830v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11830</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) present significant priority in text
understanding and generation. However, LLMs suffer from the risk of generating
harmful contents especially while being employed to applications. There are
several black-box attack methods, such as Prompt Attack, which can change the
behaviour of LLMs and induce LLMs to generate unexpected answers with harmful
contents. Researchers are interested in Prompt Attack and Defense with LLMs,
while there is no publicly available dataset to evaluate the abilities of
defending prompt attack. In this paper, we introduce a Chinese Prompt Attack
Dataset for LLMs, called CPAD. Our prompts aim to induce LLMs to generate
unexpected outputs with several carefully designed prompt attack approaches and
widely concerned attacking contents. Different from previous datasets involving
safety estimation, We construct the prompts considering three dimensions:
contents, attacking methods and goals, thus the responses can be easily
evaluated and analysed. We run several well-known Chinese LLMs on our dataset,
and the results show that our prompts are significantly harmful to LLMs, with
around 70% attack success rate. We will release CPAD to encourage further
studies on prompt attack and defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fubang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qing_L/0/1/0/all/0/1&quot;&gt;Lizhi Qing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changlong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11838">
<title>Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues. (arXiv:2309.11838v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11838</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the use of large language models (LLMs) like
ChatGPT for document-grounded response generation in the context of
information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus
of task-oriented dialogues in four social service domains previously used in
the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded
in multiple documents providing relevant information. We generate dialogue
completion responses by prompting a ChatGPT model, using two methods:
Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT
model pretraining while LlamaIndex also extracts relevant information from
documents. Observing that document-grounded response generation via LLMs cannot
be adequately assessed by automatic evaluation metrics as they are
significantly more verbose, we perform a human evaluation where annotators rate
the output of the shared task winning system, the two Chat-GPT variants
outputs, and human responses. While both ChatGPT variants are more likely to
include information not present in the relevant segments, possibly including a
presence of hallucinations, they are rated higher than both the shared task
winning system and human responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braunschweiler_N/0/1/0/all/0/1&quot;&gt;Norbert Braunschweiler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1&quot;&gt;Rama Doddipatla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keizer_S/0/1/0/all/0/1&quot;&gt;Simon Keizer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1&quot;&gt;Svetlana Stoyanchev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11849">
<title>A Discourse-level Multi-scale Prosodic Model for Fine-grained Emotion Analysis. (arXiv:2309.11849v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.11849</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores predicting suitable prosodic features for fine-grained
emotion analysis from the discourse-level text. To obtain fine-grained
emotional prosodic features as predictive values for our model, we extract a
phoneme-level Local Prosody Embedding sequence (LPEs) and a Global Style
Embedding as prosodic speech features from the speech with the help of a style
transfer model. We propose a Discourse-level Multi-scale text Prosodic Model
(D-MPM) that exploits multi-scale text to predict these two prosodic features.
The proposed model can be used to analyze better emotional prosodic features
and thus guide the speech synthesis model to synthesize more expressive speech.
To quantitatively evaluate the proposed model, we contribute a new and
large-scale Discourse-level Chinese Audiobook (DCA) dataset with more than
13,000 utterances annotated sequences to evaluate the proposed model.
Experimental results on the DCA dataset show that the multi-scale text
information effectively helps to predict prosodic features, and the
discourse-level text improves both the overall coherence and the user
experience. More interestingly, although we aim at the synthesis effect of the
style transfer model, the synthesized speech by the proposed text prosodic
analysis model is even better than the style transfer from the original speech
in some user evaluation indicators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xianhao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jia Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11852">
<title>Knowledge Sanitization of Large Language Models. (arXiv:2309.11852v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11852</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore a knowledge sanitization approach to mitigate the privacy concerns
associated with large language models (LLMs). LLMs trained on a large corpus of
Web data can memorize and potentially reveal sensitive or confidential
information, raising critical security concerns. Our technique fine-tunes these
models, prompting them to generate harmless responses such as ``I don&apos;t know&apos;&apos;
when queried about specific information. Experimental results in a closed-book
question-answering task show that our straightforward method not only minimizes
particular knowledge leakage but also preserves the overall performance of LLM.
These two advantages strengthen the defense against extraction attacks and
reduces the emission of harmful content such as hallucinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishibashi_Y/0/1/0/all/0/1&quot;&gt;Yoichi Ishibashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimodaira_H/0/1/0/all/0/1&quot;&gt;Hidetoshi Shimodaira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11853">
<title>BitCoin: Bidirectional Tagging and Supervised Contrastive Learning based Joint Relational Triple Extraction Framework. (arXiv:2309.11853v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11853</link>
<description rdf:parseType="Literal">&lt;p&gt;Relation triple extraction (RTE) is an essential task in information
extraction and knowledge graph construction. Despite recent advancements,
existing methods still exhibit certain limitations. They just employ
generalized pre-trained models and do not consider the specificity of RTE
tasks. Moreover, existing tagging-based approaches typically decompose the RTE
task into two subtasks, initially identifying subjects and subsequently
identifying objects and relations. They solely focus on extracting relational
triples from subject to object, neglecting that once the extraction of a
subject fails, it fails in extracting all triples associated with that subject.
To address these issues, we propose BitCoin, an innovative Bidirectional
tagging and supervised Contrastive learning based joint relational triple
extraction framework. Specifically, we design a supervised contrastive learning
method that considers multiple positives per anchor rather than restricting it
to just one positive. Furthermore, a penalty term is introduced to prevent
excessive similarity between the subject and object. Our framework implements
taggers in two directions, enabling triples extraction from subject to object
and object to subject. Experimental results show that BitCoin achieves
state-of-the-art results on the benchmark datasets and significantly improves
the F1 score on Normal, SEO, EPO, and multiple relation extraction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Luyao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongbao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Sen Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11869">
<title>Syntactic Variation Across the Grammar: Modelling a Complex Adaptive System. (arXiv:2309.11869v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11869</link>
<description rdf:parseType="Literal">&lt;p&gt;While language is a complex adaptive system, most work on syntactic variation
observes a few individual constructions in isolation from the rest of the
grammar. This means that the grammar, a network which connects thousands of
structures at different levels of abstraction, is reduced to a few disconnected
variables. This paper quantifies the impact of such reductions by
systematically modelling dialectal variation across 49 local populations of
English speakers in 16 countries. We perform dialect classification with both
an entire grammar as well as with isolated nodes within the grammar in order to
characterize the syntactic differences between these dialects. The results
show, first, that many individual nodes within the grammar are subject to
variation but, in isolation, none perform as well as the grammar as a whole.
This indicates that an important part of syntactic variation consists of
interactions between different parts of the grammar. Second, the results show
that the similarity between dialects depends heavily on the sub-set of the
grammar being observed: for example, New Zealand English could be more similar
to Australian English in phrasal verbs but at the same time more similar to UK
English in dative phrases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1&quot;&gt;Jonathan Dunn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11888">
<title>Is It Really Useful to Jointly Parse Constituency and Dependency Trees? A Revisit. (arXiv:2309.11888v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11888</link>
<description rdf:parseType="Literal">&lt;p&gt;This work visits the topic of jointly parsing constituency and dependency
trees, i.e., to produce compatible constituency and dependency trees
simultaneously for input sentences, which is attractive considering that the
two types of trees are complementary in representing syntax. Compared with
previous works, we make progress in four aspects: (1) adopting a much more
efficient decoding algorithm, (2) exploring joint modeling at the training
phase, instead of only at the inference phase, (3) proposing high-order scoring
components for constituent-dependency interaction, (4) gaining more insights
via in-depth experiments and analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yanggang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhefeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1&quot;&gt;Xinyu Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11895">
<title>Audio Contrastive based Fine-tuning. (arXiv:2309.11895v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2309.11895</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio classification plays a crucial role in speech and sound processing
tasks with a wide range of applications. There still remains a challenge of
striking the right balance between fitting the model to the training data
(avoiding overfitting) and enabling it to generalise well to a new domain.
Leveraging the transferability of contrastive learning, we introduce Audio
Contrastive-based Fine-tuning (AudioConFit), an efficient approach
characterised by robust generalisability. Empirical experiments on a variety of
audio classification tasks demonstrate the effectiveness and robustness of our
approach, which achieves state-of-the-art results in various settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Q/0/1/0/all/0/1&quot;&gt;Qibin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chenghao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1&quot;&gt;Noura Al Moubayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenghua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11896">
<title>Focal Inferential Infusion Coupled with Tractable Density Discrimination for Implicit Hate Speech Detection. (arXiv:2309.11896v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11896</link>
<description rdf:parseType="Literal">&lt;p&gt;Although pre-trained large language models (PLMs) have achieved
state-of-the-art on many NLP tasks, they lack understanding of subtle
expressions of implicit hate speech. Such nuanced and implicit hate is often
misclassified as non-hate. Various attempts have been made to enhance the
detection of (implicit) hate content by augmenting external context or
enforcing label separation via distance-based metrics. We combine these two
approaches and introduce FiADD, a novel Focused Inferential Adaptive Density
Discrimination framework. FiADD enhances the PLM finetuning pipeline by
bringing the surface form of an implicit hate speech closer to its implied form
while increasing the inter-cluster distance among various class labels. We test
FiADD on three implicit hate datasets and observe significant improvement in
the two-way and three-way hate classification tasks. We further experiment on
the generalizability of FiADD on three other tasks, namely detecting sarcasm,
irony, and stance, in which surface and implied forms differ, and observe
similar performance improvement. We analyze the generated latent space to
understand its evolution under FiADD, which corroborates the advantage of
employing FiADD for implicit hate speech detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masud_S/0/1/0/all/0/1&quot;&gt;Sarah Masud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajpai_A/0/1/0/all/0/1&quot;&gt;Ashutosh Bajpai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11911">
<title>InstructERC: Reforming Emotion Recognition in Conversation with a Retrieval Multi-task LLMs Framework. (arXiv:2309.11911v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11911</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of emotion recognition in dialogue (ERC) has been
consistently hindered by the complexity of pipeline designs, leading to ERC
models that often overfit to specific datasets and dialogue patterns. In this
study, we propose a novel approach, namely
&lt;/p&gt;
&lt;p&gt;InstructERC, to reformulates the ERC task from a discriminative framework to
a generative framework based on Large Language Models (LLMs) . InstructERC has
two significant contributions: Firstly, InstructERC introduces a simple yet
effective retrieval template module, which helps the model explicitly integrate
multi-granularity dialogue supervision information by concatenating the
historical dialog content, label statement, and emotional domain demonstrations
with high semantic similarity. Furthermore, we introduce two additional emotion
alignment tasks, namely speaker identification and emotion prediction tasks, to
implicitly model the dialogue role relationships and future emotional
tendencies in conversations. Our LLM-based plug-and-play plugin framework
significantly outperforms all previous models and achieves comprehensive SOTA
on three commonly used ERC datasets. Extensive analysis of parameter-efficient
and data-scaling experiments provide empirical guidance for applying
InstructERC in practical scenarios. Our code will be released after blind
review.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1&quot;&gt;Shanglin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Keheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sirui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11925">
<title>Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality Estimation Shared Task. (arXiv:2309.11925v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11925</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the joint contribution of Unbabel and Instituto Superior T\&apos;ecnico
to the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated
on all tasks: sentence- and word-level quality prediction (task 1) and
fine-grained error span detection (task 2). For all tasks, we build on the
COMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked
first for all tasks, reaching state-of-the-art performance for quality
estimation at word-, span- and sentence-level granularity. Compared to the
previous state-of-the-art COMETKIWI-22, we show large improvements in
correlation with human judgements (up to 10 Spearman points). Moreover, we
surpass the second-best multilingual submission to the shared-task with up to
3.8 absolute points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1&quot;&gt;Ricardo Rei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1&quot;&gt;Nuno M. Guerreiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pombal_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Pombal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stigt_D/0/1/0/all/0/1&quot;&gt;Daan van Stigt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1&quot;&gt;Marcos Treviso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1&quot;&gt;Luisa Coheur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souza_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; G.C. de Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; F.T. Martins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11979">
<title>Stock Market Sentiment Classification and Backtesting via Fine-tuned BERT. (arXiv:2309.11979v1 [q-fin.CP])</title>
<link>http://arxiv.org/abs/2309.11979</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of big data and computing devices, low-latency
automatic trading platforms based on real-time information acquisition have
become the main components of the stock trading market, so the topic of
quantitative trading has received widespread attention. And for non-strongly
efficient trading markets, human emotions and expectations always dominate
market trends and trading decisions. Therefore, this paper starts from the
theory of emotion, taking East Money as an example, crawling user comment
titles data from its corresponding stock bar and performing data cleaning.
Subsequently, a natural language processing model BERT was constructed, and the
BERT model was fine-tuned using existing annotated data sets. The experimental
results show that the fine-tuned model has different degrees of performance
improvement compared to the original model and the baseline model.
Subsequently, based on the above model, the user comment data crawled is
labeled with emotional polarity, and the obtained label information is combined
with the Alpha191 model to participate in regression, and significant
regression results are obtained. Subsequently, the regression model is used to
predict the average price change for the next five days, and use it as a signal
to guide automatic trading. The experimental results show that the
incorporation of emotional factors increased the return rate by 73.8\% compared
to the baseline during the trading period, and by 32.41\% compared to the
original alpha191 model. Finally, we discuss the advantages and disadvantages
of incorporating emotional factors into quantitative trading, and give possible
directions for further research in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Lou_J/0/1/0/all/0/1&quot;&gt;Jiashu Lou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11981">
<title>Rethinking the Evaluating Framework for Natural Language Understanding in AI Systems: Language Acquisition as a Core for Future Metrics. (arXiv:2309.11981v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11981</link>
<description rdf:parseType="Literal">&lt;p&gt;In the burgeoning field of artificial intelligence (AI), the unprecedented
progress of large language models (LLMs) in natural language processing (NLP)
offers an opportunity to revisit the entire approach of traditional metrics of
machine intelligence, both in form and content. As the realm of machine
cognitive evaluation has already reached Imitation, the next step is an
efficient Language Acquisition and Understanding. Our paper proposes a paradigm
shift from the established Turing Test towards an all-embracing framework that
hinges on language acquisition, taking inspiration from the recent advancements
in LLMs. The present contribution is deeply tributary of the excellent work
from various disciplines, point out the need to keep interdisciplinary bridges
open, and delineates a more robust and sustainable approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vera_P/0/1/0/all/0/1&quot;&gt;Patricio Vera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_P/0/1/0/all/0/1&quot;&gt;Pedro Moya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barraza_L/0/1/0/all/0/1&quot;&gt;Lisa Barraza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11998">
<title>LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset. (arXiv:2309.11998v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.11998</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying how people interact with large language models (LLMs) in real-world
scenarios is increasingly important due to their widespread use in various
applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset
containing one million real-world conversations with 25 state-of-the-art LLMs.
This dataset is collected from 210K unique IP addresses in the wild on our
Vicuna demo and Chatbot Arena website. We offer an overview of the dataset&apos;s
content, including its curation process, basic statistics, and topic
distribution, highlighting its diversity, originality, and scale. We
demonstrate its versatility through four use cases: developing content
moderation models that perform similarly to GPT-4, building a safety benchmark,
training instruction-following models that perform similarly to Vicuna, and
creating challenging benchmark questions. We believe that this dataset will
serve as a valuable resource for understanding and advancing LLM capabilities.
The dataset is publicly available at
\url{https://huggingface.co/datasets/lmsys/lmsys-chat-1m}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lianmin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_W/0/1/0/all/0/1&quot;&gt;Wei-Lin Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Y/0/1/0/all/0/1&quot;&gt;Ying Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1&quot;&gt;Siyuan Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhanghao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric. P Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1&quot;&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1&quot;&gt;Ion Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12030">
<title>CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation. (arXiv:2309.12030v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12030</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the limitations of manual online ad production, significant
research has been conducted in the field of automatic ad text generation (ATG).
However, comparing different methods has been challenging because of the lack
of benchmarks encompassing the entire field and the absence of well-defined
problem sets with clear model inputs and outputs. To address these challenges,
this paper aims to advance the field of ATG by introducing a redesigned task
and constructing a benchmark. Specifically, we defined ATG as a
cross-application task encompassing various aspects of the Internet
advertising. As part of our contribution, we propose a first benchmark dataset,
CA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed
for ATG to be able to leverage multi-modal information and conduct an
industry-wise evaluation. Furthermore, we demonstrate the usefulness of our
proposed benchmark through evaluation experiments using multiple baseline
models, which vary in terms of the type of pre-trained language model used and
the incorporation of multi-modal information. We also discuss the current state
of the task and the future challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mita_M/0/1/0/all/0/1&quot;&gt;Masato Mita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakami_S/0/1/0/all/0/1&quot;&gt;Soichiro Murakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kato_A/0/1/0/all/0/1&quot;&gt;Akihiko Kato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12053">
<title>AceGPT, Localizing Large Language Models in Arabic. (arXiv:2309.12053v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12053</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the imperative need and methodology for developing a
localized Large Language Model (LLM) tailored for Arabic, a language with
unique cultural characteristics that are not adequately addressed by current
mainstream models like ChatGPT. Key concerns additionally arise when
considering cultural sensitivity and local values. To this end, the paper
outlines a packaged solution, including further pre-training with Arabic texts,
supervised fine-tuning (SFT) using native Arabic instructions and GPT-4
responses in Arabic, and reinforcement learning with AI feedback (RLAIF) using
a reward model that is sensitive to local culture and values. The objective is
to train culturally aware and value-aligned Arabic LLMs that can serve the
diverse application-specific needs of Arabic-speaking communities.
&lt;/p&gt;
&lt;p&gt;Extensive evaluations demonstrated that the resulting LLM called
`\textbf{AceGPT}&apos; is the SOTA open Arabic LLM in various benchmarks, including
instruction-following benchmark (i.e., Arabic Vicuna-80 and Arabic AlpacaEval),
knowledge benchmark (i.e., Arabic MMLU and EXAMs), as well as the
newly-proposed Arabic cultural \&amp;amp; value alignment benchmark. Notably, AceGPT
outperforms ChatGPT in the popular Vicuna-80 benchmark when evaluated with
GPT-4, despite the benchmark&apos;s limited scale. % Natural Language Understanding
(NLU) benchmark (i.e., ALUE)
&lt;/p&gt;
&lt;p&gt;Codes, data, and models are in https://github.com/FreedomIntelligence/AceGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xuening Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dingjie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alharthi_A/0/1/0/all/0/1&quot;&gt;Abdulmohsen Alharthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziche Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benyou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Ruoyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haizhou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jinchao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12056">
<title>BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents BELT, a novel model and learning framework for the
pivotal topic of brain-to-language translation research. The translation from
noninvasive brain signals into readable natural language has the potential to
promote the application scenario as well as the development of brain-computer
interfaces (BCI) as a whole. The critical problem in brain signal decoding or
brain-to-language translation is the acquisition of semantically appropriate
and discriminative EEG representation from a dataset of limited scale and
quality. The proposed BELT method is a generic and efficient framework that
bootstraps EEG representation learning using off-the-shelf large-scale
pretrained language models (LMs). With a large LM&apos;s capacity for understanding
semantic information and zero-shot generalization, BELT utilizes large LMs
trained on Internet-scale datasets to bring significant improvements to the
understanding of EEG signals.
&lt;/p&gt;
&lt;p&gt;In particular, the BELT model is composed of a deep conformer encoder and a
vector quantization encoder. Semantical EEG representation is achieved by a
contrastive learning step that provides natural language supervision. We
achieve state-of-the-art results on two featuring brain decoding tasks
including the brain-to-language translation and zero-shot sentiment
classification. Specifically, our model surpasses the baseline model on both
tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%
precision on the main evaluation metrics for translation and zero-shot
sentiment classification respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinzhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yiqun Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12071">
<title>Benchmarking quantized LLaMa-based models on the Brazilian Secondary School Exam. (arXiv:2309.12071v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.12071</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Large Language Models (LLMs) represent a revolution in the way we
interact with computers, allowing the construction of complex questions and the
ability to reason over a sequence of statements, their use is restricted due to
the need for dedicated hardware for execution. In this study, we evaluate the
performance of LLMs based on the 7 and 13 billion LLaMA models, subjected to a
quantization process and run on home hardware. The models considered were
Alpaca, Koala, and Vicuna. To evaluate the effectiveness of these models, we
developed a database containing 1,006 questions from the ENEM (Brazilian
National Secondary School Exam). Our analysis revealed that the best performing
models achieved an accuracy of approximately 46% for the original texts of the
Portuguese questions and 49% on their English translations. In addition, we
evaluated the computational efficiency of the models by measuring the time
required for execution. On average, the 7 and 13 billion LLMs took
approximately 20 and 50 seconds, respectively, to process the queries on a
machine equipped with an AMD Ryzen 5 3600x processor
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Matheus L. O. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campelo_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe1;udio E. C. Campelo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12075">
<title>Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12075</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt Tuning is emerging as a scalable and cost-effective method to
fine-tune Pretrained Language Models (PLMs). This study benchmarks the
performance and computational efficiency of Prompt Tuning and baseline methods
on a multi-label text classification task. This is applied to the use case of
classifying companies into an investment firm&apos;s proprietary industry taxonomy,
supporting their thematic investment strategy. Text-to-text classification with
PLMs is frequently reported to outperform classification with a classification
head, but has several limitations when applied to a multi-label classification
problem where each label consists of multiple tokens: (a) Generated labels may
not match any label in the industry taxonomy; (b) During fine-tuning, multiple
labels must be provided in an arbitrary order; (c) The model provides a binary
decision for each label, rather than an appropriate confidence score.
Limitation (a) is addressed by applying constrained decoding using Trie Search,
which slightly improves classification performance. All limitations (a), (b),
and (c) are addressed by replacing the PLM&apos;s language head with a
classification head. This improves performance significantly, while also
reducing computational costs during inference. The results indicate the
continuing need to adapt state-of-the-art methods to domain-specific tasks,
even in the era of PLMs with strong generalization abilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchner_V/0/1/0/all/0/1&quot;&gt;Valentin Leonhard Buchner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Lele Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalo_J/0/1/0/all/0/1&quot;&gt;Jan-Christoph Kalo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12102">
<title>SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts. (arXiv:2309.12102v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12102</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe SemEval-2022 Task 7, a shared task on rating the plausibility of
clarifications in instructional texts. The dataset for this task consists of
manually clarified how-to guides for which we generated alternative
clarifications and collected human plausibility judgements. The task of
participating systems was to automatically determine the plausibility of a
clarification in the respective context. In total, 21 participants took part in
this task, with the best system achieving an accuracy of 68.9%. This report
summarizes the results and findings from 8 teams and their system descriptions.
Finally, we show in an additional evaluation that predictions by the top
participating team make it possible to identify contexts with multiple
plausible clarifications with an accuracy of 75.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1&quot;&gt;Michael Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthonio_T/0/1/0/all/0/1&quot;&gt;Talita Anthonio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1&quot;&gt;Anna Sauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12107">
<title>A Computational Analysis of Vagueness in Revisions of Instructional Texts. (arXiv:2309.12107v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12107</link>
<description rdf:parseType="Literal">&lt;p&gt;WikiHow is an open-domain repository of instructional articles for a variety
of tasks, which can be revised by users. In this paper, we extract pairwise
versions of an instruction before and after a revision was made. Starting from
a noisy dataset of revision histories, we specifically extract and analyze
edits that involve cases of vagueness in instructions. We further investigate
the ability of a neural model to distinguish between two versions of an
instruction in our data by adopting a pairwise ranking task from previous work
and showing improvements over existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1&quot;&gt;Alok Debnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1&quot;&gt;Michael Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12109">
<title>PEFTT: Parameter-Efficient Fine-Tuning for low-resource Tibetan pre-trained language models. (arXiv:2309.12109v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12109</link>
<description rdf:parseType="Literal">&lt;p&gt;In this era of large language models (LLMs), the traditional training of
models has become increasingly unimaginable for regular users and institutions.
The exploration of efficient fine-tuning for high-resource languages on these
models is an undeniable trend that is gradually gaining popularity. However,
there has been very little exploration for various low-resource languages, such
as Tibetan. Research in Tibetan NLP is inherently scarce and limited. While
there is currently no existing large language model for Tibetan due to its
low-resource nature, that day will undoubtedly arrive. Therefore, research on
efficient fine-tuning for low-resource language models like Tibetan is highly
necessary. Our research can serve as a reference to fill this crucial gap.
Efficient fine-tuning strategies for pre-trained language models (PLMs) in
Tibetan have seen minimal exploration. We conducted three types of efficient
fine-tuning experiments on the publicly available TNCC-title dataset:
&quot;prompt-tuning,&quot; &quot;Adapter lightweight fine-tuning,&quot; and &quot;prompt-tuning +
Adapter fine-tuning.&quot; The experimental results demonstrate significant
improvements using these methods, providing valuable insights for advancing
Tibetan language applications in the context of pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingjun_Z/0/1/0/all/0/1&quot;&gt;Zhou Mingjun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuoma_D/0/1/0/all/0/1&quot;&gt;Daiqing Zhuoma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuo_Q/0/1/0/all/0/1&quot;&gt;Qun Nuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tashi_N/0/1/0/all/0/1&quot;&gt;Nyima Tashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12117">
<title>How-to Guides for Specific Audiences: A Corpus and Initial Findings. (arXiv:2309.12117v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12117</link>
<description rdf:parseType="Literal">&lt;p&gt;Instructional texts for specific target groups should ideally take into
account the prior knowledge and needs of the readers in order to guide them
efficiently to their desired goals. However, targeting specific groups also
carries the risk of reflecting disparate social norms and subtle stereotypes.
In this paper, we investigate the extent to which how-to guides from one
particular platform, wikiHow, differ in practice depending on the intended
audience. We conduct two case studies in which we examine qualitative features
of texts written for specific audiences. In a generalization study, we
investigate which differences can also be systematically demonstrated using
computational methods. The results of our studies show that guides from
wikiHow, like other text genres, are subject to subtle biases. We aim to raise
awareness of these inequalities as a first step to addressing them in future
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanton_N/0/1/0/all/0/1&quot;&gt;Nicola Fanton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falenska_A/0/1/0/all/0/1&quot;&gt;Agnieszka Falenska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_M/0/1/0/all/0/1&quot;&gt;Michael Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12137">
<title>OSN-MDAD: Machine Translation Dataset for Arabic Multi-Dialectal Conversations on Online Social Media. (arXiv:2309.12137v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12137</link>
<description rdf:parseType="Literal">&lt;p&gt;While resources for English language are fairly sufficient to understand
content on social media, similar resources in Arabic are still immature. The
main reason that the resources in Arabic are insufficient is that Arabic has
many dialects in addition to the standard version (MSA). Arabs do not use MSA
in their daily communications; rather, they use dialectal versions.
Unfortunately, social users transfer this phenomenon into their use of social
media platforms, which in turn has raised an urgent need for building suitable
AI models for language-dependent applications. Existing machine translation
(MT) systems designed for MSA fail to work well with Arabic dialects. In light
of this, it is necessary to adapt to the informal nature of communication on
social networks by developing MT systems that can effectively handle the
various dialects of Arabic. Unlike for MSA that shows advanced progress in MT
systems, little effort has been exerted to utilize Arabic dialects for MT
systems. While few attempts have been made to build translation datasets for
dialectal Arabic, they are domain dependent and are not OSN cultural-language
friendly. In this work, we attempt to alleviate these limitations by proposing
an online social network-based multidialect Arabic dataset that is crafted by
contextually translating English tweets into four Arabic dialects: Gulf,
Yemeni, Iraqi, and Levantine. To perform the translation, we followed our
proposed guideline framework for content translation, which could be
universally applicable for translation between foreign languages and local
dialects. We validated the authenticity of our proposed dataset by developing
neural MT models for four Arabic dialects. Our results have shown a superior
performance of our NMT models trained using our dataset. We believe that our
dataset can reliably serve as an Arabic multidialectal translation dataset for
informal MT tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alzamzami_F/0/1/0/all/0/1&quot;&gt;Fatimah Alzamzami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12161">
<title>Code Soliloquies for Accurate Calculations in Large Language Models. (arXiv:2309.12161v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12161</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality conversational datasets are integral to the successful
development of Intelligent Tutoring Systems (ITS) that employ a Large Language
Model (LLM) backend. These datasets, when used to fine-tune the LLM backend,
significantly enhance the quality of interactions between students and ITS. A
common strategy for developing these datasets involves generating synthetic
student-teacher dialogues using advanced GPT-4 models. However, challenges
arise when these dialogues demand complex calculations, common in subjects like
physics. Despite its advanced capabilities, GPT-4&apos;s performance falls short in
reliably handling even simple multiplication tasks, marking a significant
limitation in its utility for these subjects. To address these challenges, this
paper introduces an innovative stateful prompt design. Our approach generates a
mock conversation between a student and a tutorbot, both roles simulated by
GPT-4. Each student response triggers a soliloquy (an inner monologue) in the
GPT-tutorbot, which assesses whether its response would necessitate
calculations. If so, it proceeds to script the required code in Python and then
uses the resulting output to construct its response to the student. Our
approach notably enhances the quality of synthetic conversation datasets,
especially for subjects that are calculation-intensive. Our findings show that
our Higgs model -- a LLaMA finetuned with datasets generated through our novel
stateful prompt design -- proficiently utilizes Python for computations.
Consequently, finetuning with our datasets enriched with code soliloquies
enhances not just the accuracy but also the computational reliability of Higgs&apos;
responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonkar_S/0/1/0/all/0/1&quot;&gt;Shashank Sonkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;MyCo Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Naiming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_D/0/1/0/all/0/1&quot;&gt;Debshila Basu Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1&quot;&gt;Richard G. Baraniuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12224">
<title>Towards Answering Health-related Questions from Medical Videos: Datasets and Approaches. (arXiv:2309.12224v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12224</link>
<description rdf:parseType="Literal">&lt;p&gt;The increase in the availability of online videos has transformed the way we
access information and knowledge. A growing number of individuals now prefer
instructional videos as they offer a series of step-by-step procedures to
accomplish particular tasks. The instructional videos from the medical domain
may provide the best possible visual answers to first aid, medical emergency,
and medical education questions. Toward this, this paper is focused on
answering health-related questions asked by the public by providing visual
answers from medical videos. The scarcity of large-scale datasets in the
medical domain is a key challenge that hinders the development of applications
that can help the public with their health-related questions. To address this
issue, we first proposed a pipelined approach to create two large-scale
datasets: HealthVidQA-CRF and HealthVidQA-Prompt. Later, we proposed monomodal
and multimodal approaches that can effectively provide visual answers from
medical videos to natural language questions. We conducted a comprehensive
analysis of the results, focusing on the impact of the created datasets on
model training and the significance of visual features in enhancing the
performance of the monomodal and multi-modal approaches. Our findings suggest
that these datasets have the potential to enhance the performance of medical
visual answer localization tasks and provide a promising future direction to
further enhance the performance by using pre-trained language-vision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attal_K/0/1/0/all/0/1&quot;&gt;Kush Attal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1&quot;&gt;Dina Demner-Fushman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12234">
<title>Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition. (arXiv:2309.12234v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12234</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present synchronous bilingual Connectionist Temporal
Classification (CTC), an innovative framework that leverages dual CTC to bridge
the gaps of both modality and language in the speech translation (ST) task.
Utilizing transcript and translation as concurrent objectives for CTC, our
model bridges the gap between audio and text as well as between source and
target languages. Building upon the recent advances in CTC application, we
develop an enhanced variant, BiL-CTC+, that establishes new state-of-the-art
performances on the MuST-C ST benchmarks under resource-constrained scenarios.
Intriguingly, our method also yields significant improvements in speech
recognition performance, revealing the effect of cross-lingual learning on
transcription and demonstrating its broad applicability. The source code is
available at https://github.com/xuchennlp/S2T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_E/0/1/0/all/0/1&quot;&gt;Erfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1&quot;&gt;Qianqian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_D/0/1/0/all/0/1&quot;&gt;Dapeng Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12244">
<title>ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events. (arXiv:2309.12244v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Children typically learn to identify and express emotions through sharing
their stories and feelings with others, particularly their family. However, it
is challenging for parents or siblings to have emotional communication with
children since children are still developing their communication skills. We
present ChaCha, a chatbot that encourages and guides children to share personal
events and associated emotions. ChaCha combines a state machine and large
language models (LLMs) to keep the dialogue on track while carrying on
free-form conversations. Through an exploratory study with 20 children (aged
8-12), we examine how ChaCha prompts children to share personal events and
guides them to describe associated emotions. Participants perceived ChaCha as a
close friend and shared their stories on various topics, such as family trips
and personal achievements. Based on the quantitative and qualitative findings,
we discuss opportunities for leveraging LLMs to design child-friendly chatbots
to support children in sharing their emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_W/0/1/0/all/0/1&quot;&gt;Woosuk Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chanmo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young-Ho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12247">
<title>Bad Actor, Good Advisor: Exploring the Role of Large Language Models in Fake News Detection. (arXiv:2309.12247v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting fake news requires both a delicate sense of diverse clues and a
profound understanding of the real-world background, which remains challenging
for detectors based on small language models (SLMs) due to their knowledge and
capability limitations. Recent advances in large language models (LLMs) have
shown remarkable performance in various tasks, but whether and how LLMs could
help with fake news detection remains underexplored. In this paper, we
investigate the potential of LLMs in fake news detection. First, we conduct an
empirical study and find that a sophisticated LLM such as GPT 3.5 could
generally expose fake news and provide desirable multi-perspective rationales
but still underperforms the basic SLM, fine-tuned BERT. Our subsequent analysis
attributes such a gap to the LLM&apos;s inability to select and integrate rationales
properly to conclude. Based on these findings, we propose that current LLMs may
not substitute fine-tuned SLMs in fake news detection but can be a good advisor
for SLMs by providing multi-perspective instructive rationales. To instantiate
this proposal, we design an adaptive rationale guidance network for fake news
detection (ARG), in which SLMs selectively acquire insights on news analysis
from the LLMs&apos; rationales. We further derive a rationale-free version of ARG by
distillation, namely ARG-D, which services cost-sensitive scenarios without
inquiring LLMs. Experiments on two real-world datasets demonstrate that ARG and
ARG-D outperform three types of baseline methods, including SLM-based,
LLM-based, and combinations of small and large language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Beizhe Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1&quot;&gt;Qiang Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Juan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuhui Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_P/0/1/0/all/0/1&quot;&gt;Peng Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12250">
<title>SQUARE: Automatic Question Answering Evaluation using Multiple Positive and Negative References. (arXiv:2309.12250v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12250</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluation of QA systems is very challenging and expensive, with the most
reliable approach being human annotations of correctness of answers for
questions. Recent works (AVA, BEM) have shown that transformer LM encoder based
similarity metrics transfer well for QA evaluation, but they are limited by the
usage of a single correct reference answer. We propose a new evaluation metric:
SQuArE (Sentence-level QUestion AnsweRing Evaluation), using multiple reference
answers (combining multiple correct and incorrect references) for sentence-form
QA. We evaluate SQuArE on both sentence-level extractive (Answer Selection) and
generative (GenQA) QA systems, across multiple academic and industrial
datasets, and show that it outperforms previous baselines and obtains the
highest correlation with human annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabburo_M/0/1/0/all/0/1&quot;&gt;Matteo Gabburo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddhant Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kedziorski_R/0/1/0/all/0/1&quot;&gt;Rik Koncel Kedziorski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1&quot;&gt;Alessandro Moschitti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12263">
<title>On the Relationship between Skill Neurons and Robustness in Prompt Tuning. (arXiv:2309.12263v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12263</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt Tuning is a popular parameter-efficient finetuning method for
pre-trained large language models (PLMs). Recently, based on experiments with
RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in
the transformer&apos;s feed-forward networks, that are highly predictive and
selective for the given task. In this paper, we study the robustness of Prompt
Tuning in relation to these &quot;skill neurons&quot;, using RoBERTa and T5. We show that
prompts tuned for a specific task are transferable to tasks of the same type
but are not very robust to adversarial data, with higher robustness for T5 than
RoBERTa. At the same time, we replicate the existence of skill neurons in
RoBERTa and further show that skill neurons also seem to exist in T5.
Interestingly, the skill neurons of T5 determined on non-adversarial data are
also among the most predictive neurons on the adversarial data, which is not
the case for RoBERTa. We conclude that higher adversarial robustness may be
related to a model&apos;s ability to activate the relevant skill neurons on
adversarial data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ackermann_L/0/1/0/all/0/1&quot;&gt;Leon Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1&quot;&gt;Xenia Ohmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12269">
<title>The Cambridge Law Corpus: A Corpus for Legal AI Research. (arXiv:2309.12269v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12269</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Cambridge Law Corpus (CLC), a corpus for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostling_A/0/1/0/all/0/1&quot;&gt;Andreas &amp;#xd6;stling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sargeant_H/0/1/0/all/0/1&quot;&gt;Holli Sargeant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Huiyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1&quot;&gt;Ludwig Bull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terenin_A/0/1/0/all/0/1&quot;&gt;Alexander Terenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jonsson_L/0/1/0/all/0/1&quot;&gt;Leif Jonsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magnusson_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;ns Magnusson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steffek_F/0/1/0/all/0/1&quot;&gt;Felix Steffek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12273">
<title>Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports. (arXiv:2309.12273v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12273</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid and accurate identification of Venous thromboembolism (VTE), a severe
cardiovascular condition including deep vein thrombosis (DVT) and pulmonary
embolism (PE), is important for effective treatment. Leveraging Natural
Language Processing (NLP) on radiology reports, automated methods have shown
promising advancements in identifying VTE events from retrospective data
cohorts or aiding clinical experts in identifying VTE events from radiology
reports. However, effectively training Deep Learning (DL) and the NLP models is
challenging due to limited labeled medical text data, the complexity and
heterogeneity of radiology reports, and data imbalance. This study proposes
novel method combinations of DL methods, along with data augmentation, adaptive
pre-trained NLP model selection, and a clinical expert NLP rule-based
classifier, to improve the accuracy of VTE identification in unstructured
(free-text) radiology reports. Our experimental results demonstrate the model&apos;s
efficacy, achieving an impressive 97\% accuracy and 97\% F1 score in predicting
DVT, and an outstanding 98.3\% accuracy and 98.4\% F1 score in predicting PE.
These findings emphasize the model&apos;s robustness and its potential to
significantly contribute to VTE research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jamie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yusen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayssen_H/0/1/0/all/0/1&quot;&gt;Hilary Hayssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Englum_B/0/1/0/all/0/1&quot;&gt;Brain Englum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankaria_A/0/1/0/all/0/1&quot;&gt;Aman Kankaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayorga_Carlin_M/0/1/0/all/0/1&quot;&gt;Minerva Mayorga-Carlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1&quot;&gt;Shalini Sahoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sorkin_J/0/1/0/all/0/1&quot;&gt;John Sorkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_B/0/1/0/all/0/1&quot;&gt;Brajesh Lal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yesha_Y/0/1/0/all/0/1&quot;&gt;Yelena Yesha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12276">
<title>LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.12276</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR&apos;s cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torre_F/0/1/0/all/0/1&quot;&gt;Fernanda De La Torre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Cathy Mengying Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Han Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banburski_Fahey_A/0/1/0/all/0/1&quot;&gt;Andrzej Banburski-Fahey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_J/0/1/0/all/0/1&quot;&gt;Judith Amores Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanier_J/0/1/0/all/0/1&quot;&gt;Jaron Lanier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12278">
<title>Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition. (arXiv:2309.12278v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12278</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated dominating performance in many
NLP tasks, especially on generative tasks. However, they often fall short in
some information extraction tasks, particularly those requiring domain-specific
knowledge, such as Biomedical Named Entity Recognition (NER). In this paper,
inspired by Chain-of-thought, we leverage the LLM to solve the Biomedical NER
step-by-step: break down the NER task into entity span extraction and entity
type determination. Additionally, for entity type determination, we inject
entity knowledge to address the problem that LLM&apos;s lack of domain knowledge
when predicting entity category. Experimental results show a significant
improvement in our two-step BioNER approach compared to previous few-shot LLM
baseline. Additionally, the incorporation of external knowledge significantly
enhances entity category determination performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Junyi Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shanfeng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12284">
<title>MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. (arXiv:2309.12284v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12284</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have pushed the limits of natural language
understanding and exhibited excellent problem-solving ability. Despite the
great success, most existing open-source LLMs (\eg, LLaMA-2) are still far away
from satisfactory for solving mathematical problem due to the complex reasoning
procedures. To bridge this gap, we propose \emph{MetaMath}, a fine-tuned
language model that specializes in mathematical reasoning. Specifically, we
start by bootstrapping mathematical questions by rewriting the question from
multiple perspectives without extra knowledge, which results in a new dataset
called {MetaMathQA}. Then we fine-tune the LLaMA-2 models on MetaMathQA.
Experimental results on two popular benchmarks (\ie, GSM8K and MATH) for
mathematical reasoning demonstrate that MetaMath outperforms a suite of
open-source LLMs by a significant margin. Our MetaMath-7B model achieves
$66.4\%$ on GSM8K and $19.4\%$ on MATH, exceeding the state-of-the-art models
of the same size by $11.5\%$ and $8.7\%$. Particularly, {MetaMath-70B} achieves
an accuracy of $82.3\%$ on {GSM8K}, slightly better than {GPT-3.5-Turbo}. We
release the {MetaMathQA} dataset, the {MetaMath} models with different model
sizes and the training code for public use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Longhui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Weisen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Han Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jincheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1&quot;&gt;James T. Kwok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiyang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12288">
<title>The Reversal Curse: LLMs trained on &quot;A is B&quot; fail to learn &quot;B is A&quot;. (arXiv:2309.12288v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12288</link>
<description rdf:parseType="Literal">&lt;p&gt;We expose a surprising failure of generalization in auto-regressive large
language models (LLMs). If a model is trained on a sentence of the form &quot;A is
B&quot;, it will not automatically generalize to the reverse direction &quot;B is A&quot;.
This is the Reversal Curse. For instance, if a model is trained on &quot;Olaf Scholz
was the ninth Chancellor of Germany&quot;, it will not automatically be able to
answer the question, &quot;Who was the ninth Chancellor of Germany?&quot;. Moreover, the
likelihood of the correct answer (&quot;Olaf Scholz&quot;) will not be higher than for a
random name. Thus, models exhibit a basic failure of logical deduction and do
not generalize a prevalent pattern in their training set (i.e. if &quot;A is B&apos;&apos;
occurs, &quot;B is A&quot; is more likely to occur). We provide evidence for the Reversal
Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as &quot;Uriah
Hawthorne is the composer of &apos;Abyssal Melodies&apos;&quot; and showing that they fail to
correctly answer &quot;Who composed &apos;Abyssal Melodies?&apos;&quot;. The Reversal Curse is
robust across model sizes and model families and is not alleviated by data
augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about
real-world celebrities, such as &quot;Who is Tom Cruise&apos;s mother? [A: Mary Lee
Pfeiffer]&quot; and the reverse &quot;Who is Mary Lee Pfeiffer&apos;s son?&quot;. GPT-4 correctly
answers questions like the former 79% of the time, compared to 33% for the
latter. This shows a failure of logical deduction that we hypothesize is caused
by the Reversal Curse. Code is available at
https://github.com/lukasberglund/reversal_curse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berglund_L/0/1/0/all/0/1&quot;&gt;Lukas Berglund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Max Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1&quot;&gt;Mikita Balesni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stickland_A/0/1/0/all/0/1&quot;&gt;Asa Cooper Stickland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1&quot;&gt;Tomasz Korbak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1&quot;&gt;Owain Evans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12294">
<title>Reranking for Natural Language Generation from Logical Forms: A Study based on Large Language Models. (arXiv:2309.12294v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12294</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in
natural language generation. However, their output quality can be inconsistent,
posing challenges for generating natural language from logical forms (LFs).
This task requires the generated outputs to embody the exact semantics of LFs,
without missing any LF semantics or creating any hallucinations. In this work,
we tackle this issue by proposing a novel generate-and-rerank approach. Our
approach involves initially generating a set of candidate outputs by prompting
an LLM and subsequently reranking them using a task-specific reranker model. In
addition, we curate a manually collected dataset to evaluate the alignment
between different ranking metrics and human judgements. The chosen ranking
metrics are utilized to enhance the training and evaluation of the reranker
model. By conducting extensive experiments on three diverse datasets, we
demonstrate that the candidates selected by our reranker outperform those
selected by baseline methods in terms of semantic consistency and fluency, as
measured by three comprehensive metrics. Our findings provide strong evidence
for the effectiveness of our approach in improving the quality of generated
outputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haroutunian_L/0/1/0/all/0/1&quot;&gt;Levon Haroutunian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galescu_L/0/1/0/all/0/1&quot;&gt;Lucian Galescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_P/0/1/0/all/0/1&quot;&gt;Philip Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumuluri_R/0/1/0/all/0/1&quot;&gt;Raj Tumuluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1&quot;&gt;Gholamreza Haffari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12307">
<title>LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.12307</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LongLoRA, an efficient fine-tuning approach that extends the
context sizes of pre-trained large language models (LLMs), with limited
computation cost. Typically, training LLMs with long context sizes is
computationally expensive, requiring extensive training hours and GPU
resources. For example, training on the context length of 8192 needs 16x
computational costs in self-attention layers as that of 2048. In this paper, we
speed up the context extension of LLMs in two aspects. On the one hand,
although dense global attention is needed during inference, fine-tuning the
model can be effectively and efficiently done by sparse local attention. The
proposed shift short attention effectively enables context extension, leading
to non-trivial computation saving with similar performance to fine-tuning with
vanilla attention. Particularly, it can be implemented with only two lines of
code in training, while being optional in inference. On the other hand, we
revisit the parameter-efficient fine-tuning regime for context expansion.
Notably, we find that LoRA for context extension works well under the premise
of trainable embedding and normalization. LongLoRA demonstrates strong
empirical results on various tasks on LLaMA2 models from 7B/13B to 70B.
LongLoRA adopts LLaMA2 7B from 4k context to 100k, or LLaMA2 70B to 32k on a
single 8x A100 machine. LongLoRA extends models&apos; context while retaining their
original architectures, and is compatible with most existing techniques, like
FlashAttention-2. In addition, to make LongLoRA practical, we collect a
dataset, LongQA, for supervised fine-tuning. It contains more than 3k long
context question-answer pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengju Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haotian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1&quot;&gt;Xin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12309">
<title>Rehearsal: Simulating Conflict to Teach Conflict Resolution. (arXiv:2309.12309v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.12309</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpersonal conflict is an uncomfortable but unavoidable fact of life.
Navigating conflict successfully is a skill -- one that can be learned through
deliberate practice -- but few have access to effective training or feedback.
To expand this access, we introduce Rehearsal, a system that allows users to
rehearse conflicts with a believable simulated interlocutor, explore
counterfactual &quot;what if?&quot; scenarios to identify alternative conversational
paths, and learn through feedback on how and when to apply specific conflict
strategies. Users can utilize Rehearsal to practice handling a variety of
predefined conflict scenarios, from office disputes to relationship issues, or
they can choose to create their own. To enable Rehearsal, we develop IRP
prompting, a method of conditioning output of a large language model on the
influential Interest-Rights-Power (IRP) theory from conflict resolution.
Rehearsal uses IRP to generate utterances grounded in conflict resolution
theory, guiding users towards counterfactual conflict resolution strategies
that help de-escalate difficult conversations. In a between-subjects
evaluation, 40 participants engaged in an actual conflict with a confederate
after training. Compared to a control group with lecture material covering the
same IRP theory, participants with simulated training from Rehearsal
significantly improved their performance in the unaided conflict: they reduced
their use of escalating competitive strategies by an average of 67%, while
doubling their use of cooperative strategies. Overall, Rehearsal highlights the
potential effectiveness of language models as tools for learning and practicing
interpersonal skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1&quot;&gt;Omar Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_V/0/1/0/all/0/1&quot;&gt;Valentino Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gelfand_M/0/1/0/all/0/1&quot;&gt;Michele J. Gelfand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1&quot;&gt;Michael S. Bernstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12311">
<title>LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent. (arXiv:2309.12311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.12311</link>
<description rdf:parseType="Literal">&lt;p&gt;3D visual grounding is a critical skill for household robots, enabling them
to navigate, manipulate objects, and answer questions based on their
environment. While existing approaches often rely on extensive labeled data or
exhibit limitations in handling complex language queries, we propose
LLM-Grounder, a novel zero-shot, open-vocabulary, Large Language Model
(LLM)-based 3D visual grounding pipeline. LLM-Grounder utilizes an LLM to
decompose complex natural language queries into semantic constituents and
employs a visual grounding tool, such as OpenScene or LERF, to identify objects
in a 3D scene. The LLM then evaluates the spatial and commonsense relations
among the proposed objects to make a final grounding decision. Our method does
not require any labeled training data and can generalize to novel 3D scenes and
arbitrary text queries. We evaluate LLM-Grounder on the ScanRefer benchmark and
demonstrate state-of-the-art zero-shot grounding accuracy. Our findings
indicate that LLMs significantly improve the grounding capability, especially
for complex language queries, making LLM-Grounder an effective approach for 3D
vision-language tasks in robotics. Videos and interactive demos can be found on
the project website https://chat-with-nerf.github.io/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuweiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1&quot;&gt;Shengyi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaan_N/0/1/0/all/0/1&quot;&gt;Nikhil Madaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyengar_M/0/1/0/all/0/1&quot;&gt;Madhavan Iyengar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1&quot;&gt;David F. Fouhey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12911">
<title>Grammatical cues to subjecthood are redundant in a majority of simple clauses across languages. (arXiv:2201.12911v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12911</link>
<description rdf:parseType="Literal">&lt;p&gt;Grammatical cues are sometimes redundant with word meanings in natural
language. For instance, English word order rules constrain the word order of a
sentence like &quot;The dog chewed the bone&quot; even though the status of &quot;dog&quot; as
subject and &quot;bone&quot; as object can be inferred from world knowledge and
plausibility. Quantifying how often this redundancy occurs, and how the level
of redundancy varies across typologically diverse languages, can shed light on
the function and evolution of grammar. To that end, we performed a behavioral
experiment in English and Russian and a cross-linguistic computational analysis
measuring the redundancy of grammatical cues in transitive clauses extracted
from corpus text. English and Russian speakers (n=484) were presented with
subjects, verbs, and objects (in random order and with morphological markings
removed) extracted from naturally occurring sentences and were asked to
identify which noun is the subject of the action. Accuracy was high in both
languages (~89% in English, ~87% in Russian). Next, we trained a neural network
machine classifier on a similar task: predicting which nominal in a
subject-verb-object triad is the subject. Across 30 languages from eight
language families, performance was consistently high: a median accuracy of 87%,
comparable to the accuracy observed in the human experiments. The conclusion is
that grammatical cues such as word order are necessary to convey subjecthood
and objecthood in a minority of naturally occurring transitive clauses;
nevertheless, they can (a) provide an important source of redundancy and (b)
are crucial for conveying intended meaning that cannot be inferred from the
words alone, including descriptions of human interactions, where roles are
often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing
non-prototypical meanings (e.g., &quot;The bone chewed the dog.&quot;).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1&quot;&gt;Kyle Mahowald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diachek_E/0/1/0/all/0/1&quot;&gt;Evgeniia Diachek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_E/0/1/0/all/0/1&quot;&gt;Edward Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1&quot;&gt;Richard Futrell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.05232">
<title>Survey of Aspect-based Sentiment Analysis Datasets. (arXiv:2204.05232v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2204.05232</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect-based sentiment analysis (ABSA) is a natural language processing
problem that requires analyzing user-generated reviews to determine: a) The
target entity being reviewed, b) The high-level aspect to which it belongs, and
c) The sentiment expressed toward the targets and the aspects. Numerous yet
scattered corpora for ABSA make it difficult for researchers to identify
corpora best suited for a specific ABSA subtask quickly. This study aims to
present a database of corpora that can be used to train and assess autonomous
ABSA systems. Additionally, we provide an overview of the major corpora for
ABSA and its subtasks and highlight several features that researchers should
consider when selecting a corpus. Finally, we discuss the advantages and
disadvantages of current collection approaches and make recommendations for
future corpora creation. This survey examines 65 publicly available ABSA
datasets covering over 25 domains, including 45 English and 20 other languages
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chebolu_S/0/1/0/all/0/1&quot;&gt;Siva Uday Sampreeth Chebolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1&quot;&gt;Nedim Lipka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1&quot;&gt;Thamar Solorio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13379">
<title>Faithful Chain-of-Thought Reasoning. (arXiv:2301.13379v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13379</link>
<description rdf:parseType="Literal">&lt;p&gt;While Chain-of-Thought (CoT) prompting boosts Language Models&apos; (LM)
performance on a gamut of complex reasoning tasks, the generated reasoning
chain does not necessarily reflect how the model arrives at the answer (aka.
faithfulness). We propose Faithful CoT, a reasoning framework involving two
stages: Translation (Natural Language query $\rightarrow$ symbolic reasoning
chain) and Problem Solving (reasoning chain $\rightarrow$ answer), using an LM
and a deterministic solver respectively. This guarantees that the reasoning
chain provides a faithful explanation of the final answer. Aside from
interpretability, Faithful CoT also improves empirical performance: it
outperforms standard CoT on 9 of 10 benchmarks from 4 diverse domains, with a
relative accuracy gain of 6.3% on Math Word Problems (MWP), 3.4% on Planning,
5.5% on Multi-hop Question Answering (QA), and 21.4% on Relational Inference.
Furthermore, with GPT-4 and Codex, it sets the new state-of-the-art few-shot
performance on 7 datasets (with 95.0+ accuracy on 6 of them), showing a strong
synergy between faithfulness and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1&quot;&gt;Qing Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreya Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_A/0/1/0/all/0/1&quot;&gt;Adam Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_D/0/1/0/all/0/1&quot;&gt;Delip Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1&quot;&gt;Marianna Apidianaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1&quot;&gt;Chris Callison-Burch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04456">
<title>ERNIE-Music: Text-to-Waveform Music Generation with Diffusion Models. (arXiv:2302.04456v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04456</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the burgeoning interest in diffusion models has led to
significant advances in image and speech generation. Nevertheless, the direct
synthesis of music waveforms from unrestricted textual prompts remains a
relatively underexplored domain. In response to this lacuna, this paper
introduces a pioneering contribution in the form of a text-to-waveform music
generation model, underpinned by the utilization of diffusion models. Our
methodology hinges on the innovative incorporation of free-form textual prompts
as conditional factors to guide the waveform generation process within the
diffusion model framework. Addressing the challenge of limited text-music
parallel data, we undertake the creation of a dataset by harnessing web
resources, a task facilitated by weak supervision techniques. Furthermore, a
rigorous empirical inquiry is undertaken to contrast the efficacy of two
distinct prompt formats for text conditioning, namely, music tags and
unconstrained textual descriptions. The outcomes of this comparative analysis
affirm the superior performance of our proposed model in terms of enhancing
text-music relevance. Finally, our work culminates in a demonstrative
exhibition of the excellent capabilities of our model in text-to-music
generation. We further demonstrate that our generated music in the waveform
domain outperforms previous works by a large margin in terms of diversity,
quality, and text-music relevance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1&quot;&gt;Chao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1&quot;&gt;Yekun Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuohuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hua Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06908">
<title>CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model. (arXiv:2305.06908v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06908</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion probabilistic models (DDPMs) have shown promising
performance for speech synthesis. However, a large number of iterative steps
are required to achieve high sample quality, which restricts the inference
speed. Maintaining sample quality while increasing sampling speed has become a
challenging task. In this paper, we propose a &quot;Co&quot;nsistency &quot;Mo&quot;del-based
&quot;Speech&quot; synthesis method, CoMoSpeech, which achieve speech synthesis through a
single diffusion sampling step while achieving high audio quality. The
consistency constraint is applied to distill a consistency model from a
well-designed diffusion-based teacher model, which ultimately yields superior
performances in the distilled CoMoSpeech. Our experiments show that by
generating audio recordings by a single sampling step, the CoMoSpeech achieves
an inference speed more than 150 times faster than real-time on a single NVIDIA
A100 GPU, which is comparable to FastSpeech2, making diffusion-sampling based
speech synthesis truly practical. Meanwhile, objective and subjective
evaluations on text-to-speech and singing voice synthesis show that the
proposed teacher models yield the best audio quality, and the one-step sampling
based CoMoSpeech achieves the best inference speed with better or comparable
audio quality to other conventional multi-step diffusion model baselines. Audio
samples are available at https://comospeech.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zhen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qifeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08383">
<title>The Impact of Incumbent/Opposition Status and Ideological Similitude on Emotions in Political Manifestos. (arXiv:2305.08383v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08383</link>
<description rdf:parseType="Literal">&lt;p&gt;The study involved the analysis of emotion-associated language in the UK
Conservative and Labour party general election manifestos between 2000 to 2019.
While previous research have shown a general correlation between ideological
positioning and overlap of public policies, there are still conflicting results
in matters of sentiments in such manifestos. Using new data, we present how
valence level can be swayed by party status within government with incumbent
parties presenting a higher frequency in positive emotion-associated words
while negative emotion-associated words are more prevalent in opposition
parties. We also demonstrate that parties with ideological similitude use
positive language prominently further adding to the literature on the
relationship between sentiments and party status.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishi_T/0/1/0/all/0/1&quot;&gt;Takumi Nishi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14310">
<title>Navigating Prompt Complexity for Zero-Shot Classification: A Study of Large Language Models in Computational Social Science. (arXiv:2305.14310v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14310</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuned Large Language Models (LLMs) have exhibited impressive
language understanding and the capacity to generate responses that follow
specific prompts. However, due to the computational demands associated with
training these models, their applications often adopt a zero-shot setting. In
this paper, we evaluate the zero-shot performance of two publicly accessible
LLMs, ChatGPT and OpenAssistant, in the context of six Computational Social
Science classification tasks, while also investigating the effects of various
prompting strategies. Our experiments investigate the impact of prompt
complexity, including the effect of incorporating label definitions into the
prompt; use of synonyms for label names; and the influence of integrating past
memories during foundation model training. The findings indicate that in a
zero-shot setting, current LLMs are unable to match the performance of smaller,
fine-tuned baseline transformer models (such as BERT-large). Additionally, we
find that different prompting strategies can significantly affect
classification accuracy, with variations in accuracy and F1 scores exceeding
10\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_Y/0/1/0/all/0/1&quot;&gt;Yida Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Ben P. Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorne_W/0/1/0/all/0/1&quot;&gt;William Thorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_A/0/1/0/all/0/1&quot;&gt;Ambrose Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1&quot;&gt;Nikolaos Aletras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1&quot;&gt;Carolina Scarton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bontcheva_K/0/1/0/all/0/1&quot;&gt;Kalina Bontcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xingyi Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16755">
<title>Can large language models generate salient negative statements?. (arXiv:2305.16755v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16755</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the ability of large language models (LLMs) to generate salient
(interesting) negative statements about real-world entities; an emerging
research topic of the last few years. We probe the LLMs using zero- and k-shot
unconstrained probes, and compare with traditional methods for negation
generation, i.e., pattern-based textual extractions and knowledge-graph-based
inferences, as well as crowdsourced gold statements. We measure the correctness
and salience of the generated lists about subjects from different domains. Our
evaluation shows that guided probes do in fact improve the quality of generated
negatives, compared to the zero-shot variant. Nevertheless, using both prompts,
LLMs still struggle with the notion of factuality of negatives, frequently
generating many ambiguous statements, or statements with negative keywords but
a positive meaning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1&quot;&gt;Hiba Arnaout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1&quot;&gt;Simon Razniewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14743">
<title>Turning Whisper into Real-Time Transcription System. (arXiv:2307.14743v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14743</link>
<description rdf:parseType="Literal">&lt;p&gt;Whisper is one of the recent state-of-the-art multilingual speech recognition
and translation models, however, it is not designed for real time
transcription. In this paper, we build on top of Whisper and create
Whisper-Streaming, an implementation of real-time speech transcription and
translation of Whisper-like models. Whisper-Streaming uses local agreement
policy with self-adaptive latency to enable streaming transcription. We show
that Whisper-Streaming achieves high quality and 3.3 seconds latency on
unsegmented long-form speech transcription test set, and we demonstrate its
robustness and practical usability as a component in live transcription service
at a multilingual conference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machacek_D/0/1/0/all/0/1&quot;&gt;Dominik Mach&amp;#xe1;&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1&quot;&gt;Raj Dabre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Bojar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09440">
<title>Scope is all you need: Transforming LLMs for HPC Code. (arXiv:2308.09440v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09440</link>
<description rdf:parseType="Literal">&lt;p&gt;With easier access to powerful compute resources, there is a growing trend in
the field of AI for software development to develop larger and larger language
models (LLMs) to address a variety of programming tasks. Even LLMs applied to
tasks from the high-performance computing (HPC) domain are huge in size (e.g.,
billions of parameters) and demand expensive compute resources for training. We
found this design choice confusing - why do we need large LLMs trained on
natural languages and programming languages unrelated to HPC for HPC-specific
tasks? In this line of work, we aim to question design choices made by existing
LLMs by developing smaller LLMs for specific domains - we call them
domain-specific LLMs. Specifically, we start off with HPC as a domain and
propose a novel tokenizer named Tokompiler, designed specifically for
preprocessing code in HPC and compilation-centric tasks. Tokompiler leverages
knowledge of language primitives to generate language-oriented tokens,
providing a context-aware understanding of code structure while avoiding human
semantics attributed to code structures completely. We applied Tokompiler to
pre-train two state-of-the-art models, SPT-Code and Polycoder, for a Fortran
code corpus mined from GitHub. We evaluate the performance of these models
against the conventional LLMs. Results demonstrate that Tokompiler
significantly enhances code completion accuracy and semantic understanding
compared to traditional tokenizers in normalized-perplexity tests, down to ~1
perplexity score. This research opens avenues for further advancements in
domain-specific LLMs, catering to the unique demands of HPC and compilation
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadosh_T/0/1/0/all/0/1&quot;&gt;Tal Kadosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1&quot;&gt;Niranjan Hasabnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1&quot;&gt;Vy A. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1&quot;&gt;Nadav Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krien_N/0/1/0/all/0/1&quot;&gt;Neva Krien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasay_A/0/1/0/all/0/1&quot;&gt;Abdul Wasay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1&quot;&gt;Ted Willke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamir_G/0/1/0/all/0/1&quot;&gt;Guy Tamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1&quot;&gt;Yuval Pinter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1&quot;&gt;Timothy Mattson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1&quot;&gt;Gal Oren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10792">
<title>Instruction Tuning for Large Language Models: A Survey. (arXiv:2308.10792v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper surveys research works in the quickly advancing field of
instruction tuning (IT), a crucial technique to enhance the capabilities and
controllability of large language models (LLMs). Instruction tuning refers to
the process of further training LLMs on a dataset consisting of
\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the
gap between the next-word prediction objective of LLMs and the users&apos; objective
of having LLMs adhere to human instructions. In this work, we make a systematic
review of the literature, including the general methodology of IT, the
construction of IT datasets, the training of IT models, and applications to
different modalities, domains and applications, along with an analysis on
aspects that influence the outcome of IT (e.g., generation of instruction
outputs, size of the instruction dataset, etc). We also review the potential
pitfalls of IT along with criticism against it, along with efforts pointing out
current deficiencies of existing strategies and suggest some avenues for
fruitful research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Linfeng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoya Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaofei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Runyi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10855">
<title>LatEval: An Interactive LLMs Evaluation Benchmark with Incomplete Information from Lateral Thinking Puzzles. (arXiv:2308.10855v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10855</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous evolution and refinement of LLMs, they are endowed with
impressive logical reasoning or vertical thinking capabilities. But can they
think out of the box? Do they possess proficient lateral thinking abilities?
Following the setup of Lateral Thinking Puzzles, we propose a novel evaluation
benchmark, LatEval, which assesses the model&apos;s lateral thinking within an
interactive framework. In our benchmark, we challenge LLMs with 2 aspects: the
quality of questions posed by the model and the model&apos;s capability to integrate
information for problem-solving. We find that nearly all LLMs struggle with
employing lateral thinking during interactions. For example, even the most
advanced model, GPT-4, exhibits the advantage to some extent, yet still
maintain a noticeable gap when compared to human. This evaluation benchmark
provides LLMs with a highly challenging and distinctive task that is crucial to
an effective AI assistant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shulin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shirong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Mengzuo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1&quot;&gt;Wuhe Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weidong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-Tao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11531">
<title>Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11531</link>
<description rdf:parseType="Literal">&lt;p&gt;Our project aims at helping and supporting stakeholders in refugee status
adjudications, such as lawyers, judges, governing bodies, and claimants, in
order to make better decisions through data-driven intelligence and increase
the understanding and transparency of the refugee application process for all
involved parties. This PhD project has two primary objectives: (1) to retrieve
past cases, and (2) to analyze legal decision-making processes on a dataset of
Canadian cases. In this paper, we present the current state of our work, which
includes a completed experiment on part (1) and ongoing efforts related to part
(2). We believe that NLP-based solutions are well-suited to address these
challenges, and we investigate the feasibility of automating all steps
involved. In addition, we introduce a novel benchmark for future NLP research
in refugee law. Our methodology aims to be inclusive to all end-users and
stakeholders, with expected benefits including reduced time-to-decision, fairer
and more transparent outcomes, and improved decision quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barale_C/0/1/0/all/0/1&quot;&gt;Claire Barale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03992">
<title>ConDA: Contrastive Domain Adaptation for AI-generated Text Detection. (arXiv:2309.03992v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03992</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are increasingly being used for generating text
in a variety of use cases, including journalistic news articles. Given the
potential malicious nature in which these LLMs can be used to generate
disinformation at scale, it is important to build effective detectors for such
AI-generated text. Given the surge in development of new LLMs, acquiring
labeled training data for supervised detectors is a bottleneck. However, there
might be plenty of unlabeled text data available, without information on which
generator it came from. In this work we tackle this data problem, in detecting
AI-generated news text, and frame the problem as an unsupervised domain
adaptation task. Here the domains are the different text generators, i.e. LLMs,
and we assume we have access to only the labeled source data and unlabeled
target data. We develop a Contrastive Domain Adaptation framework, called
ConDA, that blends standard domain adaptation techniques with the
representation power of contrastive learning to learn domain invariant
representations that are effective for the final unsupervised detection task.
Our experiments demonstrate the effectiveness of our framework, resulting in
average performance gains of 31.7% from the best performing baselines, and
within 0.8% margin of a fully supervised detector. All our code and data is
available at https://github.com/AmritaBh/ConDA-gen-text-detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1&quot;&gt;Amrita Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1&quot;&gt;Tharindu Kumarage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07413">
<title>CPPF: A contextual and post-processing-free model for automatic speech recognition. (arXiv:2309.07413v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07413</link>
<description rdf:parseType="Literal">&lt;p&gt;ASR systems have become increasingly widespread in recent years. However,
their textual outputs often require post-processing tasks before they can be
practically utilized. To address this issue, we draw inspiration from the
multifaceted capabilities of LLMs and Whisper, and focus on integrating
multiple ASR text processing tasks related to speech recognition into the ASR
model. This integration not only shortens the multi-stage pipeline, but also
prevents the propagation of cascading errors, resulting in direct generation of
post-processed text. In this study, we focus on ASR-related processing tasks,
including Contextual ASR and multiple ASR post processing tasks. To achieve
this objective, we introduce the CPPF model, which offers a versatile and
highly effective alternative to ASR processing. CPPF seamlessly integrates
these tasks without any significant loss in recognition performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhengkun Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiaming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1&quot;&gt;Hongyu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1&quot;&gt;Ke Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1&quot;&gt;Guanglu Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07719">
<title>L1-aware Multilingual Mispronunciation Detection Framework. (arXiv:2309.07719v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07719</link>
<description rdf:parseType="Literal">&lt;p&gt;The phonological discrepancies between a speaker&apos;s native (L1) and the
non-native language (L2) serves as a major factor for mispronunciation. This
paper introduces a novel multilingual MDD architecture, L1-MultiMDD, enriched
with L1-aware speech representation. An end-to-end speech encoder is trained on
the input signal and its corresponding reference phoneme sequence. First, an
attention mechanism is deployed to align the input audio with the reference
phoneme sequence. Afterwards, the L1-L2-speech embedding are extracted from an
auxiliary model, pretrained in a multi-task setup identifying L1 and L2
language, and are infused with the primary network. Finally, the L1-MultiMDD is
then optimized for a unified multilingual phoneme recognition task using
connectionist temporal classification (CTC) loss for the target languages:
English, Arabic, and Mandarin. Our experiments demonstrate the effectiveness of
the proposed L1-MultiMDD framework on both seen -- L2-ARTIC, LATIC, and
AraVoiceL2v2; and unseen -- EpaDB and Speechocean762 datasets. The consistent
gains in PER, and false rejection rate (FRR) across all target languages
confirm our approach&apos;s robustness, efficacy, and generalizability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheir_Y/0/1/0/all/0/1&quot;&gt;Yassine El Kheir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Shammur Absar Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Ahmed Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10916">
<title>What Learned Representations and Influence Functions Can Tell Us About Adversarial Examples. (arXiv:2309.10916v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10916</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples, deliberately crafted using small perturbations to fool
deep neural networks, were first studied in image processing and more recently
in NLP. While approaches to detecting adversarial examples in NLP have largely
relied on search over input perturbations, image processing has seen a range of
techniques that aim to characterise adversarial subspaces over the learned
representations.
&lt;/p&gt;
&lt;p&gt;In this paper, we adapt two such approaches to NLP, one based on nearest
neighbors and influence functions and one on Mahalanobis distances. The former
in particular produces a state-of-the-art detector when compared against
several strong baselines; moreover, the novel use of influence functions
provides insight into how the nature of adversarial example subspaces in NLP
relate to those in image processing, and also how they differ depending on the
kind of NLP task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonni_S/0/1/0/all/0/1&quot;&gt;Shakila Mahjabin Tonni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dras_M/0/1/0/all/0/1&quot;&gt;Mark Dras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11049">
<title>Localize, Retrieve and Fuse: A Generalized Framework for Free-Form Question Answering over Tables. (arXiv:2309.11049v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11049</link>
<description rdf:parseType="Literal">&lt;p&gt;Question answering on tabular data (a.k.a TableQA), which aims at generating
answers to questions grounded on a provided table, has gained significant
attention recently. Prior work primarily produces concise factual responses
through information extraction from individual or limited table cells, lacking
the ability to reason across diverse table cells. Yet, the realm of free-form
TableQA, which demands intricate strategies for selecting relevant table cells
and the sophisticated integration and inference of discrete data fragments,
remains mostly unexplored. To this end, this paper proposes a generalized
three-stage approach: Table-to- Graph conversion and cell localizing, external
knowledge retrieval, and the fusion of table and text (called TAG-QA), to
address the challenge of inferring long free-form answers in generative
TableQA. In particular, TAG-QA (1) locates relevant table cells using a graph
neural network to gather intersecting cells between relevant rows and columns,
(2) leverages external knowledge from Wikipedia, and (3) generates answers by
integrating both tabular data and natural linguistic information. Experiments
showcase the superior capabilities of TAG-QA in generating sentences that are
both faithful and coherent, particularly when compared to several
state-of-the-art baselines. Notably, TAG-QA surpasses the robust pipeline-based
baseline TAPAS by 17% and 14% in terms of BLEU-4 and PARENT F-score,
respectively. Furthermore, TAG-QA outperforms the end-to-end model T5 by 16%
and 12% on BLEU-4 and PARENT F-score, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenting Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhongfen Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11052">
<title>fakenewsbr: A Fake News Detection Platform for Brazilian Portuguese. (arXiv:2309.11052v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11052</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of fake news has become a significant concern in recent
times due to its potential to spread misinformation and manipulate public
opinion. This paper presents a comprehensive study on detecting fake news in
Brazilian Portuguese, focusing on journalistic-type news. We propose a machine
learning-based approach that leverages natural language processing techniques,
including TF-IDF and Word2Vec, to extract features from textual data. We
evaluate the performance of various classification algorithms, such as logistic
regression, support vector machine, random forest, AdaBoost, and LightGBM, on a
dataset containing both true and fake news articles. The proposed approach
achieves high accuracy and F1-Score, demonstrating its effectiveness in
identifying fake news. Additionally, we developed a user-friendly web platform,
fakenewsbr.com, to facilitate the verification of news articles&apos; veracity. Our
platform provides real-time analysis, allowing users to assess the likelihood
of fake news articles. Through empirical analysis and comparative studies, we
demonstrate the potential of our approach to contribute to the fight against
the spread of fake news and promote more informed media consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordani_L/0/1/0/all/0/1&quot;&gt;Luiz Giordani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daru_G/0/1/0/all/0/1&quot;&gt;Gilsiley Dar&amp;#xfa;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Queiroz_R/0/1/0/all/0/1&quot;&gt;Rhenan Queiroz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buzinaro_V/0/1/0/all/0/1&quot;&gt;Vitor Buzinaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neiva_D/0/1/0/all/0/1&quot;&gt;Davi Keglevich Neiva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_D/0/1/0/all/0/1&quot;&gt;Daniel Camilo Fuentes Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_M/0/1/0/all/0/1&quot;&gt;Marcos Jardel Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1&quot;&gt;Oilson Alberto Gonzatto Junior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Louzada_F/0/1/0/all/0/1&quot;&gt;Francisco Louzada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11206">
<title>Retrieve-Rewrite-Answer: A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering. (arXiv:2309.11206v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11206</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their competitive performance on knowledge-intensive tasks, large
language models (LLMs) still have limitations in memorizing all world knowledge
especially long tail knowledge. In this paper, we study the KG-augmented
language model approach for solving the knowledge graph question answering
(KGQA) task that requires rich world knowledge. Existing work has shown that
retrieving KG knowledge to enhance LLMs prompting can significantly improve
LLMs performance in KGQA. However, their approaches lack a well-formed
verbalization of KG knowledge, i.e., they ignore the gap between KG
representations and textual representations. To this end, we propose an
answer-sensitive KG-to-Text approach that can transform KG knowledge into
well-textualized statements most informative for KGQA. Based on this approach,
we propose a KG-to-Text enhanced LLMs framework for solving the KGQA task.
Experiments on several KGQA benchmarks show that the proposed KG-to-Text
augmented LLMs approach outperforms previous KG-augmented LLMs approaches
regarding answer accuracy and usefulness of knowledge statements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yike Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_N/0/1/0/all/0/1&quot;&gt;Nan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1&quot;&gt;Sheng Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guilin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_A/0/1/0/all/0/1&quot;&gt;Anhuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Wei Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11436">
<title>You Only Look at Screens: Multimodal Chain-of-Action Agents. (arXiv:2309.11436v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11436</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous user interface (UI) agents aim to facilitate task automation by
interacting with the user interface without manual intervention. Recent studies
have investigated eliciting the capabilities of large language models (LLMs)
for effective engagement in diverse environments. To align with the
input-output requirement of LLMs, existing approaches are developed under a
sandbox setting where they rely on external tools and application-specific APIs
to parse the environment into textual elements and interpret the predicted
actions. Consequently, those approaches often grapple with inference
inefficiency and error propagation risks. To mitigate the challenges, we
introduce Auto-UI, a multimodal solution that directly interacts with the
interface, bypassing the need for environment parsing or reliance on
application-dependent APIs. Moreover, we propose a chain-of-action technique --
leveraging a series of intermediate previous action histories and future action
plans -- to help the agent decide what action to execute. We evaluate our
approach on a new device-control benchmark AITW with 30K unique instructions,
spanning multi-step tasks such as application operation, web searching, and web
shopping. Experimental results show that Auto-UI achieves state-of-the-art
performance with an action type prediction accuracy of 90% and an overall
action success rate of 74%. Code is publicly available at
https://github.com/cooelf/Auto-UI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aston Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.11489">
<title>Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning. (arXiv:2309.11489v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.11489</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing reward functions is a longstanding challenge in reinforcement
learning (RL); it requires specialized knowledge or domain data, leading to
high costs for development. To address this, we introduce Text2Reward, a
data-free framework that automates the generation of dense reward functions
based on large language models (LLMs). Given a goal described in natural
language, Text2Reward generates dense reward functions as an executable program
grounded in a compact representation of the environment. Unlike inverse RL and
recent work that uses LLMs to write sparse reward codes, Text2Reward produces
interpretable, free-form dense reward codes that cover a wide range of tasks,
utilize existing packages, and allow iterative refinement with human feedback.
We evaluate Text2Reward on two robotic manipulation benchmarks (ManiSkill2,
MetaWorld) and two locomotion environments of MuJoCo. On 13 of the 17
manipulation tasks, policies trained with generated reward codes achieve
similar or better task success rates and convergence speed than expert-written
reward codes. For locomotion tasks, our method learns six novel locomotion
behaviors with a success rate exceeding 94%. Furthermore, we show that the
policies trained in the simulator with our method can be deployed in the real
world. Finally, Text2Reward further improves the policies by refining their
reward functions with human feedback. Video results are available at
https://text-to-reward.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tianbao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Siheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yitao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1&quot;&gt;Qian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1&quot;&gt;Victor Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tao Yu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>