<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-22T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13152" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13600" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.02673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.04819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03105" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11642" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.12799">
<title>A Fine-Grained Image Description Generation Method Based on Joint Objectives. (arXiv:2311.12799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12799</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of fine-grained image description generation techniques is to learn
detailed information from images and simulate human-like descriptions that
provide coherent and comprehensive textual details about the image content.
Currently, most of these methods face two main challenges: description
repetition and omission. Moreover, the existing evaluation metrics cannot
clearly reflect the performance of models on these two issues. To address these
challenges, we propose an innovative Fine-grained Image Description Generation
model based on Joint Objectives. Furthermore, we introduce new object-based
evaluation metrics to more intuitively assess the model&apos;s performance in
handling description repetition and omission. This novel approach combines
visual features at both the image level and object level to maximize their
advantages and incorporates an object penalty mechanism to reduce description
repetition. Experimental results demonstrate that our proposed method
significantly improves the CIDEr evaluation metric, indicating its excellent
performance in addressing description repetition and omission issues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chunzhen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1&quot;&gt;Donglin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dazhen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12800">
<title>Understanding Data Augmentation from a Robustness Perspective. (arXiv:2311.12800v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12800</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of visual recognition, data augmentation stands out as a pivotal
technique to amplify model robustness. Yet, a considerable number of existing
methodologies lean heavily on heuristic foundations, rendering their intrinsic
mechanisms ambiguous. This manuscript takes both a theoretical and empirical
approach to understanding the phenomenon. Theoretically, we frame the discourse
around data augmentation within game theory&apos;s constructs. Venturing deeper, our
empirical evaluations dissect the intricate mechanisms of emblematic data
augmentation strategies, illuminating that these techniques primarily stimulate
mid- and high-order game interactions. Beyond the foundational exploration, our
experiments span multiple datasets and diverse augmentation techniques,
underscoring the universal applicability of our findings. Recognizing the vast
array of robustness metrics with intricate correlations, we unveil a
streamlined proxy. This proxy not only simplifies robustness assessment but
also offers invaluable insights, shedding light on the inherent dynamics of
model game interactions and their relation to overarching system robustness.
These insights provide a novel lens through which we can re-evaluate model
safety and robustness in visual recognition tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhendong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qiangqiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chongjun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12801">
<title>End-to-end Phase Field Model Discovery Combining Experimentation, Crowdsourcing, Simulation and Learning. (arXiv:2311.12801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12801</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of tera-byte scale experiment data calls for AI driven
approaches which automatically discover scientific models from data.
Nonetheless, significant challenges present in AI-driven scientific discovery:
(i) The annotation of large scale datasets requires fundamental re-thinking in
developing scalable crowdsourcing tools. (ii) The learning of scientific models
from data calls for innovations beyond black-box neural nets. (iii) Novel
visualization and diagnosis tools are needed for the collaboration of
experimental and theoretical physicists, and computer scientists. We present
Phase-Field-Lab platform for end-to-end phase field model discovery, which
automatically discovers phase field physics models from experiment data,
integrating experimentation, crowdsourcing, simulation and learning.
Phase-Field-Lab combines (i) a streamlined annotation tool which reduces the
annotation time (by ~50-75%), while increasing annotation accuracy compared to
baseline; (ii) an end-to-end neural model which automatically learns phase
field models from data by embedding phase field simulation and existing domain
knowledge into learning; and (iii) novel interfaces and visualizations to
integrate our platform into the scientific discovery cycle of domain
scientists. Our platform is deployed in the analysis of nano-structure
evolution in materials under extreme conditions (high temperature and
irradiation). Our approach reveals new properties of nano-void defects, which
otherwise cannot be detected via manual analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1&quot;&gt;Md Nasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Azab_A/0/1/0/all/0/1&quot;&gt;Anter El-Azab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinghang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1&quot;&gt;Yexiang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12804">
<title>Towards the generation of synchronized and believable non-verbal facial behaviors of a talking virtual agent. (arXiv:2311.12804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12804</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new model to generate rhythmically relevant
non-verbal facial behaviors for virtual agents while they speak. The model
demonstrates perceived performance comparable to behaviors directly extracted
from the data and replayed on a virtual agent, in terms of synchronization with
speech and believability. Interestingly, we found that training the model with
two different sets of data, instead of one, did not necessarily improve its
performance. The expressiveness of the people in the dataset and the shooting
conditions are key elements. We also show that employing an adversarial model,
in which fabricated fake examples are introduced during the training phase,
increases the perception of synchronization with speech. A collection of videos
demonstrating the results and code can be accessed at:
https://github.com/aldelb/non_verbal_facial_animation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delbosc_A/0/1/0/all/0/1&quot;&gt;Alice Delbosc&lt;/a&gt; (TALEP, LIS, AMU), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochs_M/0/1/0/all/0/1&quot;&gt;Magalie Ochs&lt;/a&gt; (LIS, AMU, TALEP), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabouret_N/0/1/0/all/0/1&quot;&gt;Nicolas Sabouret&lt;/a&gt; (LISN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravenet_B/0/1/0/all/0/1&quot;&gt;Brian Ravenet&lt;/a&gt; (LISN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayache_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Ayache&lt;/a&gt; (AMU, LIS, QARMA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12805">
<title>DeepCompass: AI-driven Location-Orientation Synchronization for Navigating Platforms. (arXiv:2311.12805v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12805</link>
<description rdf:parseType="Literal">&lt;p&gt;In current navigating platforms, the user&apos;s orientation is typically
estimated based on the difference between two consecutive locations. In other
words, the orientation cannot be identified until the second location is taken.
This asynchronous location-orientation identification often leads to our
real-life question: Why does my navigator tell the wrong direction of my car at
the beginning? We propose DeepCompass to identify the user&apos;s orientation by
bridging the gap between the street-view and the user-view images. First, we
explore suitable model architectures and design corresponding input
configuration. Second, we demonstrate artificial transformation techniques
(e.g., style transfer and road segmentation) to minimize the disparity between
the street-view and the user&apos;s real-time experience. We evaluate DeepCompass
with extensive evaluation in various driving conditions. DeepCompass does not
require additional hardware and is also not susceptible to external
interference, in contrast to magnetometer-based navigator. This highlights the
potential of DeepCompass as an add-on to existing sensor-based orientation
detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jihun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;SP Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seok_H/0/1/0/all/0/1&quot;&gt;Hyekyoung Seok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1&quot;&gt;Hyoungseok Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Sanghee Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12806">
<title>MatGD: Materials Graph Digitizer. (arXiv:2311.12806v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12806</link>
<description rdf:parseType="Literal">&lt;p&gt;We have developed MatGD (Material Graph Digitizer), which is a tool for
digitizing a data line from scientific graphs. The algorithm behind the tool
consists of four steps: (1) identifying graphs within subfigures, (2)
separating axes and data sections, (3) discerning the data lines by eliminating
irrelevant graph objects and matching with the legend, and (4) data extraction
and saving. From the 62,534 papers in the areas of batteries, catalysis, and
MOFs, 501,045 figures were mined. Remarkably, our tool showcased performance
with over 99% accuracy in legend marker and text detection. Moreover, its
capability for data line separation stood at 66%, which is much higher compared
to other existing figure mining tools. We believe that this tool will be
integral to collecting both past and future data from publications, and these
data can be used to train various machine learning models that can enhance
material predictions and new materials discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewoong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wonseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jihan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12808">
<title>Improved vectorization of OpenCV algorithms for RISC-V CPUs. (arXiv:2311.12808v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2311.12808</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of an open and free RISC-V architecture is of great interest
for a wide range of areas, including high-performance computing and numerical
simulation in mathematics, physics, chemistry and other problem domains. In
this paper, we discuss the possibilities of accelerating computations on
available RISC-V processors by improving the vectorization of several computer
vision and machine learning algorithms in the widely used OpenCV library. It is
shown that improved vectorization speeds up computations on existing prototypes
of RISC-V devices by tens of percent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volokitin_V/0/1/0/all/0/1&quot;&gt;V. D. Volokitin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasiliev_E/0/1/0/all/0/1&quot;&gt;E. P. Vasiliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozinov_E/0/1/0/all/0/1&quot;&gt;E. A. Kozinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kustikova_V/0/1/0/all/0/1&quot;&gt;V. D. Kustikova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liniov_A/0/1/0/all/0/1&quot;&gt;A. V. Liniov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodimkov_Y/0/1/0/all/0/1&quot;&gt;Y. A. Rodimkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sysoyev_A/0/1/0/all/0/1&quot;&gt;A. V. Sysoyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerov_I/0/1/0/all/0/1&quot;&gt;I. B. Meyerov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12810">
<title>Combining low-dose CT-based radiomics and metabolomics for early lung cancer screening support. (arXiv:2311.12810v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12810</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to its predominantly asymptomatic or mildly symptomatic progression, lung
cancer is often diagnosed in advanced stages, resulting in poorer survival
rates for patients. As with other cancers, early detection significantly
improves the chances of successful treatment. Early diagnosis can be
facilitated through screening programs designed to detect lung tissue tumors
when they are still small, typically around 3mm in size. However, the analysis
of extensive screening program data is hampered by limited access to medical
experts. In this study, we developed a procedure for identifying potential
malignant neoplastic lesions within lung parenchyma. The system leverages
machine learning (ML) techniques applied to two types of measurements: low-dose
Computed Tomography-based radiomics and metabolomics. Using data from two
Polish screening programs, two ML algorithms were tested, along with various
integration methods, to create a final model that combines both modalities to
support lung cancer screening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zyla_J/0/1/0/all/0/1&quot;&gt;Joanna Zyla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marczyk_M/0/1/0/all/0/1&quot;&gt;Michal Marczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prazuch_W/0/1/0/all/0/1&quot;&gt;Wojciech Prazuch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Socha_M/0/1/0/all/0/1&quot;&gt;Marek Socha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suwalska_A/0/1/0/all/0/1&quot;&gt;Aleksandra Suwalska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durawa_A/0/1/0/all/0/1&quot;&gt;Agata Durawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jelitto_Gorska_M/0/1/0/all/0/1&quot;&gt;Malgorzata Jelitto-Gorska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziadziuszko_K/0/1/0/all/0/1&quot;&gt;Katarzyna Dziadziuszko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szurowska_E/0/1/0/all/0/1&quot;&gt;Edyta Szurowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rzyman_W/0/1/0/all/0/1&quot;&gt;Witold Rzyman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Widlak_P/0/1/0/all/0/1&quot;&gt;Piotr Widlak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polanska_J/0/1/0/all/0/1&quot;&gt;Joanna Polanska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12812">
<title>Personalization of Affective Models to Enable Neuropsychiatric Digital Precision Health Interventions: A Feasibility Study. (arXiv:2311.12812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12812</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile digital therapeutics for autism spectrum disorder (ASD) often target
emotion recognition and evocation, which is a challenge for children with ASD.
While such mobile applications often use computer vision machine learning (ML)
models to guide the adaptive nature of the digital intervention, a single model
is usually deployed and applied to all children. Here, we explore the potential
of model personalization, or training a single emotion recognition model per
person, to improve the performance of these underlying emotion recognition
models used to guide digital health therapies for children with ASD. We
conducted experiments on the Emognition dataset, a video dataset of human
subjects evoking a series of emotions. For a subset of 10 individuals in the
dataset with a sufficient representation of at least two ground truth emotion
labels, we trained a personalized version of three classical ML models on a set
of 51 features extracted from each video frame. We measured the importance of
each facial feature for all personalized models and observed differing ranked
lists of top features across subjects, motivating the need for model
personalization. We then compared the personalized models against a generalized
model trained using data from all 10 participants. The mean F1-scores achieved
by the personalized models were 90.48%, 92.66%, and 86.40%, respectively. By
contrast, the mean F1-scores reached by non-personalized models trained on
different human subjects and evaluated using the same test set were 88.55%,
91.78%, and 80.42%, respectively. The personalized models outperformed the
generalized models for 7 out of 10 participants. PCA analyses on the remaining
3 participants revealed relatively facial configuration differences between
emotion labels within each subject, suggesting that personalized ML will fail
when the variation among data points within a subjects data is too low.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1&quot;&gt;Ali Kargarandehkordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaisti_M/0/1/0/all/0/1&quot;&gt;Matti Kaisti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1&quot;&gt;Peter Washington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12813">
<title>Targeted Activation Penalties Help CNNs Ignore Spurious Signals. (arXiv:2311.12813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12813</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks (NNs) can learn to rely on spurious signals in the training
data, leading to poor generalisation. Recent methods tackle this problem by
training NNs with additional ground-truth annotations of such signals. These
methods may, however, let spurious signals re-emerge in deep convolutional NNs
(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the
same problem by penalising activations to control the re-emergence of spurious
signals in deep CNNs, while also lowering training times and memory usage. In
addition, ground-truth annotations can be expensive to obtain. We show that TAP
still works well with annotations generated by pre-trained models as effective
substitutes of ground-truth annotations. We demonstrate the power of TAP
against two state-of-the-art baselines on the MNIST benchmark and on two
clinical image datasets, using four different CNN architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dekai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Matthew Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12815">
<title>Proposing an intelligent mesh smoothing method with graph neural networks. (arXiv:2311.12815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12815</link>
<description rdf:parseType="Literal">&lt;p&gt;In CFD, mesh smoothing methods are commonly utilized to refine the mesh
quality to achieve high-precision numerical simulations. Specifically,
optimization-based smoothing is used for high-quality mesh smoothing, but it
incurs significant computational overhead. Pioneer works improve its smoothing
efficiency by adopting supervised learning to learn smoothing methods from
high-quality meshes. However, they pose difficulty in smoothing the mesh nodes
with varying degrees and also need data augmentation to address the node input
sequence problem. Additionally, the required labeled high-quality meshes
further limit the applicability of the proposed method. In this paper, we
present GMSNet, a lightweight neural network model for intelligent mesh
smoothing. GMSNet adopts graph neural networks to extract features of the
node&apos;s neighbors and output the optimal node position. During smoothing, we
also introduce a fault-tolerance mechanism to prevent GMSNet from generating
negative volume elements. With a lightweight model, GMSNet can effectively
smoothing mesh nodes with varying degrees and remain unaffected by the order of
input data. A novel loss function, MetricLoss, is also developed to eliminate
the need for high-quality meshes, which provides a stable and rapid convergence
during training. We compare GMSNet with commonly used mesh smoothing methods on
two-dimensional triangle meshes. The experimental results show that GMSNet
achieves outstanding mesh smoothing performances with 5% model parameters of
the previous model, and attains 8.62 times faster than optimization-based
smoothing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinhai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junjun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12816">
<title>Evolution of Convolutional Neural Network (CNN): Compute vs Memory bandwidth for Edge AI. (arXiv:2311.12816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12816</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) have greatly influenced the field of
Embedded Vision and Edge Artificial Intelligence (AI), enabling powerful
machine learning capabilities on resource-constrained devices. This article
explores the relationship between CNN compute requirements and memory bandwidth
in the context of Edge AI. We delve into the historical progression of CNN
architectures, from the early pioneering models to the current state-of-the-art
designs, highlighting the advancements in compute-intensive operations. We
examine the impact of increasing model complexity on both computational
requirements and memory access patterns. The paper presents a comparison
analysis of the evolving trade-off between compute demands and memory bandwidth
requirements in CNNs. This analysis provides insights into designing efficient
architectures and potential hardware accelerators in enhancing CNN performance
on edge devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chenna_D/0/1/0/all/0/1&quot;&gt;Dwith Chenna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12817">
<title>Semantic Face Compression for Metaverse: A Compact 3D Descriptor Based Approach. (arXiv:2311.12817v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12817</link>
<description rdf:parseType="Literal">&lt;p&gt;In this letter, we envision a new metaverse communication paradigm for
virtual avatar faces, and develop the semantic face compression with compact 3D
facial descriptors. The fundamental principle is that the communication of
virtual avatar faces primarily emphasizes the conveyance of semantic
information. In light of this, the proposed scheme offers the advantages of
being highly flexible, efficient and semantically meaningful. The semantic face
compression, which allows the communication of the descriptors for artificial
intelligence based understanding, could facilitate numerous applications
without the involvement of humans in metaverse. The promise of the proposed
paradigm is also demonstrated by performance comparisons with the
state-of-the-art video coding standard, Versatile Video Coding. A significant
improvement in terms of rate-accuracy performance has been achieved. The
proposed scheme is expected to enable numerous applications, such as digital
human communication based on machine analysis, and to form the cornerstone of
interaction and communication in the metaverse.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Binzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bolin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12818">
<title>Manifold Path Guiding for Importance Sampling Specular Chains. (arXiv:2311.12818v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12818</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex visual effects such as caustics are often produced by light paths
containing multiple consecutive specular vertices (dubbed specular chains),
which pose a challenge to unbiased estimation in Monte Carlo rendering. In this
work, we study the light transport behavior within a sub-path that is comprised
of a specular chain and two non-specular separators. We show that the specular
manifolds formed by all the sub-paths could be exploited to provide coherence
among sub-paths. By reconstructing continuous energy distributions from
historical and coherent sub-paths, seed chains can be generated in the context
of importance sampling and converge to admissible chains through manifold
walks. We verify that importance sampling the seed chain in the continuous
space reaches the goal of importance sampling the discrete admissible specular
chain. Based on these observations and theoretical analyses, a progressive
pipeline, manifold path guiding, is designed and implemented to importance
sample challenging paths featuring long specular chains. To our best knowledge,
this is the first general framework for importance sampling discrete specular
chains in regular Monte Carlo rendering. Extensive experiments demonstrate that
our method outperforms state-of-the-art unbiased solutions with up to 40x
variance reduction, especially in typical scenes containing long specular
chains and complex visibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhimin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1&quot;&gt;Pengpei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1&quot;&gt;Changqing Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Ling-Qi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12819">
<title>Fixing the problems of deep neural networks will require better training data and learning algorithms. (arXiv:2311.12819v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12819</link>
<description rdf:parseType="Literal">&lt;p&gt;Bowers and colleagues argue that DNNs are poor models of biological vision
because they often learn to rival human accuracy by relying on strategies that
differ markedly from those of humans. We show that this problem is worsening as
DNNs are becoming larger-scale and increasingly more accurate, and prescribe
methods for building DNNs that can reliably model biological vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linsley_D/0/1/0/all/0/1&quot;&gt;Drew Linsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1&quot;&gt;Thomas Serre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12820">
<title>MSG-BART: Multi-granularity Scene Graph-Enhanced Encoder-Decoder Language Model for Video-grounded Dialogue Generation. (arXiv:2311.12820v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12820</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating dialogue grounded in videos requires a high level of understanding
and reasoning about the visual scenes in the videos. However, existing large
visual-language models are not effective due to their latent features and
decoder-only structure, especially with respect to spatio-temporal relationship
reasoning. In this paper, we propose a novel approach named MSG-BART, which
enhances the integration of video information by incorporating a
multi-granularity spatio-temporal scene graph into an encoder-decoder
pre-trained language model. Specifically, we integrate the global and local
scene graph into the encoder and decoder, respectively, to improve both overall
perception and target reasoning capability. To further improve the information
selection capability, we propose a multi-pointer network to facilitate
selection between text and video. Extensive experiments are conducted on three
video-grounded dialogue benchmarks, which show the significant superiority of
the proposed MSG-BART compared to a range of state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pingjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12821">
<title>Advancing The Rate-Distortion-Computation Frontier For Neural Image Compression. (arXiv:2311.12821v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12821</link>
<description rdf:parseType="Literal">&lt;p&gt;The rate-distortion performance of neural image compression models has
exceeded the state-of-the-art for non-learned codecs, but neural codecs are
still far from widespread deployment and adoption. The largest obstacle is
having efficient models that are feasible on a wide variety of consumer
hardware. Comparative research and evaluation is difficult due to the lack of
standard benchmarking platforms and due to variations in hardware architectures
and test environments. Through our rate-distortion-computation (RDC) study we
demonstrate that neither floating-point operations (FLOPs) nor runtime are
sufficient on their own to accurately rank neural compression methods. We also
explore the RDC frontier, which leads to a family of model architectures with
the best empirical trade-off between computational requirements and RD
performance. Finally, we identify a novel neural compression architecture that
yields state-of-the-art RD performance with rate savings of 23.1% over BPG
(7.0% over VTM and 3.0% over ELIC) without requiring significantly more FLOPs
than other learning-based codecs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1&quot;&gt;David Minnen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnston_N/0/1/0/all/0/1&quot;&gt;Nick Johnston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12823">
<title>EWasteNet: A Two-Stream Data Efficient Image Transformer Approach for E-Waste Classification. (arXiv:2311.12823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12823</link>
<description rdf:parseType="Literal">&lt;p&gt;Improper disposal of e-waste poses global environmental and health risks,
raising serious concerns. The accurate classification of e-waste images is
critical for efficient management and recycling. In this paper, we have
presented a comprehensive dataset comprised of eight different classes of
images of electronic devices named the E-Waste Vision Dataset. We have also
presented EWasteNet, a novel two-stream approach for precise e-waste image
classification based on a data-efficient image transformer (DeiT). The first
stream of EWasteNet passes through a sobel operator that detects the edges
while the second stream is directed through an Atrous Spatial Pyramid Pooling
and attention block where multi-scale contextual information is captured. We
train both of the streams simultaneously and their features are merged at the
decision level. The DeiT is used as the backbone of both streams. Extensive
analysis of the e-waste dataset indicates the usefulness of our method,
providing 96% accuracy in e-waste classification. The proposed approach
demonstrates significant usefulness in addressing the global concern of e-waste
management. It facilitates efficient waste management and recycling by
accurately classifying e-waste images, reducing health and safety hazards
associated with improper disposal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_N/0/1/0/all/0/1&quot;&gt;Niful Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jony_M/0/1/0/all/0/1&quot;&gt;Md. Mehedi Hasan Jony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_E/0/1/0/all/0/1&quot;&gt;Emam Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutradhar_S/0/1/0/all/0/1&quot;&gt;Sunny Sutradhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Atikur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Motaharul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12826">
<title>LiveChat: Video Comment Generation from Audio-Visual Multimodal Contexts. (arXiv:2311.12826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12826</link>
<description rdf:parseType="Literal">&lt;p&gt;Live commenting on video, a popular feature of live streaming platforms,
enables viewers to engage with the content and share their comments, reactions,
opinions, or questions with the streamer or other viewers while watching the
video or live stream. It presents a challenging testbed for AI agents, which
involves the simultaneous understanding of audio-visual multimodal contexts
from live streams and the ability to interact with human viewers through
dialogue. As existing live streaming-based comments datasets contain limited
categories and lack a diversity, we create a large-scale audio-visual
multimodal dialogue dataset to facilitate the development of live commenting
technologies. The data is collected from Twitch, with 11 different categories
and 575 streamers for a total of 438 hours of video and 3.2 million comments.
Moreover, we propose a novel multimodal generation model capable of generating
live comments that align with the temporal and spatial events within the video,
as well as with the ongoing multimodal dialogue context. Our initial results
have demonstrated the effectiveness of the proposed model, providing a robust
foundation for further research and practical applications in the field of live
video interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalanne_J/0/1/0/all/0/1&quot;&gt;Julien Lalanne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bournet_R/0/1/0/all/0/1&quot;&gt;Raphael Bournet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12829">
<title>Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile. (arXiv:2311.12829v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12829</link>
<description rdf:parseType="Literal">&lt;p&gt;The kinematics of human movements and locomotion are closely linked to the
activation and contractions of muscles. To investigate this, we present a
multimodal dataset with benchmarks collected using a novel pair of Intelligent
Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our
system utilizes synchronized datasets that comprise time-series data from the
Knee Sleeves and the corresponding ground truth labels from the visualized
motion capture camera system. We employ these to generate 3D human models
solely based on the wearable data of individuals performing different
activities. We demonstrate the effectiveness of this camera-free system and
machine learning algorithms in the assessment of various movements and
exercises, including extension to unseen exercises and individuals. The results
show an average error of 7.21 degrees across all eight lower body joints when
compared to the ground truth, indicating the effectiveness and reliability of
the Knee Sleeve system for the prediction of different lower body joints beyond
the knees. The results enable human pose estimation in a seamless manner
without being limited by visual occlusion or the field of view of cameras. Our
results show the potential of multimodal wearable sensing in a variety of
applications from home fitness to sports, healthcare, and physical
rehabilitation focusing on pose and movement estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tashakori_A/0/1/0/all/0/1&quot;&gt;Arvin Tashakori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zenan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Servati_A/0/1/0/all/0/1&quot;&gt;Amir Servati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayana_H/0/1/0/all/0/1&quot;&gt;Harishkumar Narayana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanian_S/0/1/0/all/0/1&quot;&gt;Saeid Soltanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeap_R/0/1/0/all/0/1&quot;&gt;Rou Yi Yeap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1&quot;&gt;Meng Han Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toy_L/0/1/0/all/0/1&quot;&gt;Lauren Toy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Servati_P/0/1/0/all/0/1&quot;&gt;Peyman Servati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12831">
<title>ECNR: Efficient Compressive Neural Representation of Time-Varying Volumetric Datasets. (arXiv:2311.12831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12831</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to its conceptual simplicity and generality, compressive neural
representation has emerged as a promising alternative to traditional
compression methods for managing massive volumetric datasets. The
state-of-the-art neural compression solution, neurcomp, however, utilizes a
single large multilayer perceptron (MLP) to encode the global volume, incurring
slow training and inference. This paper presents an efficient compressive
neural representation (ECNR) solution that improves upon neurcomp to handle
large-scale time-varying datasets. At the heart of our approach is a multiscale
structure that uses the Laplacian pyramid for adaptive signal fitting via
implicit neural representation. We leverage multiple small MLPs at each scale
for fitting local content or residual blocks. By assigning similar blocks to
the same MLP via size uniformization, we enable balanced parallelization among
MLPs to significantly speed up training and inference. A deep compression
strategy is then employed to compact the resulting model. We demonstrate the
effectiveness of ECNR with multiple datasets and compare it with neurcomp and
two state-of-the-art conventional compression methods (SZ3 and TTHRESH). Our
results position ECNR as a promising alternative to neurcomp for scientific
data compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoli Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12832">
<title>Toward effective protection against diffusion based mimicry through score distillation. (arXiv:2311.12832v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12832</link>
<description rdf:parseType="Literal">&lt;p&gt;While generative diffusion models excel in producing high-quality images,
they can also be misused to mimic authorized images, posing a significant
threat to AI systems. Efforts have been made to add calibrated perturbations to
protect images from diffusion-based mimicry pipelines. However, most of the
existing methods are too ineffective and even impractical to be used by
individual users due to their high computation and memory requirements. In this
work, we present novel findings on attacking latent diffusion models (LDM) and
propose new plug-and-play strategies for more effective protection. In
particular, we explore the bottleneck in attacking an LDM, discovering that the
encoder module rather than the denoiser module is the vulnerable point. Based
on this insight, we present our strategy using Score Distillation Sampling
(SDS) to double the speed of protection and reduce memory occupation by half
without compromising its strength. Additionally, we provide a robust protection
strategy by counterintuitively minimizing the semantic loss, which can assist
in generating more natural perturbations. Finally, we conduct extensive
experiments to substantiate our findings and comprehensively evaluate our newly
proposed strategies. We hope our insights and protective measures can
contribute to better defense against malicious diffusion-based mimicry,
advancing the development of secure AI systems. The code is available in
https://github.com/xavihart/Diff-Protect
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Haotian Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chumeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12836">
<title>AI-based association analysis for medical imaging using latent-space geometric confounder correction. (arXiv:2311.12836v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12836</link>
<description rdf:parseType="Literal">&lt;p&gt;AI has greatly enhanced medical image analysis, yet its use in
epidemiological population imaging studies remains limited due to visualization
challenges in non-linear models and lack of confounder control. Addressing
this, we introduce an AI method emphasizing semantic feature interpretation and
resilience against multiple confounders. Our approach&apos;s merits are tested in
three scenarios: extracting confounder-free features from a 2D synthetic
dataset; examining the association between prenatal alcohol exposure and
children&apos;s facial shapes using 3D mesh data; exploring the relationship between
global cognition and brain images with a 3D MRI dataset. Results confirm our
method effectively reduces confounder influences, establishing less confounded
associations. Additionally, it provides a unique visual representation,
highlighting specific image alterations due to identified correlations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vernooij_M/0/1/0/all/0/1&quot;&gt;Meike W. Vernooij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolvius_E/0/1/0/all/0/1&quot;&gt;Eppo B. Wolvius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roshchupkin_G/0/1/0/all/0/1&quot;&gt;Gennady V. Roshchupkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bron_E/0/1/0/all/0/1&quot;&gt;Esther E. Bron&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12840">
<title>Wafer Map Defect Patterns Semi-Supervised Classification Using Latent Vector Representation. (arXiv:2311.12840v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12840</link>
<description rdf:parseType="Literal">&lt;p&gt;As the globalization of semiconductor design and manufacturing processes
continues, the demand for defect detection during integrated circuit
fabrication stages is becoming increasingly critical, playing a significant
role in enhancing the yield of semiconductor products. Traditional wafer map
defect pattern detection methods involve manual inspection using electron
microscopes to collect sample images, which are then assessed by experts for
defects. This approach is labor-intensive and inefficient. Consequently, there
is a pressing need to develop a model capable of automatically detecting
defects as an alternative to manual operations. In this paper, we propose a
method that initially employs a pre-trained VAE model to obtain the fault
distribution information of the wafer map. This information serves as guidance,
combined with the original image set for semi-supervised model training. During
the semi-supervised training, we utilize a teacher-student network for
iterative learning. The model presented in this paper is validated on the
benchmark dataset WM-811K wafer dataset. The experimental results demonstrate
superior classification accuracy and detection performance compared to
state-of-the-art models, fulfilling the requirements for industrial
applications. Compared to the original architecture, we have achieved
significant performance improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1&quot;&gt;Qiyu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zeng Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12841">
<title>Tool Wear Segmentation in Blanking Processes with Fully Convolutional Networks based Digital Image Processing. (arXiv:2311.12841v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12841</link>
<description rdf:parseType="Literal">&lt;p&gt;The extend of tool wear significantly affects blanking processes and has a
decisive impact on product quality and productivity. For this reason, numerous
scientists have addressed their research to wear monitoring systems in order to
identify or even predict critical wear at an early stage. Existing approaches
are mainly based on indirect monitoring using time series, which are used to
detect critical wear states via thresholds or machine learning models.
Nevertheless, differentiation between types of wear phenomena affecting the
tool during blanking as well as quantification of worn surfaces is still
limited in practice. While time series data provides partial insights into wear
occurrence and evolution, direct monitoring techniques utilizing image data
offer a more comprehensive perspective and increased robustness when dealing
with varying process parameters. However, acquiring and processing this data in
real-time is challenging. In particular, high dynamics combined with increasing
strokes rates as well as the high dimensionality of image data have so far
prevented the development of direct image-based monitoring systems. For this
reason, this paper demonstrates how high-resolution images of tools at 600 spm
can be captured and subsequently processed using semantic segmentation deep
learning algorithms, more precisely Fully Convolutional Networks (FCN). 125,000
images of the tool are taken from successive strokes, and microscope images are
captured to investigate the worn surfaces. Based on findings from the
microscope images, selected images are labeled pixel by pixel according to
their wear condition and used to train a FCN (U-Net).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlegel_C/0/1/0/all/0/1&quot;&gt;Clemens Schlegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molitor_D/0/1/0/all/0/1&quot;&gt;Dirk Alexander Molitor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubik_C/0/1/0/all/0/1&quot;&gt;Christian Kubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1&quot;&gt;Daniel Michael Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groche_P/0/1/0/all/0/1&quot;&gt;Peter Groche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12842">
<title>Multimodal Identification of Alzheimer&apos;s Disease: A Review. (arXiv:2311.12842v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12842</link>
<description rdf:parseType="Literal">&lt;p&gt;Alzheimer&apos;s disease is a progressive neurological disorder characterized by
cognitive impairment and memory loss. With the increasing aging population, the
incidence of AD is continuously rising, making early diagnosis and intervention
an urgent need. In recent years, a considerable number of teams have applied
computer-aided diagnostic techniques to early classification research of AD.
Most studies have utilized imaging modalities such as magnetic resonance
imaging (MRI), positron emission tomography (PET), and electroencephalogram
(EEG). However, there have also been studies that attempted to use other
modalities as input features for the models, such as sound, posture,
biomarkers, cognitive assessment scores, and their fusion. Experimental results
have shown that the combination of multiple modalities often leads to better
performance compared to a single modality. Therefore, this paper will focus on
different modalities and their fusion, thoroughly elucidate the mechanisms of
various modalities, explore which methods should be combined to better harness
their utility, analyze and summarize the literature in the field of early
classification of AD in recent years, in order to explore more possibilities of
modality combinations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Guian Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengsha Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuolin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiehui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhenchao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Calvin Yu-Chian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12845">
<title>A Novel Defocus-Blur Region Detection Approach Based on DCT Feature and PCNN Structure. (arXiv:2311.12845v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12845</link>
<description rdf:parseType="Literal">&lt;p&gt;The motion or out-of-focus effect in digital images is the main reason for
the blurred regions in defocused-blurred images. It may adversely affect
various image features such as texture, pixel, and region. Therefore, it is
important to detect in-focused objects in defocused-blurred images after the
segmentation of blurred and non-blurred regions. The state-of-the-art
techniques are prone to noisy pixels, and their local descriptors for
developing segmentation metrics are also complex. To address these issues, this
research, therefore, proposed a novel and hybrid-focused detection approach
based on Discrete Cosine Transform (DCT) coefficients and PC Neural Net (PCNN)
structure. The proposed approach partially resolves the limitations of the
existing contrast schemes to detect in-focused smooth objects from the
out-of-focused smooth regions in the defocus dataset. The visual and
quantitative evaluation illustrates that the proposed approach outperformed in
terms of accuracy and efficiency to referenced algorithms. The highest F-score
of the proposed approach on Zhao&apos;s dataset is 0.7940 whereas on Shi&apos;s dataset
is 0.9178.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basar_S/0/1/0/all/0/1&quot;&gt;Sadia Basar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mushtaq Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waheed_A/0/1/0/all/0/1&quot;&gt;Abdul Waheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1&quot;&gt;Muneer Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miraz_M/0/1/0/all/0/1&quot;&gt;Mahdi H. Miraz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12847">
<title>CopyScope: Model-level Copyright Infringement Quantification in the Diffusion Workflow. (arXiv:2311.12847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12847</link>
<description rdf:parseType="Literal">&lt;p&gt;Web-based AI image generation has become an innovative art form that can
generate novel artworks with the rapid development of the diffusion model.
However, this new technique brings potential copyright infringement risks as it
may incorporate the existing artworks without the owners&apos; consent. Copyright
infringement quantification is the primary and challenging step towards
AI-generated image copyright traceability. Previous work only focused on data
attribution from the training data perspective, which is unsuitable for tracing
and quantifying copyright infringement in practice because of the following
reasons: (1) the training datasets are not always available in public; (2) the
model provider is the responsible party, not the image. Motivated by this, in
this paper, we propose CopyScope, a new framework to quantify the infringement
of AI-generated images from the model level. We first rigorously identify
pivotal components within the AI image generation pipeline. Then, we propose to
take advantage of Fr\&apos;echet Inception Distance (FID) to effectively capture the
image similarity that fits human perception naturally. We further propose the
FID-based Shapley algorithm to evaluate the infringement contribution among
models. Extensive experiments demonstrate that our work not only reveals the
intricacies of infringement quantification but also effectively depicts the
infringing models quantitatively, thus promoting accountability in AI
image-generation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junlei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiashi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xuetao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12850">
<title>Meticulously Selecting 1% of the Dataset for Pre-training! Generating Differentially Private Images Data with Semantics Query. (arXiv:2311.12850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12850</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential Privacy (DP) image data synthesis, which leverages the DP
technique to generate synthetic data to replace the sensitive data, allowing
organizations to share and utilize synthetic images without privacy concerns.
Previous methods incorporate the advanced techniques of generative models and
pre-training on a public dataset to produce exceptional DP image data, but
suffer from problems of unstable training and massive computational resource
demands. This paper proposes a novel DP image synthesis method, termed
PRIVIMAGE, which meticulously selects pre-training data, promoting the
efficient creation of DP datasets with high fidelity and utility. PRIVIMAGE
first establishes a semantic query function using a public dataset. Then, this
function assists in querying the semantic distribution of the sensitive
dataset, facilitating the selection of data from the public dataset with
analogous semantics for pre-training. Finally, we pre-train an image generative
model using the selected data and then fine-tune this model on the sensitive
dataset using Differentially Private Stochastic Gradient Descent (DP-SGD).
PRIVIMAGE allows us to train a lightly parameterized generative model, reducing
the noise in the gradient during DP-SGD training and enhancing training
stability. Extensive experiments demonstrate that PRIVIMAGE uses only 1% of the
public dataset for pre-training and 7.6% of the parameters in the generative
model compared to the state-of-the-art method, whereas achieves superior
synthetic performance and conserves more computational resources. On average,
PRIVIMAGE achieves 30.1% lower FID and 12.6% higher Classification Accuracy
than the state-of-the-art method. The replication package and datasets can be
accessed online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kecen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinwen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12855">
<title>Contextualised Out-of-Distribution Detection using Pattern Identication. (arXiv:2311.12855v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12855</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose CODE, an extension of existing work from the field
of explainable AI that identifies class-specific recurring patterns to build a
robust Out-of-Distribution (OoD) detection method for visual classifiers. CODE
does not require any classifier retraining and is OoD-agnostic, i.e., tuned
directly to the training dataset. Crucially, pattern identification allows us
to provide images from the In-Distribution (ID) dataset as reference data to
provide additional context to the confidence scores. In addition, we introduce
a new benchmark based on perturbations of the ID dataset that provides a known
and quantifiable measure of the discrepancy between the ID and OoD datasets
serving as a reference value for the comparison between OoD detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Darme_R/0/1/0/all/0/1&quot;&gt;Romain Xu-Darme&lt;/a&gt; (LSL, LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girard_Satabin_J/0/1/0/all/0/1&quot;&gt;Julien Girard-Satabin&lt;/a&gt; (LSL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hond_D/0/1/0/all/0/1&quot;&gt;Darryl Hond&lt;/a&gt; (TRT UK), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Incorvaia_G/0/1/0/all/0/1&quot;&gt;Gabriele Incorvaia&lt;/a&gt; (TRT UK), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chihani_Z/0/1/0/all/0/1&quot;&gt;Zakaria Chihani&lt;/a&gt; (LSL)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12857">
<title>Adversarial sample generation and training using geometric masks for accurate and resilient license plate character recognition. (arXiv:2311.12857v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12857</link>
<description rdf:parseType="Literal">&lt;p&gt;Reading dirty license plates accurately in moving vehicles is challenging for
automatic license plate recognition systems. Moreover, license plates are often
intentionally tampered with a malicious intent to avoid police apprehension.
Usually, such groups and individuals know how to fool the existing recognition
systems by making minor unnoticeable plate changes. Designing and developing
deep learning methods resilient to such real-world &apos;attack&apos; practices remains
an active research problem. As a solution, this work develops a resilient
method to recognize license plate characters. Extracting 1057 character images
from 160 Nepalese vehicles, as the first step, we trained several standard deep
convolutional neural networks to obtain 99.5% character classification
accuracy. On adversarial images generated to simulate malicious tampering,
however, our model&apos;s accuracy dropped to 25%. Next, we enriched our dataset by
generating and adding geometrically masked images, retrained our models, and
investigated the models&apos; predictions. The proposed approach of training with
generated adversarial images helped our adversarial attack-aware license plate
character recognition (AA-LPCR) model achieves an accuracy of 99.7%. This
near-perfect accuracy demonstrates that the proposed idea of random geometric
masking is highly effective for improving the accuracy of license plate
recognition models. Furthermore, by performing interpretability studies to
understand why our models work, we identify and highlight attack-prone regions
in the input character images. In sum, although Nepal&apos;s embossed license plate
detection systems are vulnerable to malicious attacks, our findings suggest
that these systems can be upgraded to close to 100% resilience.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_B/0/1/0/all/0/1&quot;&gt;Bishal Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khakurel_G/0/1/0/all/0/1&quot;&gt;Griwan Khakurel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simkhada_K/0/1/0/all/0/1&quot;&gt;Kritika Simkhada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_B/0/1/0/all/0/1&quot;&gt;Badri Adhikari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12859">
<title>Joint Multi-View Collaborative Clustering. (arXiv:2311.12859v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12859</link>
<description rdf:parseType="Literal">&lt;p&gt;Data is increasingly being collected from multiple sources and described by
multiple views. These multi-view data provide richer information than
traditional single-view data. Fusing the former for specific tasks is an
essential component of multi-view clustering. Since the goal of multi-view
clustering algorithms is to discover the common latent structure shared by
multiple views, the majority of proposed solutions overlook the advantages of
incorporating knowledge derived from horizontal collaboration between
multi-view data and the final consensus. To fill this gap, we propose the Joint
Multi-View Collaborative Clustering (JMVCC) solution, which involves the
generation of basic partitions using Non-negative Matrix Factorization (NMF)
and the horizontal collaboration principle, followed by the fusion of these
local partitions using ensemble clustering. Furthermore, we propose a weighting
method to reduce the risk of negative collaboration (i.e., views with low
quality) during the generation and fusion of local partitions. The experimental
results, which were obtained using a variety of data sets, demonstrate that
JMVCC outperforms other multi-view clustering algorithms and is robust to noisy
views.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalafaoui_Y/0/1/0/all/0/1&quot;&gt;Yasser Khalafaoui&lt;/a&gt; (Alteca, ETIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matei_B/0/1/0/all/0/1&quot;&gt;Basarab Matei&lt;/a&gt; (LIPN), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grozavu_N/0/1/0/all/0/1&quot;&gt;Nistor Grozavu&lt;/a&gt; (ETIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovisetto_M/0/1/0/all/0/1&quot;&gt;Martino Lovisetto&lt;/a&gt; (Alteca)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12860">
<title>On the stability, correctness and plausibility of visual explanation methods based on feature importance. (arXiv:2311.12860v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12860</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of Explainable AI, multiples evaluation metrics have been
proposed in order to assess the quality of explanation methods w.r.t. a set of
desired properties. In this work, we study the articulation between the
stability, correctness and plausibility of explanations based on feature
importance for image classifiers. We show that the existing metrics for
evaluating these properties do not always agree, raising the issue of what
constitutes a good evaluation metric for explanations. Finally, in the
particular case of stability and correctness, we show the possible limitations
of some evaluation metrics and propose new ones that take into account the
local behaviour of the model under test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Darme_R/0/1/0/all/0/1&quot;&gt;Romain Xu-Darme&lt;/a&gt; (LSL, LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1&quot;&gt;Jenny Benois-Pineau&lt;/a&gt; (LaBRI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giot_R/0/1/0/all/0/1&quot;&gt;Romain Giot&lt;/a&gt; (LaBRI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quenot_G/0/1/0/all/0/1&quot;&gt;Georges Qu&amp;#xe9;not&lt;/a&gt; (LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chihani_Z/0/1/0/all/0/1&quot;&gt;Zakaria Chihani&lt;/a&gt; (LSL), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousset_M/0/1/0/all/0/1&quot;&gt;Marie-Christine Rousset&lt;/a&gt; (LIG), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhukov_A/0/1/0/all/0/1&quot;&gt;Alexey Zhukov&lt;/a&gt; (LaBRI)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12862">
<title>TorchSparse++: Efficient Training and Inference Framework for Sparse Convolution on GPUs. (arXiv:2311.12862v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2311.12862</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse convolution plays a pivotal role in emerging workloads, including
point cloud processing in AR/VR, autonomous driving, and graph understanding in
recommendation systems. Since the computation pattern is sparse and irregular,
specialized high-performance kernels are required. Existing GPU libraries offer
two dataflow types for sparse convolution. The gather-GEMM-scatter dataflow is
easy to implement but not optimal in performance, while the dataflows with
overlapped computation and memory access (e.g.implicit GEMM) are highly
performant but have very high engineering costs. In this paper, we introduce
TorchSparse++, a new GPU library that achieves the best of both worlds. We
create a highly efficient Sparse Kernel Generator that generates performant
sparse convolution kernels at less than one-tenth of the engineering cost of
the current state-of-the-art system. On top of this, we design the Sparse
Autotuner, which extends the design space of existing sparse convolution
libraries and searches for the best dataflow configurations for training and
inference workloads. Consequently, TorchSparse++ achieves 2.9x, 3.3x, 2.2x and
1.7x measured end-to-end speedup on an NVIDIA A100 GPU over state-of-the-art
MinkowskiEngine, SpConv 1.2, TorchSparse and SpConv v2 in inference; and is
1.2-1.3x faster than SpConv v2 in mixed precision training across seven
representative autonomous driving benchmarks. It also seamlessly supports graph
convolutions, achieving 2.6-7.6x faster inference speed compared with
state-of-the-art graph deep learning libraries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haotian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Ke Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhongming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiuyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guohao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Song Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12866">
<title>Modular Blended Attention Network for Video Question Answering. (arXiv:2311.12866v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12866</link>
<description rdf:parseType="Literal">&lt;p&gt;In multimodal machine learning tasks, it is due to the complexity of the
assignments that the network structure, in most cases, is assembled in a
sophisticated way. The holistic architecture can be separated into several
logical parts according to the respective ends that the modules are devised to
achieve. As the number of modalities of information representation increases,
constructing ad hoc subnetworks for processing the data from divergent
modalities while mediating the fusion of different information types has become
a cumbersome and expensive problem. In this paper, we present an approach to
facilitate the question with a reusable and composable neural unit; by
connecting the units in series or parallel, the arduous network constructing of
multimodal machine learning tasks will be accomplished in a much
straightforward way. Additionally, through parameter sharing (weights
replication) among the units, the space complexity will be significantly
reduced. We have conducted experiments on three commonly used datasets; our
method achieves impressive performance compared to several video QA baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mingjie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12871">
<title>An Embodied Generalist Agent in 3D World. (arXiv:2311.12871v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12871</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging massive knowledge and learning schemes from large language models
(LLMs), recent machine learning models show notable successes in building
generalist agents that exhibit the capability of general-purpose task solving
in diverse domains, including natural language processing, computer vision, and
robotics. However, a significant challenge remains as these models exhibit
limited ability in understanding and interacting with the 3D world. We argue
this limitation significantly hinders the current models from performing
real-world tasks and further achieving general intelligence. To this end, we
introduce an embodied multi-modal and multi-task generalist agent that excels
in perceiving, grounding, reasoning, planning, and acting in the 3D world. Our
proposed agent, referred to as LEO, is trained with shared LLM-based model
architectures, objectives, and weights in two stages: (i) 3D vision-language
alignment and (ii) 3D vision-language-action instruction tuning. To facilitate
the training, we meticulously curate and generate an extensive dataset
comprising object-level and scene-level multi-modal tasks with exceeding scale
and complexity, necessitating a deep understanding of and interaction with the
3D world. Through rigorous experiments, we demonstrate LEO&apos;s remarkable
proficiency across a wide spectrum of tasks, including 3D captioning, question
answering, embodied reasoning, embodied navigation, and robotic manipulation.
Our ablation results further provide valuable insights for the development of
future embodied generalist agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiangyong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_S/0/1/0/all/0/1&quot;&gt;Silong Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linghu_X/0/1/0/all/0/1&quot;&gt;Xiongkun Linghu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Puhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_B/0/1/0/all/0/1&quot;&gt;Baoxiong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12876">
<title>Energy efficiency in Edge TPU vs. embedded GPU for computer-aided medical imaging segmentation and classification. (arXiv:2311.12876v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12876</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we evaluate the energy usage of fully embedded medical
diagnosis aids based on both segmentation and classification of medical images
implemented on Edge TPU and embedded GPU processors. We use glaucoma diagnosis
based on color fundus images as an example to show the possibility of
performing segmentation and classification in real time on embedded boards and
to highlight the different energy requirements of the studied implementations.
&lt;/p&gt;
&lt;p&gt;Several other works develop the use of segmentation and feature extraction
techniques to detect glaucoma, among many other pathologies, with deep neural
networks. Memory limitations and low processing capabilities of embedded
accelerated systems (EAS) limit their use for deep network-based system
training. However, including specific acceleration hardware, such as NVIDIA&apos;s
Maxwell GPU or Google&apos;s Edge TPU, enables them to perform inferences using
complex pre-trained networks in very reasonable times.
&lt;/p&gt;
&lt;p&gt;In this study, we evaluate the timing and energy performance of two EAS
equipped with Machine Learning (ML) accelerators executing an example
diagnostic tool developed in a previous work. For optic disc (OD) and cup (OC)
segmentation, the obtained prediction times per image are under 29 and 43 ms
using Edge TPUs and Maxwell GPUs, respectively. Prediction times for the
classification subsystem are lower than 10 and 14 ms for Edge TPUs and Maxwell
GPUs, respectively. Regarding energy usage, in approximate terms, for OD
segmentation Edge TPUs and Maxwell GPUs use 38 and 190 mJ per image,
respectively. For fundus classification, Edge TPUs and Maxwell GPUs use 45 and
70 mJ, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Corral_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Mar&amp;#xed;a Rodr&amp;#xed;guez Corral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Civit_Masot_J/0/1/0/all/0/1&quot;&gt;Javier Civit-Masot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luna_Perejon_F/0/1/0/all/0/1&quot;&gt;Francisco Luna-Perej&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diaz_Cano_I/0/1/0/all/0/1&quot;&gt;Ignacio D&amp;#xed;az-Cano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morgado_Estevez_A/0/1/0/all/0/1&quot;&gt;Arturo Morgado-Est&amp;#xe9;vez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dominguez_Morales_M/0/1/0/all/0/1&quot;&gt;Manuel Dom&amp;#xed;nguez-Morales&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12885">
<title>Long-MIL: Scaling Long Contextual Multiple Instance Learning for Histopathology Whole Slide Image Analysis. (arXiv:2311.12885v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12885</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathology image analysis is the golden standard of clinical diagnosis
for Cancers. In doctors daily routine and computer-aided diagnosis, the Whole
Slide Image (WSI) of histopathology tissue is used for analysis. Because of the
extremely large scale of resolution, previous methods generally divide the WSI
into a large number of patches, then aggregate all patches within a WSI by
Multi-Instance Learning (MIL) to make the slide-level prediction when
developing computer-aided diagnosis tools. However, most previous WSI-MIL
models using global-attention without pairwise interaction and any positional
information, or self-attention with absolute position embedding can not well
handle shape varying large WSIs, e.g. testing WSIs after model deployment may
be larger than training WSIs, since the model development set is always limited
due to the difficulty of histopathology WSIs collection. To deal with the
problem, in this paper, we propose to amend position embedding for shape
varying long-contextual WSI by introducing Linear Bias into Attention, and
adapt it from 1-d long sequence into 2-d long-contextual WSI which helps model
extrapolate position embedding to unseen or under-fitted positions. We further
utilize Flash-Attention module to tackle the computational complexity of
Transformer, which also keep full self-attention performance compared to
previous attention approximation work. Our method, Long-contextual MIL
(Long-MIL) are evaluated on extensive experiments including 4 dataset including
WSI classification and survival prediction tasks to validate the superiority on
shape varying WSIs. The source code will be open-accessed soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Honglin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenglu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiatong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sunyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12886">
<title>Fine-Grained Open Domain Image Animation with Motion Guidance. (arXiv:2311.12886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12886</link>
<description rdf:parseType="Literal">&lt;p&gt;Image animation is a key task in computer vision which aims to generate
dynamic visual content from static image. Recent image animation methods employ
neural based rendering technique to generate realistic animations. Despite
these advancements, achieving fine-grained and controllable image animation
guided by text remains challenging, particularly for open-domain images
captured in diverse real environments. In this paper, we introduce an open
domain image animation method that leverages the motion prior of video
diffusion model. Our approach introduces targeted motion area guidance and
motion strength guidance, enabling precise control the movable area and its
motion speed. This results in enhanced alignment between the animated visual
elements and the prompting text, thereby facilitating a fine-grained and
interactive animation generation process for intricate motion sequences. We
validate the effectiveness of our method through rigorous experiments on an
open-domain dataset, with the results showcasing its superior performance. The
source code and model will be made publicly available upon publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zuozhuo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bingxue Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Siyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Long Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weizhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12889">
<title>Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge. (arXiv:2311.12889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12889</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an enhanced approach to generating scene graphs by
incorporating a relationship hierarchy and commonsense knowledge. Specifically,
we propose a Bayesian classification head that exploits an informative
hierarchical structure. It jointly predicts the super-category or type of
relationship between the two objects, along with the detailed relationship
under each super-category. We design a commonsense validation pipeline that
uses a large language model to critique the results from the scene graph
prediction system and then use that feedback to enhance the model performance.
The system requires no external large language model assistance at test time,
making it more convenient for practical applications. Experiments on the Visual
Genome and the OpenImage V6 datasets demonstrate that harnessing hierarchical
relationships enhances the model performance by a large margin. The proposed
Bayesian head can also be incorporated as a portable module in existing scene
graph generation algorithms to improve their results. In addition, the
commonsense validation enables the model to generate an extensive set of
reasonable predictions beyond dataset annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bowen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhijun Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1&quot;&gt;Camillo Jose Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12890">
<title>De-fine: Decomposing and Refining Visual Programs with Auto-Feedback. (arXiv:2311.12890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12890</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual programming, a modular and generalizable paradigm, integrates
different modules and Python operators to solve various vision-language tasks.
Unlike end-to-end models that need task-specific data, it advances in
performing visual processing and reasoning in an unsupervised manner. Current
visual programming methods generate programs in a single pass for each task
where the ability to evaluate and optimize based on feedback, unfortunately, is
lacking, which consequentially limits their effectiveness for complex,
multi-step problems. Drawing inspiration from benders decomposition, we
introduce De-fine, a general framework that automatically decomposes complex
tasks into simpler subtasks and refines programs through auto-feedback. This
model-agnostic approach can improve logical reasoning performance by
integrating the strengths of multiple models. Our experiments across various
visual tasks show that De-fine creates more accurate and robust programs,
setting new benchmarks in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minghe Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Juncheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12891">
<title>Text-Guided Texturing by Synchronized Multi-View Diffusion. (arXiv:2311.12891v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12891</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach to synthesize texture to dress up a
given 3D object, given a text prompt. Based on the pretrained text-to-image
(T2I) diffusion model, existing methods usually employ a project-and-inpaint
approach, in which a view of the given object is first generated and warped to
another view for inpainting. But it tends to generate inconsistent texture due
to the asynchronous diffusion of multiple views. We believe such asynchronous
diffusion and insufficient information sharing among views are the root causes
of the inconsistent artifact. In this paper, we propose a synchronized
multi-view diffusion approach that allows the diffusion processes from
different views to reach a consensus of the generated content early in the
process, and hence ensures the texture consistency. To synchronize the
diffusion, we share the denoised content among different views in each
denoising step, specifically blending the latent content in the texture domain
from views with overlap. Our method demonstrates superior performance in
generating consistent, seamless, highly detailed textures, comparing to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Minshan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Tien-Tsin Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12892">
<title>IMJENSE: Scan-specific Implicit Representation for Joint Coil Sensitivity and Image Estimation in Parallel MRI. (arXiv:2311.12892v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12892</link>
<description rdf:parseType="Literal">&lt;p&gt;Parallel imaging is a commonly used technique to accelerate magnetic
resonance imaging (MRI) data acquisition. Mathematically, parallel MRI
reconstruction can be formulated as an inverse problem relating the sparsely
sampled k-space measurements to the desired MRI image. Despite the success of
many existing reconstruction algorithms, it remains a challenge to reliably
reconstruct a high-quality image from highly reduced k-space measurements.
Recently, implicit neural representation has emerged as a powerful paradigm to
exploit the internal information and the physics of partially acquired data to
generate the desired object. In this study, we introduced IMJENSE, a
scan-specific implicit neural representation-based method for improving
parallel MRI reconstruction. Specifically, the underlying MRI image and coil
sensitivities were modeled as continuous functions of spatial coordinates,
parameterized by neural networks and polynomials, respectively. The weights in
the networks and coefficients in the polynomials were simultaneously learned
directly from sparsely acquired k-space measurements, without fully sampled
ground truth data for training. Benefiting from the powerful continuous
representation and joint estimation of the MRI image and coil sensitivities,
IMJENSE outperforms conventional image or k-space domain reconstruction
algorithms. With extremely limited calibration data, IMJENSE is more stable
than supervised calibrationless and calibration-based deep-learning methods.
Results show that IMJENSE robustly reconstructs the images acquired at
5$\mathbf{\times}$ and 6$\mathbf{\times}$ accelerations with only 4 or 8
calibration lines in 2D Cartesian acquisitions, corresponding to 22.0% and
19.5% undersampling rates. The high-quality results and scanning specificity
make the proposed method hold the potential for further accelerating the data
acquisition of parallel MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruimin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+She_H/0/1/0/all/0/1&quot;&gt;Huajun She&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chunlei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuyao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hongjiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12893">
<title>A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs. (arXiv:2311.12893v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12893</link>
<description rdf:parseType="Literal">&lt;p&gt;For intelligent quadcopter UAVs, a robust and reliable autonomous planning
system is crucial. Most current trajectory planning methods for UAVs are
suitable for static environments but struggle to handle dynamic obstacles,
which can pose challenges and even dangers to flight. To address this issue,
this paper proposes a vision-based planning system that combines tracking and
trajectory prediction of dynamic obstacles to achieve efficient and reliable
autonomous flight. We use a lightweight object detection algorithm to identify
dynamic obstacles and then use Kalman Filtering to track and estimate their
motion states. During the planning phase, we not only consider static obstacles
but also account for the potential movements of dynamic obstacles. For
trajectory generation, we use a B-spline-based trajectory search algorithm,
which is further optimized with various constraints to enhance safety and
alignment with the UAV&apos;s motion characteristics. We conduct experiments in both
simulation and real-world environments, and the results indicate that our
approach can successfully detect and avoid obstacles in dynamic environments in
real-time, offering greater reliability compared to existing approaches.
Furthermore, with the advancements in Natural Language Processing (NLP)
technology demonstrating exceptional zero-shot generalization capabilities,
more user-friendly human-machine interactions have become feasible, and this
study also explores the integration of autonomous planning systems with Large
Language Models (LLMs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Jiageng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yinliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zihang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haoran Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12894">
<title>Attribute-Aware Deep Hashing with Self-Consistency for Large-Scale Fine-Grained Image Retrieval. (arXiv:2311.12894v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2311.12894</link>
<description rdf:parseType="Literal">&lt;p&gt;Our work focuses on tackling large-scale fine-grained image retrieval as
ranking the images depicting the concept of interests (i.e., the same
sub-category labels) highest based on the fine-grained details in the query. It
is desirable to alleviate the challenges of both fine-grained nature of small
inter-class variations with large intra-class variations and explosive growth
of fine-grained data for such a practical task. In this paper, we propose
attribute-aware hashing networks with self-consistency for generating
attribute-aware hash codes to not only make the retrieval process efficient,
but also establish explicit correspondences between hash codes and visual
attributes. Specifically, based on the captured visual representations by
attention, we develop an encoder-decoder structure network of a reconstruction
task to unsupervisedly distill high-level attribute-specific vectors from the
appearance-specific visual representations without attribute annotations. Our
models are also equipped with a feature decorrelation constraint upon these
attribute vectors to strengthen their representative abilities. Then, driven by
preserving original entities&apos; similarity, the required hash codes can be
generated from these attribute-specific vectors and thus become
attribute-aware. Furthermore, to combat simplicity bias in deep hashing, we
consider the model design from the perspective of the self-consistency
principle and propose to further enhance models&apos; self-consistency by equipping
an additional image reconstruction path. Comprehensive quantitative experiments
under diverse empirical settings on six fine-grained retrieval datasets and two
generic retrieval datasets show the superiority of our models over competing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xuhao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yuxin Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12908">
<title>Diffusion Model Alignment Using Direct Preference Optimization. (arXiv:2311.12908v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12908</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are fine-tuned using human comparison data with
Reinforcement Learning from Human Feedback (RLHF) methods to make them better
aligned with users&apos; preferences. In contrast to LLMs, human preference learning
has not been widely explored in text-to-image diffusion models; the best
existing approach is to fine-tune a pretrained model using carefully curated
high quality images and captions to improve visual appeal and text alignment.
We propose Diffusion-DPO, a method to align diffusion models to human
preferences by directly optimizing on human comparison data. Diffusion-DPO is
adapted from the recently developed Direct Preference Optimization (DPO), a
simpler alternative to RLHF which directly optimizes a policy that best
satisfies human preferences under a classification objective. We re-formulate
DPO to account for a diffusion model notion of likelihood, utilizing the
evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic
dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model
of the state-of-the-art Stable Diffusion XL (SDXL)-1.0 model with
Diffusion-DPO. Our fine-tuned base model significantly outperforms both base
SDXL-1.0 and the larger SDXL-1.0 model consisting of an additional refinement
model in human evaluation, improving visual appeal and prompt alignment. We
also develop a variant that uses AI feedback and has comparable performance to
training on human preferences, opening the door for scaling of diffusion model
alignment methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Bram Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_M/0/1/0/all/0/1&quot;&gt;Meihua Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafailov_R/0/1/0/all/0/1&quot;&gt;Rafael Rafailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Linqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1&quot;&gt;Aaron Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1&quot;&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1&quot;&gt;Nikhil Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12912">
<title>Q-Seg: Quantum Annealing-based Unsupervised Image Segmentation. (arXiv:2311.12912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12912</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present Q-Seg, a novel unsupervised image segmentation
method based on quantum annealing, tailored for existing quantum hardware. We
formulate the pixel-wise segmentation problem, which assimilates spectral and
spatial information of the image, as a graph-cut optimization task. Our method
efficiently leverages the interconnected qubit topology of the D-Wave Advantage
device, offering superior scalability over existing quantum approaches and
outperforming state-of-the-art classical methods. Our empirical evaluations on
synthetic datasets reveal that Q-Seg offers better runtime performance against
the classical optimizer Gurobi. Furthermore, we evaluate our method on
segmentation of Earth Observation images, an area of application where the
amount of labeled data is usually very limited. In this case, Q-Seg
demonstrates near-optimal results in flood mapping detection with respect to
classical supervised state-of-the-art machine learning methods. Also, Q-Seg
provides enhanced segmentation for forest coverage compared to existing
annotated masks. Thus, Q-Seg emerges as a viable alternative for real-world
applications using available quantum hardware, particularly in scenarios where
the lack of labeled data and computational runtime are critical.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkatesh_S/0/1/0/all/0/1&quot;&gt;Supreeth Mysore Venkatesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Antonio Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuske_M/0/1/0/all/0/1&quot;&gt;Marlon Nuske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klusch_M/0/1/0/all/0/1&quot;&gt;Matthias Klusch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1&quot;&gt;Andreas Dengel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12914">
<title>Attention Deficit is Ordered! Fooling Deformable Vision Transformers with Collaborative Adversarial Patches. (arXiv:2311.12914v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12914</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest generation of transformer-based vision models have proven to be
superior to Convolutional Neural Network (CNN)-based models across several
vision tasks, largely attributed to their remarkable prowess in relation
modeling. Deformable vision transformers significantly reduce the quadratic
complexity of modeling attention by using sparse attention structures, enabling
them to be used in larger scale applications such as multi-view vision systems.
Recent work demonstrated adversarial attacks against transformers; we show that
these attacks do not transfer to deformable transformers due to their sparse
attention structure. Specifically, attention in deformable transformers is
modeled using pointers to the most relevant other tokens. In this work, we
contribute for the first time adversarial attacks that manipulate the attention
of deformable transformers, distracting them to focus on irrelevant parts of
the image. We also develop new collaborative attacks where a source patch
manipulates attention to point to a target patch that adversarially attacks the
system. In our experiments, we find that only 1% patched area of the input
field can lead to 0% AP. We also show that the attacks provide substantial
versatility to support different attacker scenarios because of their ability to
redirect attention under the attacker control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_Q/0/1/0/all/0/1&quot;&gt;Quazi Mishkatul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarchoun_B/0/1/0/all/0/1&quot;&gt;Bilel Tarchoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Ghazaleh_N/0/1/0/all/0/1&quot;&gt;Nael Abu-Ghazaleh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12919">
<title>SPOT! Revisiting Video-Language Models for Event Understanding. (arXiv:2311.12919v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12919</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding videos is an important research topic for multimodal learning.
Leveraging large-scale datasets of web-crawled video-text pairs as weak
supervision has become a pre-training paradigm for learning joint
representations and showcased remarkable potential in video understanding
tasks. However, videos can be multi-event and multi-grained, while these
video-text pairs usually contain only broad-level video captions. This raises a
question: with such weak supervision, can video representation in
video-language models gain the ability to distinguish even factual
discrepancies in textual description and understand fine-grained events? To
address this, we introduce SPOT Prober, to benchmark existing video-language
models&apos;s capacities of distinguishing event-level discrepancies as an indicator
of models&apos; event understanding ability. Our approach involves extracting events
as tuples (&amp;lt;Subject, Predicate, Object, Attribute, Timestamps&amp;gt;) from videos and
generating false event tuples by manipulating tuple components systematically.
We reevaluate the existing video-language models with these positive and
negative captions and find they fail to distinguish most of the manipulated
events. Based on our findings, we propose to plug in these manipulated event
captions as hard negative samples and find them effective in enhancing models
for event understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1&quot;&gt;Jinhe Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12956">
<title>Innovative Horizons in Aerial Imagery: LSKNet Meets DiffusionDet for Advanced Object Detection. (arXiv:2311.12956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12956</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of aerial image analysis, object detection plays a pivotal role,
with significant implications for areas such as remote sensing, urban planning,
and disaster management. This study addresses the inherent challenges in this
domain, notably the detection of small objects, managing densely packed
elements, and accounting for diverse orientations. We present an in-depth
evaluation of an object detection model that integrates the Large Selective
Kernel Network (LSKNet)as its backbone with the DiffusionDet head, utilizing
the iSAID dataset for empirical analysis. Our approach encompasses the
introduction of novel methodologies and extensive ablation studies. These
studies critically assess various aspects such as loss functions, box
regression techniques, and classification strategies to refine the model&apos;s
precision in object detection. The paper details the experimental application
of the LSKNet backbone in synergy with the DiffusionDet heads, a combination
tailored to meet the specific challenges in aerial image object detection. The
findings of this research indicate a substantial enhancement in the model&apos;s
performance, especially in the accuracy-time tradeoff. The proposed model
achieves a mean average precision (MAP) of approximately 45.7%, which is a
significant improvement, outperforming the RCNN model by 4.7% on the same
dataset. This advancement underscores the effectiveness of the proposed
modifications and sets a new benchmark in aerial image analysis, paving the way
for more accurate and efficient object detection methodologies. The code is
publicly available at https://github.com/SashaMatsun/LSKDiffDet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharshar_A/0/1/0/all/0/1&quot;&gt;Ahmed Sharshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsun_A/0/1/0/all/0/1&quot;&gt;Aleksandr Matsun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12967">
<title>Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model. (arXiv:2311.12967v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12967</link>
<description rdf:parseType="Literal">&lt;p&gt;Feedforward generalizable models for implicit shape reconstruction from
unoriented point cloud present multiple advantages, including high performance
and inference speed. However, they still suffer from generalization issues,
ranging from underfitting the input point cloud, to misrepresenting samples
outside of the training data distribution, or with toplogies unseen at
training. We propose here an efficient mechanism to remedy some of these
limitations at test time. We combine the inter-shape data prior of the network
with an intra-shape regularization prior of a Nystr\&quot;om Kernel Ridge
Regression, that we further adapt by fitting its hyperprameters to the current
shape. The resulting shape function defined in a shape specific Reproducing
Kernel Hilbert Space benefits from desirable stability and efficiency
properties and grants a shape adaptive expressiveness-robustness trade-off. We
demonstrate the improvement obtained through our method with respect to
baselines and the state-of-the-art using synthetic and real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouasfi_A/0/1/0/all/0/1&quot;&gt;Amine Ouasfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1&quot;&gt;Adnane Boukhayma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12981">
<title>SD-NAE: Generating Natural Adversarial Examples with Stable Diffusion. (arXiv:2311.12981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12981</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustly evaluating deep learning image classifiers is challenging due to
some limitations of standard datasets. Natural Adversarial Examples (NAEs),
arising naturally from the environment and capable of deceiving classifiers,
are instrumental in identifying vulnerabilities in trained models. Existing
works collect such NAEs by filtering from a huge set of real images, a process
that is passive and lacks control. In this work, we propose to actively
synthesize NAEs with the state-of-the-art Stable Diffusion. Specifically, our
method formulates a controlled optimization process, where we perturb the token
embedding that corresponds to a specified class to synthesize NAEs. The
generation is guided by the gradient of loss from the target classifier so that
the created image closely mimics the ground-truth class yet fools the
classifier. Named SD-NAE (Stable Diffusion for Natural Adversarial Examples),
our innovative method is effective in producing valid and useful NAEs, which is
demonstrated through a meticulously designed experiment. Our work thereby
provides a valuable method for obtaining challenging evaluation data, which in
turn can potentially advance the development of more robust deep learning
models. Code is available at https://github.com/linyueqian/SD-NAE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yueqian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hai Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12992">
<title>FollowMe: a Robust Person Following Framework Based on Re-Identification and Gestures. (arXiv:2311.12992v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12992</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-robot interaction (HRI) has become a crucial enabler in houses and
industries for facilitating operational flexibility. When it comes to mobile
collaborative robots, this flexibility can be further increased due to the
autonomous mobility and navigation capacity of the robotic agents, expanding
their workspace and consequently, the personalizable assistance they can
provide to the human operators. This however requires that the robot is capable
of detecting and identifying the human counterpart in all stages of the
collaborative task, and in particular while following a human in crowded
workplaces. To respond to this need, we developed a unified perception and
navigation framework, which enables the robot to identify and follow a target
person using a combination of visual Re-Identification (Re-ID), hand gestures
detection, and collision-free navigation. The Re-ID module can autonomously
learn the features of a target person and use the acquired knowledge to
visually re-identify the target. The navigation stack is used to follow the
target avoiding obstacles and other individuals in the environment. Experiments
are conducted with few subjects in a laboratory setting where some unknown
dynamic obstacles are introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rollo_F/0/1/0/all/0/1&quot;&gt;Federico Rollo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zunino_A/0/1/0/all/0/1&quot;&gt;Andrea Zunino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raiola_G/0/1/0/all/0/1&quot;&gt;Gennaro Raiola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amadio_F/0/1/0/all/0/1&quot;&gt;Fabio Amadio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajoudani_A/0/1/0/all/0/1&quot;&gt;Arash Ajoudani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsagarakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Tsagarakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12993">
<title>AI for Agriculture: the Comparison of Semantic Segmentation Methods for Crop Mapping with Sentinel-2 Imagery. (arXiv:2311.12993v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12993</link>
<description rdf:parseType="Literal">&lt;p&gt;Crop mapping is one of the most common tasks in artificial intelligence for
agriculture due to higher food demands from a growing population and increased
awareness of climate change. In case of vineyards, the texture is very
important for crop segmentation: with higher resolution satellite imagery the
texture is easily detected by majority of state-of-the-art algorithms. However,
this task becomes increasingly more difficult as the resolution of satellite
imagery decreases and the information about the texture becomes unavailable. In
this paper we aim to explore the main machine learning methods that can be used
with freely available satellite imagery and discuss how and when they can be
applied for vineyard segmentation problem. We assess the effectiveness of
various widely-used machine learning techniques and offer guidance on selecting
the most suitable model for specific scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korotkova_I/0/1/0/all/0/1&quot;&gt;Irina Korotkova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efremova_N/0/1/0/all/0/1&quot;&gt;Natalia Efremova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13009">
<title>3D Compression Using Neural Fields. (arXiv:2311.13009v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13009</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Fields (NFs) have gained momentum as a tool for compressing various
data modalities - e.g. images and videos. This work leverages previous advances
and proposes a novel NF-based compression algorithm for 3D data. We derive two
versions of our approach - one tailored to watertight shapes based on Signed
Distance Fields (SDFs) and, more generally, one for arbitrary non-watertight
shapes using Unsigned Distance Fields (UDFs). We demonstrate that our method
excels at geometry compression on 3D point clouds as well as meshes. Moreover,
we show that, due to the NF formulation, it is straightforward to extend our
compression algorithm to compress both geometry and attribute (e.g. color) of
3D data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1&quot;&gt;Janis Postels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strumpler_Y/0/1/0/all/0/1&quot;&gt;Yannick Str&amp;#xfc;mpler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichard_K/0/1/0/all/0/1&quot;&gt;Klara Reichard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13016">
<title>Image-Based Soil Organic Carbon Remote Sensing from Satellite Images with Fourier Neural Operator and Structural Similarity. (arXiv:2311.13016v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13016</link>
<description rdf:parseType="Literal">&lt;p&gt;Soil organic carbon (SOC) sequestration is the transfer and storage of
atmospheric carbon dioxide in soils, which plays an important role in climate
change mitigation. SOC concentration can be improved by proper land use, thus
it is beneficial if SOC can be estimated at a regional or global scale. As
multispectral satellite data can provide SOC-related information such as
vegetation and soil properties at a global scale, estimation of SOC through
satellite data has been explored as an alternative to manual soil sampling.
Although existing studies show promising results, they are mainly based on
pixel-based approaches with traditional machine learning methods, and
convolutional neural networks (CNNs) are uncommon. To study the use of CNNs on
SOC remote sensing, here we propose the FNO-DenseNet based on the Fourier
neural operator (FNO). By combining the advantages of the FNO and DenseNet, the
FNO-DenseNet outperformed the FNO in our experiments with hundreds of times
fewer parameters. The FNO-DenseNet also outperformed a pixel-based random
forest by 18% in the mean absolute percentage error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Ken C. L. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klein_L/0/1/0/all/0/1&quot;&gt;Levente Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ademir Ferreira da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jitendra Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1&quot;&gt;Tanveer Syeda-Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13018">
<title>Attention: Large Multimodal Model is Watching your Geo-privacy. (arXiv:2311.13018v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.13018</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic privacy, a crucial aspect of personal security, often goes
unnoticed in daily activities. This paper addresses the underestimation of this
privacy in the context of increasing online data sharing and the advancements
in information gathering technologies. With the surge in the use of Large
Multimodal Models, such as GPT-4, for Open Source Intelligence (OSINT), the
potential risks associated with geographic privacy breaches have intensified.
This study highlights the criticality of these developments, focusing on their
implications for individual privacy. The primary objective is to demonstrate
the capabilities of advanced AI tools, specifically a GPT-4 based model named
&quot;Dr. Watson,&quot; in identifying and potentially compromising geographic privacy
through online shared content. We developed &quot;Dr. Watson&quot; to analyze and extract
geographic information from publicly available data sources. The study involved
five experimental cases, each offering different perspectives on the tool&apos;s
application in extracting precise location data from partial images and social
media content. The experiments revealed that &quot;Dr. Watson&quot; could successfully
identify specific geographic details, thereby exposing the vulnerabilities in
current geo-privacy measures. These findings underscore the ease with which
geographic information can be unintentionally disclosed. The paper concludes
with a discussion on the broader implications of these findings for individuals
and the community at large. It emphasizes the urgency for enhanced awareness
and protective measures against geo-privacy leakage in the era of advanced AI
and widespread social media usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuju Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Junhong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junzhou He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13022">
<title>Unsupervised Multimodal Surface Registration with Geometric Deep Learning. (arXiv:2311.13022v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13022</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces GeoMorph, a novel geometric deep-learning framework
designed for image registration of cortical surfaces. The registration process
consists of two main steps. First, independent feature extraction is performed
on each input surface using graph convolutions, generating low-dimensional
feature representations that capture important cortical surface
characteristics. Subsequently, features are registered in a deep-discrete
manner to optimize the overlap of common structures across surfaces by learning
displacements of a set of control points. To ensure smooth and biologically
plausible deformations, we implement regularization through a deep conditional
random field implemented with a recurrent neural network. Experimental results
demonstrate that GeoMorph surpasses existing deep-learning methods by achieving
improved alignment with smoother deformations. Furthermore, GeoMorph exhibits
competitive performance compared to classical frameworks. Such versatility and
robustness suggest strong potential for various neuroscience applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suliman_M/0/1/0/all/0/1&quot;&gt;Mohamed A. Suliman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1&quot;&gt;Logan Z. J. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fawaz_A/0/1/0/all/0/1&quot;&gt;Abdulah Fawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_E/0/1/0/all/0/1&quot;&gt;Emma C. Robinson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13045">
<title>Camera-Independent Single Image Depth Estimation from Defocus Blur. (arXiv:2311.13045v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13045</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular depth estimation is an important step in many downstream tasks in
machine vision. We address the topic of estimating monocular depth from defocus
blur which can yield more accurate results than the semantic based depth
estimation methods. The existing monocular depth from defocus techniques are
sensitive to the particular camera that the images are taken from. We show how
several camera-related parameters affect the defocus blur using optical physics
equations and how they make the defocus blur depend on these parameters. The
simple correction procedure we propose can alleviate this problem which does
not require any retraining of the original model. We created a synthetic
dataset which can be used to test the camera independent performance of depth
from defocus blur models. We evaluate our model on both synthetic and real
datasets (DDFF12 and NYU depth V2) obtained with different cameras and show
that our methods are significantly more robust to the changes of cameras. Code:
https://github.com/sleekEagle/defocus_camind.git
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijayasingha_L/0/1/0/all/0/1&quot;&gt;Lahiru Wijayasingha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemzadeh_H/0/1/0/all/0/1&quot;&gt;Homa Alemzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stankovic_J/0/1/0/all/0/1&quot;&gt;John A. Stankovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13052">
<title>Novel OCT mosaicking pipeline with Feature- and Pixel-based registration. (arXiv:2311.13052v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13052</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution Optical Coherence Tomography (OCT) images are crucial for
ophthalmology studies but are limited by their relatively narrow field of view
(FoV). Image mosaicking is a technique for aligning multiple overlapping images
to obtain a larger FoV. Current mosaicking pipelines often struggle with
substantial noise and considerable displacement between the input sub-fields.
In this paper, we propose a versatile pipeline for stitching multi-view
OCT/OCTA \textit{en face} projection images. Our method combines the strengths
of learning-based feature matching and robust pixel-based registration to align
multiple images effectively. Furthermore, we advance the application of a
trained foundational model, Segment Anything Model (SAM), to validate
mosaicking results in an unsupervised manner. The efficacy of our pipeline is
validated using an in-house dataset and a large public dataset, where our
method shows superior performance in terms of both accuracy and computational
efficiency. We also made our evaluation tool for image mosaicking and the
corresponding pipeline publicly available at
\url{https://github.com/MedICL-VU/OCT-mosaicking}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yuankai K. Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13069">
<title>FuseNet: Self-Supervised Dual-Path Network for Medical Image Segmentation. (arXiv:2311.13069v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13069</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation, a crucial task in computer vision, often relies on
labor-intensive and costly annotated datasets for training. In response to this
challenge, we introduce FuseNet, a dual-stream framework for self-supervised
semantic segmentation that eliminates the need for manual annotation. FuseNet
leverages the shared semantic dependencies between the original and augmented
images to create a clustering space, effectively assigning pixels to
semantically related clusters, and ultimately generating the segmentation map.
Additionally, FuseNet incorporates a cross-modal fusion technique that extends
the principles of CLIP by replacing textual data with augmented images. This
approach enables the model to learn complex visual representations, enhancing
robustness against variations similar to CLIP&apos;s text invariance. To further
improve edge alignment and spatial consistency between neighboring pixels, we
introduce an edge refinement loss. This loss function considers edge
information to enhance spatial coherence, facilitating the grouping of nearby
pixels with similar visual features. Extensive experiments on skin lesion and
lung segmentation datasets demonstrate the effectiveness of our method.
\href{https://github.com/xmindflow/FuseNet}{Codebase.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazerouni_A/0/1/0/all/0/1&quot;&gt;Amirhossein Kazerouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimijafarbigloo_S/0/1/0/all/0/1&quot;&gt;Sanaz Karimijafarbigloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_Y/0/1/0/all/0/1&quot;&gt;Yury Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13073">
<title>FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline. (arXiv:2311.13073v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13073</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1&quot;&gt;Vladimir Arkhipkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaheen_Z/0/1/0/all/0/1&quot;&gt;Zein Shaheen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1&quot;&gt;Viacheslav Vasilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dakhova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Dakhova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1&quot;&gt;Andrey Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1&quot;&gt;Denis Dimitrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13090">
<title>On the Limitation of Diffusion Models for Synthesizing Training Datasets. (arXiv:2311.13090v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.13090</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic samples from diffusion models are promising for leveraging in
training discriminative models as replications of real training datasets.
However, we found that the synthetic datasets degrade classification
performance over real datasets even when using state-of-the-art diffusion
models. This means that modern diffusion models do not perfectly represent the
data distribution for the purpose of replicating datasets for training
discriminative tasks. This paper investigates the gap between synthetic and
real samples by analyzing the synthetic samples reconstructed from real samples
through the diffusion and reverse process. By varying the time steps starting
the reverse process in the reconstruction, we can control the trade-off between
the information in the original real data and the information added by
diffusion models. Through assessing the reconstructed samples and trained
models, we found that the synthetic data are concentrated in modes of the
training data distribution as the reverse step increases, and thus, they are
difficult to cover the outer edges of the distribution. Our findings imply that
modern diffusion models are insufficient to replicate training data
distribution perfectly, and there is room for the improvement of generative
modeling in the replication of training datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukuda_T/0/1/0/all/0/1&quot;&gt;Takuma Fukuda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13091">
<title>Stable Unlearnable Example: Enhancing the Robustness of Unlearnable Examples via Stable Error-Minimizing Noise. (arXiv:2311.13091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13091</link>
<description rdf:parseType="Literal">&lt;p&gt;The open source of large amounts of image data promotes the development of
deep learning techniques. Along with this comes the privacy risk of these
open-source image datasets being exploited by unauthorized third parties to
train deep learning models for commercial or illegal purposes. To avoid the
abuse of public data, a poisoning-based technique, the unlearnable example, is
proposed to significantly degrade the generalization performance of models by
adding a kind of imperceptible noise to the data. To further enhance its
robustness against adversarial training, existing works leverage iterative
adversarial training on both the defensive noise and the surrogate model.
However, it still remains unknown whether the robustness of unlearnable
examples primarily comes from the effect of enhancement in the surrogate model
or the defensive noise. Observing that simply removing the adversarial noise on
the training process of the defensive noise can improve the performance of
robust unlearnable examples, we identify that solely the surrogate model&apos;s
robustness contributes to the performance. Furthermore, we found a negative
correlation exists between the robustness of defensive noise and the protection
performance, indicating defensive noise&apos;s instability issue. Motivated by this,
to further boost the robust unlearnable example, we introduce stable
error-minimizing noise (SEM), which trains the defensive noise against random
perturbation instead of the time-consuming adversarial perturbation to improve
the stability of defensive noise. Through extensive experiments, we demonstrate
that SEM achieves a new state-of-the-art performance on CIFAR-10, CIFAR-100,
and ImageNet Subset in terms of both effectiveness and efficiency. The code is
available at https://github.com/liuyixin-louis/Stable-Unlearnable-Example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13099">
<title>PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF. (arXiv:2311.13099v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13099</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that physics-based simulations can be seamlessly integrated with NeRF
to generate high-quality elastodynamics of real-world objects. Unlike existing
methods, we discretize nonlinear hyperelasticity in a meshless way, obviating
the necessity for intermediate auxiliary shape proxies like a tetrahedral mesh
or voxel grid. A quadratic generalized moving least square (Q-GMLS) is employed
to capture nonlinear dynamics and large deformation on the implicit model. Such
meshless integration enables versatile simulations of complex and codimensional
shapes. We adaptively place the least-square kernels according to the NeRF
density field to significantly reduce the complexity of the nonlinear
simulation. As a result, physically realistic animations can be conveniently
synthesized using our method for a wide range of hyperelastic materials at an
interactive rate. For more information, please visit our project page at
https://fytalon.github.io/pienerf/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yutao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1&quot;&gt;Yintong Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1&quot;&gt;Tianjia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenfanfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13100">
<title>Automated Measurement of Pericoronary Adipose Tissue Attenuation and Volume in CT Angiography. (arXiv:2311.13100v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13100</link>
<description rdf:parseType="Literal">&lt;p&gt;Pericoronary adipose tissue (PCAT) is the deposition of fat in the vicinity
of the coronary arteries. It is an indicator of coronary inflammation and
associated with coronary artery disease. Non-invasive coronary CT angiography
(CCTA) is presently used to obtain measures of the thickness, volume, and
attenuation of fat deposition. However, prior works solely focus on measuring
PCAT using semi-automated approaches at the right coronary artery (RCA) over
the left coronary artery (LCA). In this pilot work, we developed a fully
automated approach for the measurement of PCAT mean attenuation and volume in
the region around both coronary arteries. First, we used a large subset of
patients from the public ImageCAS dataset (n = 735) to train a 3D full
resolution nnUNet to segment LCA and RCA. Then, we automatically measured PCAT
in the surrounding arterial regions. We evaluated our method on a held-out test
set of patients (n = 183) from the same dataset. A mean Dice score of 83% and
PCAT attenuation of -73.81 $\pm$ 12.69 HU was calculated for the RCA, while a
mean Dice score of 81% and PCAT attenuation of -77.51 $\pm$ 7.94 HU was
computed for the LCA. To the best of our knowledge, we are the first to develop
a fully automated method to measure PCAT attenuation and volume at both the RCA
and LCA. Our work underscores how automated PCAT measurement holds promise as a
biomarker for identification of inflammation and cardiac disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Andrew M. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mathai_T/0/1/0/all/0/1&quot;&gt;Tejas Sudharshan Mathai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Summers_R/0/1/0/all/0/1&quot;&gt;Ronald M. Summers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13110">
<title>White-Box Transformers via Sparse Rate Reduction: Compression Is All There Is?. (arXiv:2311.13110v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13110</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we contend that a natural objective of representation learning
is to compress and transform the distribution of the data, say sets of tokens,
towards a low-dimensional Gaussian mixture supported on incoherent subspaces.
The goodness of such a representation can be evaluated by a principled measure,
called sparse rate reduction, that simultaneously maximizes the intrinsic
information gain and extrinsic sparsity of the learned representation. From
this perspective, popular deep network architectures, including transformers,
can be viewed as realizing iterative schemes to optimize this measure.
Particularly, we derive a transformer block from alternating optimization on
parts of this objective: the multi-head self-attention operator compresses the
representation by implementing an approximate gradient descent step on the
coding rate of the features, and the subsequent multi-layer perceptron
sparsifies the features. This leads to a family of white-box transformer-like
deep network architectures, named CRATE, which are mathematically fully
interpretable. We show, by way of a novel connection between denoising and
compression, that the inverse to the aforementioned compressive encoding can be
realized by the same class of CRATE architectures. Thus, the so-derived
white-box architectures are universal to both encoders and decoders.
Experiments show that these networks, despite their simplicity, indeed learn to
compress and sparsify representations of large-scale real-world image and text
datasets, and achieve performance very close to highly engineered
transformer-based models: ViT, MAE, DINO, BERT, and GPT2. We believe the
proposed computational framework demonstrates great potential in bridging the
gap between theory and practice of deep learning, from a unified perspective of
data compression. Code is available at: https://ma-lab-berkeley.github.io/CRATE .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1&quot;&gt;Sam Buchanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_D/0/1/0/all/0/1&quot;&gt;Druv Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1&quot;&gt;Tianzhe Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Shengbang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1&quot;&gt;Hao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haeffele_B/0/1/0/all/0/1&quot;&gt;Benjamin D. Haeffele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13120">
<title>Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer. (arXiv:2311.13120v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13120</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene text recognition (STR) in the wild frequently encounters challenges
when coping with domain variations, font diversity, shape deformations, etc. A
straightforward solution is performing model fine-tuning tailored to a specific
scenario, but it is computationally intensive and requires multiple model
copies for various scenarios. Recent studies indicate that large language
models (LLMs) can learn from a few demonstration examples in a training-free
manner, termed &quot;In-Context Learning&quot; (ICL). Nevertheless, applying LLMs as a
text recognizer is unacceptably resource-consuming. Moreover, our pilot
experiments on LLMs show that ICL fails in STR, mainly attributed to the
insufficient incorporation of contextual information from diverse samples in
the training stage. To this end, we introduce E$^2$STR, a STR model trained
with context-rich scene text sequences, where the sequences are generated via
our proposed in-context training strategy. E$^2$STR demonstrates that a
regular-sized model is sufficient to achieve effective ICL capabilities in STR.
Extensive experiments show that E$^2$STR exhibits remarkable training-free
adaptation in various scenarios and outperforms even the fine-tuned
state-of-the-art approaches on public benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Can Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Binghong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chunhui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jingqun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13125">
<title>DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation. (arXiv:2311.13125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13125</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an unsupervised 3D shape co-segmentation method which learns a set
of deformable part templates from a shape collection. To accommodate structural
variations in the collection, our network composes each shape by a selected
subset of template parts which are affine-transformed. To maximize the
expressive power of the part templates, we introduce a per-part deformation
network to enable the modeling of diverse parts with substantial geometry
variations, while imposing constraints on the deformation capacity to ensure
fidelity to the originally represented parts. We also propose a training scheme
to effectively overcome local minima. Architecturally, our network is a
branched autoencoder, with a CNN encoder taking a voxel shape as input and
producing per-part transformation matrices, latent codes, and part existence
scores, and the decoder outputting point occupancies to define the
reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder,
can achieve unsupervised 3D shape co-segmentation that yields fine-grained,
compact, and meaningful parts that are consistent across diverse shapes. We
conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an
animal subset of Objaverse to show superior performance over prior methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiqin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13127">
<title>Toward Robust Imperceptible Perturbation against Unauthorized Text-to-image Diffusion-based Synthesis. (arXiv:2311.13127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13127</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models allow seamless generation of personalized
images from scant reference photos. Yet, these tools, in the wrong hands, can
fabricate misleading or harmful content, endangering individuals. To address
this problem, existing poisoning-based approaches perturb user images in an
imperceptible way to render them &quot;unlearnable&quot; from malicious uses. We identify
two limitations of these defending approaches: i) sub-optimal due to the
hand-crafted heuristics for solving the intractable bilevel optimization and
ii) lack of robustness against simple data transformations like Gaussian
filtering. To solve these challenges, we propose MetaCloak, which solves the
bi-level poisoning problem with a meta-learning framework with an additional
transformation sampling process to craft transferable and robust perturbation.
Specifically, we employ a pool of surrogate diffusion models to craft
transferable and model-agnostic perturbation. Furthermore, by incorporating an
additional transformation process, we design a simple denoising-error
maximization loss that is sufficient for causing transformation-robust semantic
distortion and degradation in a personalized generation. Extensive experiments
on the VGGFace2 and CelebA-HQ datasets show that MetaCloak outperforms existing
approaches. Notably, MetaCloak can successfully fool online training services
like Replicate, in a black-box manner, demonstrating the effectiveness of
MetaCloak in real-world scenarios. Our code is available at
https://github.com/liuyixin-louis/MetaCloak.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chenrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yutong Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13128">
<title>P2RBox: A Single Point is All You Need for Oriented Object Detection. (arXiv:2311.13128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13128</link>
<description rdf:parseType="Literal">&lt;p&gt;Oriented object detection, a specialized subfield in computer vision, finds
applications across diverse scenarios, excelling particularly when dealing with
objects of arbitrary orientations. Conversely, point annotation, which treats
objects as single points, offers a cost-effective alternative to rotated and
horizontal bounding boxes but sacrifices performance due to the loss of size
and orientation information. In this study, we introduce the P2RBox network,
which leverages point annotations and a mask generator to create mask
proposals, followed by filtration through our Inspector Module and Constrainer
Module. This process selects high-quality masks, which are subsequently
converted into rotated box annotations for training a fully supervised
detector. Specifically, we&apos;ve thoughtfully crafted an Inspector Module rooted
in multi-instance learning principles to evaluate the semantic score of masks.
We&apos;ve also proposed a more robust mask quality assessment in conjunction with
the Constrainer Module. Furthermore, we&apos;ve introduced a Symmetry Axis
Estimation (SAE) Module inspired by the spectral theorem for symmetric matrices
to transform the top-performing mask proposal into rotated bounding boxes.
P2RBox performs well with three fully supervised rotated object detectors:
RetinaNet, Rotated FCOS, and Oriented R-CNN. By combining with Oriented R-CNN,
P2RBox achieves 62.26% on DOTA-v1.0 test dataset. As far as we know, this is
the first attempt at training an oriented object detector with point
supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guangming Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuehui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenwen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xumeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guorong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhenjun Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13134">
<title>Lightweight High-Speed Photography Built on Coded Exposure and Implicit Neural Representation of Videos. (arXiv:2311.13134v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13134</link>
<description rdf:parseType="Literal">&lt;p&gt;The compact cameras recording high-speed scenes with high resolution are
highly demanded, but the required high bandwidth often leads to bulky, heavy
systems, which limits their applications on low-capacity platforms. Adopting a
coded exposure setup to encode a frame sequence into a blurry snapshot and
retrieve the latent sharp video afterward can serve as a lightweight solution.
However, restoring motion from blur is quite challenging due to the high
ill-posedness of motion blur decomposition, intrinsic ambiguity in motion
direction, and diverse motions in natural videos. In this work, by leveraging
classical coded exposure imaging technique and emerging implicit neural
representation for videos, we tactfully embed the motion direction cues into
the blurry image during the imaging process and develop a novel self-recursive
neural network to sequentially retrieve the latent video sequence from the
blurry image utilizing the embedded motion direction cues. To validate the
effectiveness and efficiency of the proposed framework, we conduct extensive
experiments on benchmark datasets and real-captured blurry images. The results
demonstrate that our proposed framework significantly outperforms existing
methods in quality and flexibility. The code for our work is available at
https://github.com/zhihongz/BDINR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Runzhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1&quot;&gt;Jinli Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1&quot;&gt;Qionghai Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13141">
<title>Diffusion360: Seamless 360 Degree Panoramic Image Generation based on Diffusion Models. (arXiv:2311.13141v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13141</link>
<description rdf:parseType="Literal">&lt;p&gt;This is a technical report on the 360-degree panoramic image generation task
based on diffusion models. Unlike ordinary 2D images, 360-degree panoramic
images capture the entire $360^\circ\times 180^\circ$ field of view. So the
rightmost and the leftmost sides of the 360 panoramic image should be
continued, which is the main challenge in this field. However, the current
diffusion pipeline is not appropriate for generating such a seamless 360-degree
panoramic image. To this end, we propose a circular blending strategy on both
the denoising and VAE decoding stages to maintain the geometry continuity.
Based on this, we present two models for \textbf{Text-to-360-panoramas} and
\textbf{Single-Image-to-360-panoramas} tasks. The code has been released as an
open-source project at
\href{https://github.com/ArcherFMY/SD-T2I-360PanoImage}{https://github.com/ArcherFMY/SD-T2I-360PanoImage}
and
\href{https://www.modelscope.cn/models/damo/cv_diffusion_text-to-360panorama-image_generation/summary}{ModelScope}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1&quot;&gt;Mengyang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Miaomiao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13144">
<title>Single Image Compressed Sensing MRI via a Self-Supervised Deep Denoising Approach. (arXiv:2311.13144v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13144</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular methods in compressed sensing (CS) are dependent on deep learning
(DL), where large amounts of data are used to train non-linear reconstruction
models. However, ensuring generalisability over and access to multiple datasets
is challenging to realise for real-world applications. To address these
concerns, this paper proposes a single image, self-supervised (SS) CS-MRI
framework that enables a joint deep and sparse regularisation of CS artefacts.
The approach effectively dampens structured CS artefacts, which can be
difficult to remove assuming sparse reconstruction, or relying solely on the
inductive biases of CNN to produce noise-free images. Image quality is thereby
improved compared to either approach alone. Metrics are evaluated using
Cartesian 1D masks on a brain and knee dataset, with PSNR improving by 2-4dB on
average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lorenzana_M/0/1/0/all/0/1&quot;&gt;Marlon Bran Lorenzana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1&quot;&gt;Shekhar S. Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13152">
<title>Test-Time Augmentation for 3D Point Cloud Classification and Segmentation. (arXiv:2311.13152v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13152</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is a powerful technique to enhance the performance of a
deep learning task but has received less attention in 3D deep learning. It is
well known that when 3D shapes are sparsely represented with low point density,
the performance of the downstream tasks drops significantly. This work explores
test-time augmentation (TTA) for 3D point clouds. We are inspired by the recent
revolution of learning implicit representation and point cloud upsampling,
which can produce high-quality 3D surface reconstruction and
proximity-to-surface, respectively. Our idea is to leverage the implicit field
reconstruction or point cloud upsampling techniques as a systematic way to
augment point cloud data. Mainly, we test both strategies by sampling points
from the reconstructed results and using the sampled point cloud as test-time
augmented data. We show that both strategies are effective in improving
accuracy. We observed that point cloud upsampling for test-time augmentation
can lead to more significant performance improvement on downstream tasks such
as object classification and segmentation on the ModelNet40, ShapeNet,
ScanObjectNN, and SemanticKITTI datasets, especially for sparse point clouds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Srinjay Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1&quot;&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13168">
<title>3D Face Style Transfer with a Hybrid Solution of NeRF and Mesh Rasterization. (arXiv:2311.13168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13168</link>
<description rdf:parseType="Literal">&lt;p&gt;Style transfer for human face has been widely researched in recent years.
Majority of the existing approaches work in 2D image domain and have 3D
inconsistency issue when applied on different viewpoints of the same face. In
this paper, we tackle the problem of 3D face style transfer which aims at
generating stylized novel views of a 3D human face with multi-view consistency.
We propose to use a neural radiance field (NeRF) to represent 3D human face and
combine it with 2D style transfer to stylize the 3D face. We find that directly
training a NeRF on stylized images from 2D style transfer brings in 3D
inconsistency issue and causes blurriness. On the other hand, training a NeRF
jointly with 2D style transfer objectives shows poor convergence due to the
identity and head pose gap between style image and content image. It also poses
challenge in training time and memory due to the need of volume rendering for
full image to apply style transfer loss functions. We therefore propose a
hybrid framework of NeRF and mesh rasterization to combine the benefits of high
fidelity geometry reconstruction of NeRF and fast rendering speed of mesh. Our
framework consists of three stages: 1. Training a NeRF model on input face
images to learn the 3D geometry; 2. Extracting a mesh from the trained NeRF
model and optimizing it with style transfer objectives via differentiable
rasterization; 3. Training a new color network in NeRF conditioned on a style
embedding to enable arbitrary style transfer to the 3D face. Experiment results
show that our approach generates high quality face style transfer with great 3D
consistency, while also enabling a flexible style control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianwei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_P/0/1/0/all/0/1&quot;&gt;Prateek Singhal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13172">
<title>Learning to Complement with Multiple Humans (LECOMH): Integrating Multi-rater and Noisy-Label Learning into Human-AI Collaboration. (arXiv:2311.13172v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13172</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of learning with noisy labels (LNL), multi-rater learning, and
human-AI collaboration has revolutionised the development of robust
classifiers, enabling them to address the challenges posed by different types
of data imperfections and complex decision processes commonly encountered in
real-world applications. While each of these methodologies has individually
made significant strides in addressing their unique challenges, the development
of techniques that can simultaneously tackle these three problems remains
underexplored. This paper addresses this research gap by integrating
noisy-label learning, multi-rater learning, and human-AI collaboration with new
benchmarks and the innovative Learning to Complement with Multiple Humans
(LECOMH) approach. LECOMH optimises the level of human collaboration during
testing, aiming to optimise classification accuracy while minimising
collaboration costs that vary from 0 to M, where M is the maximum number of
human collaborators. We quantitatively compare LECOMH with leading human-AI
collaboration methods using our proposed benchmarks. LECOMH consistently
outperforms the competition, with accuracy improving as collaboration costs
increase. Notably, LECOMH is the only method enhancing human labeller
performance across all benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wells_K/0/1/0/all/0/1&quot;&gt;Kevin Wells&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13177">
<title>Volumetric Reconstruction Resolves Off-Resonance Artifacts in Static and Dynamic PROPELLER MRI. (arXiv:2311.13177v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2311.13177</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-resonance artifacts in magnetic resonance imaging (MRI) are visual
distortions that occur when the actual resonant frequencies of spins within the
imaging volume differ from the expected frequencies used to encode spatial
information. These discrepancies can be caused by a variety of factors,
including magnetic field inhomogeneities, chemical shifts, or susceptibility
differences within the tissues. Such artifacts can manifest as blurring,
ghosting, or misregistration of the reconstructed image, and they often
compromise its diagnostic quality. We propose to resolve these artifacts by
lifting the 2D MRI reconstruction problem to 3D, introducing an additional
&quot;spectral&quot; dimension to model this off-resonance. Our approach is inspired by
recent progress in modeling radiance fields, and is capable of reconstructing
both static and dynamic MR images as well as separating fat and water, which is
of independent clinical interest. We demonstrate our approach in the context of
PROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced
Reconstruction) MRI acquisitions, which are popular for their robustness to
motion artifacts. Our method operates in a few minutes on a single GPU, and to
our knowledge is the first to correct for chemical shift in gradient echo
PROPELLER MRI reconstruction without additional measurements or pretraining
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Annesha Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fridovich_Keil_S/0/1/0/all/0/1&quot;&gt;Sara Fridovich-Keil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13182">
<title>Differentiable Radio Frequency Ray Tracing for Millimeter-Wave Sensing. (arXiv:2311.13182v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13182</link>
<description rdf:parseType="Literal">&lt;p&gt;Millimeter wave (mmWave) sensing is an emerging technology with applications
in 3D object characterization and environment mapping. However, realizing
precise 3D reconstruction from sparse mmWave signals remains challenging.
Existing methods rely on data-driven learning, constrained by dataset
availability and difficulty in generalization. We propose DiffSBR, a
differentiable framework for mmWave-based 3D reconstruction. DiffSBR
incorporates a differentiable ray tracing engine to simulate radar point clouds
from virtual 3D models. A gradient-based optimizer refines the model parameters
to minimize the discrepancy between simulated and real point clouds.
Experiments using various radar hardware validate DiffSBR&apos;s capability for
fine-grained 3D reconstruction, even for novel objects unseen by the radar
previously. By integrating physics-based simulation with gradient optimization,
DiffSBR transcends the limitations of data-driven approaches and pioneers a new
paradigm for mmWave sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1&quot;&gt;Qiyue Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xinmin Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengxiong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13186">
<title>Applications of Spiking Neural Networks in Visual Place Recognition. (arXiv:2311.13186v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13186</link>
<description rdf:parseType="Literal">&lt;p&gt;In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for
their largely-unrealized potential energy efficiency and low latency
particularly when implemented on neuromorphic hardware. Our paper highlights
three advancements for SNNs in Visual Place Recognition (VPR). First, we
propose Modular SNNs, where each SNN represents a set of non-overlapping
geographically distinct places, enabling scalable networks for large
environments. Secondly, we present Ensembles of Modular SNNs, where multiple
networks represent the same place, significantly enhancing accuracy compared to
single-network models. Our SNNs are compact and small, comprising only 1500
neurons and 474k synapses, which makes them ideally suited for ensembling due
to this small size. Lastly, we investigate the role of sequence matching in
SNN-based VPR, a technique where consecutive images are used to refine place
recognition. We analyze the responsiveness of SNNs to ensembling and sequence
matching compared to other VPR techniques. Our contributions highlight the
viability of SNNs for VPR, offering scalable and robust solutions, paving the
way for their application in various energy-sensitive robotic tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussaini_S/0/1/0/all/0/1&quot;&gt;Somayeh Hussaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tobias Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13187">
<title>NeISF: Neural Incident Stokes Field for Geometry and Material Estimation. (arXiv:2311.13187v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13187</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view inverse rendering is the problem of estimating the scene
parameters such as shapes, materials, or illuminations from a sequence of
images captured under different viewpoints. Many approaches, however, assume
single light bounce and thus fail to recover challenging scenarios like
inter-reflections. On the other hand, simply extending those methods to
consider multi-bounced light requires more assumptions to alleviate the
ambiguity. To address this problem, we propose Neural Incident Stokes Fields
(NeISF), a multi-view inverse rendering framework that reduces ambiguities
using polarization cues. The primary motivation for using polarization cues is
that it is the accumulation of multi-bounced light, providing rich information
about geometry and material. Based on this knowledge, the proposed incident
Stokes field efficiently models the accumulated polarization effect with the
aid of an original physically-based differentiable polarimetric renderer.
Lastly, experimental results show that our method outperforms the existing
works in synthetic and real scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ono_T/0/1/0/all/0/1&quot;&gt;Taishi Ono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uemori_T/0/1/0/all/0/1&quot;&gt;Takeshi Uemori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihara_H/0/1/0/all/0/1&quot;&gt;Hajime Mihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gatto_A/0/1/0/all/0/1&quot;&gt;Alexander Gatto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagahara_H/0/1/0/all/0/1&quot;&gt;Hajime Nagahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moriuchi_Y/0/1/0/all/0/1&quot;&gt;Yuseke Moriuchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13194">
<title>Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs. (arXiv:2311.13194v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13194</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of document understanding, significant advances have been made
in the fine-tuning of Multimodal Large Language Models (MLLMs) with
instruction-following data. Nevertheless, the potential of text-grounding
capability within text-rich scenarios remains underexplored. In this paper, we
present a text-grounding document understanding model, termed TGDoc, which
addresses this deficiency by enhancing MLLMs with the ability to discern the
spatial positioning of text within images. Empirical evidence suggests that
text-grounding improves the model&apos;s interpretation of textual content, thereby
elevating its proficiency in comprehending text-rich images. Specifically, we
compile a dataset containing 99K PowerPoint presentations sourced from the
internet. We formulate instruction tuning tasks including text detection,
recognition, and spotting to facilitate the cohesive alignment between the
visual encoder and large language model. Moreover, we curate a collection of
text-rich images and prompt the text-only GPT-4 to generate 12K high-quality
conversations, featuring textual locations within text-rich scenarios. By
integrating text location data into the instructions, TGDoc is adept at
discerning text locations during the visual question process. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
across multiple text-rich benchmarks, validating the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yonghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Keyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13198">
<title>DoubleAUG: Single-domain Generalized Object Detector in Urban via Color Perturbation and Dual-style Memory. (arXiv:2311.13198v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13198</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection in urban scenarios is crucial for autonomous driving in
intelligent traffic systems. However, unlike conventional object detection
tasks, urban-scene images vary greatly in style. For example, images taken on
sunny days differ significantly from those taken on rainy days. Therefore,
models trained on sunny day images may not generalize well to rainy day images.
In this paper, we aim to solve the single-domain generalizable object detection
task in urban scenarios, meaning that a model trained on images from one
weather condition should be able to perform well on images from any other
weather conditions. To address this challenge, we propose a novel Double
AUGmentation (DoubleAUG) method that includes image- and feature-level
augmentation schemes. In the image-level augmentation, we consider the
variation in color information across different weather conditions and propose
a Color Perturbation (CP) method that randomly exchanges the RGB channels to
generate various images. In the feature-level augmentation, we propose to
utilize a Dual-Style Memory (DSM) to explore the diverse style information on
the entire dataset, further enhancing the model&apos;s generalization capability.
Extensive experiments demonstrate that our proposed method outperforms
state-of-the-art methods. Furthermore, ablation studies confirm the
effectiveness of each module in our proposed method. Moreover, our method is
plug-and-play and can be integrated into existing methods to further improve
model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lei Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Peng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_T/0/1/0/all/0/1&quot;&gt;Tan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13199">
<title>DRIFu: Differentiable Rendering and Implicit Function-based Single-View 3D Reconstruction. (arXiv:2311.13199v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13199</link>
<description rdf:parseType="Literal">&lt;p&gt;The Differentiable Rendering and Implicit Function-based model (DRIFu) draws
its roots from the Pixel-aligned Implicit Function (PIFU), a pioneering 3D
digitization technique initially designed for clothed human bodies. PIFU excels
in capturing nuanced body shape variations within a low-dimensional space and
has been extensively trained on human 3D scans. However, the application of
PIFU to live animals poses significant challenges, primarily due to the
inherent difficulty in obtaining the cooperation of animals for 3D scanning. In
response to this challenge, we introduce the DRIFu model, specifically tailored
for animal digitization. To train DRIFu, we employ a curated set of synthetic
3D animal models, encompassing diverse shapes, sizes, and even accounting for
variations such as baby birds. Our innovative alignment tools play a pivotal
role in mapping these diverse synthetic animal models onto a unified template,
facilitating precise predictions of animal shape and texture. Crucially, our
template alignment strategy establishes a shared shape space, allowing for the
seamless sampling of new animal shapes, posing them realistically, animating
them, and aligning them with real-world data. This groundbreaking approach
revolutionizes our capacity to comprehensively understand and represent avian
forms. For further details and access to the project, the project website can
be found at https://github.com/kuangzijian/drifu-for-animals
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1&quot;&gt;Zijian Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1&quot;&gt;Lihang Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13200">
<title>Self-guided Few-shot Semantic Segmentation for Remote Sensing Imagery Based on Large Vision Models. (arXiv:2311.13200v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13200</link>
<description rdf:parseType="Literal">&lt;p&gt;The Segment Anything Model (SAM) exhibits remarkable versatility and
zero-shot learning abilities, owing largely to its extensive training data
(SA-1B). Recognizing SAM&apos;s dependency on manual guidance given its
category-agnostic nature, we identified unexplored potential within few-shot
semantic segmentation tasks for remote sensing imagery. This research
introduces a structured framework designed for the automation of few-shot
semantic segmentation. It utilizes the SAM model and facilitates a more
efficient generation of semantically discernible segmentation outcomes. Central
to our methodology is a novel automatic prompt learning approach, leveraging
prior guided masks to produce coarse pixel-wise prompts for SAM. Extensive
experiments on the DLRSD datasets underline the superiority of our approach,
outperforming other available few-shot methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiyu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yidan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13209">
<title>Test-time Adaptive Vision-and-Language Navigation. (arXiv:2311.13209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13209</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-and-Language Navigation (VLN) has witnessed significant advancements
in recent years, largely attributed to meticulously curated datasets and
proficiently trained models. Nevertheless, when tested in diverse environments,
the trained models inevitably encounter significant shifts in data
distribution, highlighting that relying solely on pre-trained and fixed
navigation models is insufficient. To enhance models&apos; generalization ability,
test-time adaptation (TTA) demonstrates significant potential in the computer
vision field by leveraging unlabeled test samples for model updates. However,
simply applying existing TTA methods to the VLN task cannot well handle the
adaptability-stability dilemma of VLN models, i.e., frequent updates can result
in drastic changes in model parameters, while occasional updates can make the
models ill-equipped to handle dynamically changing environments. Therefore, we
propose a Fast-Slow Test-Time Adaptation (FSTTA) approach for VLN by performing
decomposition-accumulation analysis for both gradients and parameters in a
unified framework. Specifically, in the fast update phase, gradients generated
during the recent multi-step navigation process are decomposed into components
with varying levels of consistency. Then, these components are adaptively
accumulated to pinpoint a concordant direction for fast model adaptation. In
the slow update phase, historically recorded parameters are gathered, and a
similar decomposition-accumulation analysis is conducted to revert the model to
a stable state. Extensive experiments show that our method obtains impressive
performance gains on four popular benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13222">
<title>Towards Detecting, Recognizing, and Parsing the Address Information from Bangla Signboard: A Deep Learning-based Approach. (arXiv:2311.13222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13222</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieving textual information from natural scene images is an active
research area in the field of computer vision with numerous practical
applications. Detecting text regions and extracting text from signboards is a
challenging problem due to special characteristics like reflecting lights,
uneven illumination, or shadows found in real-life natural scene images. With
the advent of deep learning-based methods, different sophisticated techniques
have been proposed for text detection and text recognition from the natural
scene. Though a significant amount of effort has been devoted to extracting
natural scene text for resourceful languages like English, little has been done
for low-resource languages like Bangla. In this research work, we have proposed
an end-to-end system with deep learning-based models for efficiently detecting,
recognizing, correcting, and parsing address information from Bangla
signboards. We have created manually annotated datasets and synthetic datasets
to train signboard detection, address text detection, address text recognition,
address text correction, and address text parser models. We have conducted a
comparative study among different CTC-based and Encoder-Decoder model
architectures for Bangla address text recognition. Moreover, we have designed a
novel address text correction model using a sequence-to-sequence
transformer-based network to improve the performance of Bangla address text
recognition model by post-correction. Finally, we have developed a Bangla
address text parser using the state-of-the-art transformer-based pre-trained
language model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murad_H/0/1/0/all/0/1&quot;&gt;Hasan Murad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohammed Eunus Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13231">
<title>Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model. (arXiv:2311.13231v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13231</link>
<description rdf:parseType="Literal">&lt;p&gt;Using reinforcement learning with human feedback (RLHF) has shown significant
promise in fine-tuning diffusion models. Previous methods start by training a
reward model that aligns with human preferences, then leverage RL techniques to
fine-tune the underlying models. However, crafting an efficient reward model
demands extensive datasets, optimal architecture, and manual hyperparameter
tuning, making the process both time and cost-intensive. The direct preference
optimization (DPO) method, effective in fine-tuning large language models,
eliminates the necessity for a reward model. However, the extensive GPU memory
requirement of the diffusion model&apos;s denoising process hinders the direct
application of the DPO method. To address this issue, we introduce the Direct
Preference for Denoising Diffusion Policy Optimization (D3PO) method to
directly fine-tune diffusion models. The theoretical analysis demonstrates that
although D3PO omits training a reward model, it effectively functions as the
optimal reward model trained using human feedback data to guide the learning
process. This approach requires no training of a reward model, proving to be
more direct, cost-effective, and minimizing computational overhead. In
experiments, our method uses the relative scale of objectives as a proxy for
human preference, delivering comparable results to methods using ground-truth
rewards. Moreover, D3PO demonstrates the ability to reduce image distortion
rates and generate safer images, overcoming challenges lacking robust reward
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jian Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiafei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chunjiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qimai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Weihan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaolong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13234">
<title>TSegFormer: 3D Tooth Segmentation in Intraoral Scans with Geometry Guided Transformer. (arXiv:2311.13234v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13234</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Intraoral Scanners (IOS) are widely used in digital dentistry to
provide detailed 3D information of dental crowns and the gingiva. Accurate 3D
tooth segmentation in IOSs is critical for various dental applications, while
previous methods are error-prone at complicated boundaries and exhibit
unsatisfactory results across patients. In this paper, we propose TSegFormer
which captures both local and global dependencies among different teeth and the
gingiva in the IOS point clouds with a multi-task 3D transformer architecture.
Moreover, we design a geometry-guided loss based on a novel point curvature to
refine boundaries in an end-to-end manner, avoiding time-consuming
post-processing to reach clinically applicable segmentation. In addition, we
create a dataset with 16,000 IOSs, the largest ever IOS dataset to the best of
our knowledge. The experimental results demonstrate that our TSegFormer
consistently surpasses existing state-of-the-art baselines. The superiority of
TSegFormer is corroborated by extensive analysis, visualizations and real-world
clinical applicability tests. Our code is available at
https://github.com/huiminxiong/TSegFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Huimin Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1&quot;&gt;Haochao Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13250">
<title>Towards Hetero-Client Federated Multi-Task Learning. (arXiv:2311.13250v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13250</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) enables joint training across distributed clients
using their local data privately. Federated Multi-Task Learning (FMTL) builds
on FL to handle multiple tasks, assuming model congruity that identical model
architecture is deployed in each client. To relax this assumption and thus
extend real-world applicability, we introduce a novel problem setting,
Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse
task setups. The main challenge of HC-FMTL is the model incongruity issue that
invalidates conventional aggregation methods. It also escalates the
difficulties in accurate model aggregation to deal with data and task
heterogeneity inherent in FMTL. To address these challenges, we propose the
FedHCA$^2$ framework, which allows for federated training of personalized
models by modeling relationships among heterogeneous clients. Drawing on our
theoretical insights into the difference between multi-task and federated
optimization, we propose the Hyper Conflict-Averse Aggregation scheme to
mitigate conflicts during encoder updates. Additionally, inspired by task
interaction in MTL, the Hyper Cross Attention Aggregation scheme uses
layer-wise cross attention to enhance decoder interactions while alleviating
model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for
each client to customize personalized parameter updates. Extensive experiments
demonstrate the superior performance of FedHCA$^2$ in various HC-FMTL scenarios
compared to representative methods. Our code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Suizhi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuwen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirejiding_S/0/1/0/all/0/1&quot;&gt;Shalayiding Sirejiding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yue Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongtao Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13254">
<title>DA-STC: Domain Adaptive Video Semantic Segmentation via Spatio-Temporal Consistency. (arXiv:2311.13254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13254</link>
<description rdf:parseType="Literal">&lt;p&gt;Video semantic segmentation is a pivotal aspect of video representation
learning. However, significant domain shifts present a challenge in effectively
learning invariant spatio-temporal features across the labeled source domain
and unlabeled target domain for video semantic segmentation. To solve the
challenge, we propose a novel DA-STC method for domain adaptive video semantic
segmentation, which incorporates a bidirectional multi-level spatio-temporal
fusion module and a category-aware spatio-temporal feature alignment module to
facilitate consistent learning for domain-invariant features. Firstly, we
perform bidirectional spatio-temporal fusion at the image sequence level and
shallow feature level, leading to the construction of two fused intermediate
video domains. This prompts the video semantic segmentation model to
consistently learn spatio-temporal features of shared patch sequences which are
influenced by domain-specific contexts, thereby mitigating the feature gap
between the source and target domain. Secondly, we propose a category-aware
feature alignment module to promote the consistency of spatio-temporal
features, facilitating adaptation to the target domain. Specifically, we
adaptively aggregate the domain-specific deep features of each category along
spatio-temporal dimensions, which are further constrained to achieve
cross-domain intra-class feature alignment and inter-class feature separation.
Extensive experiments demonstrate the effectiveness of our method, which
achieves state-of-the-art mIOUs on multiple challenging benchmarks.
Furthermore, we extend the proposed DA-STC to the image domain, where it also
exhibits superior performance for domain adaptive semantic segmentation. The
source code and models will be made available at
\url{https://github.com/ZHE-SAPI/DA-STC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gaochang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_T/0/1/0/all/0/1&quot;&gt;Tianyou Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13258">
<title>ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation. (arXiv:2311.13258v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13258</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art vision-language models (VLMs) still have limited performance
in structural knowledge extraction, such as relations between objects. In this
work, we present ViStruct, a training framework to learn VLMs for effective
visual structural knowledge extraction. Two novel designs are incorporated.
First, we propose to leverage the inherent structure of programming language to
depict visual structural information. This approach enables explicit and
consistent representation of visual structural information of multiple
granularities, such as concepts, relations, and events, in a well-organized
structured format. Second, we introduce curriculum-based learning for VLMs to
progressively comprehend visual structures, from fundamental visual concepts to
intricate event structures. Our intuition is that lower-level knowledge may
contribute to complex visual structure understanding. Furthermore, we compile
and release a collection of datasets tailored for visual structural knowledge
extraction. We adopt a weakly-supervised approach to directly generate visual
event structures from captions for ViStruct training, capitalizing on abundant
image-caption pairs from the web. In experiments, we evaluate ViStruct on
visual structure prediction tasks, demonstrating its effectiveness in improving
the understanding of visual structures. The code is public at
\url{https://github.com/Yangyi-Chen/vi-struct}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Manling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoiem_D/0/1/0/all/0/1&quot;&gt;Derek Hoiem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13261">
<title>Immunohistochemistry guided segmentation of benign epithelial cells, in situ lesions, and invasive epithelial cells in breast cancer slides. (arXiv:2311.13261v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13261</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital pathology enables automatic analysis of histopathological sections
using artificial intelligence (AI). Automatic evaluation could improve
diagnostic efficiency and help find associations between morphological features
and clinical outcome. For development of such prediction models, identifying
invasive epithelial cells, and separating these from benign epithelial cells
and in situ lesions would be the first step. In this study, we aimed to develop
an AI model for segmentation of epithelial cells in sections from breast
cancer. We generated epithelial ground truth masks by restaining hematoxylin
and eosin (HE) sections with cytokeratin (CK) AE1/AE3, and by pathologists&apos;
annotations. HE/CK image pairs were used to train a convolutional neural
network, and data augmentation was used to make the model more robust. Tissue
microarrays (TMAs) from 839 patients, and whole slide images from two patients
were used for training and evaluation of the models. The sections were derived
from four cohorts of breast cancer patients. TMAs from 21 patients from a fifth
cohort was used as a second test set. In quantitative evaluation, a mean Dice
score of 0.70, 0.79, and 0.75 for invasive epithelial cells, benign epithelial
cells, and in situ lesions, respectively, were achieved. In qualitative scoring
(0-5) by pathologists, results were best for all epithelium and invasive
epithelium, with scores of 4.7 and 4.4. Scores for benign epithelium and in
situ lesions were 3.7 and 2.0. The proposed model segmented epithelial cells in
HE stained breast cancer slides well, but further work is needed for accurate
division between the classes. Immunohistochemistry, together with pathologists&apos;
annotations, enabled the creation of accurate ground truths. The model is made
freely available in FastPathology and the code is available at
https://github.com/AICAN-Research/breast-epithelium-segmentation
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoibo_M/0/1/0/all/0/1&quot;&gt;Maren H&amp;#xf8;ib&amp;#xf8;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pedersen_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dale_V/0/1/0/all/0/1&quot;&gt;Vibeke Grotnes Dale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berget_S/0/1/0/all/0/1&quot;&gt;Sissel Marie Berget&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ytterhus_B/0/1/0/all/0/1&quot;&gt;Borgny Ytterhus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lindskog_C/0/1/0/all/0/1&quot;&gt;Cecilia Lindskog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wik_E/0/1/0/all/0/1&quot;&gt;Elisabeth Wik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akslen_L/0/1/0/all/0/1&quot;&gt;Lars A. Akslen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reinertsen_I/0/1/0/all/0/1&quot;&gt;Ingerid Reinertsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Smistad_E/0/1/0/all/0/1&quot;&gt;Erik Smistad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Valla_M/0/1/0/all/0/1&quot;&gt;Marit Valla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13263">
<title>CMFDFormer: Transformer-based Copy-Move Forgery Detection with Continual Learning. (arXiv:2311.13263v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13263</link>
<description rdf:parseType="Literal">&lt;p&gt;Copy-move forgery detection aims at detecting duplicated regions in a
suspected forged image, and deep learning based copy-move forgery detection
methods are in the ascendant. These deep learning based methods heavily rely on
synthetic training data, and the performance will degrade when facing new
tasks. In this paper, we propose a Transformer-style copy-move forgery
detection network named as CMFDFormer, and provide a novel PCSD (Pooled Cube
and Strip Distillation) continual learning framework to help CMFDFormer handle
new tasks. CMFDFormer consists of a MiT (Mix Transformer) backbone network and
a PHD (Pluggable Hybrid Decoder) mask prediction network. The MiT backbone
network is a Transformer-style network which is adopted on the basis of
comprehensive analyses with CNN-style and MLP-style backbones. The PHD network
is constructed based on self-correlation computation, hierarchical feature
integration, a multi-scale cycle fully-connected block and a mask
reconstruction block. The PHD network is applicable to feature extractors of
different styles for hierarchical multi-scale information extraction, achieving
comparable performance. Last but not least, we propose a PCSD continual
learning framework to improve the forgery detectability and avoid catastrophic
forgetting when handling new tasks. Our continual learning framework restricts
intermediate features from the PHD network, and takes advantage of both cube
pooling and strip pooling. Extensive experiments on publicly available datasets
demonstrate the good performance of CMFDFormer and the effectiveness of the
PCSD continual learning framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Song Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1&quot;&gt;Qingxiao Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wenqian Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Nenghai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13267">
<title>FedFN: Feature Normalization for Alleviating Data Heterogeneity Problem in Federated Learning. (arXiv:2311.13267v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13267</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a collaborative method for training models while
preserving data privacy in decentralized settings. However, FL encounters
challenges related to data heterogeneity, which can result in performance
degradation. In our study, we observe that as data heterogeneity increases,
feature representation in the FedAVG model deteriorates more significantly
compared to classifier weight. Additionally, we observe that as data
heterogeneity increases, the gap between higher feature norms for observed
classes, obtained from local models, and feature norms of unobserved classes
widens, in contrast to the behavior of classifier weight norms. This widening
gap extends to encompass the feature norm disparities between local and the
global models. To address these issues, we introduce Federated Averaging with
Feature Normalization Update (FedFN), a straightforward learning method. We
demonstrate the superior performance of FedFN through extensive experiments,
even when applied to pretrained ResNet18. Subsequently, we confirm the
applicability of FedFN to foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seongyoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gihun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jaehoon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Se-Young Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13297">
<title>Retargeting Visual Data with Deformation Fields. (arXiv:2311.13297v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13297</link>
<description rdf:parseType="Literal">&lt;p&gt;Seam carving is an image editing method that enable content-aware resizing,
including operations like removing objects. However, the seam-finding strategy
based on dynamic programming or graph-cut limits its applications to broader
visual data formats and degrees of freedom for editing. Our observation is that
describing the editing and retargeting of images more generally by a
displacement field yields a generalisation of content-aware deformations. We
propose to learn a deformation with a neural network that keeps the output
plausible while trying to deform it only in places with low information
content. This technique applies to different kinds of visual data, including
images, 3D scenes given as neural radiance fields, or even polygon meshes.
Experiments conducted on different visual data show that our method achieves
better content-aware retargeting compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elsner_T/0/1/0/all/0/1&quot;&gt;Tim Elsner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_J/0/1/0/all/0/1&quot;&gt;Julia Berger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czech_V/0/1/0/all/0/1&quot;&gt;Victor Czech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobbelt_L/0/1/0/all/0/1&quot;&gt;Leif Kobbelt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13307">
<title>Rethinking Radiology Report Generation via Causal Reasoning and Counterfactual Augmentation. (arXiv:2311.13307v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13307</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology Report Generation (RRG) draws attention as an interaction between
vision and language fields. Previous works inherited the ideology of
vision-to-language generation tasks,aiming to generate paragraphs with high
consistency as reports. However, one unique characteristic of RRG, the
independence between diseases, was neglected, leading to the injection of the
spurious confounder, i.e., the disease co-occurrence. Unfortunately, this
confounder confuses the process of report generation worse because of the
biased RRG data distribution. In this paper, to rethink this issue thoroughly,
we reason about its causes and effects from a novel perspective of statistics
and causality, where the Joint Vision Coupling and the Conditional Sentence
Coherence Coupling are two aspects prone to implicitly decrease the accuracy of
reports. Then, a counterfactual augmentation strategy that contains the
Counterfactual Sample Synthesis and the Counterfactual Report Reconstruction
sub-methods is proposed to break these two aspects of spurious effects.
Experimental results and further analyses on two widely used datasets justify
our reasoning and proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiafan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenbin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13317">
<title>Recognition-Guided Diffusion Model for Scene Text Image Super-Resolution. (arXiv:2311.13317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13317</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Text Image Super-Resolution (STISR) aims to enhance the resolution and
legibility of text within low-resolution (LR) images, consequently elevating
recognition accuracy in Scene Text Recognition (STR). Previous methods
predominantly employ discriminative Convolutional Neural Networks (CNNs)
augmented with diverse forms of text guidance to address this issue.
Nevertheless, they remain deficient when confronted with severely blurred
images, due to their insufficient generation capability when little structural
or semantic information can be extracted from original images. Therefore, we
introduce RGDiffSR, a Recognition-Guided Diffusion model for scene text image
Super-Resolution, which exhibits great generative diversity and fidelity even
in challenging scenarios. Moreover, we propose a Recognition-Guided Denoising
Network, to guide the diffusion model generating LR-consistent results through
succinct semantic guidance. Experiments on the TextZoom dataset demonstrate the
superiority of RGDiffSR over prior state-of-the-art methods in both text
recognition accuracy and image fidelity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Liangcai Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1&quot;&gt;Baole Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13319">
<title>Deep Learning for Vascular Segmentation and Applications in Phase Contrast Tomography Imaging. (arXiv:2311.13319v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13319</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated blood vessel segmentation is vital for biomedical imaging, as
vessel changes indicate many pathologies. Still, precise segmentation is
difficult due to the complexity of vascular structures, anatomical variations
across patients, the scarcity of annotated public datasets, and the quality of
images. We present a thorough literature review, highlighting the state of
machine learning techniques across diverse organs. Our goal is to provide a
foundation on the topic and identify a robust baseline model for application to
vascular segmentation in a new imaging modality, Hierarchical Phase Contrast
Tomography (HiP CT). Introduced in 2020 at the European Synchrotron Radiation
Facility, HiP CT enables 3D imaging of complete organs at an unprecedented
resolution of ca. 20mm per voxel, with the capability for localized zooms in
selected regions down to 1mm per voxel without sectioning. We have created a
training dataset with double annotator validated vascular data from three
kidneys imaged with HiP CT in the context of the Human Organ Atlas Project.
Finally, utilising the nnU Net model, we conduct experiments to assess the
models performance on both familiar and unseen samples, employing vessel
specific metrics. Our results show that while segmentations yielded reasonably
high scores such as clDice values ranging from 0.82 to 0.88, certain errors
persisted. Large vessels that collapsed due to the lack of hydrostatic pressure
(HiP CT is an ex vivo technique) were segmented poorly. Moreover, decreased
connectivity in finer vessels and higher segmentation errors at vessel
boundaries were observed. Such errors obstruct the understanding of the
structures by interrupting vascular tree connectivity. Through our review and
outputs, we aim to set a benchmark for subsequent model evaluations using
various modalities, especially with the HiP CT imaging database.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yagis_E/0/1/0/all/0/1&quot;&gt;Ekin Yagis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aslani_S/0/1/0/all/0/1&quot;&gt;Shahab Aslani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jain_Y/0/1/0/all/0/1&quot;&gt;Yashvardhan Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahmani_S/0/1/0/all/0/1&quot;&gt;Shahrokh Rahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brunet_J/0/1/0/all/0/1&quot;&gt;Joseph Brunet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bellier_A/0/1/0/all/0/1&quot;&gt;Alexandre Bellier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Werlein_C/0/1/0/all/0/1&quot;&gt;Christopher Werlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ackermann_M/0/1/0/all/0/1&quot;&gt;Maximilian Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jonigk_D/0/1/0/all/0/1&quot;&gt;Danny Jonigk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tafforeau_P/0/1/0/all/0/1&quot;&gt;Paul Tafforeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Peter D Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_C/0/1/0/all/0/1&quot;&gt;Claire Walsh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13321">
<title>Revisiting Supervision for Continual Representation Learning. (arXiv:2311.13321v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13321</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of continual learning, models are designed to learn tasks one
after the other. While most research has centered on supervised continual
learning, recent studies have highlighted the strengths of self-supervised
continual representation learning. The improved transferability of
representations built with self-supervised methods is often associated with the
role played by the multi-layer perceptron projector. In this work, we depart
from this observation and reexamine the role of supervision in continual
representation learning. We reckon that additional information, such as human
annotations, should not deteriorate the quality of representations. Our
findings show that supervised models when enhanced with a multi-layer
perceptron head, can outperform self-supervised models in continual
representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marczak_D/0/1/0/all/0/1&quot;&gt;Daniel Marczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1&quot;&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13335">
<title>Quantum learning and essential cognition under the traction of meta-characteristics in an open world. (arXiv:2311.13335v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.13335</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence has made significant progress in the Close World
problem, being able to accurately recognize old knowledge through training and
classification. However, AI faces significant challenges in the Open World
problem, as it involves a new and unknown exploration journey. AI is not
inherently proactive in exploration, and its challenge lies in not knowing how
to approach and adapt to the unknown world. How do humans acquire knowledge of
the unknown world. Humans identify new knowledge through intrinsic cognition.
In the process of recognizing new colors, the cognitive cues are different from
known color features and involve hue, saturation, brightness, and other
characteristics. When AI encounters objects with different features in the new
world, it faces another challenge: where are the distinguishing features
between influential features of new and old objects? AI often mistakes a new
world&apos;s brown bear for a known dog because it has not learned the differences
in feature distributions between knowledge systems. This is because things in
the new and old worlds have different units and dimensions for their features.
This paper proposes an open-world model and elemental feature system that
focuses on fundamentally recognizing the distribution differences in objective
features between the new and old worlds. The quantum tunneling effect of
learning ability in the new and old worlds is realized through the tractive
force of meta-characteristic. The outstanding performance of the model system
in learning new knowledge (using pedestrian re-identification datasets as an
example) demonstrates that AI has acquired the ability to recognize the new
world with an accuracy of $96.71\%$ at most and has gained the capability to
explore new knowledge, similar to humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Changlin Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13338">
<title>High-Quality Face Caricature via Style Translation. (arXiv:2311.13338v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13338</link>
<description rdf:parseType="Literal">&lt;p&gt;Caricature is an exaggerated form of artistic portraiture that accentuates
unique yet subtle characteristics of human faces. Recently, advancements in
deep end-to-end techniques have yielded encouraging outcomes in capturing both
style and elevated exaggerations in creating face caricatures. Most of these
approaches tend to produce cartoon-like results that could be more practical
for real-world applications. In this study, we proposed a high-quality,
unpaired face caricature method that is appropriate for use in the real world
and uses computer vision techniques and GAN models. We attain the exaggeration
of facial features and the stylization of appearance through a two-step
process: Face caricature generation and face caricature projection. The face
caricature generation step creates new caricature face datasets from real
images and trains a generative model using the real and newly created
caricature datasets. The Face caricature projection employs an encoder trained
with real and caricature faces with the pretrained generator to project real
and caricature faces. We perform an incremental facial exaggeration from the
real image to the caricature faces using the encoder and generator&apos;s latent
space. Our projection preserves the facial identity, attributes, and
expressions from the input image. Also, it accounts for facial occlusions, such
as reading glasses or sunglasses, to enhance the robustness of our model.
Furthermore, we conducted a comprehensive comparison of our approach with
various state-of-the-art face caricature methods, highlighting our process&apos;s
distinctiveness and exceptional realism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laishram_L/0/1/0/all/0/1&quot;&gt;Lamyanba Laishram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaheryar_M/0/1/0/all/0/1&quot;&gt;Muhammad Shaheryar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jong Taek Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soon Ki Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13355">
<title>Unified Classification and Rejection: A One-versus-All Framework. (arXiv:2311.13355v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13355</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifying patterns of known classes and rejecting ambiguous and novel (also
called as out-of-distribution (OOD)) inputs are involved in open world pattern
recognition. Deep neural network models usually excel in closed-set
classification while performing poorly in rejecting OOD. To tackle this
problem, numerous methods have been designed to perform open set recognition
(OSR) or OOD rejection/detection tasks. Previous methods mostly take
post-training score transformation or hybrid models to ensure low scores on OOD
inputs while separating known classes. In this paper, we attempt to build a
unified framework for building open set classifiers for both classification and
OOD rejection. We formulate the open set recognition of $ K $-known-class as a
$ (K + 1) $-class classification problem with model trained on known-class
samples only. By decomposing the $ K $-class problem into $ K $ one-versus-all
(OVA) binary classification tasks and binding some parameters, we show that
combining the scores of OVA classifiers can give $ (K + 1) $-class posterior
probabilities, which enables classification and OOD rejection in a unified
framework. To maintain the closed-set classification accuracy of the OVA
trained classifier, we propose a hybrid training strategy combining OVA loss
and multi-class cross-entropy loss. We implement the OVA framework and hybrid
training strategy on the recently proposed convolutional prototype network.
Experiments on popular OSR and OOD detection datasets demonstrate that the
proposed framework, using a single multi-class classifier, yields competitive
performance in closed-set classification, OOD detection, and misclassification
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu-Yao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng-Lin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13372">
<title>MRGazer: Decoding Eye Gaze Points from Functional Magnetic Resonance Imaging in Individual Space. (arXiv:2311.13372v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13372</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye-tracking research has proven valuable in understanding numerous cognitive
functions. Recently, Frey et al. provided an exciting deep learning method for
learning eye movements from fMRI data. However, it needed to co-register fMRI
into standard space to obtain eyeballs masks, and thus required additional
templates and was time consuming. To resolve this issue, in this paper, we
propose a framework named MRGazer for predicting eye gaze points from fMRI in
individual space. The MRGazer consisted of eyeballs extraction module and a
residual network-based eye gaze prediction. Compared to the previous method,
the proposed framework skips the fMRI co-registration step, simplifies the
processing protocol and achieves end-to-end eye gaze regression. The proposed
method achieved superior performance in a variety of eye movement tasks than
the co-registration-based method, and delivered objective results within a
shorter time (~ 0.02 Seconds for each volume) than prior method (~0.3 Seconds
for each volume).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiuwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rongjie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bensheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13378">
<title>Point Projection Mapping System for Tracking, Registering, Labeling and Validating Optical Tissue Measurements. (arXiv:2311.13378v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13378</link>
<description rdf:parseType="Literal">&lt;p&gt;Validation of newly developed optical tissue sensing techniques for tumor
detection during cancer surgery requires an accurate correlation with
histological results. Additionally, such accurate correlation facilitates
precise data labeling for developing high-performance machine-learning tissue
classification models. In this paper, a newly developed Point Projection
Mapping system will be introduced, which allows non-destructive tracking of the
measurement locations on tissue specimens. Additionally, a framework for
accurate registration, validation, and labeling with histopathology results is
proposed and validated on a case study. The proposed framework provides a more
robust and accurate method for tracking and validation of optical tissue
sensing techniques, which saves time and resources compared to conventional
techniques available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feenstra_L/0/1/0/all/0/1&quot;&gt;Lianne Feenstra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stel_S/0/1/0/all/0/1&quot;&gt;Stefan D.van der Stel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guimaraes_M/0/1/0/all/0/1&quot;&gt;Marcos Da Silva Guimaraes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruers_T/0/1/0/all/0/1&quot;&gt;Theo J.M Ruers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dashtbozorg_B/0/1/0/all/0/1&quot;&gt;Behdad Dashtbozorg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13384">
<title>LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes. (arXiv:2311.13384v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13384</link>
<description rdf:parseType="Literal">&lt;p&gt;With the widespread usage of VR devices and contents, demands for 3D scene
generation techniques become more popular. Existing 3D scene generation models,
however, limit the target scene to specific domain, primarily due to their
training strategies using 3D scan dataset that is far from the real-world. To
address such limitation, we propose LucidDreamer, a domain-free scene
generation pipeline by fully leveraging the power of existing large-scale
diffusion-based generative model. Our LucidDreamer has two alternate steps:
Dreaming and Alignment. First, to generate multi-view consistent images from
inputs, we set the point cloud as a geometrical guideline for each image
generation. Specifically, we project a portion of point cloud to the desired
view and provide the projection as a guidance for inpainting using the
generative model. The inpainted images are lifted to 3D space with estimated
depth maps, composing a new points. Second, to aggregate the new points into
the 3D scene, we propose an aligning algorithm which harmoniously integrates
the portions of newly generated 3D scenes. The finally obtained 3D scene serves
as initial points for optimizing Gaussian splats. LucidDreamer produces
Gaussian splats that are highly-detailed compared to the previous 3D scene
generation methods, with no constraint on domain of the target scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Suyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1&quot;&gt;Hyeongjin Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaerin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13385">
<title>SegVol: Universal and Interactive Volumetric Medical Image Segmentation. (arXiv:2311.13385v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13385</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise image segmentation provides clinical study with meaningful and
well-structured information. Despite the remarkable progress achieved in
medical image segmentation, there is still an absence of foundation
segmentation model that can segment a wide range of anatomical categories with
easy user interaction. In this paper, we propose a universal and interactive
volumetric medical image segmentation model, named SegVol. By training on 90k
unlabeled Computed Tomography (CT) volumes and 6k labeled CTs, this foundation
model supports the segmentation of over 200 anatomical categories using
semantic and spatial prompts. Extensive experiments verify that SegVol
outperforms the state of the art by a large margin on multiple segmentation
benchmarks. Notably, on three challenging lesion datasets, our method achieves
around 20% higher Dice score than nnU-Net. The model and data are publicly
available at: https://github.com/BAAI-DCAI/SegVol.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuxin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1&quot;&gt;Fan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13398">
<title>Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot Images. (arXiv:2311.13398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13398</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jeongtaek Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13404">
<title>Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions. (arXiv:2311.13404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13404</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1&quot;&gt;Keyang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1&quot;&gt;Tianjia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13409">
<title>CompenHR: Efficient Full Compensation for High-resolution Projector. (arXiv:2311.13409v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13409</link>
<description rdf:parseType="Literal">&lt;p&gt;Full projector compensation is a practical task of projector-camera systems.
It aims to find a projector input image, named compensation image, such that
when projected it cancels the geometric and photometric distortions due to the
physical environment and hardware. State-of-the-art methods use deep learning
to address this problem and show promising performance for low-resolution
setups. However, directly applying deep learning to high-resolution setups is
impractical due to the long training time and high memory cost. To address this
issue, this paper proposes a practical full compensation solution. Firstly, we
design an attention-based grid refinement network to improve geometric
correction quality. Secondly, we integrate a novel sampling scheme into an
end-to-end compensation network to alleviate computation and introduce
attention blocks to preserve key features. Finally, we construct a benchmark
dataset for high-resolution projector full compensation. In experiments, our
method demonstrates clear advantages in both efficiency and quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haibin Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bingyao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13435">
<title>PG-Video-LLaVA: Pixel Grounding Large Video-Language Models. (arXiv:2311.13435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13435</link>
<description rdf:parseType="Literal">&lt;p&gt;Extending image-based Large Multimodal Models (LMM) to videos is challenging
due to the inherent complexity of video data. The recent approaches extending
image-based LMM to videos either lack the grounding capabilities (e.g.,
VideoChat, Video-ChatGPT, Video-LLaMA) or do not utilize the audio-signals for
better video understanding (e.g., Video-ChatGPT). Addressing these gaps, we
propose Video-LLaVA, the first LMM with pixel-level grounding capability,
integrating audio cues by transcribing them into text to enrich video-context
understanding. Our framework uses an off-the-shelf tracker and a novel
grounding module, enabling it to spatially and temporally localize objects in
videos following user instructions. We evaluate Video-LLaVA using video-based
generative and question-answering benchmarks and introduce new benchmarks
specifically designed to measure prompt-based object grounding performance in
videos. Further, we propose the use of Vicuna over GPT-3.5, as utilized in
Video-ChatGPT, for video-based conversation benchmarking, ensuring
reproducibility of results which is a concern with the proprietary nature of
GPT-3.5. Our framework builds on SoTA image-based LLaVA model and extends its
advantages to the video domain, delivering promising gains on video-based
conversation and grounding tasks. Project Page:
https://github.com/mbzuai-oryx/Video-LLaVA
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munasinghe_S/0/1/0/all/0/1&quot;&gt;Shehan Munasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thushara_R/0/1/0/all/0/1&quot;&gt;Rusiru Thushara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Maaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1&quot;&gt;Hanoona Abdul Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13443">
<title>Guided Flows for Generative Modeling and Decision Making. (arXiv:2311.13443v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.13443</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifier-free guidance is a key component for improving the performance of
conditional generative models for many downstream tasks. It drastically
improves the quality of samples produced, but has so far only been used for
diffusion models. Flow Matching (FM), an alternative simulation-free approach,
trains Continuous Normalizing Flows (CNFs) based on regressing vector fields.
It remains an open question whether classifier-free guidance can be performed
for Flow Matching models, and to what extent does it improve performance. In
this paper, we explore the usage of Guided Flows for a variety of downstream
applications involving conditional image generation, speech synthesis, and
reinforcement learning. In particular, we are the first to apply flow models to
the offline reinforcement learning setting. We also show that Guided Flows
significantly improves the sample quality in image generation and zero-shot
text-to-speech synthesis, and can make use of drastically low amounts of
computation without affecting the agent&apos;s overall performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Matt Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaul_N/0/1/0/all/0/1&quot;&gt;Neta Shaul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1&quot;&gt;Yaron Lipman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ricky T. Q. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13444">
<title>SkeletonGait: Gait Recognition Using Skeleton Maps. (arXiv:2311.13444v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13444</link>
<description rdf:parseType="Literal">&lt;p&gt;The choice of the representations is essential for deep gait recognition
methods. The binary silhouettes and skeletal coordinates are two dominant
representations in recent literature, achieving remarkable advances in many
scenarios. However, inherent challenges remain, in which silhouettes are not
always guaranteed in unconstrained scenes, and structural cues have not been
fully utilized from skeletons. In this paper, we introduce a novel skeletal
gait representation named Skeleton Map, together with SkeletonGait, a
skeleton-based method to exploit structural information from human skeleton
maps. Specifically, the skeleton map represents the coordinates of human joints
as a heatmap with Gaussian approximation, exhibiting a silhouette-like image
devoid of exact body structure. Beyond achieving state-of-the-art performances
over five popular gait datasets, more importantly, SkeletonGait uncovers novel
insights about how important structural features are in describing gait and
when do they play a role. Furthermore, we propose a multi-branch architecture,
named SkeletonGait++, to make use of complementary features from both skeletons
and silhouettes. Experiments indicate that SkeletonGait++ outperforms existing
state-of-the-art methods by a significant margin in various scenarios. For
instance, it achieves an impressive rank-1 accuracy of over $85\%$ on the
challenging GREW dataset. All the source code will be available at
https://github.com/ShiqiYu/OpenGait.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jingzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Dongyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuanfu Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shiqi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13485">
<title>Deep-learning-based acceleration of MRI for radiotherapy planning of pediatric patients with brain tumors. (arXiv:2311.13485v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.13485</link>
<description rdf:parseType="Literal">&lt;p&gt;Magnetic Resonance Imaging (MRI) is a non-invasive diagnostic and
radiotherapy (RT) planning tool, offering detailed insights into the anatomy of
the human body. The extensive scan time is stressful for patients, who must
remain motionless in a prolonged imaging procedure that prioritizes reduction
of imaging artifacts. This is challenging for pediatric patients who may
require measures for managing voluntary motions such as anesthesia. Several
computational approaches reduce scan time (fast MRI), by recording fewer
measurements and digitally recovering full information via post-acquisition
reconstruction. However, most fast MRI approaches were developed for diagnostic
imaging, without addressing reconstruction challenges specific to RT planning.
In this work, we developed a deep learning-based method (DeepMRIRec) for MRI
reconstruction from undersampled data acquired with RT-specific receiver coil
arrangements. We evaluated our method against fully sampled data of T1-weighted
MR images acquired from 73 children with brain tumors/surgical beds using loop
and posterior coils (12 channels), with and without applying virtual
compression of coil elements. DeepMRIRec reduced scanning time by a factor of
four producing a structural similarity score surpassing the evaluated
state-of-the-art method (0.960 vs 0.896), thereby demonstrating its potential
for accelerating MRI scanning for RT planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Shahinur Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uh_J/0/1/0/all/0/1&quot;&gt;Jinsoo Uh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dresner_A/0/1/0/all/0/1&quot;&gt;Alexander Dresner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hua_C/0/1/0/all/0/1&quot;&gt;Chia-ho Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khairy_K/0/1/0/all/0/1&quot;&gt;Khaled Khairy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13512">
<title>Hybrid Whale-Mud-Ring Optimization for Precise Color Skin Cancer Image Segmentation. (arXiv:2311.13512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13512</link>
<description rdf:parseType="Literal">&lt;p&gt;Timely identification and treatment of rapidly progressing skin cancers can
significantly contribute to the preservation of patients&apos; health and
well-being. Dermoscopy, a dependable and accessible tool, plays a pivotal role
in the initial stages of skin cancer detection. Consequently, the effective
processing of digital dermoscopy images holds significant importance in
elevating the accuracy of skin cancer diagnoses. Multilevel thresholding is a
key tool in medical imaging that extracts objects within the image to
facilitate its analysis. In this paper, an enhanced version of the Mud Ring
Algorithm hybridized with the Whale Optimization Algorithm, named WMRA, is
proposed. The proposed approach utilizes bubble-net attack and mud ring
strategy to overcome stagnation in local optima and obtain optimal thresholds.
The experimental results show that WMRA is powerful against a cluster of recent
methods in terms of fitness, Peak Signal to Noise Ratio (PSNR), and Mean Square
Error (MSE).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1&quot;&gt;Amir Hamza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lekouaghet_B/0/1/0/all/0/1&quot;&gt;Badis Lekouaghet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13531">
<title>Leveraging CNNs and Ensemble Learning for Automated Disaster Image Classification. (arXiv:2311.13531v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13531</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural disasters act as a serious threat globally, requiring effective and
efficient disaster management and recovery. This paper focuses on classifying
natural disaster images using Convolutional Neural Networks (CNNs). Multiple
CNN architectures were built and trained on a dataset containing images of
earthquakes, floods, wildfires, and volcanoes. A stacked CNN ensemble approach
proved to be the most effective, achieving 95% accuracy and an F1 score going
up to 0.96 for individual classes. Tuning hyperparameters of individual models
for optimization was critical to maximize the models&apos; performance. The stacking
of CNNs with XGBoost acting as the meta-model utilizes the strengths of the CNN
and ResNet models to improve the overall accuracy of the classification.
Results obtained from the models illustrated the potency of CNN-based models
for automated disaster image classification. This lays the foundation for
expanding these techniques to build robust systems for disaster response,
damage assessment, and recovery management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathod_A/0/1/0/all/0/1&quot;&gt;Archit Rathod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pariawala_V/0/1/0/all/0/1&quot;&gt;Veer Pariawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surana_M/0/1/0/all/0/1&quot;&gt;Mokshit Surana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_K/0/1/0/all/0/1&quot;&gt;Kumkum Saxena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13535">
<title>DiffusionMat: Alpha Matting as Sequential Refinement Learning. (arXiv:2311.13535v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13535</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce DiffusionMat, a novel image matting framework
that employs a diffusion model for the transition from coarse to refined alpha
mattes. Diverging from conventional methods that utilize trimaps merely as
loose guidance for alpha matte prediction, our approach treats image matting as
a sequential refinement learning process. This process begins with the addition
of noise to trimaps and iteratively denoises them using a pre-trained diffusion
model, which incrementally guides the prediction towards a clean alpha matte.
The key innovation of our framework is a correction module that adjusts the
output at each denoising step, ensuring that the final result is consistent
with the input image&apos;s structures. We also introduce the Alpha Reliability
Propagation, a novel technique designed to maximize the utility of available
guidance by selectively enhancing the trimap regions with confident alpha
information, thus simplifying the correction task. To train the correction
module, we devise specialized loss functions that target the accuracy of the
alpha matte&apos;s edges and the consistency of its opaque and transparent regions.
We evaluate our model across several image matting benchmarks, and the results
indicate that DiffusionMat consistently outperforms existing methods. Project
page at~\url{https://cnnlstm.github.io/DiffusionMat
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shengfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kwan-Yee K. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13547">
<title>Medical Image Retrieval Using Pretrained Embeddings. (arXiv:2311.13547v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13547</link>
<description rdf:parseType="Literal">&lt;p&gt;A wide range of imaging techniques and data formats available for medical
images make accurate retrieval from image databases challenging.
&lt;/p&gt;
&lt;p&gt;Efficient retrieval systems are crucial in advancing medical research,
enabling large-scale studies and innovative diagnostic tools. Thus, addressing
the challenges of medical image retrieval is essential for the continued
enhancement of healthcare and research.
&lt;/p&gt;
&lt;p&gt;In this study, we evaluated the feasibility of employing four
state-of-the-art pretrained models for medical image retrieval at modality,
body region, and organ levels and compared the results of two similarity
indexing approaches. Since the employed networks take 2D images, we analyzed
the impacts of weighting and sampling strategies to incorporate 3D information
during retrieval of 3D volumes. We showed that medical image retrieval is
feasible using pretrained networks without any additional training or
fine-tuning steps. Using pretrained embeddings, we achieved a recall of 1 for
various tasks at modality, body region, and organ level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jush_F/0/1/0/all/0/1&quot;&gt;Farnaz Khun Jush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Tuan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogler_S/0/1/0/all/0/1&quot;&gt;Steffen Vogler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenga_M/0/1/0/all/0/1&quot;&gt;Matthias Lenga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13549">
<title>ADriver-I: A General World Model for Autonomous Driving. (arXiv:2311.13549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13549</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, autonomous driving adopts a modular design, which divides the full
stack into perception, prediction, planning and control parts. Though
interpretable, such modular design tends to introduce a substantial amount of
redundancy. Recently, multimodal large language models (MLLM) and diffusion
techniques have demonstrated their superior performance on comprehension and
generation ability. In this paper, we first introduce the concept of
interleaved vision-action pair, which unifies the format of visual features and
control signals. Based on the vision-action pairs, we construct a general world
model based on MLLM and diffusion model for autonomous driving, termed
ADriver-I. It takes the vision-action pairs as inputs and autoregressively
predicts the control signal of the current frame. The generated control signals
together with the historical vision-action pairs are further conditioned to
predict the future frames. With the predicted next frame, ADriver-I performs
further control signal prediction. Such a process can be repeated infinite
times, ADriver-I achieves autonomous driving in the world created by itself.
Extensive experiments are conducted on nuScenes and our large-scale private
datasets. ADriver-I shows impressive performance compared to several
constructed baselines. We hope our ADriver-I can provide some new insights for
future autonomous driving and embodied intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1&quot;&gt;Fan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weixin Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yingfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yucheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuqing Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tiancai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13559">
<title>Transfer Learning-based Real-time Handgun Detection. (arXiv:2311.13559v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13559</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional surveillance systems rely on human attention, limiting their
effectiveness. This study employs convolutional neural networks and transfer
learning to develop a real-time computer vision system for automatic handgun
detection. Comprehensive analysis of online handgun detection methods is
conducted, emphasizing reducing false positives and learning time. Transfer
learning is demonstrated as an effective approach. Despite technical
challenges, the proposed system achieves a precision rate of 84.74%,
demonstrating promising performance comparable to related works, enabling
faster learning and accurate automatic handgun detection for enhanced security.
This research advances security measures by reducing human monitoring
dependence, showcasing the potential of transfer learning-based approaches for
efficient and reliable handgun detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmir_Y/0/1/0/all/0/1&quot;&gt;Youssef Elmir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laouar_S/0/1/0/all/0/1&quot;&gt;Sid Ahmed Laouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamdaoui_L/0/1/0/all/0/1&quot;&gt;Larbi Hamdaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13562">
<title>Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object. (arXiv:2311.13562v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13562</link>
<description rdf:parseType="Literal">&lt;p&gt;Image style transfer occupies an important place in both computer graphics
and computer vision. However, most current methods require reference to
stylized images and cannot individually stylize specific objects. To overcome
this limitation, we propose the &quot;Soulstyler&quot; framework, which allows users to
guide the stylization of specific objects in an image through simple textual
descriptions. We introduce a large language model to parse the text and
identify stylization goals and specific styles. Combined with a CLIP-based
semantic visual embedding encoder, the model understands and matches text and
image content. We also introduce a novel localized text-image block matching
loss that ensures that style transfer is performed only on specified target
objects, while non-target regions remain in their original style. Experimental
results demonstrate that our model is able to accurately perform style transfer
on target objects according to textual descriptions without affecting the style
of background regions. Our code will be available at
https://github.com/yisuanwang/Soulstyler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_P/0/1/0/all/0/1&quot;&gt;Peng Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingbo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1&quot;&gt;Hongwu Lv&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13570">
<title>WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space. (arXiv:2311.13570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13570</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern learning-based approaches to 3D-aware image synthesis achieve high
photorealism and 3D-consistent viewpoint changes for the generated images.
Existing approaches represent instances in a shared canonical space. However,
for in-the-wild datasets a shared canonical system can be difficult to define
or might not even exist. In this work, we instead model instances in view
space, alleviating the need for posed images and learned camera distributions.
We find that in this setting, existing GAN-based methods are prone to
generating flat geometry and struggle with distribution coverage. We hence
propose WildFusion, a new approach to 3D-aware image synthesis based on latent
diffusion models (LDMs). We first train an autoencoder that infers a compressed
latent representation, which additionally captures the images&apos; underlying 3D
structure and enables not only reconstruction but also novel view synthesis. To
learn a faithful 3D representation, we leverage cues from monocular depth
prediction. Then, we train a diffusion model in the 3D-aware latent space,
thereby enabling synthesis of high-quality 3D-consistent image samples,
outperforming recent state-of-the-art GAN-based methods. Importantly, our
3D-aware LDM is trained without any direct supervision from multiview images or
3D geometry and does not require posed images or learned pose or camera
distributions. It directly learns a 3D representation without relying on
canonical camera coordinates. This opens up promising research avenues for
scalable 3D-aware image synthesis and 3D content creation from in-the-wild
image data. See https://katjaschwarz.github.io/wildfusion for videos of our 3D
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1&quot;&gt;Katja Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1&quot;&gt;Karsten Kreis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13574">
<title>XAGen: 3D Expressive Human Avatars Generation. (arXiv:2311.13574v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13574</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in 3D-aware GAN models have enabled the generation of
realistic and controllable human body images. However, existing methods focus
on the control of major body joints, neglecting the manipulation of expressive
attributes, such as facial expressions, jaw poses, hand poses, and so on. In
this work, we present XAGen, the first 3D generative model for human avatars
capable of expressive control over body, face, and hands. To enhance the
fidelity of small-scale regions like face and hands, we devise a multi-scale
and multi-part 3D representation that models fine details. Based on this
representation, we propose a multi-part rendering technique that disentangles
the synthesis of body, face, and hands to ease model training and enhance
geometric quality. Furthermore, we design multi-part discriminators that
evaluate the quality of the generated avatars with respect to their appearance
and fine-grained control capabilities. Experiments show that XAGen surpasses
state-of-the-art methods in terms of realism, diversity, and expressive control
abilities. Code and data will be made available at
https://showlab.github.io/xagen.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhongcong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13596">
<title>T-Rex: Counting by Visual Prompting. (arXiv:2311.13596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13596</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce T-Rex, an interactive object counting model designed to first
detect and then count any objects. We formulate object counting as an open-set
object detection task with the integration of visual prompts. Users can specify
the objects of interest by marking points or boxes on a reference image, and
T-Rex then detects all objects with a similar pattern. Guided by the visual
feedback from T-Rex, users can also interactively refine the counting results
by prompting on missing or falsely-detected objects. T-Rex has achieved
state-of-the-art performance on several class-agnostic counting benchmarks. To
further exploit its potential, we established a new counting benchmark
encompassing diverse scenarios and challenges. Both quantitative and
qualitative results show that T-Rex possesses exceptional zero-shot counting
capabilities. We also present various practical application scenarios for
T-Rex, illustrating its potential in the realm of visual prompting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kent Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13600">
<title>ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs. (arXiv:2311.13600v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13600</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for finetuning generative models for concept-driven personalization
generally achieve strong results for subject-driven or style-driven generation.
Recently, low-rank adaptations (LoRA) have been proposed as a
parameter-efficient way of achieving concept-driven personalization. While
recent work explores the combination of separate LoRAs to achieve joint
generation of learned styles and subjects, existing techniques do not reliably
address the problem; they often compromise either subject fidelity or style
fidelity. We propose ZipLoRA, a method to cheaply and effectively merge
independently trained style and subject LoRAs in order to achieve generation of
any user-provided subject in any user-provided style. Experiments on a wide
range of subject and style combinations show that ZipLoRA can generate
compelling results with meaningful improvements over baselines in subject and
style fidelity while preserving the ability to recontextualize. Project page:
https://ziplora.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_V/0/1/0/all/0/1&quot;&gt;Viraj Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1&quot;&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1&quot;&gt;Forrester Cole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_E/0/1/0/all/0/1&quot;&gt;Erika Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1&quot;&gt;Svetlana Lazebnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13601">
<title>Visual In-Context Prompting. (arXiv:2311.13601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13601</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context prompting in large language models (LLMs) has become a prevalent
approach to improve zero-shot capabilities, but this idea is less explored in
the vision domain. Existing visual prompting methods focus on referring
segmentation to segment the most relevant object, falling short of addressing
many generic vision tasks like open-set segmentation and detection. In this
paper, we introduce a universal visual in-context prompting framework for both
tasks. In particular, we build on top of an encoder-decoder architecture, and
develop a versatile prompt encoder to support a variety of prompts like
strokes, boxes, and points. We further enhance it to take an arbitrary number
of reference image segments as the context. Our extensive explorations show
that the proposed visual in-context prompting elicits extraordinary referring
and generic segmentation capabilities to refer and detect, yielding competitive
performance to close-set in-domain datasets and showing promising results on
many open-set segmentation datasets. By joint training on COCO and SA-1B, our
model achieves $57.7$ PQ on COCO and $23.2$ PQ on ADE20K. Code will be
available at https://github.com/UX-Decoder/DINOv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xueyan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Huaizhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chunyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jianwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13602">
<title>Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation. (arXiv:2311.13602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.13602</link>
<description rdf:parseType="Literal">&lt;p&gt;Content-aware graphic layout generation aims to automatically arrange visual
elements along with a given content, such as an e-commerce product image. In
this paper, we argue that the current layout generation approaches suffer from
the limited training data for the high-dimensional layout structure. We show
that a simple retrieval augmentation can significantly improve the generation
quality. Our model, which is named Retrieval-Augmented Layout Transformer
(RALF), retrieves nearest neighbor layout examples based on an input image and
feeds these results into an autoregressive generator. Our model can apply
retrieval augmentation to various controllable generation tasks and yield
high-quality layouts within a unified architecture. Our extensive experiments
show that RALF successfully generates content-aware layouts in both constrained
and unconstrained settings and significantly outperforms the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horita_D/0/1/0/all/0/1&quot;&gt;Daichi Horita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1&quot;&gt;Naoto Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kikuchi_K/0/1/0/all/0/1&quot;&gt;Kotaro Kikuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1&quot;&gt;Kota Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1&quot;&gt;Kiyoharu Aizawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.02673">
<title>Attention-based Adversarial Appearance Learning of Augmented Pedestrians. (arXiv:2107.02673v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.02673</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data became already an essential component of machine
learning-based perception in the field of autonomous driving. Yet it still
cannot replace real data completely due to the sim2real domain shift. In this
work, we propose a method that leverages the advantages of the augmentation
process and adversarial training to synthesize realistic data for the
pedestrian recognition task. Our approach utilizes an attention mechanism
driven by an adversarial loss to learn domain discrepancies and improve
sim2real adaptation. Our experiments confirm that the proposed adaptation
method is robust to such discrepancies and reveals both visual realism and
semantic consistency. Furthermore, we evaluate our data generation pipeline on
the task of pedestrian recognition and demonstrate that generated data resemble
properties of the real domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strauss_K/0/1/0/all/0/1&quot;&gt;Kevin Strauss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1&quot;&gt;Artem Savkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1&quot;&gt;Federico Tombari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.04819">
<title>Deep Rank-Consistent Pyramid Model for Enhanced Crowd Counting. (arXiv:2201.04819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.04819</link>
<description rdf:parseType="Literal">&lt;p&gt;Most conventional crowd counting methods utilize a fully-supervised learning
framework to establish a mapping between scene images and crowd density maps.
They usually rely on a large quantity of costly and time-intensive pixel-level
annotations for training supervision. One way to mitigate the intensive
labeling effort and improve counting accuracy is to leverage large amounts of
unlabeled images. This is attributed to the inherent self-structural
information and rank consistency within a single image, offering additional
qualitative relation supervision during training. Contrary to earlier methods
that utilized the rank relations at the original image level, we explore such
rank-consistency relation within the latent feature spaces. This approach
enables the incorporation of numerous pyramid partial orders, strengthening the
model representation capability. A notable advantage is that it can also
increase the utilization ratio of unlabeled samples. Specifically, we propose a
Deep Rank-consistEnt pyrAmid Model (DREAM), which makes full use of rank
consistency across coarse-to-fine pyramid features in latent spaces for
enhanced crowd counting with massive unlabeled images. In addition, we have
collected a new unlabeled crowd counting dataset, FUDAN-UCC, comprising 4,000
images for training purposes. Extensive experiments on four benchmark datasets,
namely UCF-QNRF, ShanghaiTech PartA and PartB, and UCF-CC-50, show the
effectiveness of our method compared with previous semi-supervised methods. The
codes are available at https://github.com/bridgeqiqi/DREAM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiaqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yiming Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;James Z. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei-Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14605">
<title>Looking at the posterior: accuracy and uncertainty of neural-network predictions. (arXiv:2211.14605v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14605</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference can quantify uncertainty in the predictions of neural
networks using posterior distributions for model parameters and network output.
By looking at these posterior distributions, one can separate the origin of
uncertainty into aleatoric and epistemic contributions. One goal of uncertainty
quantification is to inform on prediction accuracy. Here we show that
prediction accuracy depends on both epistemic and aleatoric uncertainty in an
intricate fashion that cannot be understood in terms of marginalized
uncertainty distributions alone. How the accuracy relates to epistemic and
aleatoric uncertainties depends not only on the model architecture, but also on
the properties of the dataset. We discuss the significance of these results for
active learning and introduce a novel acquisition function that outperforms
common uncertainty-based methods. To arrive at our results, we approximated the
posteriors using deep ensembles, for fully-connected, convolutional and
attention-based neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1&quot;&gt;H. Linander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balabanov_O/0/1/0/all/0/1&quot;&gt;O. Balabanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;H. Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehlig_B/0/1/0/all/0/1&quot;&gt;B. Mehlig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02931">
<title>Leveraging Different Learning Styles for Improved Knowledge Distillation in Biomedical Imaging. (arXiv:2212.02931v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02931</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning style refers to a type of training mechanism adopted by an
individual to gain new knowledge. As suggested by the VARK model, humans have
different learning preferences, like Visual (V), Auditory (A), Read/Write (R),
and Kinesthetic (K), for acquiring and effectively processing information. Our
work endeavors to leverage this concept of knowledge diversification to improve
the performance of model compression techniques like Knowledge Distillation
(KD) and Mutual Learning (ML). Consequently, we use a single-teacher and
two-student network in a unified framework that not only allows for the
transfer of knowledge from teacher to students (KD) but also encourages
collaborative learning between students (ML). Unlike the conventional approach,
where the teacher shares the same knowledge in the form of predictions or
feature representations with the student network, our proposed approach employs
a more diversified strategy by training one student with predictions and the
other with feature maps from the teacher. We further extend this knowledge
diversification by facilitating the exchange of predictions and feature maps
between the two student networks, enriching their learning experiences. We have
conducted comprehensive experiments with three benchmark datasets for both
classification and segmentation tasks using two different network architecture
combinations. These experimental results demonstrate that knowledge
diversification in a combined KD and ML framework outperforms conventional KD
or ML techniques (with similar network configuration) that only use predictions
with an average improvement of 2%. Furthermore, consistent improvement in
performance across different tasks, with various network architectures, and
over state-of-the-art techniques establishes the robustness and
generalizability of the proposed model
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyaz_U/0/1/0/all/0/1&quot;&gt;Usma Niyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sambyal_A/0/1/0/all/0/1&quot;&gt;Abhishek Singh Sambyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bathula_D/0/1/0/all/0/1&quot;&gt;Deepti R. Bathula&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01959">
<title>Randomized Adversarial Style Perturbations for Domain Generalization. (arXiv:2304.01959v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01959</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel domain generalization technique, referred to as Randomized
Adversarial Style Perturbation (RASP), which is motivated by the observation
that the characteristics of each domain are captured by the feature statistics
corresponding to style. The proposed algorithm perturbs the style of a feature
in an adversarial direction towards a randomly selected class, and makes the
model learn against being misled by the unexpected styles observed in unseen
target domains. While RASP is effective to handle domain shifts, its naive
integration into the training procedure might degrade the capability of
learning knowledge from source domains because it has no restriction on the
perturbations of representations. This challenge is alleviated by Normalized
Feature Mixup (NFM), which facilitates the learning of the original features
while achieving robustness to perturbed representations via their mixup during
training. We evaluate the proposed algorithm via extensive experiments on
various benchmarks and show that our approach improves domain generalization
performance, especially in large-scale benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07647">
<title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision. (arXiv:2304.07647v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07647</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiani Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_M/0/1/0/all/0/1&quot;&gt;Mayur Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16213">
<title>ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation. (arXiv:2305.16213v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16213</link>
<description rdf:parseType="Literal">&lt;p&gt;Score distillation sampling (SDS) has shown great promise in text-to-3D
generation by distilling pretrained large-scale text-to-image diffusion models,
but suffers from over-saturation, over-smoothing, and low-diversity problems.
In this work, we propose to model the 3D parameter as a random variable instead
of a constant as in SDS and present variational score distillation (VSD), a
principled particle-based variational framework to explain and address the
aforementioned issues in text-to-3D generation. We show that SDS is a special
case of VSD and leads to poor samples with both small and large CFG weights. In
comparison, VSD works well with various CFG weights as ancestral sampling from
diffusion models and simultaneously improves the diversity and sample quality
with a common CFG weight (i.e., $7.5$). We further present various improvements
in the design space for text-to-3D such as distillation time schedule and
density initialization, which are orthogonal to the distillation algorithm yet
not well explored. Our overall approach, dubbed ProlificDreamer, can generate
high rendering resolution (i.e., $512\times512$) and high-fidelity NeRF with
rich structure and complex effects (e.g., smoke and drops). Further,
initialized from NeRF, meshes fine-tuned by VSD are meticulously detailed and
photo-realistic. Project page and codes:
https://ml.cs.tsinghua.edu.cn/prolificdreamer/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1&quot;&gt;Fan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04225">
<title>Efficient Vision Transformer for Human Pose Estimation via Patch Selection. (arXiv:2306.04225v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04225</link>
<description rdf:parseType="Literal">&lt;p&gt;While Convolutional Neural Networks (CNNs) have been widely successful in 2D
human pose estimation, Vision Transformers (ViTs) have emerged as a promising
alternative to CNNs, boosting state-of-the-art performance. However, the
quadratic computational complexity of ViTs has limited their applicability for
processing high-resolution images. In this paper, we propose three methods for
reducing ViT&apos;s computational complexity, which are based on selecting and
processing a small number of most informative patches while disregarding
others. The first two methods leverage a lightweight pose estimation network to
guide the patch selection process, while the third method utilizes a set of
learnable joint tokens to ensure that the selected patches contain the most
important information about body joints. Experiments across six benchmarks show
that our proposed methods achieve a significant reduction in computational
complexity, ranging from 30% to 44%, with only a minimal drop in accuracy
between 0% and 3.5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kinfu_K/0/1/0/all/0/1&quot;&gt;Kaleab A. Kinfu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1&quot;&gt;Rene Vidal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04889">
<title>ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering. (arXiv:2306.04889v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04889</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ShaDDR, an example-based deep generative neural network which
produces a high-resolution textured 3D shape through geometry detailization and
conditional texture generation applied to an input coarse voxel shape. Trained
on a small set of detailed and textured exemplar shapes, our method learns to
detailize the geometry via multi-resolution voxel upsampling and generate
textures on voxel surfaces via differentiable rendering against exemplar
texture images from a few views. The generation is interactive, taking less
than 1 second to produce a 3D model with voxel resolutions up to 512^3. The
generated shape preserves the overall structure of the input coarse voxel
model, while the style of the generated geometric details and textures can be
manipulated through learned latent codes. In the experiments, we show that our
method can generate higher-resolution shapes with plausible and improved
geometric details and clean textures compared to prior works. Furthermore, we
showcase the ability of our method to learn geometric details and textures from
shapes reconstructed from real-world photos. In addition, we have developed an
interactive modeling application to demonstrate the generalizability of our
method to various user inputs and the controllability it offers, allowing users
to interactively sculpt a coarse voxel shape to define the overall structure of
the detailized 3D shape. Code and data are available at
https://github.com/qiminchen/ShaDDR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qimin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiqin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05705">
<title>On the Challenges and Perspectives of Foundation Models for Medical Image Analysis. (arXiv:2306.05705v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05705</link>
<description rdf:parseType="Literal">&lt;p&gt;This article discusses the opportunities, applications and future directions
of large-scale pre-trained models, i.e., foundation models, for analyzing
medical images. Medical foundation models have immense potential in solving a
wide range of downstream tasks, as they can help to accelerate the development
of accurate and robust models, reduce the large amounts of required labeled
data, preserve the privacy and confidentiality of patient data. Specifically,
we illustrate the &quot;spectrum&quot; of medical foundation models, ranging from general
vision models, modality-specific models, to organ/task-specific models,
highlighting their challenges, opportunities and applications. We also discuss
how foundation models can be leveraged in downstream medical tasks to enhance
the accuracy and efficiency of medical image analysis, leading to more precise
diagnosis and treatment decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08326">
<title>Early Detection of Late Blight Tomato Disease using Histogram Oriented Gradient based Support Vector Machine. (arXiv:2306.08326v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08326</link>
<description rdf:parseType="Literal">&lt;p&gt;The tomato is one of the most important fruits on earth. It plays an
important and useful role in the agricultural production of any country. This
research propose a novel smart technique for early detection of late blight
diseases in tomatoes. This work improve the dataset with an increase in images
from the field (the Plant Village dataset) and proposed a hybrid algorithm
composed of support vector machines (SVM) and histogram-oriented gradients
(HOG) for real-time detection of late blight tomato disease. To propose a
HOG-based SVM model for early detection of late blight tomato leaf disease. To
check the performance of the proposed model in terms of MSE, accuracy,
precision, and recall as compared to Decision Tree and KNN. The integration of
advanced technology in agriculture has the potential to revolutionize the
industry, making it more efficient, sustainable, and profitable. This research
work on the early detection of tomato diseases contributes to the growing
importance of smart farming, the need for climate-smart agriculture, the rising
need to more efficiently utilize natural resources, and the demand for higher
crop yields. The proposed hybrid algorithm of SVM and HOG has significant
potential for the early detection of late blight disease in tomato plants. The
performance of the proposed model against decision tree and KNN algorithms and
the results may assist in selecting the best algorithm for future applications.
The research work can help farmers make data-driven decisions to optimize crop
yield and quality while also reducing the environmental impact of farming
practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishaq_M/0/1/0/all/0/1&quot;&gt;M. Ishaq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waqas_M/0/1/0/all/0/1&quot;&gt;M. Waqas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15670">
<title>Symphonize 3D Semantic Scene Completion with Contextual Instance Queries. (arXiv:2306.15670v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15670</link>
<description rdf:parseType="Literal">&lt;p&gt;`3D Semantic Scene Completion (SSC) has emerged as a nascent and pivotal
undertaking in autonomous driving, aiming to predict voxel occupancy within
volumetric scenes. However, prevailing methodologies primarily focus on
voxel-wise feature aggregation, while neglecting instance semantics and scene
context. In this paper, we present a novel paradigm termed Symphonies
(Scene-from-Insts), that delves into the integration of instance queries to
orchestrate 2D-to-3D reconstruction and 3D scene modeling. Leveraging our
proposed Serial Instance-Propagated Attentions, Symphonies dynamically encodes
instance-centric semantics, facilitating intricate interactions between
image-based and volumetric domains. Simultaneously, Symphonies enables holistic
scene comprehension by capturing context through the efficient fusion of
instance queries, alleviating geometric ambiguity such as occlusion and
perspective errors through contextual scene reasoning. Experimental results
demonstrate that Symphonies achieves state-of-the-art performance on
challenging benchmarks SemanticKITTI and SSCBench-KITTI-360, yielding
remarkable mIoU scores of 15.04 and 18.58, respectively. These results showcase
the paradigm&apos;s promising advancements. The code is available at
https://github.com/hustvl/Symphonies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Haoyi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tianheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1&quot;&gt;Naiyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15755">
<title>Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving. (arXiv:2306.15755v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15755</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving, behavior prediction is fundamental for safe motion
planning, hence the security and robustness of prediction models against
adversarial attacks are of paramount importance. We propose a novel adversarial
backdoor attack against trajectory prediction models as a means of studying
their potential vulnerabilities. Our attack affects the victim at training time
via naturalistic, hence stealthy, poisoned samples crafted using a novel
two-step approach. First, the triggers are crafted by perturbing the trajectory
of attacking vehicle and then disguised by transforming the scene using a
bi-level optimization technique. The proposed attack does not depend on a
particular model architecture and operates in a black-box manner, thus can be
effective without any knowledge of the victim model. We conduct extensive
empirical studies using state-of-the-art prediction models on two benchmark
datasets using metrics customized for trajectory prediction. We show that the
proposed attack is highly effective, as it can significantly hinder the
performance of prediction models, unnoticeable by the victims, and efficient as
it forces the victim to generate malicious behavior even under constrained
conditions. Via ablative studies, we analyze the impact of different attack
design choices followed by an evaluation of existing defence mechanisms against
the proposed attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourkeshavarz_M/0/1/0/all/0/1&quot;&gt;Mozhgan Pourkeshavarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohammad Sabokrou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1&quot;&gt;Amir Rasouli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01121">
<title>Artifacts Mapping: Multi-Modal Semantic Mapping for Object Detection and 3D Localization. (arXiv:2307.01121v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01121</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric navigation is nowadays a well-established field of robotics and the
research focus is shifting towards higher-level scene understanding, such as
Semantic Mapping. When a robot needs to interact with its environment, it must
be able to comprehend the contextual information of its surroundings. This work
focuses on classifying and localising objects within a map, which is under
construction (SLAM) or already built. To further explore this direction, we
propose a framework that can autonomously detect and localize predefined
objects in a known environment using a multi-modal sensor fusion approach
(combining RGB and depth data from an RGB-D camera and a lidar). The framework
consists of three key elements: understanding the environment through RGB data,
estimating depth through multi-modal sensor fusion, and managing artifacts
(i.e., filtering and stabilizing measurements). The experiments show that the
proposed framework can accurately detect 98% of the objects in the real sample
environment, without post-processing, while 85% and 80% of the objects were
mapped using the single RGBD camera or RGB + lidar setup respectively. The
comparison with single-sensor (camera or lidar) experiments is performed to
show that sensor fusion allows the robot to accurately detect near and far
obstacles, which would have been noisy or imprecise in a purely visual or
laser-based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rollo_F/0/1/0/all/0/1&quot;&gt;Federico Rollo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raiola_G/0/1/0/all/0/1&quot;&gt;Gennaro Raiola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zunino_A/0/1/0/all/0/1&quot;&gt;Andrea Zunino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsagarakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Tsagarakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajoudani_A/0/1/0/all/0/1&quot;&gt;Arash Ajoudani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09936">
<title>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10631">
<title>PsyMo: A Dataset for Estimating Self-Reported Psychological Traits from Gait. (arXiv:2308.10631v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10631</link>
<description rdf:parseType="Literal">&lt;p&gt;Psychological trait estimation from external factors such as movement and
appearance is a challenging and long-standing problem in psychology, and is
principally based on the psychological theory of embodiment. To date, attempts
to tackle this problem have utilized private small-scale datasets with
intrusive body-attached sensors. Potential applications of an automated system
for psychological trait estimation include estimation of occupational fatigue
and psychology, and marketing and advertisement. In this work, we propose PsyMo
(Psychological traits from Motion), a novel, multi-purpose and multi-modal
dataset for exploring psychological cues manifested in walking patterns. We
gathered walking sequences from 312 subjects in 7 different walking variations
and 6 camera angles. In conjunction with walking sequences, participants filled
in 6 psychological questionnaires, totalling 17 psychometric attributes related
to personality, self-esteem, fatigue, aggressiveness and mental health. We
propose two evaluation protocols for psychological trait estimation. Alongside
the estimation of self-reported psychological traits from gait, the dataset can
be used as a drop-in replacement to benchmark methods for gait recognition. We
anonymize all cues related to the identity of the subjects and publicly release
only silhouettes, 2D / 3D human skeletons and 3D SMPL human meshes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1&quot;&gt;Adrian Cosma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radoi_E/0/1/0/all/0/1&quot;&gt;Emilian Radoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00168">
<title>Pose-Graph Attentional Graph Neural Network for Lidar Place Recognition. (arXiv:2309.00168v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00168</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a pose-graph attentional graph neural network, called
P-GAT, which compares (key)nodes between sequential and non-sequential
sub-graphs for place recognition tasks as opposed to a common frame-to-frame
retrieval problem formulation currently implemented in SOTA place recognition
methods. P-GAT uses the maximum spatial and temporal information between
neighbour cloud descriptors -- generated by an existing encoder -- utilising
the concept of pose-graph SLAM. Leveraging intra- and inter-attention and graph
neural network, P-GAT relates point clouds captured in nearby locations in
Euclidean space and their embeddings in feature space. Experimental results on
the large-scale publically available datasets demonstrate the effectiveness of
our approach in scenes lacking distinct features and when training and testing
environments have different distributions (domain adaptation). Further, an
exhaustive comparison with the state-of-the-art shows improvements in
performance gains. Code is available at
https://github.com/csiro-robotics/P-GAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1&quot;&gt;Milad Ramezani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1&quot;&gt;Joshua Knights&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pounds_P/0/1/0/all/0/1&quot;&gt;Pauline Pounds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00846">
<title>pSTarC: Pseudo Source Guided Target Clustering for Fully Test-Time Adaptation. (arXiv:2309.00846v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00846</link>
<description rdf:parseType="Literal">&lt;p&gt;Test Time Adaptation (TTA) is a pivotal concept in machine learning, enabling
models to perform well in real-world scenarios, where test data distribution
differs from training. In this work, we propose a novel approach called pseudo
Source guided Target Clustering (pSTarC) addressing the relatively unexplored
area of TTA under real-world domain shifts. This method draws inspiration from
target clustering techniques and exploits the source classifier for generating
pseudo-source samples. The test samples are strategically aligned with these
pseudo-source samples, facilitating their clustering and thereby enhancing TTA
performance. pSTarC operates solely within the fully test-time adaptation
protocol, removing the need for actual source data. Experimental validation on
a variety of domain shift datasets, namely VisDA, Office-Home, DomainNet-126,
CIFAR-100C verifies pSTarC&apos;s effectiveness. This method exhibits significant
improvements in prediction accuracy along with efficient computational
requirements. Furthermore, we also demonstrate the universality of the pSTarC
framework by showing its effectiveness for the continuous TTA framework. The
source code for our method is available at https://manogna-s.github.io/pstarc
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreenivas_M/0/1/0/all/0/1&quot;&gt;Manogna Sreenivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarty_G/0/1/0/all/0/1&quot;&gt;Goirik Chakrabarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Soma Biswas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02185">
<title>BEVTrack: A Simple and Strong Baseline for 3D Single Object Tracking in Bird&apos;s-Eye View. (arXiv:2309.02185v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02185</link>
<description rdf:parseType="Literal">&lt;p&gt;3D Single Object Tracking (SOT) is a fundamental task of computer vision,
proving essential for applications like autonomous driving. It remains
challenging to localize the target from surroundings due to appearance
variations, distractors, and the high sparsity of point clouds. The spatial
information indicating objects&apos; spatial adjacency across consecutive frames is
crucial for effective object tracking. However, existing trackers typically
employ point-wise representation with irregular formats, leading to
insufficient use of this important spatial knowledge. As a result, these
trackers usually require elaborate designs and solving multiple subtasks. In
this paper, we propose BEVTrack, a simple yet effective baseline that performs
tracking in Bird&apos;s-Eye View (BEV). This representation greatly retains spatial
information owing to its ordered structure and inherently encodes the implicit
motion relations of the target as well as distractors. To achieve accurate
regression for targets with diverse attributes (\textit{e.g.}, sizes and motion
patterns), BEVTrack constructs the likelihood function with the learned
underlying distributions adapted to different targets, rather than making a
fixed Laplace or Gaussian assumption as in previous works. This provides
valuable priors for tracking and thus further boosts performance. While only
using a single regression loss with a plain convolutional architecture,
BEVTrack achieves state-of-the-art performance on three large-scale datasets,
KITTI, NuScenes, and Waymo Open Dataset while maintaining a high inference
speed of about 200 FPS. The code will be released at
https://github.com/xmm-prio/BEVTrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yingqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiahao Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04153">
<title>Mapping EEG Signals to Visual Stimuli: A Deep Learning Approach to Match vs. Mismatch Classification. (arXiv:2309.04153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04153</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing approaches to modeling associations between visual stimuli and brain
responses are facing difficulties in handling between-subject variance and
model generalization. Inspired by the recent progress in modeling speech-brain
response, we propose in this work a &quot;match-vs-mismatch&quot; deep learning model to
classify whether a video clip induces excitatory responses in recorded EEG
signals and learn associations between the visual content and corresponding
neural recordings. Using an exclusive experimental dataset, we demonstrate that
the proposed model is able to achieve the highest accuracy on unseen subjects
as compared to other baseline models. Furthermore, we analyze the inter-subject
noise using a subject-level silhouette score in the embedding space and show
that the developed model is able to mitigate inter-subject noise and
significantly reduce the silhouette score. Moreover, we examine the Grad-CAM
activation score and show that the brain regions associated with language
processing contribute most to the model predictions, followed by regions
associated with visual processing. These results have the potential to
facilitate the development of neural recording-based video reconstruction and
its related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiqian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhengqiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05254">
<title>Towards Better Data Exploitation in Self-Supervised Monocular Depth Estimation. (arXiv:2309.05254v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05254</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth estimation plays an important role in the robotic perception system.
Self-supervised monocular paradigm has gained significant attention since it
can free training from the reliance on depth annotations. Despite recent
advancements, existing self-supervised methods still underutilize the available
training data, limiting their generalization ability. In this paper, we take
two data augmentation techniques, namely Resizing-Cropping and
Splitting-Permuting, to fully exploit the potential of training datasets.
Specifically, the original image and the generated two augmented images are fed
into the training pipeline simultaneously and we leverage them to conduct
self-distillation. Additionally, we introduce the detail-enhanced DepthNet with
an extra full-scale branch in the encoder and a grid decoder to enhance the
restoration of fine details in depth maps. Experimental results demonstrate our
method can achieve state-of-the-art performance on the KITTI benchmark, with
both raw ground truth and improved ground truth. Moreover, our models also show
superior generalization performance when transferring to Make3D and NYUv2
datasets. Our codes are available at https://github.com/Sauf4896/BDEdepth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1&quot;&gt;Lingtong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08273">
<title>Unsupervised Disentangling of Facial Representations with 3D-aware Latent Diffusion Models. (arXiv:2309.08273v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08273</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised learning of facial representations has gained increasing
attention for face understanding ability without heavily relying on large-scale
annotated datasets. However, it remains unsolved due to the coupling of facial
identities, expressions, and external factors like pose and light. Prior
methods primarily focus on 2D factors and pixel-level consistency, leading to
incomplete disentangling and suboptimal performance in downstream tasks. In
this paper, we propose LatentFace, a novel unsupervised disentangling framework
for facial expression and identity representation. We suggest the disentangling
problem should be performed in latent space and propose the solution using a
3D-aware latent diffusion model. First, we introduce a 3D-aware autoencoder to
encode face images into 3D latent embeddings. Second, we propose a novel
representation diffusion model (RDM) to disentangle 3D latent into facial
identity and expression. Consequently, our method achieves state-of-the-art
performance in facial expression recognition and face verification among
unsupervised facial representation learning models. Codes are available at
\url{https://github.com/ryanhe312/LatentFace}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13289">
<title>USL-Net: Uncertainty Self-Learning Network for Unsupervised Skin Lesion Segmentation. (arXiv:2309.13289v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13289</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised skin lesion segmentation offers several benefits, including
conserving expert human resources, reducing discrepancies due to subjective
human labeling, and adapting to novel environments. However, segmenting
dermoscopic images without manual labeling guidance presents significant
challenges due to dermoscopic image artifacts such as hair noise, blister
noise, and subtle edge differences. To address these challenges, we introduce
an innovative Uncertainty Self-Learning Network (USL-Net) designed for skin
lesion segmentation. The USL-Net can effectively segment a range of lesions,
eliminating the need for manual labeling guidance. Initially, features are
extracted using contrastive learning, followed by the generation of Class
Activation Maps (CAMs) as saliency maps using these features. The different CAM
locations correspond to the importance of the lesion region based on their
saliency. High-saliency regions in the map serve as pseudo-labels for lesion
regions while low-saliency regions represent the background. However,
intermediate regions can be hard to classify, often due to their proximity to
lesion edges or interference from hair or blisters. Rather than risk potential
pseudo-labeling errors or learning confusion by forcefully classifying these
regions, we consider them as uncertainty regions, exempting them from
pseudo-labeling and allowing the network to self-learn. Further, we employ
connectivity detection and centrality detection to refine foreground
pseudo-labels and reduce noise-induced errors. The application of cycle
refining enhances performance further. Our method underwent thorough
experimental validation on the ISIC-2017, ISIC-2018, and PH2 datasets,
demonstrating that its performance is on par with weakly supervised and
supervised methods, and exceeds that of other existing unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaofan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Changyou Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Daipeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhuyang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16020">
<title>GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization. (arXiv:2309.16020v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16020</link>
<description rdf:parseType="Literal">&lt;p&gt;Worldwide Geo-localization aims to pinpoint the precise location of images
taken anywhere on Earth. This task has considerable challenges due to immense
variation in geographic landscapes. The image-to-image retrieval-based
approaches fail to solve this problem on a global scale as it is not feasible
to construct a large gallery of images covering the entire world. Instead,
existing approaches divide the globe into discrete geographic cells,
transforming the problem into a classification task. However, their performance
is limited by the predefined classes and often results in inaccurate
localizations when an image&apos;s location significantly deviates from its class
center. To overcome these limitations, we propose GeoCLIP, a novel
CLIP-inspired Image-to-GPS retrieval approach that enforces alignment between
the image and its corresponding GPS locations. GeoCLIP&apos;s location encoder
models the Earth as a continuous function by employing positional encoding
through random Fourier features and constructing a hierarchical representation
that captures information at varying resolutions to yield a semantically rich
high-dimensional feature suitable to use even beyond geo-localization. To the
best of our knowledge, this is the first work employing GPS encoding for
geo-localization. We demonstrate the efficacy of our method via extensive
experiments and ablations on benchmark datasets. We achieve competitive
performance with just 20% of training data, highlighting its effectiveness even
in limited-data settings. Furthermore, we qualitatively demonstrate
geo-localization using a text query by leveraging CLIP backbone of our image
encoder. The project webpage is available at:
https://vicentevivan.github.io/GeoCLIP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cepeda_V/0/1/0/all/0/1&quot;&gt;Vicente Vivanco Cepeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00615">
<title>Scene-aware Human Motion Forecasting via Mutual Distance Prediction. (arXiv:2310.00615v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00615</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we tackle the problem of scene-aware 3D human motion
forecasting. A key challenge of this task is to predict future human motions
that are consistent with the scene by modeling the human-scene interactions.
While recent works have demonstrated that explicit constraints on human-scene
interactions can prevent the occurrence of ghost motion, they only provide
constraints on partial human motion e.g., the global motion of the human or a
few joints contacting the scene, leaving the rest of the motion unconstrained.
To address this limitation, we propose to model the human-scene interaction
with the mutual distance between the human body and the scene. Such mutual
distances constrain both the local and global human motion, resulting in a
whole-body motion constrained prediction. In particular, mutual distance
constraints consist of two components, the signed distance of each vertex on
the human mesh to the scene surface and the distance of basis scene points to
the human mesh. We further introduce a global scene representation learned from
a signed distance function (SDF) volume to ensure coherence between the global
scene representation and the explicit constraint from the mutual distance. We
develop a pipeline with two sequential steps: predicting the future mutual
distances first, followed by forecasting future human motion. During training,
we explicitly encourage consistency between predicted poses and mutual
distances. Extensive evaluations on the existing synthetic and real datasets
demonstrate that our approach consistently outperforms the state-of-the-art
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1&quot;&gt;Chaoyue Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Wei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08420">
<title>Visual Attention-Prompted Prediction and Learning. (arXiv:2310.08420v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08420</link>
<description rdf:parseType="Literal">&lt;p&gt;Explanation(attention)-guided learning is a method that enhances a model&apos;s
predictive power by incorporating human understanding during the training
phase. While attention-guided learning has shown promising results, it often
involves time-consuming and computationally expensive model retraining. To
address this issue, we introduce the attention-prompted prediction technique,
which enables direct prediction guided by the attention prompt without the need
for model retraining. However, this approach presents several challenges,
including: 1) How to incorporate the visual attention prompt into the model&apos;s
decision-making process and leverage it for future predictions even in the
absence of a prompt? and 2) How to handle the incomplete information from the
visual attention prompt? To tackle these challenges, we propose a novel
framework called Visual Attention-Prompted Prediction and Learning, which
seamlessly integrates visual attention prompts into the model&apos;s decision-making
process and adapts to images both with and without attention prompts for
prediction. To address the incomplete information of the visual attention
prompt, we introduce a perturbation-based attention map modification method.
Additionally, we propose an optimization-based mask aggregation method with a
new weight learning function for adaptive perturbed annotation aggregation in
the attention map modification process. Our overall framework is designed to
learn in an attention-prompt guided multi-task manner to enhance future
predictions even for samples without attention prompts and trained in an
alternating manner for better convergence. Extensive experiments conducted on
two datasets demonstrate the effectiveness of our proposed framework in
enhancing predictions for samples, both with and without provided prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Siyi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Bo Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_G/0/1/0/all/0/1&quot;&gt;Guangji Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08897">
<title>Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jina Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Youngtaek Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;Dawun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yeonggul Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1&quot;&gt;Jaeik Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Sihyeon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_T/0/1/0/all/0/1&quot;&gt;Taekgeun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yeonyee E. Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moon_I/0/1/0/all/0/1&quot;&gt;Inki Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-Ah Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyuk-Jae Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03105">
<title>Pelvic floor MRI segmentation based on semi-supervised deep learning. (arXiv:2311.03105v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03105</link>
<description rdf:parseType="Literal">&lt;p&gt;The semantic segmentation of pelvic organs via MRI has important clinical
significance. Recently, deep learning-enabled semantic segmentation has
facilitated the three-dimensional geometric reconstruction of pelvic floor
organs, providing clinicians with accurate and intuitive diagnostic results.
However, the task of labeling pelvic floor MRI segmentation, typically
performed by clinicians, is labor-intensive and costly, leading to a scarcity
of labels. Insufficient segmentation labels limit the precise segmentation and
reconstruction of pelvic floor organs. To address these issues, we propose a
semi-supervised framework for pelvic organ segmentation. The implementation of
this framework comprises two stages. In the first stage, it performs
self-supervised pre-training using image restoration tasks. Subsequently,
fine-tuning of the self-supervised model is performed, using labeled data to
train the segmentation model. In the second stage, the self-supervised
segmentation model is used to generate pseudo labels for unlabeled data.
Ultimately, both labeled and unlabeled data are utilized in semi-supervised
training. Upon evaluation, our method significantly enhances the performance in
the semantic segmentation and geometric reconstruction of pelvic organs, Dice
coefficient can increase by 2.65% averagely. Especially for organs that are
difficult to segment, such as the uterus, the accuracy of semantic segmentation
can be improved by up to 3.70%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_J/0/1/0/all/0/1&quot;&gt;Jianwei Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashton_Miller_J/0/1/0/all/0/1&quot;&gt;James A. Ashton-Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delancey_J/0/1/0/all/0/1&quot;&gt;John O.L. Delancey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiajia Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03943">
<title>CLIP Guided Image-perceptive Prompt Learning for Image Enhancement. (arXiv:2311.03943v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03943</link>
<description rdf:parseType="Literal">&lt;p&gt;Image enhancement is a significant research area in the fields of computer
vision and image processing. In recent years, many learning-based methods for
image enhancement have been developed, where the Look-up-table (LUT) has proven
to be an effective tool. In this paper, we delve into the potential of
Contrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning,
proposing a simple structure called CLIP-LUT for image enhancement. We found
that the prior knowledge of CLIP can effectively discern the quality of
degraded images, which can provide reliable guidance. To be specific, We
initially learn image-perceptive prompts to distinguish between original and
target images using CLIP model, in the meanwhile, we introduce a very simple
network by incorporating a simple baseline to predict the weights of three
different LUT as enhancement network. The obtained prompts are used to steer
the enhancement network like a loss function and improve the performance of
model. We demonstrate that by simply combining a straightforward method with
CLIP, we can obtain satisfactory results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zinuo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08936">
<title>Confident Naturalness Explanation (CNE): A Framework to Explain and Assess Patterns Forming Naturalness. (arXiv:2311.08936v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08936</link>
<description rdf:parseType="Literal">&lt;p&gt;Protected natural areas are regions that have been minimally affected by
human activities such as urbanization, agriculture, and other human
interventions. To better understand and map the naturalness of these areas,
machine learning models can be used to analyze satellite imagery. Specifically,
explainable machine learning methods show promise in uncovering patterns that
contribute to the concept of naturalness within these protected environments.
Additionally, addressing the uncertainty inherent in machine learning models is
crucial for a comprehensive understanding of this concept. However, existing
approaches have limitations. They either fail to provide explanations that are
both valid and objective or struggle to offer a quantitative metric that
accurately measures the contribution of specific patterns to naturalness, along
with the associated confidence. In this paper, we propose a novel framework
called the Confident Naturalness Explanation (CNE) framework. This framework
combines explainable machine learning and uncertainty quantification to assess
and explain naturalness. We introduce a new quantitative metric that describes
the confident contribution of patterns to the concept of naturalness.
Furthermore, we generate an uncertainty-aware segmentation mask for each input
sample, highlighting areas where the model lacks knowledge. To demonstrate the
effectiveness of our framework, we apply it to a study site in Fennoscandia
using two open-source satellite datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emam_A/0/1/0/all/0/1&quot;&gt;Ahmed Emam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farag_M/0/1/0/all/0/1&quot;&gt;Mohamed Farag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roscher_R/0/1/0/all/0/1&quot;&gt;Ribana Roscher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10813">
<title>A Language Agent for Autonomous Driving. (arXiv:2311.10813v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-level driving is an ultimate goal of autonomous driving. Conventional
approaches formulate autonomous driving as a perception-prediction-planning
framework, yet their systems do not capitalize on the inherent reasoning
ability and experiential knowledge of humans. In this paper, we propose a
fundamental paradigm shift from current pipelines, exploiting Large Language
Models (LLMs) as a cognitive agent to integrate human-like intelligence into
autonomous driving systems. Our approach, termed Agent-Driver, transforms the
traditional autonomous driving pipeline by introducing a versatile tool library
accessible via function calls, a cognitive memory of common sense and
experiential knowledge for decision-making, and a reasoning engine capable of
chain-of-thought reasoning, task planning, motion planning, and
self-reflection. Powered by LLMs, our Agent-Driver is endowed with intuitive
common sense and robust reasoning capabilities, thus enabling a more nuanced,
human-like approach to autonomous driving. We evaluate our approach on the
large-scale nuScenes benchmark, and extensive experiments substantiate that our
Agent-Driver significantly outperforms the state-of-the-art driving methods by
a large margin. Our approach also demonstrates superior interpretability and
few-shot learning ability to these methods. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiageng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junjie Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuxi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11284">
<title>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching. (arXiv:2311.11284v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11284</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advancements in text-to-3D generation mark a significant milestone
in generative models, unlocking new possibilities for creating imaginative 3D
assets across various real-world scenarios. While recent advancements in
text-to-3D generation have shown promise, they often fall short in rendering
detailed and high-quality 3D models. This problem is especially prevalent as
many methods base themselves on Score Distillation Sampling (SDS). This paper
identifies a notable deficiency in SDS, that it brings inconsistent and
low-quality updating direction for the 3D model, causing the over-smoothing
effect. To address this, we propose a novel approach called Interval Score
Matching (ISM). ISM employs deterministic diffusing trajectories and utilizes
interval-based score matching to counteract over-smoothing. Furthermore, we
incorporate 3D Gaussian Splatting into our text-to-3D generation pipeline.
Extensive experiments show that our model largely outperforms the
state-of-the-art in quality and training efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yixun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiantao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haodong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingcong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11317">
<title>Discrete approximations of Gaussian smoothing and Gaussian derivatives. (arXiv:2311.11317v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11317</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops an in-depth treatment concerning the problem of
approximating the Gaussian smoothing and Gaussian derivative computations in
scale-space theory for application on discrete data. With close connections to
previous axiomatic treatments of continuous and discrete scale-space theory, we
consider three main ways discretizing these scale-space operations in terms of
explicit discrete convolutions, based on either (i) sampling the Gaussian
kernels and the Gaussian derivative kernels, (ii) locally integrating the
Gaussian kernels and the Gaussian derivative kernels over each pixel support
region and (iii) basing the scale-space analysis on the discrete analogue of
the Gaussian kernel, and then computing derivative approximations by applying
small-support central difference operators to the spatially smoothed image
data.
&lt;/p&gt;
&lt;p&gt;We study the properties of these three main discretization methods both
theoretically and experimentally, and characterize their performance by
quantitative measures, including the results they give rise to with respect to
the task of scale selection, investigated for four different use cases, and
with emphasis on the behaviour at fine scales. The results show that the
sampled Gaussian kernels and derivatives as well as the integrated Gaussian
kernels and derivatives perform very poorly at very fine scales. At very fine
scales, the discrete analogue of the Gaussian kernel with its corresponding
discrete derivative approximations performs substantially better. The sampled
Gaussian kernel and the sampled Gaussian derivatives do, on the other hand,
lead to numerically very good approximations of the corresponding continuous
results, when the scale parameter is sufficiently large, in the experiments
presented in the paper, when the scale parameter is greater than a value of
about 1, in units of the grid spacing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1&quot;&gt;Tony Lindeberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11772">
<title>A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology. (arXiv:2311.11772v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11772</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1&quot;&gt;Georg W&amp;#xf6;lflein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferber_D/0/1/0/all/0/1&quot;&gt;Dyke Ferber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneghetti_A/0/1/0/all/0/1&quot;&gt;Asier Rabasco Meneghetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nahhas_O/0/1/0/all/0/1&quot;&gt;Omar S. M. El Nahhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrero_Z/0/1/0/all/0/1&quot;&gt;Zunamys I. Carrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1&quot;&gt;David J. Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1&quot;&gt;Jakob N. Kather&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11819">
<title>Generalized super-resolution 4D Flow MRI $\unicode{x2013}$ using ensemble learning to extend across the cardiovascular system. (arXiv:2311.11819v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11819</link>
<description rdf:parseType="Literal">&lt;p&gt;4D Flow Magnetic Resonance Imaging (4D Flow MRI) is a non-invasive
measurement technique capable of quantifying blood flow across the
cardiovascular system. While practical use is limited by spatial resolution and
image noise, incorporation of trained super-resolution (SR) networks has
potential to enhance image quality post-scan. However, these efforts have
predominantly been restricted to narrowly defined cardiovascular domains, with
limited exploration of how SR performance extends across the cardiovascular
system; a task aggravated by contrasting hemodynamic conditions apparent across
the cardiovasculature. The aim of our study was to explore the generalizability
of SR 4D Flow MRI using a combination of heterogeneous training sets and
dedicated ensemble learning. With synthetic training data generated across
three disparate domains (cardiac, aortic, cerebrovascular), varying
convolutional base and ensemble learners were evaluated as a function of domain
and architecture, quantifying performance on both in-silico and acquired
in-vivo data from the same three domains. Results show that both bagging and
stacking ensembling enhance SR performance across domains, accurately
predicting high-resolution velocities from low-resolution input data in-silico.
Likewise, optimized networks successfully recover native resolution velocities
from downsampled in-vivo data, as well as show qualitative potential in
generating denoised SR-images from clinical level input data. In conclusion,
our work presents a viable approach for generalized SR 4D Flow MRI, with
ensemble learning extending utility across various clinical areas of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ericsson_L/0/1/0/all/0/1&quot;&gt;Leon Ericsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hjalmarsson_A/0/1/0/all/0/1&quot;&gt;Adam Hjalmarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akbar_M/0/1/0/all/0/1&quot;&gt;Muhammad Usman Akbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ferdian_E/0/1/0/all/0/1&quot;&gt;Edward Ferdian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonini_M/0/1/0/all/0/1&quot;&gt;Mia Bonini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hardy_B/0/1/0/all/0/1&quot;&gt;Brandon Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schollenberger_J/0/1/0/all/0/1&quot;&gt;Jonas Schollenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aristova_M/0/1/0/all/0/1&quot;&gt;Maria Aristova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winter_P/0/1/0/all/0/1&quot;&gt;Patrick Winter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Burris_N/0/1/0/all/0/1&quot;&gt;Nicholas Burris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fyrdahl_A/0/1/0/all/0/1&quot;&gt;Alexander Fyrdahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sigfridsson_A/0/1/0/all/0/1&quot;&gt;Andreas Sigfridsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnell_S/0/1/0/all/0/1&quot;&gt;Susanne Schnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Figueroa_C/0/1/0/all/0/1&quot;&gt;C. Alberto Figueroa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nordsletten_D/0/1/0/all/0/1&quot;&gt;David Nordsletten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Alistair A. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marlevi_D/0/1/0/all/0/1&quot;&gt;David Marlevi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12068">
<title>Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12068</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1&quot;&gt;Rohit Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12144">
<title>Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12144</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12161">
<title>ChemScraper: Graphics Extraction, Molecular Diagram Parsing, and Annotated Data Generation for PDF Images. (arXiv:2311.12161v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12161</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing visual parsers for molecule diagrams translate pixel-based raster
images such as PNGs to chemical structure representations (e.g., SMILES).
However, PDFs created by word processors including LaTeX and Word provide
explicit locations and shapes for characters, lines, and polygons. We extract
symbols from born-digital PDF molecule images and then apply simple graph
transformations to capture both visual and chemical structure in editable
ChemDraw files (CDXML). Our fast ( PDF $\rightarrow$ visual graph $\rightarrow$
chemical graph ) pipeline does not require GPUs, Optical Character Recognition
(OCR) or vectorization. We evaluate on standard benchmarks using SMILES
strings, along with a novel evaluation that provides graph-based metrics and
error compilation using LgEval. The geometric information in born-digital PDFs
produces a highly accurate parser, motivating generating training data for
visual parsers that recognize from raster images, with extracted graphics,
visual structure, and chemical structure as annotations. To do this we render
SMILES strings in Indigo, parse molecule structure, and then validate
recognized structure to select correct files.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ayush Kumar Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amador_B/0/1/0/all/0/1&quot;&gt;Bryan Manrique Amador&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1&quot;&gt;Abhisek Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creekmore_M/0/1/0/all/0/1&quot;&gt;Ming Creekmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ocampo_B/0/1/0/all/0/1&quot;&gt;Blake Ocampo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denmark_S/0/1/0/all/0/1&quot;&gt;Scott Denmark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanibbi_R/0/1/0/all/0/1&quot;&gt;Richard Zanibbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12198">
<title>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v2 [cs.GR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12198</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, &quot;cage meshes,&quot; or
any other geometry embedding, highlighting the principle of &quot;what you see is
what you simulate (WS$^2$).&quot; Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tianyi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zeshun Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuxing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yutao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenfanfu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12437">
<title>Learning Site-specific Styles for Multi-institutional Unsupervised Cross-modality Domain Adaptation. (arXiv:2311.12437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12437</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised cross-modality domain adaptation is a challenging task in
medical image analysis, and it becomes more challenging when source and target
domain data are collected from multiple institutions. In this paper, we present
our solution to tackle the multi-institutional unsupervised domain adaptation
for the crossMoDA 2023 challenge. First, we perform unpaired image translation
to translate the source domain images to the target domain, where we design a
dynamic network to generate synthetic target domain images with controllable,
site-specific styles. Afterwards, we train a segmentation model using the
synthetic images and further reduce the domain gap by self-training. Our
solution achieved the 1st place during both the validation and testing phases
of the challenge. The code repository is publicly available at
https://github.com/MedICL-VU/crossmoda2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yubo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhoubing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawant_B/0/1/0/all/0/1&quot;&gt;Benoit M. Dawant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12467">
<title>GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation with Large Domain Gap. (arXiv:2311.12467v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we tackle the challenging problem of unsupervised video domain
adaptation (UVDA) for action recognition. We specifically focus on scenarios
with a substantial domain gap, in contrast to existing works primarily deal
with small domain gaps between labeled source domains and unlabeled target
domains. To establish a more realistic setting, we introduce a novel UVDA
scenario, denoted as Kinetics-&amp;gt;BABEL, with a more considerable domain gap in
terms of both temporal dynamics and background shifts. To tackle the temporal
shift, i.e., action duration difference between the source and target domains,
we propose a global-local view alignment approach. To mitigate the background
shift, we propose to learn temporal order sensitive representations by temporal
order learning and background invariant representations by background
augmentation. We empirically validate that the proposed method shows
significant improvement over the existing methods on the Kinetics-&amp;gt;BABEL
dataset with a large domain gap. The code is available at
https://github.com/KHUVLL/GLAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyogun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Kyungho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Seong Jong Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1&quot;&gt;Yumin Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gyeong-Moon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinwoo Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12603">
<title>Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition. (arXiv:2311.12603v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12603</link>
<description rdf:parseType="Literal">&lt;p&gt;To assist surgeons in the operating theatre, surgical phase recognition is
critical for developing computer-assisted surgical systems, which requires
comprehensive understanding of surgical videos. Although existing studies made
great progress, there are still two significant limitations worthy of
improvement. First, due to the compromise of resource consumption, frame-wise
visual features are extracted by 2D networks and disregard spatial and temporal
knowledge of surgical actions, which hinders subsequent inter-frame modeling
for phase prediction. Second, these works simply utilize ordinary
classification loss with one-hot phase labels to optimize the phase
predictions, and cannot fully explore surgical videos under inadequate
supervision. To overcome these two limitations, we propose a Surgical Temporal
Action-aware Network with sequence Regularization, named STAR-Net, to recognize
surgical phases more accurately from input videos. Specifically, we propose an
efficient multi-scale surgical temporal action (MS-STA) module, which
integrates visual features with spatial and temporal knowledge of surgical
actions at the cost of 2D networks. Moreover, we devise the dual-classifier
sequence regularization (DSR) to facilitate the training of STAR-Net by the
sequence guidance of an auxiliary classifier with a smaller capacity. Our
STAR-Net with MS-STA and DSR can exploit visual features of surgical actions
with effective regularization, thereby leading to the superior performance of
surgical phase recognition. Extensive experiments on a large-scale gastrectomy
surgery dataset and the public Cholec80 benchmark prove that our STAR-Net
significantly outperforms state-of-the-arts of surgical phase recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinqiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12655">
<title>Hand-Eye Calibration. (arXiv:2311.12655v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12655</link>
<description rdf:parseType="Literal">&lt;p&gt;Whenever a sensor is mounted on a robot hand it is important to know the
relationship between the sensor and the hand. The problem of determining this
relationship is referred to as hand-eye calibration, which is important in at
least two types of tasks: (i) map sensor centered measurements into the robot
workspace and (ii) allow the robot to precisely move the sensor. In the past
some solutions were proposed in the particular case of a camera. With almost no
exception, all existing solutions attempt to solve the homogeneous matrix
equation AX=XB. First we show that there are two possible formulations of the
hand-eye calibration problem. One formulation is the classical one that we just
mentioned. A second formulation takes the form of the following homogeneous
matrix equation: MY=M&apos;YB. The advantage of the latter is that the extrinsic and
intrinsic camera parameters need not be made explicit. Indeed, this formulation
directly uses the 3 by 4 perspective matrices (M and M&apos;) associated with two
positions of the camera. Moreover, this formulation together with the classical
one cover a wider range of camera-based sensors to be calibrated with respect
to the robot hand. Second, we develop a common mathematical framework to solve
for the hand-eye calibration problem using either of the two formulations. We
present two methods, (i) a rotation then translation and (ii) a non-linear
solver for rotation and translation. Third, we perform a stability analysis
both for our two methods and for the classical linear method of Tsai and Lenz
(1989). In the light of this comparison, the non-linear optimization method,
that solves for rotation and translation simultaneously, seems to be the most
robust one with respect to noise and to measurement errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dornaika_F/0/1/0/all/0/1&quot;&gt;Fadi Dornaika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12764">
<title>Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection. (arXiv:2311.12764v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12764</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) exhibit superior performance in various machine
learning tasks, e.g., image classification, speech recognition, biometric
recognition, object detection, etc. However, it is essential to analyze their
sensitivity to parameter perturbations before deploying them in real-world
applications. In this work, we assess the sensitivity of DNNs against
perturbations to their weight and bias parameters. The sensitivity analysis
involves three DNN architectures (VGG, ResNet, and DenseNet), three types of
parameter perturbations (Gaussian noise, weight zeroing, and weight scaling),
and two settings (entire network and layer-wise). We perform experiments in the
context of iris presentation attack detection and evaluate on two publicly
available datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the
sensitivity analysis, we propose improved models simply by perturbing
parameters of the network without undergoing training. We further combine these
perturbed models at the score-level and at the parameter-level to improve the
performance over the original model. The ensemble at the parameter-level shows
an average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on
the LivDet-Iris-2020 dataset. The source code is available at
https://github.com/redwankarimsony/WeightPerturbation-MSU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Renu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sony_R/0/1/0/all/0/1&quot;&gt;Redwan Sony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06054">
<title>Refining the ONCE Benchmark with Hyperparameter Tuning. (arXiv:2311.06054v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.06054</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the growing demand for 3D object detection in applications
such as autonomous driving, robotics, and augmented reality, this work focuses
on the evaluation of semi-supervised learning approaches for point cloud data.
The point cloud representation provides reliable and consistent observations
regardless of lighting conditions, thanks to advances in LiDAR sensors. Data
annotation is of paramount importance in the context of LiDAR applications, and
automating 3D data annotation with semi-supervised methods is a pivotal
challenge that promises to reduce the associated workload and facilitate the
emergence of cost-effective LiDAR solutions. Nevertheless, the task of
semi-supervised learning in the context of unordered point cloud data remains
formidable due to the inherent sparsity and incomplete shapes that hinder the
generation of accurate pseudo-labels. In this study, we consider these
challenges by posing the question: &quot;To what extent does unlabelled data
contribute to the enhancement of model performance?&quot; We show that improvements
from previous semi-supervised methods may not be as profound as previously
thought. Our results suggest that simple grid search hyperparameter tuning
applied to a supervised model can lead to state-of-the-art performance on the
ONCE dataset, while the contribution of unlabelled data appears to be
comparatively less exceptional.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golyadkin_M/0/1/0/all/0/1&quot;&gt;Maksim Golyadkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gambashidze_A/0/1/0/all/0/1&quot;&gt;Alexander Gambashidze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nurgaliev_I/0/1/0/all/0/1&quot;&gt;Ildar Nurgaliev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makarov_I/0/1/0/all/0/1&quot;&gt;Ilya Makarov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11642">
<title>Video Face Re-Aging: Toward Temporally Consistent Face Re-Aging. (arXiv:2311.11642v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.11642</link>
<description rdf:parseType="Literal">&lt;p&gt;Video face re-aging deals with altering the apparent age of a person to the
target age in videos. This problem is challenging due to the lack of paired
video datasets maintaining temporal consistency in identity and age. Most
re-aging methods process each image individually without considering the
temporal consistency of videos. While some existing works address the issue of
temporal coherence through video facial attribute manipulation in latent space,
they often fail to deliver satisfactory performance in age transformation. To
tackle the issues, we propose (1) a novel synthetic video dataset that features
subjects across a diverse range of age groups; (2) a baseline architecture
designed to validate the effectiveness of our proposed dataset, and (3) the
development of three novel metrics tailored explicitly for evaluating the
temporal consistency of video re-aging techniques. Our comprehensive
experiments on public datasets, such as VFHQ and CelebV-HQ, show that our
method outperforms the existing approaches in terms of both age transformation
and temporal consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muqeet_A/0/1/0/all/0/1&quot;&gt;Abdul Muqeet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyuchul Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yohan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungrae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woonggon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kwang Hee Lee&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>