<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12481" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.07207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.14719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12436" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.12439">
<title>Single-pixel 3D imaging based on fusion temporal data of single photon detector and millimeter-wave radar. (arXiv:2312.12439v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12439</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, there has been increased attention towards 3D imaging using
single-pixel single-photon detection (also known as temporal data) due to its
potential advantages in terms of cost and power efficiency. However, to
eliminate the symmetry blur in the reconstructed images, a fixed background is
required. This paper proposes a fusion-data-based 3D imaging method that
utilizes a single-pixel single-photon detector and a millimeter-wave radar to
capture temporal histograms of a scene from multiple perspectives.
Subsequently, the 3D information can be reconstructed from the one-dimensional
fusion temporal data by using Artificial Neural Network (ANN). Both the
simulation and experimental results demonstrate that our fusion method
effectively eliminates symmetry blur and improves the quality of the
reconstructed images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1&quot;&gt;Tingqin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinyi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Lianye Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xuelin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Ping Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shihai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12441">
<title>DiffSpectralNet : Unveiling the Potential of Diffusion Models for Hyperspectral Image Classification. (arXiv:2312.12441v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12441</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSI) have become popular for analysing remotely sensed
images in multiple domain like agriculture, medical. However, existing models
struggle with complex relationships and characteristics of spectral-spatial
data due to the multi-band nature and data redundancy of hyperspectral data. To
address this limitation, we propose a new network called DiffSpectralNet, which
combines diffusion and transformer techniques. Our approach involves a two-step
process. First, we use an unsupervised learning framework based on the
diffusion model to extract both high-level and low-level spectral-spatial
features. The diffusion method is capable of extracting diverse and meaningful
spectral-spatial features, leading to improvement in HSI classification. Then,
we employ a pretrained denoising U-Net to extract intermediate hierarchical
features for classification. Finally, we use a supervised transformer-based
classifier to perform the HSI classification. Through comprehensive experiments
on HSI datasets, we evaluate the classification performance of DiffSpectralNet.
The results demonstrate that our framework significantly outperforms existing
approaches, achieving state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigger_N/0/1/0/all/0/1&quot;&gt;Neetu Sigger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuan Thanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tozzi_G/0/1/0/all/0/1&quot;&gt;Gianluca Tozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_Q/0/1/0/all/0/1&quot;&gt;Quoc-Tuan Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Sinh Van Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12442">
<title>Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC) -- an end-to-end model for characterizing severity and diagnosis. (arXiv:2312.12442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12442</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated classification of cancer pathology reports can extract information
from unstructured reports and categorize each report into structured diagnosis
and severity categories. Thus, such system can reduce the burden for populating
tumor registries, help registration for clinical trial as well as developing
large dataset for deep learning model development using true pathologic ground
truth. However, the content of breast pathology reports can be difficult for
categorize due to the high linguistic variability in content and wide variety
of potential diagnoses &amp;gt;50. Existing NLP models are primarily focused on
developing classifier for primary breast cancer types (e.g. IDC, DCIS, ILC) and
tumor characteristics, and ignore the rare diagnosis of cancer subtypes. We
then developed a hierarchical hybrid transformer-based pipeline (59 labels) -
Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC),
which utilizes the potential of the transformer context-preserving NLP
technique and compared our model to several state of the art ML and DL models.
We trained the model on the EUH data and evaluated our model&apos;s performance on
two external datasets - MGH and Mayo Clinic. We publicly release the code and a
live application under Huggingface spaces repository
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_T/0/1/0/all/0/1&quot;&gt;Thiago Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1&quot;&gt;Harish Kamath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAdams_C/0/1/0/all/0/1&quot;&gt;Christopher R. McAdams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newell_M/0/1/0/all/0/1&quot;&gt;Mary S. Newell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosunjac_M/0/1/0/all/0/1&quot;&gt;Marina Mosunjac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprea_Ilies_G/0/1/0/all/0/1&quot;&gt;Gabriela Oprea-Ilies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_G/0/1/0/all/0/1&quot;&gt;Geoffrey Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_C/0/1/0/all/0/1&quot;&gt;Constance Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1&quot;&gt;Imon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1&quot;&gt;Hari Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12444">
<title>What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?. (arXiv:2312.12444v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12444</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the success of transfer learning in computer vision, roboticists
have investigated visual pre-training as a means to improve the learning
efficiency and generalization ability of policies learned from pixels. To that
end, past work has favored large object interaction datasets, such as
first-person videos of humans completing diverse tasks, in pursuit of
manipulation-relevant features. Although this approach improves the efficiency
of policy learning, it remains unclear how reliable these representations are
in the presence of distribution shifts that arise commonly in robotic
applications. Surprisingly, we find that visual representations designed for
manipulation and control tasks do not necessarily generalize under subtle
changes in lighting and scene texture or the introduction of distractor
objects. To understand what properties do lead to robust representations, we
compare the performance of 15 pre-trained vision models under different visual
appearances. We find that emergent segmentation ability is a strong predictor
of out-of-distribution generalization among ViT models. The rank order induced
by this metric is more predictive than metrics that have previously guided
generalization research within computer vision and machine learning, such as
downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by
cue-conflict performance. We test this finding extensively on a suite of
distribution shifts in ten tasks across two simulated manipulation
environments. On the ALOHA setup, segmentation score predicts real-world
performance after offline training with 50 demonstrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1&quot;&gt;Kaylee Burns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witzel_Z/0/1/0/all/0/1&quot;&gt;Zach Witzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamid_J/0/1/0/all/0/1&quot;&gt;Jubayer Ibn Hamid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12463">
<title>Open Vocabulary Semantic Scene Sketch Understanding. (arXiv:2312.12463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12463</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the underexplored but fundamental vision problem of machine
understanding of abstract freehand scene sketches. We introduce a sketch
encoder that results in semantically-aware feature space, which we evaluate by
testing its performance on a semantic sketch segmentation task. To train our
model we rely only on the availability of bitmap sketches with their brief
captions and do not require any pixel-level annotations. To obtain
generalization to a large set of sketches and categories, we build on a vision
transformer encoder pretrained with the CLIP model. We freeze the text encoder
and perform visual-prompt tuning of the visual encoder branch while introducing
a set of critical modifications. Firstly, we augment the classical key-query
(k-q) self-attention blocks with value-value (v-v) self-attention blocks.
Central to our model is a two-level hierarchical network design that enables
efficient semantic disentanglement: The first level ensures holistic scene
sketch encoding, and the second level focuses on individual categories. We,
then, in the second level of the hierarchy, introduce a cross-attention between
textual and visual branches. Our method outperforms zero-shot CLIP pixel
accuracy of segmentation results by 37 points, reaching an accuracy of $85.5\%$
on the FS-COCO sketch dataset. Finally, we conduct a user study that allows us
to identify further improvements needed over our method to reconcile machine
and human understanding of scene sketches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bourouis_A/0/1/0/all/0/1&quot;&gt;Ahmed Bourouis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Judith Ellen Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1&quot;&gt;Yulia Gryaditskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12468">
<title>MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers. (arXiv:2312.12468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12468</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative AI have significantly enhanced image and video
editing, particularly in the context of text prompt control. State-of-the-art
approaches predominantly rely on diffusion models to accomplish these tasks.
However, the computational demands of diffusion-based methods are substantial,
often necessitating large-scale paired datasets for training, and therefore
challenging the deployment in practical applications. This study addresses this
challenge by breaking down the text-based video editing process into two
separate stages. In the first stage, we leverage an existing text-to-image
diffusion model to simultaneously edit a few keyframes without additional
fine-tuning. In the second stage, we introduce an efficient model called
MaskINT, which is built on non-autoregressive masked generative transformers
and specializes in frame interpolation between the keyframes, benefiting from
structural guidance provided by intermediate frames. Our comprehensive set of
experiments illustrates the efficacy and efficiency of MaskINT when compared to
other diffusion-based methodologies. This research offers a practical solution
for text-based video editing and showcases the potential of non-autoregressive
masked generative transformers in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haoyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdizadehaghdam_S/0/1/0/all/0/1&quot;&gt;Shahin Mahdizadehaghdam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuchao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapira_L/0/1/0/all/0/1&quot;&gt;Lior Shapira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12470">
<title>Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation. (arXiv:2312.12470v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12470</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring Remote Sensing Image Segmentation (RRSIS) is a new challenge that
combines computer vision and natural language processing, delineating specific
regions in aerial images as described by textual queries. Traditional Referring
Image Segmentation (RIS) approaches have been impeded by the complex spatial
scales and orientations found in aerial imagery, leading to suboptimal
segmentation results. To address these challenges, we introduce the Rotated
Multi-Scale Interaction Network (RMSIN), an innovative approach designed for
the unique demands of RRSIS. RMSIN incorporates an Intra-scale Interaction
Module (IIM) to effectively address the fine-grained detail required at
multiple scales and a Cross-scale Interaction Module (CIM) for integrating
these details coherently across the network. Furthermore, RMSIN employs an
Adaptive Rotated Convolution (ARC) to account for the diverse orientations of
objects, a novel contribution that significantly enhances segmentation
accuracy. To assess the efficacy of RMSIN, we have curated an expansive dataset
comprising 17,402 image-caption-mask triplets, which is unparalleled in terms
of scale and variety. This dataset not only presents the model with a wide
range of spatial and rotational scenarios but also establishes a stringent
benchmark for the RRSIS task, ensuring a rigorous evaluation of performance.
Our experimental evaluations demonstrate the exceptional performance of RMSIN,
surpassing existing state-of-the-art models by a significant margin. All
datasets and code are made available at https://github.com/Lsan2401/RMSIN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12471">
<title>Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion. (arXiv:2312.12471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12471</link>
<description rdf:parseType="Literal">&lt;p&gt;Monocular depth estimation has experienced significant progress on
terrestrial images in recent years, largely due to deep learning advancements.
However, it remains inadequate for underwater scenes, primarily because of data
scarcity. Given the inherent challenges of light attenuation and backscattering
in water, acquiring clear underwater images or precise depth information is
notably difficult and costly. Consequently, learning-based approaches often
rely on synthetic data or turn to unsupervised or self-supervised methods to
mitigate this lack of data. Nonetheless, the performance of these methods is
often constrained by the domain gap and looser constraints. In this paper, we
propose a novel pipeline for generating photorealistic underwater images using
accurate terrestrial depth data. This approach facilitates the training of
supervised models for underwater depth estimation, effectively reducing the
performance disparity between terrestrial and underwater environments. Contrary
to prior synthetic datasets that merely apply style transfer to terrestrial
images without altering the scene content, our approach uniquely creates
vibrant, non-existent underwater scenes by leveraging terrestrial depth data
through the innovative Stable Diffusion model. Specifically, we introduce a
unique Depth2Underwater ControlNet, trained on specially prepared \{Underwater,
Depth, Text\} data triplets, for this generation task. Our newly developed
dataset enables terrestrial depth estimation models to achieve considerable
improvements, both quantitatively and qualitatively, on unseen underwater
images, surpassing their terrestrial pre-trained counterparts. Moreover, the
enhanced depth accuracy for underwater scenes also aids underwater image
restoration techniques that rely on depth maps, further demonstrating our
dataset&apos;s utility. The dataset will be available at
https://github.com/zkawfanx/Atlantis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shaodi You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Ying Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12478">
<title>ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval. (arXiv:2312.12478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12478</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust
performance in generalized test scenarios, wherein data may belong to strictly
unknown domains and categories during training. Recently, pre-trained models
with prompt tuning have shown strong generalization capabilities and attained
noteworthy achievements in various downstream tasks, such as few-shot learning
and video-text retrieval. However, applying them directly to UCDR may not
sufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)
and semantic shift (i.e., transferring to unknown categories). To this end, we
propose Prompting-to-Simulate (ProS), the first method to apply prompt tuning
for UCDR. ProS employs a two-step process to simulate Content-aware Dynamic
Prompts (CaDP) which can impact models to produce generalized features for
UCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units
to individually capture domain and semantic knowledge in a mask-and-align way.
Then, in Context-aware Simulator Learning stage, we train a Content-aware
Prompt Simulator under a simulated test scenarios to produce the corresponding
CaDP. Extensive experiments conducted on three benchmark datasets show that our
method achieves new state-of-the-art performance without bringing excessive
parameters. Our method is publicly available at
https://anonymous.4open.science/r/ProS
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1&quot;&gt;Pengpeng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12479">
<title>Zero-shot Building Attribute Extraction from Large-Scale Vision and Language Models. (arXiv:2312.12479v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12479</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing building recognition methods, exemplified by BRAILS, utilize
supervised learning to extract information from satellite and street-view
images for classification and segmentation. However, each task module requires
human-annotated data, hindering the scalability and robustness to regional
variations and annotation imbalances. In response, we propose a new zero-shot
workflow for building attribute extraction that utilizes large-scale vision and
language models to mitigate reliance on external annotations. The proposed
workflow contains two key components: image-level captioning and segment-level
captioning for the building images based on the vocabularies pertinent to
structural and civil engineering. These two components generate descriptive
captions by computing feature representations of the image and the
vocabularies, and facilitating a semantic match between the visual and textual
representations. Consequently, our framework offers a promising avenue to
enhance AI-driven captioning for building attribute extraction in the
structural and civil engineering domains, ultimately reducing reliance on human
annotations while bolstering performance and adaptability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1&quot;&gt;Fei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1&quot;&gt;Sangryul Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Brian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mckenna_F/0/1/0/all/0/1&quot;&gt;Frank Mckenna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Stella X. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12480">
<title>Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation. (arXiv:2312.12480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12480</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Test-Time Adaptation (CTTA) is proposed to migrate a source
pre-trained model to continually changing target distributions, addressing
real-world dynamism. Existing CTTA methods mainly rely on entropy minimization
or teacher-student pseudo-labeling schemes for knowledge extraction in
unlabeled target domains. However, dynamic data distributions cause
miscalibrated predictions and noisy pseudo-labels in existing self-supervised
learning methods, hindering the effective mitigation of error accumulation and
catastrophic forgetting problems during the continual adaptation process. To
tackle these issues, we propose a continual self-supervised method, Adaptive
Distribution Masked Autoencoders (ADMA), which enhances the extraction of
target domain knowledge while mitigating the accumulation of distribution
shifts. Specifically, we propose a Distribution-aware Masking (DaM) mechanism
to adaptively sample masked positions, followed by establishing consistency
constraints between the masked target samples and the original target samples.
Additionally, for masked tokens, we utilize an efficient decoder to reconstruct
a hand-crafted feature descriptor (e.g., Histograms of Oriented Gradients),
leveraging its invariant properties to boost task-relevant representations.
Through conducting extensive experiments on four widely recognized benchmarks,
our proposed method attains state-of-the-art performance in both classification
and segmentation CTTA tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Senqiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zehui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yandong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12481">
<title>Unveiling Spaces: Architecturally meaningful semantic descriptions from images of interior spaces. (arXiv:2312.12481v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12481</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been a growing adoption of computer vision tools and technologies
in architectural design workflows over the past decade. Notable use cases
include point cloud generation, visual content analysis, and spatial awareness
for robotic fabrication. Multiple image classification, object detection, and
semantic pixel segmentation models have become popular for the extraction of
high-level symbolic descriptions and semantic content from two-dimensional
images and videos. However, a major challenge in this regard has been the
extraction of high-level architectural structures (walls, floors, ceilings
windows etc.) from diverse imagery where parts of these elements are occluded
by furniture, people, or other non-architectural elements. This project aims to
tackle this problem by proposing models that are capable of extracting
architecturally meaningful semantic descriptions from two-dimensional scenes of
populated interior spaces. 1000 virtual classrooms are parametrically
generated, randomized along key spatial parameters such as length, width,
height, and door/window positions. The positions of cameras, and
non-architectural visual obstructions (furniture/objects) are also randomized.
A Generative Adversarial Network (GAN) for image-to-image translation (Pix2Pix)
is trained on synthetically generated rendered images of these enclosures,
along with corresponding image abstractions representing high-level
architectural structure. The model is then tested on unseen synthetic imagery
of new enclosures, and outputs are compared to ground truth using pixel-wise
comparison for evaluation. A similar model evaluation is also carried out on
photographs of existing indoor enclosures, to measure its performance in
real-world settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tas_D/0/1/0/all/0/1&quot;&gt;Demircan Tas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanatani_R/0/1/0/all/0/1&quot;&gt;Rohit Priyadarshi Sanatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12483">
<title>SCoTTi: Save Computation at Training Time with an adaptive framework. (arXiv:2312.12483v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12483</link>
<description rdf:parseType="Literal">&lt;p&gt;On-device training is an emerging approach in machine learning where models
are trained on edge devices, aiming to enhance privacy protection and real-time
performance. However, edge devices typically possess restricted computational
power and resources, making it challenging to perform computationally intensive
model training tasks. Consequently, reducing resource consumption during
training has become a pressing concern in this field. To this end, we propose
SCoTTi (Save Computation at Training Time), an adaptive framework that
addresses the aforementioned challenge. It leverages an optimizable threshold
parameter to effectively reduce the number of neuron updates during training
which corresponds to a decrease in memory and computation footprint. Our
proposed approach demonstrates superior performance compared to the
state-of-the-art methods regarding computational resource savings on various
commonly employed benchmarks and popular architectures, including ResNets,
MobileNet, and Swin-T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Ziyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van-Tam Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12486">
<title>Vision-Based Automatic Groceries Tracking System -- Smart Homes. (arXiv:2312.12486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12486</link>
<description rdf:parseType="Literal">&lt;p&gt;With advanced AI, while every industry is growing at rocket speed, the smart
home industry has not reached the next generation. There is still a huge leap
of innovation that needs to happen before we call a home a Smart home. A Smart
home should predict residents&apos; needs and fulfill them in a timely manner. One
of the important tasks of maintaining a home is timely grocery tracking and
supply maintenance. Grocery tracking models are very famous in the retail
industry but they are nonexistent in the common household. Groceries detection
in household refrigerators or storage closets is very complicated compared to
retail shelving data. In this paper, home grocery tracking problem is resolved
by combining retail shelving data and fruits dataset with real-time 360 view
data points collected from home groceries storage. By integrating this
vision-based object detection system along with supply chain and user food
interest prediction systems, complete automation of groceries ordering can be
achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mereddy_D/0/1/0/all/0/1&quot;&gt;Divya Mereddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12488">
<title>Foreseeing Reconstruction Quality of Gradient Inversion: An Optimization Perspective. (arXiv:2312.12488v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12488</link>
<description rdf:parseType="Literal">&lt;p&gt;Gradient inversion attacks can leak data privacy when clients share weight
updates with the server in federated learning (FL). Existing studies mainly use
L2 or cosine distance as the loss function for gradient matching in the attack.
Our empirical investigation shows that the vulnerability ranking varies with
the loss function used. Gradient norm, which is commonly used as a
vulnerability proxy for gradient inversion attack, cannot explain this as it
remains constant regardless of the loss function for gradient matching. In this
paper, we propose a loss-aware vulnerability proxy (LAVP) for the first time.
LAVP refers to either the maximum or minimum eigenvalue of the Hessian with
respect to gradient matching loss at ground truth. This suggestion is based on
our theoretical findings regarding the local optimization of the gradient
inversion in proximity to the ground truth, which corresponds to the worst case
attack scenario. We demonstrate the effectiveness of LAVP on various
architectures and datasets, showing its consistent superiority over the
gradient norm in capturing sample vulnerabilities. The performance of each
proxy is measured in terms of Spearman&apos;s rank correlation with respect to
several similarity scores. This work will contribute to enhancing FL security
against any potential loss functions beyond L2 or cosine distance in the
future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1&quot;&gt;HyeongGwon Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1&quot;&gt;Yooshin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hanbyel Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1&quot;&gt;Jaesung Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junmo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12490">
<title>InstructVideo: Instructing Video Diffusion Models with Human Feedback. (arXiv:2312.12490v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12490</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as the de facto paradigm for video generation.
However, their reliance on web-scale data of varied quality often yields
results that are visually unappealing and misaligned with the textual prompts.
To tackle this problem, we propose InstructVideo to instruct text-to-video
diffusion models with human feedback by reward fine-tuning. InstructVideo has
two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by
generating through the full DDIM sampling chain, we recast reward fine-tuning
as editing. By leveraging the diffusion process to corrupt a sampled video,
InstructVideo requires only partial inference of the DDIM sampling chain,
reducing fine-tuning cost while improving fine-tuning efficiency. 2) To
mitigate the absence of a dedicated video reward model for human preferences,
we repurpose established image reward models, e.g., HPSv2. To this end, we
propose Segmental Video Reward, a mechanism to provide reward signals based on
segmental sparse sampling, and Temporally Attenuated Reward, a method that
mitigates temporal modeling degradation during fine-tuning. Extensive
experiments, both qualitative and quantitative, validate the practicality and
efficacy of using image reward models in InstructVideo, significantly enhancing
the visual quality of generated videos without compromising generalization
capabilities. Code and models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hangjie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yujie Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yining Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12491">
<title>StreamDiffusion: A Pipeline-level Solution for Real-time Interactive Generation. (arXiv:2312.12491v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12491</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce StreamDiffusion, a real-time diffusion pipeline designed for
interactive image generation. Existing diffusion models are adept at creating
images from text or image prompts, yet they often fall short in real-time
interaction. This limitation becomes particularly evident in scenarios
involving continuous input, such as Metaverse, live video streaming, and
broadcasting, where high throughput is imperative. To address this, we present
a novel approach that transforms the original sequential denoising into the
batching denoising process. Stream Batch eliminates the conventional
wait-and-interact approach and enables fluid and high throughput streams. To
handle the frequency disparity between data input and model throughput, we
design a novel input-output queue for parallelizing the streaming process.
Moreover, the existing diffusion pipeline uses classifier-free guidance(CFG),
which requires additional U-Net computation. To mitigate the redundant
computations, we propose a novel residual classifier-free guidance (RCFG)
algorithm that reduces the number of negative conditional denoising steps to
only one or even zero. Besides, we introduce a stochastic similarity
filter(SSF) to optimize power consumption. Our Stream Batch achieves around
1.5x speedup compared to the sequential denoising method at different denoising
levels. The proposed RCFG leads to speeds up to 2.05x higher than the
conventional CFG. Combining the proposed strategies and existing mature
acceleration tools makes the image-to-image generation achieve up-to 91.07fps
on one RTX4090, improving the throughputs of AutoPipline developed by Diffusers
over 59.56x. Furthermore, our proposed StreamDiffusion also significantly
reduces the energy consumption by 2.39x on one RTX3060 and 1.99x on one
RTX4090, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodaira_A/0/1/0/all/0/1&quot;&gt;Akio Kodaira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazama_T/0/1/0/all/0/1&quot;&gt;Toshiki Hazama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshimoto_T/0/1/0/all/0/1&quot;&gt;Takanori Yoshimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohno_K/0/1/0/all/0/1&quot;&gt;Kohei Ohno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsuhori_S/0/1/0/all/0/1&quot;&gt;Shogo Mitsuhori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugano_S/0/1/0/all/0/1&quot;&gt;Soichi Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hanying Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12494">
<title>DDOS: The Drone Depth and Obstacle Segmentation Dataset. (arXiv:2312.12494v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12494</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate depth and semantic segmentation are crucial for various computer
vision tasks. However, the scarcity of annotated real-world aerial datasets
poses a significant challenge for training and evaluating robust models.
Additionally, the detection and segmentation of thin objects, such as wires,
cables, and fences, present a critical concern for ensuring the safe operation
of drones. To address these limitations, we present a novel synthetic dataset
specifically designed for depth and semantic segmentation tasks in aerial
views. Leveraging photo-realistic rendering techniques, our dataset provides a
valuable resource for training models using a synthetic-supervision training
scheme while introducing new drone-specific metrics for depth accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolbeinsson_B/0/1/0/all/0/1&quot;&gt;Benedikt Kolbeinsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12540">
<title>Fixed-point Inversion for Text-to-image diffusion models. (arXiv:2312.12540v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12540</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-guided diffusion models offer powerful new ways to generate and
manipulate images. Several applications of these models, including image
editing interpolation, and semantic augmentation, require diffusion inversion.
This is the process of finding a noise seed that can be used to generate a
given image. Current techniques for inverting a given image can be slow or
inaccurate. The technical challenge for inverting the diffusion process arises
from an implicit equation over the latent that cannot be solved in closed form.
Previous approaches proposed to solve this issue by approximation or various
learning schemes. Here, we formulate the problem as a fixed-point equation
problem and solve it using fixed-point iterations, a well-studied approach in
numerical analysis. We further identify a source of inconsistency that
significantly hurts the inversion of real images encoded to the latent space.
We show how to correct it by applying a prompt-aware adjustment of the
encoding. Our solution, Fixed-point inversion, is much faster than previous
techniques like EDICT and Null-text, with similar inversion quality. It can be
combined with any pretrained diffusion model and requires no model training,
prompt tuning, or additional parameters. In a series of experiments, we find
that Fixed-point inversion shows improved results in several downstream tasks:
image editing, image interpolation, and generation of rare objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meiri_B/0/1/0/all/0/1&quot;&gt;Barak Meiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1&quot;&gt;Dvir Samuel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1&quot;&gt;Nir Darshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1&quot;&gt;Shai Avidan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1&quot;&gt;Rami Ben-Ari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12599">
<title>Unsupervised Segmentation of Colonoscopy Images. (arXiv:2312.12599v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12599</link>
<description rdf:parseType="Literal">&lt;p&gt;Colonoscopy plays a crucial role in the diagnosis and prognosis of various
gastrointestinal diseases. Due to the challenges of collecting large-scale
high-quality ground truth annotations for colonoscopy images, and more
generally medical images, we explore using self-supervised features from vision
transformers in three challenging tasks for colonoscopy images. Our results
indicate that image-level features learned from DINO models achieve image
classification performance comparable to fully supervised models, and
patch-level features contain rich semantic information for object detection.
Furthermore, we demonstrate that self-supervised features combined with
unsupervised segmentation can be used to discover multiple clinically relevant
structures in a fully unsupervised manner, demonstrating the tremendous
potential of applying these methods in medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Heming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luscher_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me L&amp;#xfc;scher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Becker_B/0/1/0/all/0/1&quot;&gt;Benjamin Gutierrez Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arus_Pous_J/0/1/0/all/0/1&quot;&gt;Josep Ar&amp;#xfa;s-Pous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biancalani_T/0/1/0/all/0/1&quot;&gt;Tommaso Biancalani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bigorgne_A/0/1/0/all/0/1&quot;&gt;Amelie Bigorgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Richmond_D/0/1/0/all/0/1&quot;&gt;David Richmond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12606">
<title>Optimizing Neural Networks with Gradient Lexicase Selection. (arXiv:2312.12606v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12606</link>
<description rdf:parseType="Literal">&lt;p&gt;One potential drawback of using aggregated performance measurement in machine
learning is that models may learn to accept higher errors on some training
cases as compromises for lower errors on others, with the lower errors actually
being instances of overfitting. This can lead to both stagnation at local
optima and poor generalization. Lexicase selection is an uncompromising method
developed in evolutionary computation, which selects models on the basis of
sequences of individual training case errors instead of using aggregated
metrics such as loss and accuracy. In this paper, we investigate how lexicase
selection, in its general form, can be integrated into the context of deep
learning to enhance generalization. We propose Gradient Lexicase Selection, an
optimization framework that combines gradient descent and lexicase selection in
an evolutionary fashion. Our experimental results demonstrate that the proposed
method improves the generalization performance of various widely-used deep
neural network architectures across three image classification benchmarks.
Additionally, qualitative analysis suggests that our method assists networks in
learning more diverse representations. Our source code is available on GitHub:
https://github.com/ld-ing/gradient-lexicase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Li Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spector_L/0/1/0/all/0/1&quot;&gt;Lee Spector&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12619">
<title>Hierarchical Vision Transformers for Context-Aware Prostate Cancer Grading in Whole Slide Images. (arXiv:2312.12619v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12619</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have ushered in a new era in computer vision,
showcasing unparalleled performance in many challenging tasks. However, their
practical deployment in computational pathology has largely been constrained by
the sheer size of whole slide images (WSIs), which result in lengthy input
sequences. Transformers faced a similar limitation when applied to long
documents, and Hierarchical Transformers were introduced to circumvent it.
Given the analogous challenge with WSIs and their inherent hierarchical
structure, Hierarchical Vision Transformers (H-ViTs) emerge as a promising
solution in computational pathology. This work delves into the capabilities of
H-ViTs, evaluating their efficiency for prostate cancer grading in WSIs. Our
results show that they achieve competitive performance against existing
state-of-the-art solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grisi_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Grisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1&quot;&gt;Geert Litjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laak_J/0/1/0/all/0/1&quot;&gt;Jeroen van der Laak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12634">
<title>MotionScript: Natural Language Descriptions for Expressive 3D Human Motions. (arXiv:2312.12634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes MotionScript, a motion-to-text conversion algorithm and
natural language representation for human body motions. MotionScript aims to
describe movements in greater detail and with more accuracy than previous
natural language approaches. Many motion datasets describe relatively objective
and simple actions with little variation on the way they are expressed (e.g.
sitting, walking, dribbling a ball). But for expressive actions that contain a
diversity of movements in the class (e.g. being sad, dancing), or for actions
outside the domain of standard motion capture datasets (e.g. stylistic walking,
sign-language), more specific and granular natural language descriptions are
needed. Our proposed MotionScript descriptions differ from existing natural
language representations in that it provides direct descriptions in natural
language instead of simple action labels or high-level human captions. To the
best of our knowledge, this is the first attempt at translating 3D motions to
natural language descriptions without requiring training data. Our experiments
show that when MotionScript representations are used in a text-to-motion neural
task, body movements are more accurately reconstructed, and large language
models can be used to generate unseen complex motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdian_P/0/1/0/all/0/1&quot;&gt;Payam Jome Yazdian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Eric Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1&quot;&gt;Angelica Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12635">
<title>RealCraft: Attention Control as A Solution for Zero-shot Long Video Editing. (arXiv:2312.12635v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12635</link>
<description rdf:parseType="Literal">&lt;p&gt;Although large-scale text-to-image generative models have shown promising
performance in synthesizing high-quality images, directly applying these models
to image editing remains a significant challenge. This challenge is further
amplified in video editing due to the additional dimension of time. Especially
for editing real videos as it necessitates maintaining a stable semantic layout
across the frames while executing localized edits precisely without disrupting
the existing backgrounds. In this paper, we propose \textit{RealCraft}, an
attention-control-based method for zero-shot editing in real videos. By
employing the object-centric manipulation of cross-attention between prompts
and frames and spatial-temporal attention within the frames, we achieve precise
shape-wise editing along with enhanced consistency. Our model can be used
directly with Stable Diffusion and operates without the need for additional
localized information. We showcase our zero-shot attention-control-based method
across a range of videos, demonstrating localized, high-fidelity, shape-precise
and time-consistent editing in videos of various lengths, up to 64 frames.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Shutong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokorny_F/0/1/0/all/0/1&quot;&gt;Florian T. Pokorny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12644">
<title>Rotational Augmented Noise2Inverse for Low-dose Computed Tomography Reconstruction. (arXiv:2312.12644v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12644</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a novel self-supervised method for Low Dose Computed
Tomography (LDCT) reconstruction. Reducing the radiation dose to patients
during a CT scan is a crucial challenge since the quality of the reconstruction
highly degrades because of low photons or limited measurements. Supervised deep
learning methods have shown the ability to remove noise in images but require
accurate ground truth which can be obtained only by performing additional
high-radiation CT scans. Therefore, we propose a novel self-supervised
framework for LDCT, in which ground truth is not required for training the
convolutional neural network (CNN). Based on the Noise2Inverse (N2I) method, we
enforce in the training loss the equivariant property of rotation
transformation, which is induced by the CT imaging system, to improve the
quality of the CT image in a lower dose. Numerical and experimental results
show that the reconstruction accuracy of N2I with sparse views is degrading
while the proposed rotational augmented Noise2Inverse (RAN2I) method keeps
better image quality over a different range of sampling angles. Finally, the
quantitative results demonstrate that RAN2I achieves higher image quality
compared to N2I, and experimental results of RAN2I on real projection data show
comparable performance to supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Perelli_A/0/1/0/all/0/1&quot;&gt;Alessandro Perelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12648">
<title>IS-DARTS: Stabilizing DARTS through Precise Measurement on Candidate Importance. (arXiv:2312.12648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12648</link>
<description rdf:parseType="Literal">&lt;p&gt;Among existing Neural Architecture Search methods, DARTS is known for its
efficiency and simplicity. This approach applies continuous relaxation of
network representation to construct a weight-sharing supernet and enables the
identification of excellent subnets in just a few GPU days. However,
performance collapse in DARTS results in deteriorating architectures filled
with parameter-free operations and remains a great challenge to the robustness.
To resolve this problem, we reveal that the fundamental reason is the biased
estimation of the candidate importance in the search space through theoretical
and experimental analysis, and more precisely select operations via
information-based measurements. Furthermore, we demonstrate that the excessive
concern over the supernet and inefficient utilization of data in bi-level
optimization also account for suboptimal results. We adopt a more realistic
objective focusing on the performance of subnets and simplify it with the help
of the information-based measurements. Finally, we explain theoretically why
progressively shrinking the width of the supernet is necessary and reduce the
approximation error of optimal weights in DARTS. Our proposed method, named
IS-DARTS, comprehensively improves DARTS and resolves the aforementioned
problems. Extensive experiments on NAS-Bench-201 and DARTS-based search space
demonstrate the effectiveness of IS-DARTS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hongyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Longjun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haonan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12649">
<title>Surf-CDM: Score-Based Surface Cold-Diffusion Model For Medical Image Segmentation. (arXiv:2312.12649v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12649</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have shown impressive performance for image generation,
often times outperforming other generative models. Since their introduction,
researchers have extended the powerful noise-to-image denoising pipeline to
discriminative tasks, including image segmentation. In this work we propose a
conditional score-based generative modeling framework for medical image
segmentation which relies on a parametric surface representation for the
segmentation masks. The surface re-parameterization allows the direct
application of standard diffusion theory, as opposed to when the mask is
represented as a binary mask. Moreover, we adapted an extended variant of the
diffusion technique known as the &quot;cold-diffusion&quot; where the diffusion model can
be constructed with deterministic perturbations instead of Gaussian noise,
which facilitates significantly faster convergence in the reverse diffusion. We
evaluated our method on the segmentation of the left ventricle from 65
transthoracic echocardiogram videos (2230 echo image frames) and compared its
performance to the most popular and widely used image segmentation models. Our
proposed model not only outperformed the compared methods in terms of
segmentation accuracy, but also showed potential in estimating segmentation
uncertainties for further downstream analyses due to its inherent generative
nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Fahim Ahmed Zaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacob_M/0/1/0/all/0/1&quot;&gt;Mathews Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Amanda Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sonka_M/0/1/0/all/0/1&quot;&gt;Milan Sonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12653">
<title>Diagnosis Of Takotsubo Syndrome By Robust Feature Selection From The Complex Latent Space Of DL-based Segmentation Network. (arXiv:2312.12653v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12653</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have shown significant correlations among segmented objects in
various medical imaging modalities and disease related pathologies. Several
studies showed that using hand crafted features for disease prediction neglects
the immense possibility to use latent features from deep learning (DL) models
which may reduce the overall accuracy of differential diagnosis. However,
directly using classification or segmentation models on medical to learn latent
features opt out robust feature selection and may lead to overfitting. To fill
this gap, we propose a novel feature selection technique using the latent space
of a segmentation model that can aid diagnosis. We evaluated our method in
differentiating a rare cardiac disease: Takotsubo Syndrome (TTS) from the ST
elevation myocardial infarction (STEMI) using echocardiogram videos (echo). TTS
can mimic clinical features of STEMI in echo and extremely hard to distinguish.
Our approach shows promising results in differential diagnosis of TTS with 82%
diagnosis accuracy beating the previous state-of-the-art (SOTA) approach.
Moreover, the robust feature selection technique using LASSO algorithm shows
great potential in reducing the redundant features and creates a robust
pipeline for short- and long-term disease prognoses in the downstream analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Fahim Ahmed Zaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_W/0/1/0/all/0/1&quot;&gt;Wahidul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Tarun Kanti Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Amanda Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12659">
<title>Expediting Contrastive Language-Image Pretraining via Self-distilled Encoders. (arXiv:2312.12659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12659</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision language pretraining (VLP) have been largely
attributed to the large-scale data collected from the web. However, uncurated
dataset contains weakly correlated image-text pairs, causing data inefficiency.
To address the issue, knowledge distillation have been explored at the expense
of extra image and text momentum encoders to generate teaching signals for
misaligned image-text pairs. In this paper, our goal is to resolve the
misalignment problem with an efficient distillation framework. To this end, we
propose ECLIPSE: Expediting Contrastive Language-Image Pretraining with
Self-distilled Encoders. ECLIPSE features a distinctive distillation
architecture wherein a shared text encoder is utilized between an online image
encoder and a momentum image encoder. This strategic design choice enables the
distillation to operate within a unified projected space of text embedding,
resulting in better performance. Based on the unified text embedding space,
ECLIPSE compensates for the additional computational cost of the momentum image
encoder by expediting the online image encoder. Through our extensive
experiments, we validate that there is a sweet spot between expedition and
distillation where the partial view from the expedited online image encoder
interacts complementarily with the momentum teacher. As a result, ECLIPSE
outperforms its counterparts while achieving substantial acceleration in
inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinhyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yeonsik Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Hwan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12661">
<title>Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pretraining. (arXiv:2312.12661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12661</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pretraining has emerged as a prominent approach
for training vision and text encoders with uncurated image-text pairs from the
web. To enhance data-efficiency, recent efforts have introduced additional
supervision terms that involve random-augmented views of the image. However,
since the image augmentation process is unaware of its text counterpart, this
procedure could cause various degrees of image-text misalignments during
training. Prior methods either disregarded this discrepancy or introduced
external models to mitigate the impact of misalignments during training. In
contrast, we propose a novel metric learning approach that capitalizes on these
misalignments as an additional training source, which we term &quot;Misalign,
Contrast then Distill (MCD)&quot;. Unlike previous methods that treat augmented
images and their text counterparts as simple positive pairs, MCD predicts the
continuous scales of misalignment caused by the augmentation. Our extensive
experimental results show that our proposed MCD achieves state-of-the-art
transferability in multiple classification and retrieval downstream datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yeonsik Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinhyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Hwan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12664">
<title>UnionDet: Union-Level Detector Towards Real-Time Human-Object Interaction Detection. (arXiv:2312.12664v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12664</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep neural networks have achieved significant progress in
detecting individual objects from an image. However, object detection is not
sufficient to fully understand a visual scene. Towards a deeper visual
understanding, the interactions between objects, especially humans and objects
are essential. Most prior works have obtained this information with a bottom-up
approach, where the objects are first detected and the interactions are
predicted sequentially by pairing the objects. This is a major bottleneck in
HOI detection inference time. To tackle this problem, we propose UnionDet, a
one-stage meta-architecture for HOI detection powered by a novel union-level
detector that eliminates this additional inference stage by directly capturing
the region of interaction. Our one-stage detector for human-object interaction
shows a significant reduction in interaction prediction time 4x~14x while
outperforming state-of-the-art methods on two public datasets: V-COCO and
HICO-DET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bumsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1&quot;&gt;Taeho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12668">
<title>Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm. (arXiv:2312.12668v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12668</link>
<description rdf:parseType="Literal">&lt;p&gt;The Forward-Forward (FF) Algorithm has been recently proposed to alleviate
the issues of backpropagation (BP) commonly used to train deep neural networks.
However, its current formulation exhibits limitations such as the generation of
negative data, slower convergence, and inadequate performance on complex tasks.
In this paper, we take the main ideas of FF and improve them by leveraging
channel-wise competitive learning in the context of convolutional neural
networks for image classification tasks. A layer-wise loss function is
introduced that promotes competitive learning and eliminates the need for
negative data construction. To enhance both the learning of compositional
features and feature space partitioning, a channel-wise feature separator and
extractor block is proposed that complements the competitive learning process.
Our method outperforms recent FF-based models on image classification tasks,
achieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST,
Fashion-MNIST, CIFAR-10 and CIFAR-100 respectively. Our approach bridges the
performance gap between FF learning and BP methods, indicating the potential of
our proposed approach to learn useful representations in a layer-wise modular
fashion, enabling more efficient and flexible learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papachristodoulou_A/0/1/0/all/0/1&quot;&gt;Andreas Papachristodoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1&quot;&gt;Christos Kyrkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timotheou_S/0/1/0/all/0/1&quot;&gt;Stelios Timotheou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theocharides_T/0/1/0/all/0/1&quot;&gt;Theocharis Theocharides&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12680">
<title>Trajectory Approximation of Video Based on Phase Correlation for Forward Facing Camera. (arXiv:2312.12680v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12680</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce an innovative approach for extracting
trajectories from a camera sensor in GPS-denied environments, leveraging visual
odometry. The system takes video footage captured by a forward-facing camera
mounted on a vehicle as input, with the output being a chain code representing
the camera&apos;s trajectory. The proposed methodology involves several key steps.
Firstly, we employ phase correlation between consecutive frames of the video to
extract essential information. Subsequently, we introduce a novel chain code
method termed &quot;dynamic chain code,&quot; which is based on the x-shift values
derived from the phase correlation. The third step involves determining
directional changes (forward, left, right) by establishing thresholds and
extracting the corresponding chain code. This extracted code is then stored in
a buffer for further processing. Notably, our system outperforms traditional
methods reliant on spatial features, exhibiting greater speed and robustness in
noisy environments. Importantly, our approach operates without external camera
calibration information. Moreover, by incorporating visual odometry, our system
enhances its accuracy in estimating camera motion, providing a more
comprehensive understanding of trajectory dynamics. Finally, the system
culminates in the visualization of the normalized camera motion trajectory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulkadhem_A/0/1/0/all/0/1&quot;&gt;Abdulkadhem A. Abdulkadhem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12691">
<title>How Good Are Deep Generative Models for Solving Inverse Problems?. (arXiv:2312.12691v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12691</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models, such as diffusion models, GANs, and IMLE, have shown
impressive capability in tackling inverse problems. However, the validity of
model-generated solutions w.r.t. the forward problem and the reliability of
associated uncertainty estimates remain understudied. This study evaluates
recent diffusion-based, GAN-based, and IMLE-based methods on three inverse
problems, i.e., $16\times$ super-resolution, colourization, and image
decompression. We assess the validity of these models&apos; outputs as solutions to
the inverse problems and conduct a thorough analysis of the reliability of the
models&apos; estimates of uncertainty over the solution. Overall, we find that the
IMLE-based CHIMLE method outperforms other methods in terms of producing valid
solutions and reliable uncertainty estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Shichong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moazeni_A/0/1/0/all/0/1&quot;&gt;Alireza Moazeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12716">
<title>BloomVQA: Assessing Hierarchical Multi-modal Comprehension. (arXiv:2312.12716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12716</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel VQA dataset, based on picture stories designed for
educating young children, that aims to facilitate comprehensive evaluation and
characterization of vision-language models on comprehension tasks. Unlike
current VQA datasets that often focus on fact-based memorization and simple
reasoning tasks without principled scientific grounding, we collect data
containing tasks reflecting different levels of comprehension and underlying
cognitive processes, as laid out in Bloom&apos;s Taxonomy, a classic framework
widely adopted in education research. The proposed BloomVQA dataset can be
mapped to a hierarchical graph-based representation of visual stories, enabling
automatic data augmentation and novel measures characterizing model consistency
across the underlying taxonomy. We demonstrate graded evaluation and
reliability analysis based on our proposed consistency metrics on
state-of-the-art vision-language models. Our results suggest that, while
current models achieve the most gain on low-level comprehension tasks, they
generally fall short on high-level tasks requiring more advanced comprehension
and cognitive skills, as 38.0% drop in VQA accuracy is observed comparing
lowest and highest level tasks. Furthermore, current models show consistency
patterns misaligned with human comprehension in various scenarios, suggesting
emergent structures of model behaviors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yunye Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_R/0/1/0/all/0/1&quot;&gt;Robik Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Claypoole_J/0/1/0/all/0/1&quot;&gt;Jared Claypoole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cogswell_M/0/1/0/all/0/1&quot;&gt;Michael Cogswell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Arijit Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Divakaran_A/0/1/0/all/0/1&quot;&gt;Ajay Divakaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12720">
<title>AdvST: Revisiting Data Augmentations for Single Domain Generalization. (arXiv:2312.12720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12720</link>
<description rdf:parseType="Literal">&lt;p&gt;Single domain generalization (SDG) aims to train a robust model against
unknown target domain shifts using data from a single source domain. Data
augmentation has been proven an effective approach to SDG. However, the utility
of standard augmentations, such as translate, or invert, has not been fully
exploited in SDG; practically, these augmentations are used as a part of a data
preprocessing procedure. Although it is intuitive to use many such
augmentations to boost the robustness of a model to out-of-distribution domain
shifts, we lack a principled approach to harvest the benefit brought from
multiple these augmentations. Here, we conceptualize standard data
augmentations with learnable parameters as semantics transformations that can
manipulate certain semantics of a sample, such as the geometry or color of an
image. Then, we propose Adversarial learning with Semantics Transformations
(AdvST) that augments the source domain data with semantics transformations and
learns a robust model with the augmented data. We theoretically show that AdvST
essentially optimizes a distributionally robust optimization objective defined
on a set of semantics distributions induced by the parameters of semantics
transformations. We demonstrate that AdvST can produce samples that expand the
coverage on target domain data. Compared with the state-of-the-art methods,
AdvST, despite being a simple method, is surprisingly competitive and achieves
the best average SDG performance on the Digits, PACS, and DomainNet datasets.
Our code is available at https://github.com/gtzheng/AdvST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guangtao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huai_M/0/1/0/all/0/1&quot;&gt;Mengdi Huai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aidong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12721">
<title>Cross-Modal Reasoning with Event Correlation for Video Question Answering. (arXiv:2312.12721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12721</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Question Answering (VideoQA) is a very attractive and challenging
research direction aiming to understand complex semantics of heterogeneous data
from two domains, i.e., the spatio-temporal video content and the word sequence
in question. Although various attention mechanisms have been utilized to manage
contextualized representations by modeling intra- and inter-modal relationships
of the two modalities, one limitation of the predominant VideoQA methods is the
lack of reasoning with event correlation, that is, sensing and analyzing
relationships among abundant and informative events contained in the video. In
this paper, we introduce the dense caption modality as a new auxiliary and
distill event-correlated information from it to infer the correct answer. To
this end, we propose a novel end-to-end trainable model, Event-Correlated Graph
Neural Networks (EC-GNNs), to perform cross-modal reasoning over information
from the three modalities (i.e., caption, video, and question). Besides the
exploitation of a brand new modality, we employ cross-modal reasoning modules
for explicitly modeling inter-modal relationships and aggregating relevant
information across different modalities, and we propose a question-guided
self-adaptive multi-modal fusion module to collect the question-oriented and
event-correlated evidence through multi-step reasoning. We evaluate our model
on two widely-used benchmark datasets and conduct an ablation study to justify
the effectiveness of each proposed component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Q/0/1/0/all/0/1&quot;&gt;Qinru Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12722">
<title>Fine-Grained Knowledge Selection and Restoration for Non-Exemplar Class Incremental Learning. (arXiv:2312.12722v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12722</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-exemplar class incremental learning aims to learn both the new and old
tasks without accessing any training data from the past. This strict
restriction enlarges the difficulty of alleviating catastrophic forgetting
since all techniques can only be applied to current task data. Considering this
challenge, we propose a novel framework of fine-grained knowledge selection and
restoration. The conventional knowledge distillation-based methods place too
strict constraints on the network parameters and features to prevent
forgetting, which limits the training of new tasks. To loose this constraint,
we proposed a novel fine-grained selective patch-level distillation to
adaptively balance plasticity and stability. Some task-agnostic patches can be
used to preserve the decision boundary of the old task. While some patches
containing the important foreground are favorable for learning the new task.
&lt;/p&gt;
&lt;p&gt;Moreover, we employ a task-agnostic mechanism to generate more realistic
prototypes of old tasks with the current task sample for reducing classifier
bias for fine-grained knowledge restoration. Extensive experiments on CIFAR100,
TinyImageNet and ImageNet-Subset demonstrate the effectiveness of our method.
Code is available at https://github.com/scok30/vit-cil.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1&quot;&gt;Jiang-Tian Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xialei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Ming-Ming Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12723">
<title>Multi-Clue Reasoning with Memory Augmentation for Knowledge-based Visual Question Answering. (arXiv:2312.12723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12723</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Question Answering (VQA) has emerged as one of the most challenging
tasks in artificial intelligence due to its multi-modal nature. However, most
existing VQA methods are incapable of handling Knowledge-based Visual Question
Answering (KB-VQA), which requires external knowledge beyond visible contents
to answer questions about a given image. To address this issue, we propose a
novel framework that endows the model with capabilities of answering more
general questions, and achieves a better exploitation of external knowledge
through generating Multiple Clues for Reasoning with Memory Neural Networks
(MCR-MemNN). Specifically, a well-defined detector is adopted to predict
image-question related relation phrases, each of which delivers two
complementary clues to retrieve the supporting facts from external knowledge
base (KB), which are further encoded into a continuous embedding space using a
content-addressable memory. Afterwards, mutual interactions between
visual-semantic representation and the supporting facts stored in memory are
captured to distill the most relevant information in three modalities (i.e.,
image, question, and KB). Finally, the optimal answer is predicted by choosing
the supporting fact with the highest score. We conduct extensive experiments on
two widely-used benchmarks. The experimental results well justify the
effectiveness of MCR-MemNN, as well as its superiority over other KB-VQA
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12726">
<title>Reducing Shape-Radiance Ambiguity in Radiance Fields with a Closed-Form Color Estimation Method. (arXiv:2312.12726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12726</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance field (NeRF) enables the synthesis of cutting-edge realistic
novel view images of a 3D scene. It includes density and color fields to model
the shape and radiance of a scene, respectively. Supervised by the photometric
loss in an end-to-end training manner, NeRF inherently suffers from the
shape-radiance ambiguity problem, i.e., it can perfectly fit training views but
does not guarantee decoupling the two fields correctly. To deal with this
issue, existing works have incorporated prior knowledge to provide an
independent supervision signal for the density field, including total variation
loss, sparsity loss, distortion loss, etc. These losses are based on general
assumptions about the density field, e.g., it should be smooth, sparse, or
compact, which are not adaptive to a specific scene. In this paper, we propose
a more adaptive method to reduce the shape-radiance ambiguity. The key is a
rendering method that is only based on the density field. Specifically, we
first estimate the color field based on the density field and posed images in a
closed form. Then NeRF&apos;s rendering process can proceed. We address the problems
in estimating the color field, including occlusion and non-uniformly
distributed views. Afterward, it is applied to regularize NeRF&apos;s density field.
As our regularization is guided by photometric loss, it is more adaptive
compared to existing ones. Experimental results show that our method improves
the density field of NeRF both qualitatively and quantitatively. Our code is
available at https://github.com/qihangGH/Closed-form-color-field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qihang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yafei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Keqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12729">
<title>Segment Anything Model Meets Image Harmonization. (arXiv:2312.12729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12729</link>
<description rdf:parseType="Literal">&lt;p&gt;Image harmonization is a crucial technique in image composition that aims to
seamlessly match the background by adjusting the foreground of composite
images. Current methods adopt either global-level or pixel-level feature
matching. Global-level feature matching ignores the proximity prior, treating
foreground and background as separate entities. On the other hand, pixel-level
feature matching loses contextual information. Therefore, it is necessary to
use the information from semantic maps that describe different objects to guide
harmonization. In this paper, we propose Semantic-guided Region-aware Instance
Normalization (SRIN) that can utilize the semantic segmentation maps output by
a pre-trained Segment Anything Model (SAM) to guide the visual consistency
learning of foreground and background features. Abundant experiments
demonstrate the superiority of our method for image harmonization over
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaohui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zhangxuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuoer Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jun Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaxiong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12730">
<title>A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models. (arXiv:2312.12730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12730</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient transfer learning (ETL) is receiving increasing attention to adapt
large pre-trained language-vision models on downstream tasks with a few labeled
samples. While significant progress has been made, we reveal that
state-of-the-art ETL approaches exhibit strong performance only in
narrowly-defined experimental setups, and with a careful adjustment of
hyperparameters based on a large corpus of labeled samples. In particular, we
make two interesting, and surprising empirical observations. First, to
outperform a simple Linear Probing baseline, these methods require to optimize
their hyper-parameters on each target task. And second, they typically
underperform -- sometimes dramatically -- standard zero-shot predictions in the
presence of distributional drifts. Motivated by the unrealistic assumptions
made in the existing literature, i.e., access to a large validation set and
case-specific grid-search for optimal hyperparameters, we propose a novel
approach that meets the requirements of real-world scenarios. More concretely,
we introduce a CLass-Adaptive linear Probe (CLAP) objective, whose balancing
term is optimized via an adaptation of the general Augmented Lagrangian method
tailored to this context. We comprehensively evaluate CLAP on a broad span of
datasets and scenarios, demonstrating that it consistently outperforms SoTA
approaches, while yet being a much more efficient alternative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1&quot;&gt;Julio Silva-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajimiri_S/0/1/0/all/0/1&quot;&gt;Sina Hajimiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12735">
<title>MetaSegNet: Metadata-collaborative Vision-Language Representation Learning for Semantic Segmentation of Remote Sensing Images. (arXiv:2312.12735v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12735</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of remote sensing images plays a vital role in a wide
range of Earth Observation (EO) applications, such as land use land cover
mapping, environment monitoring, and sustainable development. Driven by rapid
developments in Artificial Intelligence (AI), deep learning (DL) has emerged as
the mainstream tool for semantic segmentation and achieved many breakthroughs
in the field of remote sensing. However, the existing DL-based methods mainly
focus on unimodal visual data while ignoring the rich multimodal information
involved in the real world, usually demonstrating weak reliability and
generlization. Inspired by the success of Vision Transformers and large
language models, we propose a novel metadata-collaborative multimodal
segmentation network (MetaSegNet) that applies vision-language representation
learning for semantic segmentation of remote sensing images. Unlike the common
model structure that only uses unimodal visual data, we extract the key
characteristic (i.e. the climate zone) from freely available remote sensing
image metadata and transfer it into knowledge-based text prompts via the
generic ChatGPT. Then, we construct an image encoder, a text encoder and a
crossmodal attention fusion subnetwork to extract the image and text feature
and apply image-text interaction. Benefiting from such a design, the proposed
MetaSegNet demonstrates superior generalization and achieves competitive
accuracy with state-of-the-art semantic segmentation methods on the large-scale
OpenEarthMap dataset (68.6% mIoU) and Potsdam dataset (93.3% mean F1 score) as
well as LoveDA dataset (52.2% mIoU).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Libo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Sijun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shenghui Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12742">
<title>Cached Transformers: Improving Transformers with Differentiable Memory Cache. (arXiv:2312.12742v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12742</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a new Transformer model called Cached Transformer, which
uses Gated Recurrent Cached (GRC) attention to extend the self-attention
mechanism with a differentiable memory cache of tokens. GRC attention enables
attending to both past and current tokens, increasing the receptive field of
attention and allowing for exploring long-range dependencies. By utilizing a
recurrent gating unit to continuously update the cache, our model achieves
significant advancements in \textbf{six} language and vision tasks, including
language modeling, machine translation, ListOPs, image classification, object
detection, and instance segmentation. Furthermore, our approach surpasses
previous memory-based techniques in tasks such as language modeling and
displays the ability to be applied to a broader range of situations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wenqi Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinwei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12743">
<title>PointeNet: A Lightweight Framework for Effective and Efficient Point Cloud Analysis. (arXiv:2312.12743v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12743</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methodologies in point cloud analysis predominantly explore 3D
geometries, often achieved through the introduction of intricate learnable
geometric extractors in the encoder or by deepening networks with repeated
blocks. However, these approaches inevitably lead to a significant number of
learnable parameters, resulting in substantial computational costs and imposing
memory burdens on CPU/GPU. Additionally, the existing strategies are primarily
tailored for object-level point cloud classification and segmentation tasks,
with limited extensions to crucial scene-level applications, such as autonomous
driving. In response to these limitations, we introduce PointeNet, an efficient
network designed specifically for point cloud analysis. PointeNet distinguishes
itself with its lightweight architecture, low training cost, and plug-and-play
capability, effectively capturing representative features. The network consists
of a Multivariate Geometric Encoding (MGE) module and an optional
Distance-aware Semantic Enhancement (DSE) module. The MGE module employs
operations of sampling, grouping, and multivariate geometric aggregation to
lightweightly capture and adaptively aggregate multivariate geometric features,
providing a comprehensive depiction of 3D geometries. The DSE module, designed
for real-world autonomous driving scenarios, enhances the semantic perception
of point clouds, particularly for distant points. Our method demonstrates
flexibility by seamlessly integrating with a classification/segmentation head
or embedding into off-the-shelf 3D object detection networks, achieving notable
performance improvements at a minimal cost. Extensive experiments on
object-level datasets, including ModelNet40, ScanObjectNN, ShapeNetPart, and
the scene-level dataset KITTI, demonstrate the superior performance of
PointeNet over state-of-the-art methods in point cloud analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Lipeng Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xuefeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1&quot;&gt;Liangliang Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dingkun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Honghua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mingqiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12754">
<title>Spectral Prompt Tuning:Unveiling Unseen Classes for Zero-Shot Semantic Segmentation. (arXiv:2312.12754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12754</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, CLIP has found practical utility in the domain of pixel-level
zero-shot segmentation tasks. The present landscape features two-stage
methodologies beset by issues such as intricate pipelines and elevated
computational costs. While current one-stage approaches alleviate these
concerns and incorporate Visual Prompt Training (VPT) to uphold CLIP&apos;s
generalization capacity, they still fall short in fully harnessing CLIP&apos;s
potential for pixel-level unseen class demarcation and precise pixel
predictions. To further stimulate CLIP&apos;s zero-shot dense prediction capability,
we propose SPT-SEG, a one-stage approach that improves CLIP&apos;s adaptability from
image to pixel. Specifically, we initially introduce Spectral Prompt Tuning
(SPT), incorporating spectral prompts into the CLIP visual encoder&apos;s shallow
layers to capture structural intricacies of images, thereby enhancing
comprehension of unseen classes. Subsequently, we introduce the Spectral Guided
Decoder (SGD), utilizing both high and low-frequency information to steer the
network&apos;s spatial focus towards more prominent classification features,
enabling precise pixel-level prediction outcomes. Through extensive experiments
on two public datasets, we demonstrate the superiority of our method over
state-of-the-art approaches, performing well across all classes and
particularly excelling in handling unseen classes. Code is available
at:https://github.com/clearxu/SPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rongtao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shibiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Li Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Man Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12763">
<title>AMD:Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion. (arXiv:2312.12763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12763</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating realistic human motion sequences from text descriptions is a
challenging task that requires capturing the rich expressiveness of both
natural language and human motion.Recent advances in diffusion models have
enabled significant progress in human motion synthesis.However, existing
methods struggle to handle text inputs that describe complex or long motions.In
this paper, we propose the Adaptable Motion Diffusion (AMD) model, which
leverages a Large Language Model (LLM) to parse the input text into a sequence
of concise and interpretable anatomical scripts that correspond to the target
motion.This process exploits the LLM&apos;s ability to provide anatomical guidance
for complex motion synthesis.We then devise a two-branch fusion scheme that
balances the influence of the input text and the anatomical scripts on the
inverse diffusion process, which adaptively ensures the semantic fidelity and
diversity of the synthesized motion.Our method can effectively handle texts
with complex or long motion descriptions, where existing methods often fail.
Experiments on datasets with relatively more complex motions, such as CLCD1 and
CLCD2, demonstrate that our AMD significantly outperforms existing
state-of-the-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1&quot;&gt;Beibei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zikai Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12768">
<title>Mutual-modality Adversarial Attack with Semantic Perturbation. (arXiv:2312.12768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12768</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks constitute a notable threat to machine learning systems,
given their potential to induce erroneous predictions and classifications.
However, within real-world contexts, the essential specifics of the deployed
model are frequently treated as a black box, consequently mitigating the
vulnerability to such attacks. Thus, enhancing the transferability of the
adversarial samples has become a crucial area of research, which heavily relies
on selecting appropriate surrogate models. To address this challenge, we
propose a novel approach that generates adversarial attacks in a
mutual-modality optimization scheme. Our approach is accomplished by leveraging
the pre-trained CLIP model. Firstly, we conduct a visual attack on the clean
image that causes semantic perturbations on the aligned embedding space with
the other textual modality. Then, we apply the corresponding defense on the
textual modality by updating the prompts, which forces the re-matching on the
perturbed embedding space. Finally, to enhance the attack transferability, we
utilize the iterative training strategy on the visual attack and the textual
defense, where the two processes optimize from each other. We evaluate our
approach on several benchmark datasets and demonstrate that our mutual-modal
attack strategy can effectively produce high-transferable attacks, which are
stable regardless of the target networks. Our approach outperforms
state-of-the-art attack methods and can be readily deployed as a plug-and-play
solution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jingwen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruonan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Songhua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12773">
<title>Segmenting Messy Text: Detecting Boundaries in Text Derived from Historical Newspaper Images. (arXiv:2312.12773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12773</link>
<description rdf:parseType="Literal">&lt;p&gt;Text segmentation, the task of dividing a document into sections, is often a
prerequisite for performing additional natural language processing tasks.
Existing text segmentation methods have typically been developed and tested
using clean, narrative-style text with segments containing distinct topics.
Here we consider a challenging text segmentation task: dividing newspaper
marriage announcement lists into units of one announcement each. In many cases
the information is not structured into sentences, and adjacent segments are not
topically distinct from each other. In addition, the text of the announcements,
which is derived from images of historical newspapers via optical character
recognition, contains many typographical errors. As a result, these
announcements are not amenable to segmentation with existing techniques. We
present a novel deep learning-based model for segmenting such text and show
that it significantly outperforms an existing state-of-the-art method on our
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1&quot;&gt;Carol Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crone_P/0/1/0/all/0/1&quot;&gt;Phil Crone&lt;/a&gt; (Ancestry.com)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12789">
<title>SLP-Net:An efficient lightweight network for segmentation of skin lesions. (arXiv:2312.12789v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12789</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chenggang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaohui Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xianzhong Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12804">
<title>Multi-stages attention Breast cancer classification based on nonlinear spiking neural P neurons with autapses. (arXiv:2312.12804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12804</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast cancer(BC) is a prevalent type of malignant tumor in women. Early
diagnosis and treatment are vital for enhancing the patients&apos; survival rate.
Downsampling in deep networks may lead to loss of information, so for
compensating the detail and edge information and allowing convolutional neural
networks to pay more attention to seek the lesion region, we propose a
multi-stages attention architecture based on NSNP neurons with autapses. First,
unlike the single-scale attention acquisition methods of existing methods, we
set up spatial attention acquisition at each feature map scale of the
convolutional network to obtain an fusion global information on attention
guidance. Then we introduce a new type of NSNP variants called NSNP neurons
with autapses. Specifically, NSNP systems are modularized as feature encoders,
recoding the features extracted from convolutional neural network as well as
the fusion of attention information and preserve the key characteristic
elements in feature maps. This ensures the retention of valuable data while
gradually transforming high-dimensional complicated info into low-dimensional
ones. The proposed method is evaluated on the public dataset BreakHis at
various magnifications and classification tasks. It achieves a classification
accuracy of 96.32% at all magnification cases, outperforming state-of-the-art
methods. Ablation studies are also performed, verifying the proposed model&apos;s
efficacy. The source code is available at
XhuBobYoung/Breast-cancer-Classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hong Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaohui Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xianzhong Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12807">
<title>All but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models. (arXiv:2312.12807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12807</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Image models such as Stable Diffusion have shown impressive image
generation synthesis, thanks to the utilization of large-scale datasets.
However, these datasets may contain sexually explicit, copyrighted, or
undesirable content, which allows the model to directly generate them. Given
that retraining these large models on individual concept deletion requests is
infeasible, fine-tuning algorithms have been developed to tackle concept
erasing in diffusion models. While these algorithms yield good concept erasure,
they all present one of the following issues: 1) the corrupted feature space
yields synthesis of disintegrated objects, 2) the initially synthesized content
undergoes a divergence in both spatial structure and semantics in the generated
images, and 3) sub-optimal training updates heighten the model&apos;s susceptibility
to utility harm. These issues severely degrade the original utility of
generative models. In this work, we present a new approach that solves all of
these challenges. We take inspiration from the concept of classifier guidance
and propose a surgical update on the classifier guidance term while
constraining the drift of the unconditional score term. Furthermore, our
algorithm empowers the user to select an alternative to the erasing concept,
allowing for more controllability. Our experimental results show that our
algorithm not only erases the target concept effectively but also preserves the
model&apos;s generation capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Seunghoo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juhun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Simon S. Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12815">
<title>OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using Semantic Understanding in Mixed Reality. (arXiv:2312.12815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12815</link>
<description rdf:parseType="Literal">&lt;p&gt;One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1&quot;&gt;Luke Yoffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aditya Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1&quot;&gt;Tobias H&amp;#xf6;llerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12816">
<title>Object-aware Adaptive-Positivity Learning for Audio-Visual Question Answering. (arXiv:2312.12816v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12816</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the Audio-Visual Question Answering (AVQA) task that
aims to answer questions derived from untrimmed audible videos. To generate
accurate answers, an AVQA model is expected to find the most informative
audio-visual clues relevant to the given questions. In this paper, we propose
to explicitly consider fine-grained visual objects in video frames
(object-level clues) and explore the multi-modal relations(i.e., the object,
audio, and question) in terms of feature interaction and model optimization.
For the former, we present an end-to-end object-oriented network that adopts a
question-conditioned clue discovery module to concentrate audio/visual
modalities on respective keywords of the question and designs a
modality-conditioned clue collection module to highlight closely associated
audio segments or visual objects. For model optimization, we propose an
object-aware adaptive-positivity learning strategy that selects the highly
semantic-matched multi-modal pair as positivity. Specifically, we design two
object-aware contrastive loss functions to identify the highly relevant
question-object pairs and audio-object pairs, respectively. These selected
pairs are constrained to have larger similarity values than the mismatched
pairs. The positivity-selecting process is adaptive as the positivity pairs
selected in each video frame may be different. These two object-aware
objectives help the model understand which objects are exactly relevant to the
question and which are making sounds. Extensive experiments on the MUSIC-AVQA
dataset demonstrate the proposed method is effective in finding favorable
audio-visual clues and also achieves new state-of-the-art question-answering
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhangbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinxing Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12824">
<title>FedSODA: Federated Cross-assessment and Dynamic Aggregation for Histopathology Segmentation. (arXiv:2312.12824v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12824</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) for histopathology image segmentation involving
multiple medical sites plays a crucial role in advancing the field of accurate
disease diagnosis and treatment. However, it is still a task of great
challenges due to the sample imbalance across clients and large data
heterogeneity from disparate organs, variable segmentation tasks, and diverse
distribution. Thus, we propose a novel FL approach for histopathology nuclei
and tissue segmentation, FedSODA, via synthetic-driven cross-assessment
operation (SO) and dynamic stratified-layer aggregation (DA). Our SO constructs
a cross-assessment strategy to connect clients and mitigate the representation
bias under sample imbalance. Our DA utilizes layer-wise interaction and dynamic
aggregation to diminish heterogeneity and enhance generalization. The
effectiveness of our FedSODA has been evaluated on the most extensive
histopathology image segmentation dataset from 7 independent datasets. The code
is available at https://github.com/yuanzhang7/FedSODA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yaolei Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaoming Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Senhadji_L/0/1/0/all/0/1&quot;&gt;Lotfi Senhadji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yongyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12826">
<title>ReCo-Diff: Explore Retinex-Based Condition Strategy in Diffusion Model for Low-Light Image Enhancement. (arXiv:2312.12826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12826</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement (LLIE) has achieved promising performance by
employing conditional diffusion models. In this study, we propose ReCo-Diff, a
novel approach that incorporates Retinex-based prior as an additional
pre-processing condition to regulate the generating capabilities of the
diffusion model. ReCo-Diff first leverages a pre-trained decomposition network
to produce initial reflectance and illumination maps of the low-light image.
Then, an adjustment network is introduced to suppress the noise in the
reflectance map and brighten the illumination map, thus forming the learned
Retinex-based condition. The condition is integrated into a refinement network,
implementing Retinex-based conditional modules that offer sufficient guidance
at both feature- and image-levels. By treating Retinex theory as a condition,
ReCo-Diff presents a unique perspective for establishing an LLIE-specific
diffusion model. Extensive experiments validate the rationality and superiority
of our ReCo-Diff approach. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuhui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12828">
<title>TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training. (arXiv:2312.12828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12828</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) has demonstrated impressive
capabilities in open-vocabulary classification. The class token in the image
encoder is trained to capture the global features to distinguish different text
descriptions supervised by contrastive loss, making it highly effective for
single-label classification. However, it shows poor performance on multi-label
datasets because the global feature tends to be dominated by the most prominent
class and the contrastive nature of softmax operation aggravates it. In this
study, we observe that the multi-label classification results heavily rely on
discriminative local features but are overlooked by CLIP. As a result, we
dissect the preservation of patch-wise spatial information in CLIP and proposed
a local-to-global framework to obtain image tags. It comprises three steps: (1)
patch-level classification to obtain coarse scores; (2) dual-masking attention
refinement (DMAR) module to refine the coarse scores; (3) class-wise
reidentification (CWR) module to remedy predictions from a global perspective.
This framework is solely based on frozen CLIP and significantly enhances its
multi-label classification performance on various benchmarks without
dataset-specific training. Besides, to comprehensively assess the quality and
practicality of generated tags, we extend their application to the downstream
task, i.e., weakly supervised semantic segmentation (WSSS) with generated tags
as image-level pseudo labels. Experiments demonstrate that this
classify-then-segment paradigm dramatically outperforms other annotation-free
segmentation methods and validates the effectiveness of generated tags. Our
code is available at https://github.com/linyq2117/TagCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1&quot;&gt;Dongqin Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Binbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haifeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12833">
<title>Learning Exhaustive Correlation for Spectral Super-Resolution: Where Unified Spatial-Spectral Attention Meets Mutual Linear Dependence. (arXiv:2312.12833v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12833</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral super-resolution from the easily obtainable RGB image to
hyperspectral image (HSI) has drawn increasing interest in the field of
computational photography. The crucial aspect of spectral super-resolution lies
in exploiting the correlation within HSIs. However, two types of bottlenecks in
existing Transformers limit performance improvement and practical applications.
First, existing Transformers often separately emphasize either spatial-wise or
spectral-wise correlation, disrupting the 3D features of HSI and hindering the
exploitation of unified spatial-spectral correlation. Second, the existing
self-attention mechanism learns the correlation between pairs of tokens and
captures the full-rank correlation matrix, leading to its inability to
establish mutual linear dependence among multiple tokens. To address these
issues, we propose a novel Exhaustive Correlation Transformer (ECT) for
spectral super-resolution. First, we propose a Spectral-wise Discontinuous 3D
(SD3D) splitting strategy, which models unified spatial-spectral correlation by
simultaneously utilizing spatial-wise continuous splitting and spectral-wise
discontinuous splitting. Second, we propose a Dynamic Low-Rank Mapping (DLRM)
model, which captures mutual linear dependence among multiple tokens through a
dynamically calculated low-rank dependence map. By integrating unified
spatial-spectral attention with mutual linear dependence, our ECT can establish
exhaustive correlation within HSI. The experimental results on both simulated
and real data indicate that our method achieves state-of-the-art performance.
Codes and pretrained models will be available later.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_F/0/1/0/all/0/1&quot;&gt;Fenglong Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Youliang Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12838">
<title>FedA3I: Annotation Quality-Aware Aggregation for Federated Medical Image Segmentation Against Heterogeneous Annotation Noise. (arXiv:2312.12838v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12838</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has emerged as a promising paradigm for training
segmentation models on decentralized medical data, owing to its
privacy-preserving property. However, existing research overlooks the prevalent
annotation noise encountered in real-world medical datasets, which limits the
performance ceilings of FL. In this paper, we, for the first time, identify and
tackle this problem. For problem formulation, we propose a contour evolution
for modeling non-independent and identically distributed (Non-IID) noise across
pixels within each client and then extend it to the case of multi-source data
to form a heterogeneous noise model (\textit{i.e.}, Non-IID annotation noise
across clients). For robust learning from annotations with such two-level
Non-IID noise, we emphasize the importance of data quality in model
aggregation, allowing high-quality clients to have a greater impact on FL. To
achieve this, we propose \textbf{Fed}erated learning with \textbf{A}nnotation
qu\textbf{A}lity-aware \textbf{A}ggregat\textbf{I}on, named \textbf{FedA$^3$I},
by introducing a quality factor based on client-wise noise estimation.
Specifically, noise estimation at each client is accomplished through the
Gaussian mixture model and then incorporated into model aggregation in a
layer-wise manner to up-weight high-quality clients. Extensive experiments on
two real-world medical image segmentation datasets demonstrate the superior
performance of FedA$^3$I against the state-of-the-art approaches in dealing
with cross-client annotation noise. The code is available at
\color{blue}{https://github.com/wnn2000/FedAAAI}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1&quot;&gt;Nannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaobin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zengqiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12848">
<title>Quantum Annealing for Computer Vision Minimization Problems. (arXiv:2312.12848v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2312.12848</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer Vision (CV) labelling algorithms play a pivotal role in the domain
of low-level vision. For decades, it has been known that these problems can be
elegantly formulated as discrete energy minimization problems derived from
probabilistic graphical models (such as Markov Random Fields). Despite recent
advances in inference algorithms (such as graph-cut and message-passing
algorithms), the resulting energy minimization problems are generally viewed as
intractable. The emergence of quantum computations, which offer the potential
for faster solutions to certain problems than classical methods, has led to an
increased interest in utilizing quantum properties to overcome intractable
problems. Recently, there has also been a growing interest in Quantum Computer
Vision (QCV), with the hope of providing a credible alternative or assistant to
deep learning solutions in the field. This study investigates a new Quantum
Annealing based inference algorithm for CV discrete energy minimization
problems. Our contribution is focused on Stereo Matching as a significant CV
labeling problem. As a proof of concept, we also use a hybrid quantum-classical
solver provided by D-Wave System to compare our results with the best classical
inference algorithms in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Heidari_S/0/1/0/all/0/1&quot;&gt;Shahrokh Heidari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dinneen_M/0/1/0/all/0/1&quot;&gt;Michael J. Dinneen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Delmas_P/0/1/0/all/0/1&quot;&gt;Patrice Delmas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12856">
<title>SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing. (arXiv:2312.12856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12856</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing imagery, despite its broad applications in helping achieve
Sustainable Development Goals and tackle climate change, has not yet benefited
from the recent advancements of versatile, task-agnostic vision language models
(VLMs). A key reason is that the large-scale, semantically diverse image-text
dataset required for developing VLMs is still absent for remote sensing images.
Unlike natural images, remote sensing images and their associated text
descriptions cannot be efficiently collected from the public Internet at scale.
In this work, we bridge this gap by using geo-coordinates to automatically
connect open, unlabeled remote sensing images with rich semantics covered in
OpenStreetMap, and thus construct SkyScript, a comprehensive vision-language
dataset for remote sensing images, comprising 2.6 million image-text pairs
covering 29K distinct semantic tags. With continual pre-training on this
dataset, we obtain a VLM that surpasses baseline models with a 6.2% average
accuracy gain in zero-shot scene classification across seven benchmark
datasets. It also demonstrates the ability of zero-shot transfer for
fine-grained object attribute classification and cross-modal retrieval. We hope
this dataset can support the advancement of VLMs for various multi-modal tasks
in remote sensing, such as open-vocabulary classification, retrieval,
captioning, and text-to-image synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhecheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabha_R/0/1/0/all/0/1&quot;&gt;Rajanie Prabha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12865">
<title>RadEdit: stress-testing biomedical vision models via diffusion image editing. (arXiv:2312.12865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12865</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical imaging datasets are often small and biased, meaning that
real-world performance of predictive models can be substantially lower than
expected from internal testing. This work proposes using generative image
editing to simulate dataset shifts and diagnose failure modes of biomedical
vision models; this can be used in advance of deployment to assess readiness,
potentially reducing cost and patient harm. Existing editing methods can
produce undesirable changes, with spurious correlations learned due to the
co-occurrence of disease and treatment interventions, limiting practical
applicability. To address this, we train a text-to-image diffusion model on
multiple chest X-ray datasets and introduce a new editing method RadEdit that
uses multiple masks, if present, to constrain changes and ensure consistency in
the edited images. We consider three types of dataset shifts: acquisition
shift, manifestation shift, and population shift, and demonstrate that our
approach can diagnose failures and quantify model robustness without additional
data collection, complementing more qualitative tools for explainable AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1&quot;&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1&quot;&gt;Sam Bond-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro P. Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breugel_B/0/1/0/all/0/1&quot;&gt;Boris van Breugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel C. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1&quot;&gt;Valentina Salvatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1&quot;&gt;Maria T. A. Wetscherek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_H/0/1/0/all/0/1&quot;&gt;Hannah Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1&quot;&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1&quot;&gt;Ozan Oktay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1&quot;&gt;Maximilian Ilse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12870">
<title>The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective. (arXiv:2312.12870v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12870</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the thriving development of research related to egocentric
videos has provided a unique perspective for the study of conversational
interactions, where both visual and audio signals play a crucial role. While
most prior work focus on learning about behaviors that directly involve the
camera wearer, we introduce the Ego-Exocentric Conversational Graph Prediction
problem, marking the first attempt to infer exocentric conversational
interactions from egocentric videos. We propose a unified multi-modal,
multi-task framework -- Audio-Visual Conversational Attention (Av-CONV), for
the joint prediction of conversation behaviors -- speaking and listening -- for
both the camera wearer as well as all other social partners present in the
egocentric video. Specifically, we customize the self-attention mechanism to
model the representations across-time, across-subjects, and across-modalities.
To validate our method, we conduct experiments on a challenging egocentric
video dataset that includes first-person perspective, multi-speaker, and
multi-conversation scenarios. Our results demonstrate the superior performance
of our method compared to a series of baselines. We also present detailed
ablation studies to assess the contribution of each component in our model.
Project page: https://vjwq.github.io/AV-CONV/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wenqi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananthabhotla_I/0/1/0/all/0/1&quot;&gt;Ishwarya Ananthabhotla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1&quot;&gt;James M. Rehg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ithapu_V/0/1/0/all/0/1&quot;&gt;Vamsi Krishna Ithapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Ruohan Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12872">
<title>Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms. (arXiv:2312.12872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12872</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the analysis of the application effectiveness of the
integration of deep learning and computer vision technologies. Deep learning
achieves a historic breakthrough by constructing hierarchical neural networks,
enabling end-to-end feature learning and semantic understanding of images. The
successful experiences in the field of computer vision provide strong support
for training deep learning algorithms. The tight integration of these two
fields has given rise to a new generation of advanced computer vision systems,
significantly surpassing traditional methods in tasks such as machine vision
image classification and object detection. In this paper, typical image
classification cases are combined to analyze the superior performance of deep
neural network models while also pointing out their limitations in
generalization and interpretability, proposing directions for future
improvements. Overall, the efficient integration and development trend of deep
learning with massive visual data will continue to drive technological
breakthroughs and application expansion in the field of computer vision, making
it possible to build truly intelligent machine vision systems. This deepening
fusion paradigm will powerfully promote unprecedented tasks and functions in
computer vision, providing stronger development momentum for related
disciplines and industries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1&quot;&gt;Chang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qunwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12876">
<title>COVID-19 Diagnosis: ULGFBP-ResNet51 approach on the CT and the Chest X-ray Images Classification. (arXiv:2312.12876v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12876</link>
<description rdf:parseType="Literal">&lt;p&gt;The contagious and pandemic COVID-19 disease is currently considered as the
main health concern and posed widespread panic across human-beings. It affects
the human respiratory tract and lungs intensely. So that it has imposed
significant threats for premature death. Although, its early diagnosis can play
a vital role in revival phase, the radiography tests with the manual
intervention are a time-consuming process. Time is also limited for such manual
inspecting of numerous patients in the hospitals. Thus, the necessity of
automatic diagnosis on the chest X-ray or the CT images with a high efficient
performance is urgent. Toward this end, we propose a novel method, named as the
ULGFBP-ResNet51 to tackle with the COVID-19 diagnosis in the images. In fact,
this method includes Uniform Local Binary Pattern (ULBP), Gabor Filter (GF),
and ResNet51. According to our results, this method could offer superior
performance in comparison with the other methods, and attain maximum accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Esmaeili_V/0/1/0/all/0/1&quot;&gt;Vida Esmaeili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feghhi_M/0/1/0/all/0/1&quot;&gt;Mahmood Mohassel Feghhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shahdi_S/0/1/0/all/0/1&quot;&gt;Seyed Omid Shahdi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12877">
<title>Relightable and Animatable Neural Avatars from Videos. (arXiv:2312.12877v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12877</link>
<description rdf:parseType="Literal">&lt;p&gt;Lightweight creation of 3D digital avatars is a highly desirable but
challenging task. With only sparse videos of a person under unknown
illumination, we propose a method to create relightable and animatable neural
avatars, which can be used to synthesize photorealistic images of humans under
novel viewpoints, body poses, and lighting. The key challenge here is to
disentangle the geometry, material of the clothed body, and lighting, which
becomes more difficult due to the complex geometry and shadow changes caused by
body motions. To solve this ill-posed problem, we propose novel techniques to
better model the geometry and shadow changes. For geometry change modeling, we
propose an invertible deformation field, which helps to solve the inverse
skinning problem and leads to better geometry quality. To model the spatial and
temporal varying shading cues, we propose a pose-aware part-wise light
visibility network to estimate light occlusion. Extensive experiments on
synthetic and real datasets show that our approach reconstructs high-quality
geometry and generates realistic shadows under different body poses. Code and
data are available at
\url{https://wenbin-lin.github.io/RelightableAvatar-page/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wenbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chengwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1&quot;&gt;Jun-Hai Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Feng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12880">
<title>Testing the Segment Anything Model on radiology data. (arXiv:2312.12880v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12880</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models trained with large amounts of data have become a recent
and effective approach to predictive problem solving -- these have become known
as &quot;foundation models&quot; as they can be used as fundamental tools for other
applications. While the paramount examples of image classification (earlier)
and large language models (more recently) led the way, the Segment Anything
Model (SAM) was recently proposed and stands as the first foundation model for
image segmentation, trained on over 10 million images and with recourse to over
1 billion masks. However, the question remains -- what are the limits of this
foundation? Given that magnetic resonance imaging (MRI) stands as an important
method of diagnosis, we sought to understand whether SAM could be used for a
few tasks of zero-shot segmentation using MRI data. Particularly, we wanted to
know if selecting masks from the pool of SAM predictions could lead to good
segmentations.
&lt;/p&gt;
&lt;p&gt;Here, we provide a critical assessment of the performance of SAM on magnetic
resonance imaging data. We show that, while acceptable in a very limited set of
cases, the overall trend implies that these models are insufficient for MRI
segmentation across the whole volume, but can provide good segmentations in a
few, specific slices. More importantly, we note that while foundation models
trained on natural images are set to become key aspects of predictive
modelling, they may prove ineffective when used on other imaging modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Guilherme de Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rodrigues_N/0/1/0/all/0/1&quot;&gt;Nuno M. Rodrigues&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Sara Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Papanikolaou_N/0/1/0/all/0/1&quot;&gt;Nickolas Papanikolaou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12908">
<title>The Common Optical Music Recognition Evaluation Framework. (arXiv:2312.12908v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12908</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of Optical Music Recognition (OMR) systems is a rather difficult
magnitude to measure. There is no lingua franca shared among OMR datasets that
allows to compare systems&apos; performance on equal grounds, since most of them are
specialised on certain approaches. As a result, most state-of-the-art works
currently report metrics that cannot be compared directly. In this paper we
identify the need of a common music representation language and propose the
Music Tree Notation (MTN) format, thanks to which the definition of standard
metrics is possible. This format represents music as a set of primitives that
group together into higher-abstraction nodes, a compromise between the
expression of fully graph-based and sequential notation formats. We have also
developed a specific set of OMR metrics and a typeset score dataset as a proof
of concept of this idea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torras_P/0/1/0/all/0/1&quot;&gt;Pau Torras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Sanket Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1&quot;&gt;Alicia Forn&amp;#xe9;s&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12913">
<title>Produce Once, Utilize Twice for Anomaly Detection. (arXiv:2312.12913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12913</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual anomaly detection aims at classifying and locating the regions that
deviate from the normal appearance. Embedding-based methods and
reconstruction-based methods are two main approaches for this task. However,
they are either not efficient or not precise enough for the industrial
detection. To deal with this problem, we derive POUTA (Produce Once Utilize
Twice for Anomaly detection), which improves both the accuracy and efficiency
by reusing the discriminant information potential in the reconstructive
network. We observe that the encoder and decoder representations of the
reconstructive network are able to stand for the features of the original and
reconstructed image respectively. And the discrepancies between the symmetric
reconstructive representations provides roughly accurate anomaly information.
To refine this information, a coarse-to-fine process is proposed in POUTA,
which calibrates the semantics of each discriminative layer by the high-level
representations and supervision loss. Equipped with the above modules, POUTA is
endowed with the ability to provide a more precise anomaly location than the
prior arts. Besides, the representation reusage also enables to exclude the
feature extraction process in the discriminative network, which reduces the
parameters and improves the efficiency. Extensive experiments show that, POUTA
is superior or comparable to the prior methods with even less cost.
Furthermore, POUTA also achieves better performance than the state-of-the-art
few-shot anomaly detection methods without any special design, showing that
POUTA has strong ability to learn representations inherent in the training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Huiyuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengkan Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12917">
<title>Sign Language Production with Latent Motion Transformer. (arXiv:2312.12917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12917</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign Language Production (SLP) is the tough task of turning sign language
into sign videos. The main goal of SLP is to create these videos using a sign
gloss. In this research, we&apos;ve developed a new method to make high-quality sign
videos without using human poses as a middle step. Our model works in two main
parts: first, it learns from a generator and the video&apos;s hidden features, and
next, it uses another model to understand the order of these hidden features.
To make this method even better for sign videos, we make several significant
improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn
downsampled latent representations. (ii) In the second stage, we introduce
sequence-to-sequence attention to better leverage conditional information.
(iii) The separated two-stage training discards the realistic visual semantic
of the latent codes in the second stage. To endow the latent sequences semantic
information, we extend the token-level autoregressive latent codes learning
with perceptual loss and reconstruction loss for the prior model with visual
perception. Compared with previous state-of-the-art approaches, our model
performs consistently better on two word-level sign language datasets, i.e.,
WLASL and NMFs-CSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Taiyi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qipeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12954">
<title>TADAP: Trajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised features in winter driving conditions. (arXiv:2312.12954v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12954</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of the drivable area in all conditions is crucial for autonomous
driving and advanced driver assistance systems. However, the amount of labeled
data in adverse driving conditions is limited, especially in winter, and
supervised methods generalize poorly to conditions outside the training
distribution. For easy adaption to all conditions, the need for human
annotation should be removed from the learning process. In this paper,
Trajectory-Aided Drivable area Auto-labeling with Pre-trained self-supervised
features (TADAP) is presented for automated annotation of the drivable area in
winter driving conditions. A sample of the drivable area is extracted based on
the trajectory estimate from the global navigation satellite system. Similarity
with the sample area is determined based on pre-trained self-supervised visual
features. Image areas similar to the sample area are considered to be drivable.
These TADAP labels were evaluated with a novel winter-driving dataset,
collected in varying driving scenes. A prediction model trained with the TADAP
labels achieved a +9.6 improvement in intersection over union compared to the
previous state-of-the-art of self-supervised drivable area detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alamikkotervo_E/0/1/0/all/0/1&quot;&gt;Eerik Alamikkotervo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojala_R/0/1/0/all/0/1&quot;&gt;Risto Ojala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seppanen_A/0/1/0/all/0/1&quot;&gt;Alvari Sepp&amp;#xe4;nen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tammi_K/0/1/0/all/0/1&quot;&gt;Kari Tammi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12961">
<title>Radar Fields: An Extension of Radiance Fields to SAR. (arXiv:2312.12961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12961</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiance fields have been a major breakthrough in the field of inverse
rendering, novel view synthesis and 3D modeling of complex scenes from
multi-view image collections. Since their introduction, it was shown that they
could be extended to other modalities such as LiDAR, radio frequencies, X-ray
or ultrasound. In this paper, we show that, despite the important difference
between optical and synthetic aperture radar (SAR) image formation models, it
is possible to extend radiance fields to radar images thus presenting the first
&quot;radar fields&quot;. This allows us to learn surface models using only collections
of radar images, similar to how regular radiance fields are learned and with
the same computational complexity on average. Thanks to similarities in how
both fields are defined, this work also shows a potential for hybrid methods
combining both optical and SAR images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehret_T/0/1/0/all/0/1&quot;&gt;Thibaud Ehret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mari_R/0/1/0/all/0/1&quot;&gt;Roger Mar&amp;#xed;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derksen_D/0/1/0/all/0/1&quot;&gt;Dawa Derksen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasnier_N/0/1/0/all/0/1&quot;&gt;Nicolas Gasnier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12970">
<title>D3Former: Jointly Learning Repeatable Dense Detectors and Feature-enhanced Descriptors via Saliency-guided Transformer. (arXiv:2312.12970v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12970</link>
<description rdf:parseType="Literal">&lt;p&gt;Establishing accurate and representative matches is a crucial step in
addressing the point cloud registration problem. A commonly employed approach
involves detecting keypoints with salient geometric features and subsequently
mapping these keypoints from one frame of the point cloud to another. However,
methods within this category are hampered by the repeatability of the sampled
keypoints. In this paper, we introduce a saliency-guided trans\textbf{former},
referred to as \textit{D3Former}, which entails the joint learning of
repeatable \textbf{D}ense \textbf{D}etectors and feature-enhanced
\textbf{D}escriptors. The model comprises a Feature Enhancement Descriptor
Learning (FEDL) module and a Repetitive Keypoints Detector Learning (RKDL)
module. The FEDL module utilizes a region attention mechanism to enhance
feature distinctiveness, while the RKDL module focuses on detecting repeatable
keypoints to enhance matching capabilities. Extensive experimental results on
challenging indoor and outdoor benchmarks demonstrate that our proposed method
consistently outperforms state-of-the-art point cloud matching methods.
Notably, tests on 3DLoMatch, even with a low overlap ratio, show that our
method consistently outperforms recently published approaches such as RoReg and
RoITr. For instance, with the number of extracted keypoints reduced to 250, the
registration recall scores for RoReg, RoITr, and our method are 64.3\%, 73.6\%,
and 76.5\%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junjie Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1&quot;&gt;Qiujie Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qiong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1&quot;&gt;Shiqing Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Caiming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12990">
<title>Multi-task Learning To Improve Semantic Segmentation Of CBCT Scans Using Image Reconstruction. (arXiv:2312.12990v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12990</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation is a crucial task in medical image processing,
essential for segmenting organs or lesions such as tumors. In this study we aim
to improve automated segmentation in CBCTs through multi-task learning. To
evaluate effects on different volume qualities, a CBCT dataset is synthesised
from the CT Liver Tumor Segmentation Benchmark (LiTS) dataset. To improve
segmentation, two approaches are investigated. First, we perform multi-task
learning to add morphology based regularization through a volume reconstruction
task. Second, we use this reconstruction task to reconstruct the best quality
CBCT (most similar to the original CT), facilitating denoising effects. We
explore both holistic and patch-based approaches. Our findings reveal that,
especially using a patch-based approach, multi-task learning improves
segmentation in most cases and that these results can further be improved by
our denoising approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tschuchnig_M/0/1/0/all/0/1&quot;&gt;Maximilian Ernst Tschuchnig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Coste_Marin_J/0/1/0/all/0/1&quot;&gt;Julia Coste-Marin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Steininger_P/0/1/0/all/0/1&quot;&gt;Philipp Steininger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gadermayr_M/0/1/0/all/0/1&quot;&gt;Michael Gadermayr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12995">
<title>Aggregating Multiple Bio-Inspired Image Region Classifiers For Effective And Lightweight Visual Place Recognition. (arXiv:2312.12995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12995</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual place recognition (VPR) enables autonomous systems to localize
themselves within an environment using image information. While VPR techniques
built upon a Convolutional Neural Network (CNN) backbone dominate
state-of-the-art VPR performance, their high computational requirements make
them unsuitable for platforms equipped with low-end hardware. Recently, a
lightweight VPR system based on multiple bio-inspired classifiers, dubbed
DrosoNets, has been proposed, achieving great computational efficiency at the
cost of reduced absolute place retrieval performance. In this work, we propose
a novel multi-DrosoNet localization system, dubbed RegionDrosoNet, with
significantly improved VPR performance, while preserving a low-computational
profile. Our approach relies on specializing distinct groups of DrosoNets on
differently sliced partitions of the original image, increasing extrinsic model
differentiation. Furthermore, we introduce a novel voting module to combine the
outputs of all DrosoNets into the final place prediction which considers
multiple top refence candidates from each DrosoNet. RegionDrosoNet outperforms
other lightweight VPR techniques when dealing with both appearance changes and
viewpoint variations. Moreover, it competes with computationally expensive
methods on some benchmark datasets at a small fraction of their online
inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcanjo_B/0/1/0/all/0/1&quot;&gt;Bruno Arcanjo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrarini_B/0/1/0/all/0/1&quot;&gt;Bruno Ferrarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fasli_M/0/1/0/all/0/1&quot;&gt;Maria Fasli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1&quot;&gt;Klaus D. McDonald-Maier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1&quot;&gt;Shoaib Ehsan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13008">
<title>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision. (arXiv:2312.13008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13008</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised approaches for video have shown impressive results in video
understanding tasks. However, unlike early works that leverage temporal
self-supervision, current state-of-the-art methods primarily rely on tasks from
the image domain (e.g., contrastive learning) that do not explicitly promote
the learning of temporal features. We identify two factors that limit existing
temporal self-supervision: 1) tasks are too simple, resulting in saturated
training performance, and 2) we uncover shortcuts based on local appearance
statistics that hinder the learning of high-level features. To address these
issues, we propose 1) a more challenging reformulation of temporal
self-supervision as frame-level (rather than clip-level) recognition tasks and
2) an effective augmentation strategy to mitigate shortcuts. Our model extends
a representation of single video frames, pre-trained through contrastive
learning, with a transformer that we train through temporal self-supervision.
We demonstrate experimentally that our more challenging frame-level task
formulations and the removal of shortcuts drastically improve the quality of
features learned through temporal self-supervision. The generalization
capability of our self-supervised video method is evidenced by its
state-of-the-art performance in a wide range of high-level semantic tasks,
including video retrieval, action classification, and video attribute
recognition (such as object and scene identification), as well as low-level
temporal correspondence tasks like video object segmentation and pose tracking.
Additionally, we show that the video representations learned through our method
exhibit increased robustness to the input perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1&quot;&gt;Ishan Rajendrakumar Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1&quot;&gt;Simon Jenni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13016">
<title>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis. (arXiv:2312.13016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13016</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DiffPortrait3D, a conditional diffusion model that is capable of
synthesizing 3D-consistent photo-realistic novel views from as few as a single
in-the-wild portrait. Specifically, given a single RGB input, we aim to
synthesize plausible but consistent facial details rendered from novel camera
views with retained both identity and facial expression. In lieu of
time-consuming optimization and fine-tuning, our zero-shot method generalizes
well to arbitrary face portraits with unposed camera views, extreme facial
expressions, and diverse artistic depictions. At its core, we leverage the
generative prior of 2D diffusion models pre-trained on large-scale image
datasets as our rendering backbone, while the denoising is guided with
disentangled attentive control of appearance and camera pose. To achieve this,
we first inject the appearance context from the reference image into the
self-attention layers of the frozen UNets. The rendering view is then
manipulated with a novel conditional control module that interprets the camera
pose by watching a condition image of a crossed subject from the same view.
Furthermore, we insert a trainable cross-view attention module to enhance view
consistency, which is further strengthened with a novel 3D-aware noise
generation process during inference. We demonstrate state-of-the-art results
both qualitatively and quantitatively on our challenging in-the-wild and
multi-view benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;You Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoxian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Lingjie Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13027">
<title>Doubly Perturbed Task-Free Continual Learning. (arXiv:2312.13027v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13027</link>
<description rdf:parseType="Literal">&lt;p&gt;Task-free online continual learning (TF-CL) is a challenging problem where
the model incrementally learns tasks without explicit task information.
Although training with entire data from the past, present as well as future is
considered as the gold standard, naive approaches in TF-CL with the current
samples may be conflicted with learning with samples in the future, leading to
catastrophic forgetting and poor plasticity. Thus, a proactive consideration of
an unseen future sample in TF-CL becomes imperative. Motivated by this
intuition, we propose a novel TF-CL framework considering future samples and
show that injecting adversarial perturbations on both input data and
decision-making is effective. Then, we propose a novel method named Doubly
Perturbed Continual Learning (DPCL) to efficiently implement these input and
decision-making perturbations. Specifically, for input perturbation, we propose
an approximate perturbation method that injects noise into the input data as
well as the feature vector and then interpolates the two perturbed samples. For
decision-making process perturbation, we devise multiple stochastic
classifiers. We also investigate a memory management scheme and learning rate
scheduling reflecting our proposed double perturbations. We demonstrate that
our proposed method outperforms the state-of-the-art baseline methods by large
margins on various TF-CL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1&quot;&gt;Min-hwan Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Se Young Chun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13053">
<title>Quantifying Bias in Text-to-Image Generative Models. (arXiv:2312.13053v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13053</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias in text-to-image (T2I) models can propagate unfair social
representations and may be used to aggressively market ideas or push
controversial agendas. Existing T2I model bias evaluation methods only focus on
social biases. We look beyond that and instead propose an evaluation
methodology to quantify general biases in T2I generative models, without any
preconceived notions. We assess four state-of-the-art T2I models and compare
their baseline bias characteristics to their respective variants (two for
each), where certain biases have been intentionally induced. We propose three
evaluation metrics to assess model biases including: (i) Distribution bias,
(ii) Jaccard hallucination and (iii) Generative miss-rate. We conduct two
evaluation studies, modelling biases under general, and task-oriented
conditions, using a marketing scenario as the domain for the latter. We also
quantify social biases to compare our findings to related works. Finally, our
methodology is transferred to evaluate captioned-image datasets and measure
their bias. Our approach is objective, domain-agnostic and consistently
measures different forms of T2I model biases. We have developed a web
application and practical implementation of what has been proposed in this
work, which is at https://huggingface.co/spaces/JVice/try-before-you-bias. A
video series with demonstrations is available at
https://www.youtube.com/channel/UCk-0xyUyT0MSd_hkp4jQt1Q
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vice_J/0/1/0/all/0/1&quot;&gt;Jordan Vice&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1&quot;&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1&quot;&gt;Ajmal Mian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13066">
<title>PPEA-Depth: Progressive Parameter-Efficient Adaptation for Self-Supervised Monocular Depth Estimation. (arXiv:2312.13066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13066</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised monocular depth estimation is of significant importance with
applications spanning across autonomous driving and robotics. However, the
reliance on self-supervision introduces a strong static-scene assumption,
thereby posing challenges in achieving optimal performance in dynamic scenes,
which are prevalent in most real-world situations. To address these issues, we
propose PPEA-Depth, a Progressive Parameter-Efficient Adaptation approach to
transfer a pre-trained image model for self-supervised depth estimation. The
training comprises two sequential stages: an initial phase trained on a dataset
primarily composed of static scenes, succeeded by an expansion to more
intricate datasets involving dynamic scenes. To facilitate this process, we
design compact encoder and decoder adapters to enable parameter-efficient
tuning, allowing the network to adapt effectively. They not only uphold
generalized patterns from pre-trained image models but also retain knowledge
gained from the preceding phase into the subsequent one. Extensive experiments
demonstrate that PPEA-Depth achieves state-of-the-art performance on KITTI,
CityScapes and DDAD datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yue-Jiang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ying-Tian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fang-Lue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13071">
<title>Point Deformable Network with Enhanced Normal Embedding for Point Cloud Analysis. (arXiv:2312.13071v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13071</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently MLP-based methods have shown strong performance in point cloud
analysis. Simple MLP architectures are able to learn geometric features in
local point groups yet fail to model long-range dependencies directly. In this
paper, we propose Point Deformable Network (PDNet), a concise MLP-based network
that can capture long-range relations with strong representation ability.
Specifically, we put forward Point Deformable Aggregation Module (PDAM) to
improve representation capability in both long-range dependency and adaptive
aggregation among points. For each query point, PDAM aggregates information
from deformable reference points rather than points in limited local areas. The
deformable reference points are generated data-dependent, and we initialize
them according to the input point positions. Additional offsets and modulation
scalars are learned on the whole point features, which shift the deformable
reference points to the regions of interest. We also suggest estimating the
normal vector for point clouds and applying Enhanced Normal Embedding (ENE) to
the geometric extractors to improve the representation ability of single-point.
Extensive experiments and ablation studies on various benchmarks demonstrate
the effectiveness and superiority of our PDNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xingyilang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liangchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13081">
<title>BEVSeg2TP: Surround View Camera Bird&apos;s-Eye-View Based Joint Vehicle Segmentation and Ego Vehicle Trajectory Prediction. (arXiv:2312.13081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13081</link>
<description rdf:parseType="Literal">&lt;p&gt;Trajectory prediction is, naturally, a key task for vehicle autonomy. While
the number of traffic rules is limited, the combinations and uncertainties
associated with each agent&apos;s behaviour in real-world scenarios are nearly
impossible to encode. Consequently, there is a growing interest in
learning-based trajectory prediction. The proposed method in this paper
predicts trajectories by considering perception and trajectory prediction as a
unified system. In considering them as unified tasks, we show that there is the
potential to improve the performance of perception. To achieve these goals, we
present BEVSeg2TP - a surround-view camera bird&apos;s-eye-view-based joint vehicle
segmentation and ego vehicle trajectory prediction system for autonomous
vehicles. The proposed system uses a network trained on multiple camera views.
The images are transformed using several deep learning techniques to perform
semantic segmentation of objects, including other vehicles, in the scene. The
segmentation outputs are fused across the camera views to obtain a
comprehensive representation of the surrounding vehicles from the
bird&apos;s-eye-view perspective. The system further predicts the future trajectory
of the ego vehicle using a spatiotemporal probabilistic network (STPN) to
optimize trajectory prediction. This network leverages information from
encoder-decoder transformers and joint vehicle segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Sushil Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arindam Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halton_M/0/1/0/all/0/1&quot;&gt;Mark Halton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13090">
<title>Perception Test 2023: A Summary of the First Challenge And Outcome. (arXiv:2312.13090v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13090</link>
<description rdf:parseType="Literal">&lt;p&gt;The First Perception Test challenge was held as a half-day workshop alongside
the IEEE/CVF International Conference on Computer Vision (ICCV) 2023, with the
goal of benchmarking state-of-the-art video models on the recently proposed
Perception Test benchmark. The challenge had six tracks covering low-level and
high-level tasks, with both a language and non-language interface, across
video, audio, and text modalities, and covering: object tracking, point
tracking, temporal action localisation, temporal sound localisation,
multiple-choice video question-answering, and grounded video
question-answering. We summarise in this report the task descriptions, metrics,
baselines, and results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyward_J/0/1/0/all/0/1&quot;&gt;Joseph Heyward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1&quot;&gt;Dima Damen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica P&amp;#x103;tr&amp;#x103;ucean&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13091">
<title>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading. (arXiv:2312.13091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13091</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing an avatar from a portrait image has many applications in
multimedia, but remains a challenging research problem. Extracting reflectance
maps and geometry from one image is ill-posed: recovering geometry is a
one-to-many mapping problem and reflectance and light are difficult to
disentangle. Accurate geometry and reflectance can be captured under the
controlled conditions of a light stage, but it is costly to acquire large
datasets in this fashion. Moreover, training solely with this type of data
leads to poor generalization with in-the-wild images. This motivates the
introduction of MoSAR, a method for 3D avatar generation from monocular images.
We propose a semi-supervised training scheme that improves generalization by
learning from both light stage and in-the-wild datasets. This is achieved using
a novel differentiable shading formulation. We show that our approach
effectively disentangles the intrinsic face parameters, producing relightable
avatars. As a result, MoSAR estimates a richer set of skin reflectance maps,
and generates more realistic avatars than existing state-of-the-art methods. We
also introduce a new dataset, named FFHQ-UV-Intrinsics, the first public
dataset providing intrisic face attributes at scale (diffuse, specular, ambient
occlusion and translucency maps) for a total of 10k subjects. The project
website and the dataset are available on the following link:
https://ubisoftlaforge.github.io/character/mosar
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dib_A/0/1/0/all/0/1&quot;&gt;Abdallah Dib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafemann_L/0/1/0/all/0/1&quot;&gt;Luiz Gustavo Hafemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Got_E/0/1/0/all/0/1&quot;&gt;Emeline Got&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_T/0/1/0/all/0/1&quot;&gt;Trevor Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fadaeinejad_A/0/1/0/all/0/1&quot;&gt;Amin Fadaeinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_R/0/1/0/all/0/1&quot;&gt;Rafael M. O. Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonneau_M/0/1/0/all/0/1&quot;&gt;Marc-Andre Carbonneau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13100">
<title>SEER-ZSL: Semantic Encoder-Enhanced Representations for Generalized Zero-Shot Learning. (arXiv:2312.13100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13100</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Zero-Shot Learning (GZSL) recognizes unseen classes by
transferring knowledge from the seen classes, depending on the inherent
interactions between visual and semantic data. However, the discrepancy between
well-prepared training data and unpredictable real-world test scenarios remains
a significant challenge. This paper introduces a dual strategy to address the
generalization gap. Firstly, we incorporate semantic information through an
innovative encoder. This encoder effectively integrates class-specific semantic
information by targeting the performance disparity, enhancing the produced
features to enrich the semantic space for class-specific attributes. Secondly,
we refine our generative capabilities using a novel compositional loss
function. This approach generates discriminative classes, effectively
classifying both seen and unseen classes. In addition, we extend the
exploitation of the learned latent space by utilizing controlled semantic
inputs, ensuring the robustness of the model in varying environments. This
approach yields a model that outperforms the state-of-the-art models in terms
of both generalization and diverse settings, notably without requiring
hyperparameter tuning or domain-specific adaptations. We also propose a set of
novel evaluation metrics to provide a more detailed assessment of the
reliability and reproducibility of the results. The complete code is made
available on https://github.com/william-heyden/SEER-ZeroShotLearning/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heyden_W/0/1/0/all/0/1&quot;&gt;William Heyden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullah_H/0/1/0/all/0/1&quot;&gt;Habib Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddiqui_M/0/1/0/all/0/1&quot;&gt;M. Salman Siddiqui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machot_F/0/1/0/all/0/1&quot;&gt;Fadi Al Machot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13102">
<title>SpecNeRF: Gaussian Directional Encoding for Specular Reflections. (arXiv:2312.13102v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13102</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields have achieved remarkable performance in modeling the
appearance of 3D scenes. However, existing approaches still struggle with the
view-dependent appearance of glossy surfaces, especially under complex lighting
of indoor environments. Unlike existing methods, which typically assume distant
lighting like an environment map, we propose a learnable Gaussian directional
encoding to better model the view-dependent effects under near-field lighting
conditions. Importantly, our new directional encoding captures the
spatially-varying nature of near-field lighting and emulates the behavior of
prefiltered environment maps. As a result, it enables the efficient evaluation
of preconvolved specular color at any 3D location with varying roughness
coefficients. We further introduce a data-driven geometry prior that helps
alleviate the shape radiance ambiguity in reflection modeling. We show that our
Gaussian directional encoding and geometry prior significantly improve the
modeling of challenging specular reflections in neural radiance fields, which
helps decompose appearance into more physically meaningful components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Li Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_V/0/1/0/all/0/1&quot;&gt;Vasu Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1&quot;&gt;Haithem Turki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Changil Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sander_P/0/1/0/all/0/1&quot;&gt;Pedro Sander&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhofer_M/0/1/0/all/0/1&quot;&gt;Michael Zollh&amp;#xf6;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardt_C/0/1/0/all/0/1&quot;&gt;Christian Richardt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13103">
<title>Exploring Multimodal Large Language Models for Radiology Report Error-checking. (arXiv:2312.13103v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.13103</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes one of the first clinical applications of multimodal
large language models (LLMs) as an assistant for radiologists to check errors
in their reports. We created an evaluation dataset from two real-world
radiology datasets (MIMIC-CXR and IU-Xray), with 1,000 subsampled reports each.
A subset of original reports was modified to contain synthetic errors by
introducing various type of mistakes. The evaluation contained two difficulty
levels: SIMPLE for binary error-checking and COMPLEX for identifying error
types. LLaVA (Large Language and Visual Assistant) variant models, including
our instruction-tuned model, were used for the evaluation. Additionally, a
domain expert evaluation was conducted on a small test set. At the SIMPLE
level, the LLaVA v1.5 model outperformed other publicly available models.
Instruction tuning significantly enhanced performance by 47.4% and 25.4% on
MIMIC-CXR and IU-Xray data, respectively. The model also surpassed the domain
experts accuracy in the MIMIC-CXR dataset by 1.67%. Notably, among the subsets
(N=21) of the test set where a clinician did not achieve the correct
conclusion, the LLaVA ensemble mode correctly identified 71.4% of these cases.
This study marks a promising step toward utilizing multi-modal LLMs to enhance
diagnostic accuracy in radiology. The ensemble model demonstrated comparable
performance to clinicians, even capturing errors overlooked by humans.
Nevertheless, future work is needed to improve the model ability to identify
the types of inconsistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinge Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yunsoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keller_E/0/1/0/all/0/1&quot;&gt;Eva C. Keller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_J/0/1/0/all/0/1&quot;&gt;Jamie Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1&quot;&gt;Adam P. Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pontikos_N/0/1/0/all/0/1&quot;&gt;Nikolas Pontikos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1&quot;&gt;Zina Ibrahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_P/0/1/0/all/0/1&quot;&gt;Paul Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1&quot;&gt;Michelle C. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Honghan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13104">
<title>Optimizing Ego Vehicle Trajectory Prediction: The Graph Enhancement Approach. (arXiv:2312.13104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13104</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the trajectory of an ego vehicle is a critical component of
autonomous driving systems. Current state-of-the-art methods typically rely on
Deep Neural Networks (DNNs) and sequential models to process front-view images
for future trajectory prediction. However, these approaches often struggle with
perspective issues affecting object features in the scene. To address this, we
advocate for the use of Bird&apos;s Eye View (BEV) perspectives, which offer unique
advantages in capturing spatial relationships and object homogeneity. In our
work, we leverage Graph Neural Networks (GNNs) and positional encoding to
represent objects in a BEV, achieving competitive performance compared to
traditional DNN-based methods. While the BEV-based approach loses some detailed
information inherent to front-view images, we balance this by enriching the BEV
data by representing it as a graph where relationships between the objects in a
scene are captured effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Sushil Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aryan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sistu_G/0/1/0/all/0/1&quot;&gt;Ganesh Sistu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halton_M/0/1/0/all/0/1&quot;&gt;Mark Halton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1&quot;&gt;Ciar&amp;#xe1;n Eising&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13108">
<title>ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation. (arXiv:2312.13108v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13108</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphical User Interface (GUI) automation holds significant promise for
assisting users with complex tasks, thereby boosting human productivity.
Existing works leveraging Large Language Model (LLM) or LLM-based AI agents
have shown capabilities in automating tasks on Android and Web platforms.
However, these tasks are primarily aimed at simple device usage and
entertainment operations. This paper presents a novel benchmark, AssistGUI, to
evaluate whether models are capable of manipulating the mouse and keyboard on
the Windows platform in response to user-requested tasks. We carefully
collected a set of 100 tasks from nine widely-used software applications, such
as, After Effects and MS Word, each accompanied by the necessary project files
for better evaluation. Moreover, we propose an advanced Actor-Critic Embodied
Agent framework, which incorporates a sophisticated GUI parser driven by an
LLM-agent and an enhanced reasoning mechanism adept at handling lengthy
procedural tasks. Our experimental results reveal that our GUI Parser and
Reasoning mechanism outshine existing methods in performance. Nevertheless, the
potential remains substantial, with the best model attaining only a 46% success
rate on our benchmark. We conclude with a thorough analysis of the current
methods&apos; limitations, setting the stage for future breakthroughs in this
domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1&quot;&gt;Difei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zechen Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_M/0/1/0/all/0/1&quot;&gt;Mingyu Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peiran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1&quot;&gt;Dongxing Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qinchen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xiangwu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hengxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1&quot;&gt;Mike Zheng Shou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13114">
<title>Investigating Color Illusions from the Perspective of Computational Color Constancy. (arXiv:2312.13114v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13114</link>
<description rdf:parseType="Literal">&lt;p&gt;Color constancy and color illusion perception are two phenomena occurring in
the human visual system, which can help us reveal unknown mechanisms of human
perception. For decades computer vision scientists have developed numerous
color constancy methods, which estimate the reflectance of the surface by
discounting the illuminant. However, color illusions have not been analyzed in
detail in the field of computational color constancy, which we find surprising
since the relationship they share is significant and may let us design more
robust systems. We argue that any model that can reproduce our sensation on
color illusions should also be able to provide pixel-wise estimates of the
light source. In other words, we suggest that the analysis of color illusions
helps us to improve the performance of the existing global color constancy
methods, and enable them to provide pixel-wise estimates for scenes illuminated
by multiple light sources. In this study, we share the outcomes of our
investigation in which we take several color constancy methods and modify them
to reproduce the behavior of the human visual system on color illusions. Also,
we show that parameters purely extracted from illusions are able to improve the
performance of color constancy methods. A noteworthy outcome is that our
strategy based on the investigation of color illusions outperforms the
state-of-the-art methods that are specifically designed to transform global
color constancy algorithms into multi-illuminant algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulucan_O/0/1/0/all/0/1&quot;&gt;Oguzhan Ulucan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulucan_D/0/1/0/all/0/1&quot;&gt;Diclehan Ulucan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebner_M/0/1/0/all/0/1&quot;&gt;Marc Ebner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13116">
<title>VSR-Net: Vessel-like Structure Rehabilitation Network with Graph Clustering. (arXiv:2312.13116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13116</link>
<description rdf:parseType="Literal">&lt;p&gt;The morphologies of vessel-like structures, such as blood vessels and nerve
fibres, play significant roles in disease diagnosis, e.g., Parkinson&apos;s disease.
Deep network-based refinement segmentation methods have recently achieved
promising vessel-like structure segmentation results. There are still two
challenges: (1) existing methods have limitations in rehabilitating subsection
ruptures in segmented vessel-like structures; (2) they are often overconfident
in predicted segmentation results. To tackle these two challenges, this paper
attempts to leverage the potential of spatial interconnection relationships
among subsection ruptures from the structure rehabilitation perspective. Based
on this, we propose a novel Vessel-like Structure Rehabilitation Network
(VSR-Net) to rehabilitate subsection ruptures and improve the model calibration
based on coarse vessel-like structure segmentation results. VSR-Net first
constructs subsection rupture clusters with Curvilinear Clustering Module
(CCM). Then, the well-designed Curvilinear Merging Module (CMM) is applied to
rehabilitate the subsection ruptures to obtain the refined vessel-like
structures. Extensive experiments on five 2D/3D medical image datasets show
that VSR-Net significantly outperforms state-of-the-art (SOTA) refinement
segmentation methods with lower calibration error. Additionally, we provide
quantitative analysis to explain the morphological difference between the
rehabilitation results of VSR-Net and ground truth (GT), which is smaller than
SOTA methods and GT, demonstrating that our method better rehabilitates
vessel-like structures by restoring subsection ruptures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haili Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13127">
<title>Pixel-to-Abundance Translation: Conditional Generative Adversarial Networks Based on Patch Transformer for Hyperspectral Unmixing. (arXiv:2312.13127v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.13127</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectral unmixing is a significant challenge in hyperspectral image
processing. Existing unmixing methods utilize prior knowledge about the
abundance distribution to solve the regularization optimization problem, where
the difficulty lies in choosing appropriate prior knowledge and solving the
complex regularization optimization problem. To solve these problems, we
propose a hyperspectral conditional generative adversarial network (HyperGAN)
method as a generic unmixing framework, based on the following assumption: the
unmixing process from pixel to abundance can be regarded as a transformation of
two modalities with an internal specific relationship. The proposed HyperGAN is
composed of a generator and discriminator, the former completes the modal
conversion from mixed hyperspectral pixel patch to the abundance of
corresponding endmember of the central pixel and the latter is used to
distinguish whether the distribution and structure of generated abundance are
the same as the true ones. We propose hyperspectral image (HSI) Patch
Transformer as the main component of the generator, which utilize adaptive
attention score to capture the internal pixels correlation of the HSI patch and
leverage the spatial-spectral information in a fine-grained way to achieve
optimization of the unmixing process. Experiments on synthetic data and real
hyperspectral data achieve impressive results compared to state-of-the-art
competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Longfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_H/0/1/0/all/0/1&quot;&gt;Hongyun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xianghai Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13139">
<title>Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. (arXiv:2312.13139v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13139</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative pre-trained models have demonstrated remarkable effectiveness in
language and vision domains by learning useful representations. In this paper,
we extend the scope of this effectiveness by showing that visual robot
manipulation can significantly benefit from large-scale video generative
pre-training. We introduce GR-1, a straightforward GPT-style model designed for
multi-task language-conditioned visual robot manipulation. GR-1 takes as inputs
a language instruction, a sequence of observation images, and a sequence of
robot states. It predicts robot actions as well as future images in an
end-to-end manner. Thanks to a flexible design, GR-1 can be seamlessly
finetuned on robot data after pre-trained on a large-scale video dataset. We
perform extensive experiments on the challenging CALVIN benchmark and a real
robot. On CALVIN benchmark, our method outperforms state-of-the-art baseline
methods and improves the success rate from 88.9% to 94.9%. In the setting of
zero-shot unseen scene generalization, GR-1 improves the success rate from
53.3% to 85.4%. In real robot experiments, GR-1 also outperforms baseline
methods and shows strong potentials in generalization to unseen scenes and
objects. We provide inaugural evidence that a unified GPT-style transformer,
augmented with large-scale video generative pre-training, exhibits remarkable
generalization to multi-task visual robot manipulation. Project page:
https://GR1-Manipulation.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hongtao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1&quot;&gt;Ya Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1&quot;&gt;Chilam Cheang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangzeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiafeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinghang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1&quot;&gt;Tao Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13150">
<title>Splatter Image: Ultra-Fast Single-View 3D Reconstruction. (arXiv:2312.13150v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13150</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the Splatter Image, an ultra-fast approach for monocular 3D
object reconstruction which operates at 38 FPS. Splatter Image is based on
Gaussian Splatting, which has recently brought real-time rendering, fast
training, and excellent scaling to multi-view reconstruction. For the first
time, we apply Gaussian Splatting in a monocular reconstruction setting. Our
approach is learning-based, and, at test time, reconstruction only requires the
feed-forward evaluation of a neural network. The main innovation of Splatter
Image is the surprisingly straightforward design: it uses a 2D image-to-image
network to map the input image to one 3D Gaussian per pixel. The resulting
Gaussians thus have the form of an image, the Splatter Image. We further extend
the method to incorporate more than one image as input, which we do by adding
cross-view attention. Owning to the speed of the renderer (588 FPS), we can use
a single GPU for training while generating entire images at each iteration in
order to optimize perceptual metrics like LPIPS. On standard benchmarks, we
demonstrate not only fast reconstruction but also better results than recent
and much more expensive baselines in terms of PSNR, LPIPS, and other metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szymanowicz_S/0/1/0/all/0/1&quot;&gt;Stanislaw Szymanowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1&quot;&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1&quot;&gt;Andrea Vedaldi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13162">
<title>Brain-Inspired Visual Odometry: Balancing Speed and Interpretability through a System of Systems Approach. (arXiv:2312.13162v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we address the critical challenge of balancing speed and
accuracy while maintaining interpretablity in visual odometry (VO) systems, a
pivotal aspect in the field of autonomous navigation and robotics. Traditional
VO systems often face a trade-off between computational speed and the precision
of pose estimation. To tackle this issue, we introduce an innovative system
that synergistically combines traditional VO methods with a specifically
tailored fully connected network (FCN). Our system is unique in its approach to
handle each degree of freedom independently within the FCN, placing a strong
emphasis on causal inference to enhance interpretability. This allows for a
detailed and accurate assessment of relative pose error (RPE) across various
degrees of freedom, providing a more comprehensive understanding of parameter
variations and movement dynamics in different environments. Notably, our system
demonstrates a remarkable improvement in processing speed without compromising
accuracy. In certain scenarios, it achieves up to a 5% reduction in Root Mean
Square Error (RMSE), showcasing its ability to effectively bridge the gap
between speed and accuracy that has long been a limitation in VO research. This
advancement represents a significant step forward in developing more efficient
and reliable VO systems, with wide-ranging applications in real-time navigation
and robotic systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabrizi_H/0/1/0/all/0/1&quot;&gt;Habib Boloorchi Tabrizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crick_C/0/1/0/all/0/1&quot;&gt;Christopher Crick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13216">
<title>Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps. (arXiv:2312.13216v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13216</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in self-supervised representation learning has resulted in
models that are capable of extracting image features that are not only
effective at encoding image level, but also pixel-level, semantics. These
features have been shown to be effective for dense visual semantic
correspondence estimation, even outperforming fully-supervised methods.
Nevertheless, current self-supervised approaches still fail in the presence of
challenging image characteristics such as symmetries and repeated parts. To
address these limitations, we propose a new approach for semantic
correspondence estimation that supplements discriminative self-supervised
features with 3D understanding via a weak geometric spherical prior. Compared
to more involved 3D pipelines, our model only requires weak viewpoint
information, and the simplicity of our spherical representation enables us to
inject informative geometric priors into the model during training. We propose
a new evaluation metric that better accounts for repeated part and
symmetry-induced mistakes. We present results on the challenging SPair-71k
dataset, where we show that our approach demonstrates is capable of
distinguishing between symmetric views and repeated parts across many object
categories, and also demonstrate that we can generalize to unseen classes on
the AwA dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariotti_O/0/1/0/all/0/1&quot;&gt;Octave Mariotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aodha_O/0/1/0/all/0/1&quot;&gt;Oisin Mac Aodha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1&quot;&gt;Hakan Bilen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13219">
<title>Interactive Visual Task Learning for Robots. (arXiv:2312.13219v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.13219</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for robots to learn novel visual concepts and tasks
via in-situ linguistic interactions with human users. Previous approaches have
either used large pre-trained visual models to infer novel objects zero-shot,
or added novel concepts along with their attributes and representations to a
concept hierarchy. We extend the approaches that focus on learning visual
concept hierarchies by enabling them to learn novel concepts and solve unseen
robotics tasks with them. To enable a visual concept learner to solve robotics
tasks one-shot, we developed two distinct techniques. Firstly, we propose a
novel approach, Hi-Viscont(HIerarchical VISual CONcept learner for Task), which
augments information of a novel concept to its parent nodes within a concept
hierarchy. This information propagation allows all concepts in a hierarchy to
update as novel concepts are taught in a continual learning setting. Secondly,
we represent a visual task as a scene graph with language annotations, allowing
us to create novel permutations of a demonstrated task zero-shot in-situ. We
present two sets of results. Firstly, we compare Hi-Viscont with the baseline
model (FALCON) on visual question answering(VQA) in three domains. While being
comparable to the baseline model on leaf level concepts, Hi-Viscont achieves an
improvement of over 9% on non-leaf concepts on average. We compare our model&apos;s
performance against the baseline FALCON model. Our framework achieves 33%
improvements in success rate metric, and 19% improvements in the object level
accuracy compared to the baseline model. With both of these results we
demonstrate the ability of our model to learn tasks and concepts in a continual
learning setting on the robot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Weiwei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_A/0/1/0/all/0/1&quot;&gt;Anant Sah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_N/0/1/0/all/0/1&quot;&gt;Nakul Gopalan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09169">
<title>Rich Action-semantic Consistent Knowledge for Early Action Prediction. (arXiv:2201.09169v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09169</link>
<description rdf:parseType="Literal">&lt;p&gt;Early action prediction (EAP) aims to recognize human actions from a part of
action execution in ongoing videos, which is an important task for many
practical applications. Most prior works treat partial or full videos as a
whole, ignoring rich action knowledge hidden in videos, i.e., semantic
consistencies among different partial videos. In contrast, we partition
original partial or full videos to form a new series of partial videos and mine
the Action-Semantic Consistent Knowledge (ASCK) among these new partial videos
evolving in arbitrary progress levels. Moreover, a novel Rich Action-semantic
Consistent Knowledge network (RACK) under the teacher-student framework is
proposed for EAP. Firstly, we use a two-stream pre-trained model to extract
features of videos. Secondly, we treat the RGB or flow features of the partial
videos as nodes and their action semantic consistencies as edges. Next, we
build a bi-directional semantic graph for the teacher network and a
single-directional semantic graph for the student network to model rich ASCK
among partial videos. The MSE and MMD losses are incorporated as our
distillation loss to enrich the ASCK of partial videos from the teacher to the
student network. Finally, we obtain the final prediction by summering the
logits of different subnetworks and applying a softmax layer. Extensive
experiments and ablative studies have been conducted, demonstrating the
effectiveness of modeling rich ASCK for EAP. With the proposed RACK, we have
achieved state-of-the-art performance on three benchmarks. The code is
available at https://github.com/lily2lab/RACK.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02980">
<title>3D Object Detection from Images for Autonomous Driving: A Survey. (arXiv:2202.02980v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02980</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object detection from images, one of the fundamental and challenging
problems in autonomous driving, has received increasing attention from both
industry and academia in recent years. Benefiting from the rapid development of
deep learning technologies, image-based 3D detection has achieved remarkable
progress. Particularly, more than 200 works have studied this problem from 2015
to 2021, encompassing a broad spectrum of theories, algorithms, and
applications. However, to date no recent survey exists to collect and organize
this knowledge. In this paper, we fill this gap in the literature and provide
the first comprehensive survey of this novel and continuously growing research
field, summarizing the most commonly used pipelines for image-based 3D
detection and deeply analyzing each of their components. Additionally, we also
propose two new taxonomies to organize the state-of-the-art methods into
different categories, with the intent of providing a more systematic review of
existing methods and facilitating fair comparisons with future works. In
retrospect of what has been achieved so far, we also analyze the current
challenges in the field and discuss future directions for image-based 3D
detection research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simonelli_A/0/1/0/all/0/1&quot;&gt;Andrea Simonelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.07207">
<title>Beyond Grounding: Extracting Fine-Grained Event Hierarchies Across Modalities. (arXiv:2206.07207v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.07207</link>
<description rdf:parseType="Literal">&lt;p&gt;Events describe happenings in our world that are of importance. Naturally,
understanding events mentioned in multimedia content and how they are related
forms an important way of comprehending our world. Existing literature can
infer if events across textual and visual (video) domains are identical (via
grounding) and thus, on the same semantic level. However, grounding fails to
capture the intricate cross-event relations that exist due to the same events
being referred to on many semantic levels. For example, in Figure 1, the
abstract event of &quot;war&quot; manifests at a lower semantic level through subevents
&quot;tanks firing&quot; (in video) and airplane &quot;shot&quot; (in text), leading to a
hierarchical, multimodal relationship between the events.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose the task of extracting event hierarchies from
multimodal (video and text) data to capture how the same event manifests itself
in different modalities at different semantic levels. This reveals the
structure of events and is critical to understanding them. To support research
on this task, we introduce the Multimodal Hierarchical Events (MultiHiEve)
dataset. Unlike prior video-language datasets, MultiHiEve is composed of news
video-article pairs, which makes it rich in event hierarchies. We densely
annotate a part of the dataset to construct the test benchmark. We show the
limitations of state-of-the-art unimodal and multimodal baselines on this task.
Further, we address these limitations via a new weakly supervised model,
leveraging only unannotated video-article pairs from MultiHiEve. We perform a
thorough evaluation of our proposed method which demonstrates improved
performance on this task and highlight opportunities for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayyubi_H/0/1/0/all/0/1&quot;&gt;Hammad A. Ayyubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_C/0/1/0/all/0/1&quot;&gt;Christopher Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chum_L/0/1/0/all/0/1&quot;&gt;Lovish Chum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lokesh_R/0/1/0/all/0/1&quot;&gt;Rahul Lokesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yulei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xuande Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Jaywon Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Sounak Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shih-Fu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.14719">
<title>In Search of Projectively Equivariant Networks. (arXiv:2209.14719v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.14719</link>
<description rdf:parseType="Literal">&lt;p&gt;Equivariance of linear neural network layers is well studied. In this work,
we relax the equivariance condition to only be true in a projective sense. We
propose a way to construct a projectively equivariant neural network through
building a standard equivariant network where the linear group representations
acting on each intermediate feature space are &quot;multiplicatively modified lifts&quot;
of projective group representations. By theoretically studying the relation of
projectively and linearly equivariant linear layers, we show that our approach
is the most general possible when building a network out of linear layers. The
theory is showcased in two simple experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bokman_G/0/1/0/all/0/1&quot;&gt;Georg B&amp;#xf6;kman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flinth_A/0/1/0/all/0/1&quot;&gt;Axel Flinth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahl_F/0/1/0/all/0/1&quot;&gt;Fredrik Kahl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03087">
<title>Iterative Vision-and-Language Navigation. (arXiv:2210.03087v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03087</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Iterative Vision-and-Language Navigation (IVLN), a paradigm for
evaluating language-guided agents navigating in a persistent environment over
time. Existing Vision-and-Language Navigation (VLN) benchmarks erase the
agent&apos;s memory at the beginning of every episode, testing the ability to
perform cold-start navigation with no prior information. However, deployed
robots occupy the same environment for long periods of time. The IVLN paradigm
addresses this disparity by training and evaluating VLN agents that maintain
memory across tours of scenes that consist of up to 100 ordered
instruction-following Room-to-Room (R2R) episodes, each defined by an
individual language instruction and a target path. We present discrete and
continuous Iterative Room-to-Room (IR2R) benchmarks comprising about 400 tours
each in 80 indoor scenes. We find that extending the implicit memory of
high-performing transformer VLN agents is not sufficient for IVLN, but agents
that build maps can benefit from environment persistence, motivating a renewed
focus on map-building agents in VLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krantz_J/0/1/0/all/0/1&quot;&gt;Jacob Krantz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Shurjo Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1&quot;&gt;Jason Corso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_P/0/1/0/all/0/1&quot;&gt;Peter Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Stefan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1&quot;&gt;Jesse Thomason&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15563">
<title>Multimodal Transformer Distillation for Audio-Visual Synchronization. (arXiv:2210.15563v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15563</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual synchronization aims to determine whether the mouth movements
and speech in the video are synchronized. VocaLiST reaches state-of-the-art
performance by incorporating multimodal Transformers to model audio-visual
interact information. However, it requires high computing resources, making it
impractical for real-world applications. This paper proposed an MTDVocaLiST
model, which is trained by our proposed multimodal Transformer distillation
(MTD) loss. MTD loss enables MTDVocaLiST model to deeply mimic the
cross-attention distribution and value-relation in the Transformer of VocaLiST.
Additionally, we harness uncertainty weighting to fully exploit the interaction
information across all layers. Our proposed method is effective in two aspects:
From the distillation method perspective, MTD loss outperforms other strong
distillation baselines. From the distilled model&apos;s performance perspective: 1)
MTDVocaLiST outperforms similar-size SOTA models, SyncNet, and Perfect Match
models by 15.65% and 3.35%; 2) MTDVocaLiST reduces the model size of VocaLiST
by 83.52%, yet still maintaining similar performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuanjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chung-Che Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jyh-Shing Roger Jang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04155">
<title>Latent Graph Representations for Critical View of Safety Assessment. (arXiv:2212.04155v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04155</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing the critical view of safety in laparoscopic cholecystectomy
requires accurate identification and localization of key anatomical structures,
reasoning about their geometric relationships to one another, and determining
the quality of their exposure. Prior works have approached this task by
including semantic segmentation as an intermediate step, using predicted
segmentation masks to then predict the CVS. While these methods are effective,
they rely on extremely expensive ground-truth segmentation annotations and tend
to fail when the predicted segmentation is incorrect, limiting generalization.
In this work, we propose a method for CVS prediction wherein we first represent
a surgical image using a disentangled latent scene graph, then process this
representation using a graph neural network. Our graph representations
explicitly encode semantic information - object location, class information,
geometric relations - to improve anatomy-driven reasoning, as well as visual
features to retain differentiability and thereby provide robustness to semantic
errors. Finally, to address annotation cost, we propose to train our method
using only bounding box annotations, incorporating an auxiliary image
reconstruction objective to learn fine-grained object boundaries. We show that
our method not only outperforms several baseline methods when trained with
bounding box annotations, but also scales effectively when trained with
segmentation masks, maintaining state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Aditya Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1&quot;&gt;Deepak Alapatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vardazaryan_A/0/1/0/all/0/1&quot;&gt;Armine Vardazaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_A/0/1/0/all/0/1&quot;&gt;Alain Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okamoto_N/0/1/0/all/0/1&quot;&gt;Nariaki Okamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02515">
<title>Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey. (arXiv:2302.02515v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02515</link>
<description rdf:parseType="Literal">&lt;p&gt;Time Series Classification and Extrinsic Regression are important and
challenging machine learning tasks. Deep learning has revolutionized natural
language processing and computer vision and holds great promise in other fields
such as time series analysis where the relevant features must often be
abstracted from the raw data but are not known a priori. This paper surveys the
current state of the art in the fast-moving field of deep learning for time
series classification and extrinsic regression. We review different network
architectures and training methods used for these tasks and discuss the
challenges and opportunities when applying deep learning to time series data.
We also summarize two critical applications of time series classification and
extrinsic regression, human activity recognition and satellite earth
observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foumani_N/0/1/0/all/0/1&quot;&gt;Navid Mohammadi Foumani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_L/0/1/0/all/0/1&quot;&gt;Lynn Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chang Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1&quot;&gt;Germain Forestier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1&quot;&gt;Mahsa Salehi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05122">
<title>M-Tuning: Prompt Tuning with Mitigated Label Bias in Open-Set Scenarios. (arXiv:2303.05122v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05122</link>
<description rdf:parseType="Literal">&lt;p&gt;In realistic open-set scenarios where labels of a part of testing data are
totally unknown, when vision-language (VL) prompt learning methods encounter
inputs related to unknown classes (i.e., not seen during training), they always
predict them as one of the training classes. The exhibited label bias causes
difficulty in open set recognition (OSR), in which an image should be correctly
predicted as one of the known classes or the unknown one. To achieve this goal,
we propose a vision-language prompt tuning method with mitigated label bias
(M-Tuning). It introduces open words from the WordNet to extend the range of
words forming the prompt texts from only closed-set label words to more, and
thus prompts are tuned in a simulated open-set scenario. Besides, inspired by
the observation that classifying directly on large datasets causes a much
higher false positive rate than on small datasets, we propose a Combinatorial
Tuning and Testing (CTT) strategy for improving performance. CTT decomposes
M-Tuning on large datasets as multiple independent group-wise tuning on fewer
classes, then makes accurate and comprehensive predictions by selecting the
optimal sub-prompt. Finally, given the lack of VL-based OSR baselines in the
literature, especially for prompt methods, we contribute new baselines for fair
comparisons. Our method achieves the best performance on datasets with various
scales, and extensive ablation studies also validate its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_N/0/1/0/all/0/1&quot;&gt;Ning Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Min Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06854">
<title>Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks. (arXiv:2303.06854v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06854</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive vision-language representation learning has achieved
state-of-the-art performance for zero-shot classification, by learning from
millions of image-caption pairs crawled from the internet. However, the massive
data that powers large multimodal models such as CLIP, makes them extremely
vulnerable to various types of targeted data poisoning and backdoor attacks.
Despite this vulnerability, robust contrastive vision-language pre-training
against such attacks has remained unaddressed. In this work, we propose ROCLIP,
the first effective method for robust pre-training multimodal vision-language
models against targeted data poisoning and backdoor attacks. ROCLIP effectively
breaks the association between poisoned image-caption pairs by considering a
relatively large and varying pool of random captions, and matching every image
with the text that is most similar to it in the pool instead of its own
caption, every few epochs.It also leverages image and text augmentations to
further strengthen the defense and improve the performance of the model. Our
extensive experiments show that ROCLIP renders state-of-the-art targeted data
poisoning and backdoor attacks ineffective during pre-training CLIP models. In
particular, ROCLIP decreases the success rate for targeted data poisoning
attacks from 93.75% to 12.5% and that of backdoor attacks down to 0%, while
improving the model&apos;s linear probe performance by 10% and maintains a similar
zero shot performance compared to CLIP. By increasing the frequency of
matching, ROCLIP is able to defend strong attacks, which add up to 1% poisoned
examples to the data, and successfully maintain a low attack success rate of
12.5%, while trading off the performance on some tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jingdong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirzasoleiman_B/0/1/0/all/0/1&quot;&gt;Baharan Mirzasoleiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09429">
<title>Data Roaming and Quality Assessment for Composed Image Retrieval. (arXiv:2303.09429v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09429</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Composed Image Retrieval (CoIR) involves queries that combine
image and text modalities, allowing users to express their intent more
effectively. However, current CoIR datasets are orders of magnitude smaller
compared to other vision and language (V&amp;amp;L) datasets. Additionally, some of
these datasets have noticeable issues, such as queries containing redundant
modalities. To address these shortcomings, we introduce the Large Scale
Composed Image Retrieval (LaSCo) dataset, a new CoIR dataset which is ten times
larger than existing ones. Pre-training on our LaSCo, shows a noteworthy
improvement in performance, even in zero-shot. Furthermore, we propose a new
approach for analyzing CoIR datasets and methods, which detects modality
redundancy or necessity, in queries. We also introduce a new CoIR baseline, the
Cross-Attention driven Shift Encoder (CASE). This baseline allows for early
fusion of modalities using a cross-attention module and employs an additional
auxiliary task during training. Our experiments demonstrate that this new
baseline outperforms the current state-of-the-art methods on established
benchmarks like FashionIQ and CIRR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_M/0/1/0/all/0/1&quot;&gt;Matan Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1&quot;&gt;Rami Ben-Ari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1&quot;&gt;Nir Darshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11048">
<title>SGFormer: Semantic Graph Transformer for Point Cloud-based 3D Scene Graph Generation. (arXiv:2303.11048v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11048</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel model called SGFormer, Semantic Graph
TransFormer for point cloud-based 3D scene graph generation. The task aims to
parse a point cloud-based scene into a semantic structural graph, with the core
challenge of modeling the complex global structure. Existing methods based on
graph convolutional networks (GCNs) suffer from the over-smoothing dilemma and
can only propagate information from limited neighboring nodes. In contrast,
SGFormer uses Transformer layers as the base building block to allow global
information passing, with two types of newly-designed layers tailored for the
3D scene graph generation task. Specifically, we introduce the graph embedding
layer to best utilize the global information in graph edges while maintaining
comparable computation costs. Furthermore, we propose the semantic injection
layer to leverage linguistic knowledge from large-scale language model (i.e.,
ChatGPT), to enhance objects&apos; visual features. We benchmark our SGFormer on the
established 3DSSG dataset and achieve a 40.94% absolute improvement in
relationship prediction&apos;s R@50 and an 88.36% boost on the subset with complex
scenes over the state-of-the-art. Our analyses further show SGFormer&apos;s
superiority in the long-tail and zero-shot scenarios. Our source code is
available at https://github.com/Andy20178/SGFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Changsheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11938">
<title>3D-CLFusion: Fast Text-to-3D Rendering with Contrastive Latent Diffusion. (arXiv:2303.11938v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11938</link>
<description rdf:parseType="Literal">&lt;p&gt;We tackle the task of text-to-3D creation with pre-trained latent-based NeRFs
(NeRFs that generate 3D objects given input latent code). Recent works such as
DreamFusion and Magic3D have shown great success in generating 3D content using
NeRFs and text prompts, but the current approach of optimizing a NeRF for every
text prompt is 1) extremely time-consuming and 2) often leads to low-resolution
outputs. To address these challenges, we propose a novel method named
3D-CLFusion which leverages the pre-trained latent-based NeRFs and performs
fast 3D content creation in less than a minute. In particular, we introduce a
latent diffusion prior network for learning the w latent from the input CLIP
text/image embeddings. This pipeline allows us to produce the w latent without
further optimization during inference and the pre-trained NeRF is able to
perform multi-view high-resolution 3D synthesis based on the latent. We note
that the novelty of our model lies in that we introduce contrastive learning
during training the diffusion prior which enables the generation of the valid
view-invariant latent code. We demonstrate through experiments the
effectiveness of our proposed view-invariant diffusion process for fast
text-to-3D creation, e.g., 100 times faster than DreamFusion. We note that our
model is able to serve as the role of a plug-and-play tool for text-to-3D with
pre-trained NeRFs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu-Jhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Ji Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1&quot;&gt;Albert Pumarola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peizhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12332">
<title>Weakly-Supervised Temporal Action Localization by Inferring Salient Snippet-Feature. (arXiv:2303.12332v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12332</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised temporal action localization aims to locate action regions
and identify action categories in untrimmed videos simultaneously by taking
only video-level labels as the supervision. Pseudo label generation is a
promising strategy to solve the challenging problem, but the current methods
ignore the natural temporal structure of the video that can provide rich
information to assist such a generation process. In this paper, we propose a
novel weakly-supervised temporal action localization method by inferring
salient snippet-feature. First, we design a saliency inference module that
exploits the variation relationship between temporal neighbor snippets to
discover salient snippet-features, which can reflect the significant dynamic
change in the video. Secondly, we introduce a boundary refinement module that
enhances salient snippet-features through the information interaction unit.
Then, a discrimination enhancement module is introduced to enhance the
discriminative nature of snippet-features. Finally, we adopt the refined
snippet-features to produce high-fidelity pseudo labels, which could be used to
supervise the training of the action localization network. Extensive
experiments on two publicly available datasets, i.e., THUMOS14 and ActivityNet
v1.3, demonstrate our proposed method achieves significant improvements
compared to the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1&quot;&gt;Wulian Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Mengshi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuanming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Huadong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12484">
<title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12484</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has seen rapid growth in recent years and achieved
state-of-the-art performance in a wide range of applications. However, training
models typically requires expensive and time-consuming collection of large
quantities of labeled data. This is particularly true within the scope of
medical imaging analysis (MIA), where data are limited and labels are expensive
to be acquired. Thus, label-efficient deep learning methods are developed to
make comprehensive use of the labeled data as well as the abundance of
unlabeled and weak-labeled data. In this survey, we extensively investigated
over 300 recent papers to provide a comprehensive overview of recent progress
on label-efficient learning strategies in MIA. We first present the background
of label-efficient learning and categorize the approaches into different
schemes. Next, we examine the current state-of-the-art methods in detail
through each scheme. Specifically, we provide an in-depth investigation,
covering not only canonical semi-supervised, self-supervised, and
multi-instance learning schemes, but also recently emerged active and
annotation-efficient learning strategies. Moreover, as a comprehensive
contribution to the field, this survey not only elucidates the commonalities
and unique features of the surveyed methods but also presents a detailed
analysis of the current challenges in the field and suggests potential avenues
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengrui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Luyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15413">
<title>Debiasing Scores and Prompts of 2D Diffusion for View-consistent Text-to-3D Generation. (arXiv:2303.15413v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15413</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing score-distilling text-to-3D generation techniques, despite their
considerable promise, often encounter the view inconsistency problem. One of
the most notable issues is the Janus problem, where the most canonical view of
an object (\textit{e.g}., face or head) appears in other views. In this work,
we explore existing frameworks for score-distilling text-to-3D generation and
identify the main causes of the view inconsistency problem -- the embedded bias
of 2D diffusion models. Based on these findings, we propose two approaches to
debias the score-distillation frameworks for view-consistent text-to-3D
generation. Our first approach, called score debiasing, involves cutting off
the score estimated by 2D diffusion models and gradually increasing the
truncation value throughout the optimization process. Our second approach,
called prompt debiasing, identifies conflicting words between user prompts and
view prompts using a language model, and adjusts the discrepancy between view
prompts and the viewing direction of an object. Our experimental results show
that our methods improve the realism of the generated 3D objects by
significantly reducing artifacts and achieve a good trade-off between
faithfulness to the 2D diffusion models and 3D consistency with little
overhead. Our project page is available
at~\url{https://susunghong.github.io/Debiased-Score-Distillation-Sampling/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Susung Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_D/0/1/0/all/0/1&quot;&gt;Donghoon Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17908">
<title>Trade-offs in Fine-tuned Diffusion Models Between Accuracy and Interpretability. (arXiv:2303.17908v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17908</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in diffusion models have significantly impacted the
trajectory of generative machine learning research, with many adopting the
strategy of fine-tuning pre-trained models using domain-specific text-to-image
datasets. Notably, this method has been readily employed for medical
applications, such as X-ray image synthesis, leveraging the plethora of
associated radiology reports. Yet, a prevailing concern is the lack of
assurance on whether these models genuinely comprehend their generated content.
With the evolution of text-conditional image generation, these models have
grown potent enough to facilitate object localization scrutiny. Our research
underscores this advancement in the critical realm of medical imaging,
emphasizing the crucial role of interpretability. We further unravel a
consequential trade-off between image fidelity as gauged by conventional
metrics and model interpretability in generative diffusion models.
Specifically, the adoption of learnable text encoders when fine-tuning results
in diminished interpretability. Our in-depth exploration uncovers the
underlying factors responsible for this divergence. Consequently, we present a
set of design principles for the development of truly interpretable generative
models. Code is available at https://github.com/MischaD/chest-distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dombrowski_M/0/1/0/all/0/1&quot;&gt;Mischa Dombrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1&quot;&gt;Hadrien Reynaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1&quot;&gt;Johanna P. M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baugh_M/0/1/0/all/0/1&quot;&gt;Matthew Baugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1&quot;&gt;Bernhard Kainz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02150">
<title>Re-Evaluating LiDAR Scene Flow for Autonomous Driving. (arXiv:2304.02150v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02150</link>
<description rdf:parseType="Literal">&lt;p&gt;Popular benchmarks for self-supervised LiDAR scene flow (stereoKITTI, and
FlyingThings3D) have unrealistic rates of dynamic motion, unrealistic
correspondences, and unrealistic sampling patterns. As a result, progress on
these benchmarks is misleading and may cause researchers to focus on the wrong
problems. We evaluate a suite of top methods on a suite of real-world datasets
(Argoverse 2.0, Waymo, and NuScenes) and report several conclusions. First, we
find that performance on stereoKITTI is negatively correlated with performance
on real-world data. Second, we find that one of this task&apos;s key components --
removing the dominant ego-motion -- is better solved by classic ICP than any
tested method. Finally, we show that despite the emphasis placed on learning,
most performance gains are caused by pre- and post-processing steps:
piecewise-rigid refinement and ground removal. We demonstrate this through a
baseline method that combines these processing steps with a learning-free
test-time flow optimization. This baseline outperforms every evaluated method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chodosh_N/0/1/0/all/0/1&quot;&gt;Nathaniel Chodosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1&quot;&gt;Simon Lucey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03483">
<title>RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging. (arXiv:2304.03483v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03483</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic imaging addresses the recovery of a time-varying 2D or 3D object at
each time instant using its undersampled measurements. In particular, in the
case of dynamic tomography, only a single projection at a single view angle may
be available at a time, making the problem severely ill-posed. In this work, we
propose an approach, RED-PSM, which combines for the first time two powerful
techniques to address this challenging imaging problem. The first, are
partially separable models, which have been used to efficiently introduce a
low-rank prior for the spatio-temporal object. The second is the recent
\textit{Regularization by Denoising (RED)}, which provides a flexible framework
to exploit the impressive performance of state-of-the-art image denoising
algorithms, for various inverse problems. We propose a partially separable
objective with RED and a computationally efficient and scalable optimization
scheme with variable splitting and ADMM. Theoretical analysis proves the
convergence of our objective to a value corresponding to a stationary point
satisfying the first-order optimality conditions. Convergence is accelerated by
a particular projection-domain-based initialization. We demonstrate the
performance and computational improvements of our proposed RED-PSM with a
learned image denoiser by comparing it to a recent deep-prior-based method
known as TD-DIP. Although the main focus is on dynamic tomography, we also show
performance advantages of RED-PSM in a cardiac dynamic MRI setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iskender_B/0/1/0/all/0/1&quot;&gt;Berk Iskender&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klasky_M/0/1/0/all/0/1&quot;&gt;Marc L. Klasky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bresler_Y/0/1/0/all/0/1&quot;&gt;Yoram Bresler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03693">
<title>Model-Agnostic Gender Debiased Image Captioning. (arXiv:2304.03693v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03693</link>
<description rdf:parseType="Literal">&lt;p&gt;Image captioning models are known to perpetuate and amplify harmful societal
bias in the training set. In this work, we aim to mitigate such gender bias in
image captioning models. While prior work has addressed this problem by forcing
models to focus on people to reduce gender misclassification, it conversely
generates gender-stereotypical words at the expense of predicting the correct
gender. From this observation, we hypothesize that there are two types of
gender bias affecting image captioning models: 1) bias that exploits context to
predict gender, and 2) bias in the probability of generating certain (often
stereotypical) words because of gender. To mitigate both types of gender
biases, we propose a framework, called LIBRA, that learns from synthetically
biased samples to decrease both types of biases, correcting gender
misclassification and changing gender-stereotypical words to more neutral ones.
Code is available at https://github.com/rebnej/LIBRA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1&quot;&gt;Yusuke Hirota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1&quot;&gt;Yuta Nakashima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Noa Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06385">
<title>TransHP: Image Classification with Hierarchical Prompting. (arXiv:2304.06385v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores a hierarchical prompting mechanism for the hierarchical
image classification (HIC) task. Different from prior HIC methods, our
hierarchical prompting is the first to explicitly inject ancestor-class
information as a tokenized hint that benefits the descendant-class
discrimination. We think it well imitates human visual recognition, i.e.,
humans may use the ancestor class as a prompt to draw focus on the subtle
differences among descendant classes. We model this prompting mechanism into a
Transformer with Hierarchical Prompting (TransHP). TransHP consists of three
steps: 1) learning a set of prompt tokens to represent the coarse (ancestor)
classes, 2) on-the-fly predicting the coarse class of the input image at an
intermediate block, and 3) injecting the prompt token of the predicted coarse
class into the intermediate feature. Though the parameters of TransHP maintain
the same for all input images, the injected coarse-class prompt conditions
(modifies) the subsequent feature extraction and encourages a dynamic focus on
relatively subtle differences among the descendant classes. Extensive
experiments show that TransHP improves image classification on accuracy (e.g.,
improving ViT-B/16 by +2.83% ImageNet classification accuracy), training data
efficiency (e.g., +12.69% improvement under 10% ImageNet training data), and
model explainability. Moreover, TransHP also performs favorably against prior
HIC methods, showing that TransHP well exploits the hierarchical information.
The code is available at: https://github.com/WangWenhao0716/TransHP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10701">
<title>Personalization as a Shortcut for Few-Shot Backdoor Attack against Text-to-Image Diffusion Models. (arXiv:2305.10701v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10701</link>
<description rdf:parseType="Literal">&lt;p&gt;Although recent personalization methods have democratized high-resolution
image synthesis by enabling swift concept acquisition with minimal examples and
lightweight computation, they also present an exploitable avenue for high
accessible backdoor attacks. This paper investigates a critical and unexplored
aspect of text-to-image (T2I) diffusion models - their potential vulnerability
to backdoor attacks via personalization. Our study focuses on a zero-day
backdoor vulnerability prevalent in two families of personalization methods,
epitomized by Textual Inversion and DreamBooth.Compared to traditional backdoor
attacks, our proposed method can facilitate more precise, efficient, and easily
accessible attacks with a lower barrier to entry. We provide a comprehensive
review of personalization in T2I diffusion models, highlighting the operation
and exploitation potential of this backdoor vulnerability. To be specific, by
studying the prompt processing of Textual Inversion and DreamBooth, we have
devised dedicated backdoor attacks according to the different ways of dealing
with unseen tokens and analyzed the influence of triggers and concept images on
the attack effect. Through comprehensive empirical study, we endorse the
utilization of the nouveau-token backdoor attack due to its impressive
effectiveness, stealthiness, and integrity, markedly outperforming the
legacy-token backdoor attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yihao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1&quot;&gt;Felix Juefei-Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yutong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Ming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1&quot;&gt;Geguang Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12554">
<title>Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion. (arXiv:2305.12554v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12554</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Human Motion Prediction (HMP) aims to predict multiple possible
upcoming pose sequences based on past human motion trajectories. Although
previous approaches have shown impressive performance, they face several
issues, including complex training processes and a tendency to generate
predictions that are often inconsistent with the provided history, and
sometimes even becoming entirely unreasonable. To overcome these issues, we
propose DiffMotion, an end-to-end diffusion-based stochastic HMP framework.
DiffMotion&apos;s motion predictor is composed of two modules, including (1) a
Transformer-based network for initial motion reconstruction from corrupted
motion, and (2) a Graph Convolutional Network (GCN) to refine the generated
motion considering past observations. Our method, facilitated by this novel
Transformer-GCN module design and a proposed variance scheduler, excels in
predicting accurate, realistic, and consistent motions, while maintaining an
appropriate level of diversity. Our results on benchmark datasets show that
DiffMotion significantly outperforms previous methods in terms of both accuracy
and fidelity, while demonstrating superior robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiarui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1&quot;&gt;Girish Chowdhary&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15296">
<title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation. (arXiv:2305.15296v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15296</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent popularity of text-to-image diffusion models (DM) can largely be
attributed to the intuitive interface they provide to users. The intended
generation can be expressed in natural language, with the model producing
faithful interpretations of text prompts. However, expressing complex or
nuanced ideas in text alone can be difficult. To ease image generation, we
propose MultiFusion that allows one to express complex and nuanced concepts
with arbitrarily interleaved inputs of multiple modalities and languages.
MutliFusion leverages pre-trained models and aligns them for integration into a
cohesive system, thereby avoiding the need for extensive training from scratch.
Our experimental results demonstrate the efficient transfer of capabilities
from individual modules to the downstream model. Specifically, the fusion of
all independent components allows the image generation module to utilize
multilingual, interleaved multimodal inputs despite being trained solely on
monomodal data in a single language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Deiseroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1&quot;&gt;Constantin Eichenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1&quot;&gt;Robert Baldock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_S/0/1/0/all/0/1&quot;&gt;Souradeep Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_Salinas_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Cruz-Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1&quot;&gt;Samuel Weinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16172">
<title>Masked and Permuted Implicit Context Learning for Scene Text Recognition. (arXiv:2305.16172v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16172</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Text Recognition (STR) is difficult because of the variations in text
styles, shapes, and backgrounds. Though the integration of linguistic
information enhances models&apos; performance, existing methods based on either
permuted language modeling (PLM) or masked language modeling (MLM) have their
pitfalls. PLM&apos;s autoregressive decoding lacks foresight into subsequent
characters, while MLM overlooks inter-character dependencies. Addressing these
problems, we propose a masked and permuted implicit context learning network
for STR, which unifies PLM and MLM within a single decoder, inheriting the
advantages of both approaches. We utilize the training procedure of PLM, and to
integrate MLM, we incorporate word length information into the decoding process
and replace the undetermined characters with mask tokens. Besides, perturbation
training is employed to train a more robust model against potential length
prediction errors. Our empirical evaluations demonstrate the performance of our
model. It not only achieves superior performance on the common benchmarks but
also achieves a substantial improvement of $9.1\%$ on the more challenging
Union14M-Benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1&quot;&gt;Zhi Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jin Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dongbao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03373">
<title>CiT-Net: Convolutional Neural Networks Hand in Hand with Vision Transformers for Medical Image Segmentation. (arXiv:2306.03373v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03373</link>
<description rdf:parseType="Literal">&lt;p&gt;The hybrid architecture of convolutional neural networks (CNNs) and
Transformer are very popular for medical image segmentation. However, it
suffers from two challenges. First, although a CNNs branch can capture the
local image features using vanilla convolution, it cannot achieve adaptive
feature learning. Second, although a Transformer branch can capture the global
features, it ignores the channel and cross-dimensional self-attention,
resulting in a low segmentation accuracy on complex-content images. To address
these challenges, we propose a novel hybrid architecture of convolutional
neural networks hand in hand with vision Transformers (CiT-Net) for medical
image segmentation. Our network has two advantages. First, we design a dynamic
deformable convolution and apply it to the CNNs branch, which overcomes the
weak feature extraction ability due to fixed-size convolution kernels and the
stiff design of sharing kernel parameters among different inputs. Second, we
design a shifted-window adaptive complementary attention module and a compact
convolutional projection. We apply them to the Transformer branch to learn the
cross-dimensional long-term dependency for medical images. Experimental results
show that our CiT-Net provides better medical image segmentation results than
popular SOTA methods. Besides, our CiT-Net requires lower parameters and less
computational costs and does not rely on pre-training. The code is publicly
available at https://github.com/SR0920/CiT-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lei_T/0/1/0/all/0/1&quot;&gt;Tao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Rui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nandi_A/0/1/0/all/0/1&quot;&gt;Asoke Nandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04086">
<title>TEC-Net: Vision Transformer Embrace Convolutional Neural Networks for Medical Image Segmentation. (arXiv:2306.04086v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04086</link>
<description rdf:parseType="Literal">&lt;p&gt;The hybrid architecture of convolution neural networks (CNN) and Transformer
has been the most popular method for medical image segmentation. However, the
existing networks based on the hybrid architecture suffer from two problems.
First, although the CNN branch can capture image local features by using
convolution operation, the vanilla convolution is unable to achieve adaptive
extraction of image features. Second, although the Transformer branch can model
the global information of images, the conventional self-attention only focuses
on the spatial self-attention of images and ignores the channel and
cross-dimensional self-attention leading to low segmentation accuracy for
medical images with complex backgrounds. To solve these problems, we propose
vision Transformer embrace convolutional neural networks for medical image
segmentation (TEC-Net). Our network has two advantages. First, dynamic
deformable convolution (DDConv) is designed in the CNN branch, which not only
overcomes the difficulty of adaptive feature extraction using fixed-size
convolution kernels, but also solves the defect that different inputs share the
same convolution kernel parameters, effectively improving the feature
expression ability of CNN branch. Second, in the Transformer branch, a
(shifted)-window adaptive complementary attention module ((S)W-ACAM) and
compact convolutional projection are designed to enable the network to fully
learn the cross-dimensional long-range dependency of medical images with few
parameters and calculations. Experimental results show that the proposed
TEC-Net provides better medical image segmentation results than SOTA methods
including CNN and Transformer networks. In addition, our TEC-Net requires fewer
parameters and computational costs and does not rely on pre-training. The code
is publicly available at https://github.com/SR0920/TEC-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_R/0/1/0/all/0/1&quot;&gt;Rui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lei_T/0/1/0/all/0/1&quot;&gt;Tao Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weichuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yong Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nandi_A/0/1/0/all/0/1&quot;&gt;Asoke K. Nandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12045">
<title>Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v6 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12045</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing paradigm.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a {\it natural paradigm}. We show
that TeCoS-LVM models can produce more realistic spike activities and
accurately fit spike statistics than powerful alternatives. Additionally,
learned TeCoS-LVM models can generalize well to longer time scales. Overall,
while remaining computationally tractable, our model effectively captures key
features of neural coding systems. It thus provides a useful tool for building
accurate predictive computational accounts for various sensory perception
circuits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gehua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Runhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Huajin Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02273">
<title>Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient Neural Image Compression. (arXiv:2307.02273v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02273</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the performance of neural image compression (NIC) has steadily
improved thanks to the last line of study, reaching or outperforming
state-of-the-art conventional codecs. Despite significant progress, current NIC
methods still rely on ConvNet-based entropy coding, limited in modeling
long-range dependencies due to their local connectivity and the increasing
number of architectural biases and priors, resulting in complex underperforming
models with high decoding latency. Motivated by the efficiency investigation of
the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose
to enhance the latter, as first, with a more straightforward yet effective
Tranformer-based channel-wise auto-regressive prior model, resulting in an
absolute image compression transformer (ICT). Through the proposed ICT, we can
capture both global and local contexts from the latent representations and
better parameterize the distribution of the quantized latents. Further, we
leverage a learnable scaling module with a sandwich ConvNeXt-based
pre-/post-processor to accurately extract more compact latent codes while
reconstructing higher-quality images. Extensive experimental results on
benchmark datasets showed that the proposed framework significantly improves
the trade-off between coding efficiency and decoder complexity over the
versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM. Moreover, we provide model scaling studies to verify the
computational efficiency of our approach and conduct several objective and
subjective analyses to bring to the fore the performance gap between the
adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_A/0/1/0/all/0/1&quot;&gt;Ahmed Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1&quot;&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1&quot;&gt;Luce Morin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07063">
<title>Bootstrapping Vision-Language Learning with Decoupled Language Pre-training. (arXiv:2307.07063v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07063</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel methodology aimed at optimizing the application of frozen
large language models (LLMs) for resource-intensive vision-language (VL)
pre-training. The current paradigm uses visual features as prompts to guide
language models, with a focus on determining the most relevant visual features
for corresponding text. Our approach diverges by concentrating on the language
component, specifically identifying the optimal prompts to align with visual
features. We introduce the Prompt-Transformer (P-Former), a model that predicts
these ideal prompts, which is trained exclusively on linguistic data, bypassing
the need for image-text pairings. This strategy subtly bifurcates the
end-to-end VL training process into an additional, separate stage. Our
experiments reveal that our framework significantly enhances the performance of
a robust image-to-text baseline (BLIP-2), and effectively narrows the
performance gap between models trained with either 4M or 129M image-text pairs.
Importantly, our framework is modality-agnostic and flexible in terms of
architectural design, as validated by its successful application in a video
learning task using varied base modules. The code will be made available at
https://github.com/yiren-jian/BLIText.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1&quot;&gt;Yiren Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chongyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1&quot;&gt;Soroush Vosoughi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13986">
<title>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities. (arXiv:2307.13986v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13986</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Manual annotations for training deep learning (DL) models in
auto-segmentation are time-intensive. This study introduces a hybrid
representation-enhanced sampling strategy that integrates both density and
diversity criteria within an uncertainty-based Bayesian active learning (BAL)
framework to reduce annotation efforts by selecting the most informative
training samples. Methods: The experiments are performed on two lower extremity
(LE) datasets of MRI and CT images, focusing on the segmentation of the femur,
pelvis, sacrum, quadriceps femoris, hamstrings, adductors, sartorius, and
iliopsoas, utilizing a U-net-based BAL framework. Our method selects uncertain
samples with high density and diversity for manual revision, optimizing for
maximal similarity to unlabeled instances and minimal similarity to existing
training data. We assess the accuracy and efficiency using Dice and a proposed
metric called reduced annotation cost (RAC), respectively. We further evaluate
the impact of various acquisition rules on BAL performance and design an
ablation study for effectiveness estimation. Results: In MRI and CT datasets,
our method was superior or comparable to existing ones, achieving a 0.8\% Dice
and 1.0\% RAC increase in CT (statistically significant), and a 0.8\% Dice and
1.1\% RAC increase in MRI (not statistically significant) in volume-wise
acquisition. Our ablation study indicates that combining density and diversity
criteria enhances the efficiency of BAL in musculoskeletal segmentation
compared to using either criterion alone. Conclusion: Our sampling method is
proven efficient in reducing annotation costs in image segmentation tasks. The
combination of the proposed method and our BAL framework provides a
semi-automatic way for efficient annotation of medical image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ganping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1&quot;&gt;Yoshito Otake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soufi_M/0/1/0/all/0/1&quot;&gt;Mazen Soufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taniguchi_M/0/1/0/all/0/1&quot;&gt;Masashi Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yagi_M/0/1/0/all/0/1&quot;&gt;Masahide Yagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ichihashi_N/0/1/0/all/0/1&quot;&gt;Noriaki Ichihashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uemura_K/0/1/0/all/0/1&quot;&gt;Keisuke Uemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takao_M/0/1/0/all/0/1&quot;&gt;Masaki Takao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sugano_N/0/1/0/all/0/1&quot;&gt;Nobuhiko Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15409">
<title>Uncertainty-aware Unsupervised Multi-Object Tracking. (arXiv:2307.15409v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15409</link>
<description rdf:parseType="Literal">&lt;p&gt;Without manually annotated identities, unsupervised multi-object trackers are
inferior to learning reliable feature embeddings. It causes the
similarity-based inter-frame association stage also be error-prone, where an
uncertainty problem arises. The frame-by-frame accumulated uncertainty prevents
trackers from learning the consistent feature embedding against time variation.
To avoid this uncertainty problem, recent self-supervised techniques are
adopted, whereas they failed to capture temporal relations. The interframe
uncertainty still exists. In fact, this paper argues that though the
uncertainty problem is inevitable, it is possible to leverage the uncertainty
itself to improve the learned consistency in turn. Specifically, an
uncertainty-based metric is developed to verify and rectify the risky
associations. The resulting accurate pseudo-tracklets boost learning the
feature consistency. And accurate tracklets can incorporate temporal
information into spatial transformation. This paper proposes a tracklet-guided
augmentation strategy to simulate tracklets&apos; motion, which adopts a
hierarchical uncertainty-based sampling mechanism for hard sample mining. The
ultimate unsupervised MOT framework, namely U2MOT, is proven effective on
MOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performance
among the published supervised and unsupervised trackers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Sheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Zhihang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Rongxin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jieping Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03108">
<title>SAAM: Stealthy Adversarial Attack on Monocular Depth Estimation. (arXiv:2308.03108v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03108</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the vulnerability of MDE to adversarial
patches. We propose a novel \underline{S}tealthy \underline{A}dversarial
\underline{A}ttacks on \underline{M}DE (SAAM) that compromises MDE by either
corrupting the estimated distance or causing an object to seamlessly blend into
its surroundings. Our experiments, demonstrate that the designed stealthy patch
successfully causes a DNN-based MDE to misestimate the depth of objects. In
fact, our proposed adversarial patch achieves a significant 60\% depth error
with 99\% ratio of the affected region. Importantly, despite its adversarial
nature, the patch maintains a naturalistic appearance, making it inconspicuous
to human observers. We believe that this work sheds light on the threat of
adversarial attacks in the context of MDE on edge devices. We hope it raises
awareness within the community about the potential real-life harm of such
attacks and encourages further research into developing more robust and
adaptive defense mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1&quot;&gt;Amira Guesmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouni_B/0/1/0/all/0/1&quot;&gt;Bassem Ouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08511">
<title>Two-and-a-half Order Score-based Model for Solving 3D Ill-posed Inverse Problems. (arXiv:2308.08511v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08511</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography (CT) and Magnetic Resonance Imaging (MRI) are crucial
technologies in the field of medical imaging. Score-based models have proven to
be effective in addressing different inverse problems encountered in CT and
MRI, such as sparse-view CT and fast MRI reconstruction. However, these models
face challenges in achieving accurate three dimensional (3D) volumetric
reconstruction. The existing score-based models primarily focus on
reconstructing two dimensional (2D) data distribution, leading to
inconsistencies between adjacent slices in the reconstructed 3D volumetric
images. To overcome this limitation, we propose a novel two-and-a-half order
score-based model (TOSM). During the training phase, our TOSM learns data
distributions in 2D space, which reduces the complexity of training compared to
directly working on 3D volumes. However, in the reconstruction phase, the TOSM
updates the data distribution in 3D space, utilizing complementary scores along
three directions (sagittal, coronal, and transaxial) to achieve a more precise
reconstruction. The development of TOSM is built on robust theoretical
principles, ensuring its reliability and efficacy. Through extensive
experimentation on large-scale sparse-view CT and fast MRI datasets, our method
demonstrates remarkable advancements and attains state-of-the-art results in
solving 3D ill-posed inverse problems. Notably, the proposed TOSM effectively
addresses the inter-slice inconsistency issue, resulting in high-quality 3D
volumetric reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zirong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weiwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hengyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08658">
<title>A Data-Theoretic Approach to Identifying Violent Facial Expressions in Social Crime Contexts. (arXiv:2308.08658v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08658</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Facial Expressions plays an important role in identifying human actions
or intention. Facial expressions can represent any specific action of any
person and the pattern of violent behavior of any person strongly depends on
the geographic region. Here we have designed an automated system by using a
Convolutional Neural Network which can detect whether a person has any
intention to commit any crime or not. Here we proposed a new method that can
identify criminal intentions or violent behavior of any person before executing
crimes more efficiently by using very little data on facial expressions before
executing a crime or any violent tasks. Instead of using image features which
is a time-consuming and faulty method we used an automated feature selector
Convolutional Neural Network model which can capture exact facial expressions
for training and then can predict that target facial expressions more
accurately. Here we used only the facial data of a specific geographic region
which can represent the violent and before-crime before-crime facial patterns
of the people of the whole region.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1&quot;&gt;Arindam Kumar Paul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10079">
<title>MeDM: Mediating Image Diffusion Models for Video-to-Video Translation with Temporal Correspondence Guidance. (arXiv:2308.10079v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10079</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces an efficient and effective method, MeDM, that utilizes
pre-trained image Diffusion Models for video-to-video translation with
consistent temporal flow. The proposed framework can render videos from scene
position information, such as a normal G-buffer, or perform text-guided editing
on videos captured in real-world scenarios. We employ explicit optical flows to
construct a practical coding that enforces physical constraints on generated
frames and mediates independent frame-wise scores. By leveraging this coding,
maintaining temporal consistency in the generated videos can be framed as an
optimization problem with a closed-form solution. To ensure compatibility with
Stable Diffusion, we also suggest a workaround for modifying observation-space
scores in latent Diffusion Models. Notably, MeDM does not require fine-tuning
or test-time optimization of the Diffusion Models. Through extensive
qualitative, quantitative, and subjective experiments on various benchmarks,
the study demonstrates the effectiveness and superiority of the proposed
approach. Our project page can be found at https://medm2023.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_E/0/1/0/all/0/1&quot;&gt;Ernie Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzuhsuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shuo-Yen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jun-Cheng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10542">
<title>Learning Weakly Convex Regularizers for Convergent Image-Reconstruction Algorithms. (arXiv:2308.10542v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10542</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to learn non-convex regularizers with a prescribed upper bound on
their weak-convexity modulus. Such regularizers give rise to variational
denoisers that minimize a convex energy. They rely on few parameters (less than
15,000) and offer a signal-processing interpretation as they mimic handcrafted
sparsity-promoting regularizers. Through numerical experiments, we show that
such denoisers outperform convex-regularization methods as well as the popular
BM3D denoiser. Additionally, the learned regularizer can be deployed to solve
inverse problems with iterative schemes that provably converge. For both CT and
MRI reconstruction, the regularizer generalizes well and offers an excellent
tradeoff between performance, number of parameters, guarantees, and
interpretability when compared to other data-driven approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goujon_A/0/1/0/all/0/1&quot;&gt;Alexis Goujon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Neumayer_S/0/1/0/all/0/1&quot;&gt;Sebastian Neumayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Unser_M/0/1/0/all/0/1&quot;&gt;Michael Unser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12535">
<title>SCP: Spherical-Coordinate-based Learned Point Cloud Compression. (arXiv:2308.12535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12535</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the task of learned point cloud compression has gained
prominence. An important type of point cloud, the spinning LiDAR point cloud,
is generated by spinning LiDAR on vehicles. This process results in numerous
circular shapes and azimuthal angle invariance features within the point
clouds. However, these two features have been largely overlooked by previous
methodologies. In this paper, we introduce a model-agnostic method called
Spherical-Coordinate-based learned Point cloud compression (SCP), designed to
leverage the aforementioned features fully. Additionally, we propose a
multi-level Octree for SCP to mitigate the reconstruction error for distant
areas within the Spherical-coordinate-based Octree. SCP exhibits excellent
universality, making it applicable to various learned point cloud compression
techniques. Experimental results demonstrate that SCP surpasses previous
state-of-the-art methods by up to 29.14% in point-to-point PSNR BD-Rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1&quot;&gt;Ao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nonaka_K/0/1/0/all/0/1&quot;&gt;Keisuke Nonaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unno_K/0/1/0/all/0/1&quot;&gt;Kyohei Unno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Heming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goto_M/0/1/0/all/0/1&quot;&gt;Masayuki Goto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katto_J/0/1/0/all/0/1&quot;&gt;Jiro Katto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13739">
<title>Devignet: High-Resolution Vignetting Removal via a Dual Aggregated Fusion Transformer With Adaptive Channel Expansion. (arXiv:2308.13739v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13739</link>
<description rdf:parseType="Literal">&lt;p&gt;Vignetting commonly occurs as a degradation in images resulting from factors
such as lens design, improper lens hood usage, and limitations in camera
sensors. This degradation affects image details, color accuracy, and presents
challenges in computational photography. Existing vignetting removal algorithms
predominantly rely on ideal physics assumptions and hand-crafted parameters,
resulting in the ineffective removal of irregular vignetting and suboptimal
results. Moreover, the substantial lack of real-world vignetting datasets
hinders the objective and comprehensive evaluation of vignetting removal. To
address these challenges, we present Vigset, a pioneering dataset for
vignetting removal. Vigset includes 983 pairs of both vignetting and
vignetting-free high-resolution ($5340\times3697$) real-world images under
various conditions. In addition, We introduce DeVigNet, a novel frequency-aware
Transformer architecture designed for vignetting removal. Through the Laplacian
Pyramid decomposition, we propose the Dual Aggregated Fusion Transformer to
handle global features and remove vignetting in the low-frequency domain.
Additionally, we propose the Adaptive Channel Expansion Module to enhance
details in the high-frequency domain. The experiments demonstrate that the
proposed model outperforms existing state-of-the-art methods. The code, models,
and dataset are available at \url{https://github.com/CXH-Research/DeVigNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shenghong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zinuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14078">
<title>Sparse3D: Distilling Multiview-Consistent Diffusion for Object Reconstruction from Sparse Views. (arXiv:2308.14078v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14078</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing 3D objects from extremely sparse views is a long-standing and
challenging problem. While recent techniques employ image diffusion models for
generating plausible images at novel viewpoints or for distilling pre-trained
diffusion priors into 3D representations using score distillation sampling
(SDS), these methods often struggle to simultaneously achieve high-quality,
consistent, and detailed results for both novel-view synthesis (NVS) and
geometry. In this work, we present Sparse3D, a novel 3D reconstruction method
tailored for sparse view inputs. Our approach distills robust priors from a
multiview-consistent diffusion model to refine a neural radiance field.
Specifically, we employ a controller that harnesses epipolar features from
input views, guiding a pre-trained diffusion model, such as Stable Diffusion,
to produce novel-view images that maintain 3D consistency with the input. By
tapping into 2D priors from powerful image diffusion models, our integrated
model consistently delivers high-quality results, even when faced with
open-world objects. To address the blurriness introduced by conventional SDS,
we introduce the category-score distillation sampling (C-SDS) to enhance
detail. We conduct experiments on CO3DV2 which is a multi-view dataset of
real-world objects. Both quantitative and qualitative evaluations demonstrate
that our approach outperforms previous state-of-the-art works on the metrics
regarding NVS and geometry reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zi-Xin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Weihao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yan-Pei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shi-Sheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10689">
<title>ReShader: View-Dependent Highlights for Single Image View-Synthesis. (arXiv:2309.10689v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10689</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, novel view synthesis from a single image has seen
significant progress thanks to the rapid advancements in 3D scene
representation and image inpainting techniques. While the current approaches
are able to synthesize geometrically consistent novel views, they often do not
handle the view-dependent effects properly. Specifically, the highlights in
their synthesized images usually appear to be glued to the surfaces, making the
novel views unrealistic. To address this major problem, we make a key
observation that the process of synthesizing novel views requires changing the
shading of the pixels based on the novel camera, and moving them to appropriate
locations. Therefore, we propose to split the view synthesis process into two
independent tasks of pixel reshading and relocation. During the reshading
process, we take the single image as the input and adjust its shading based on
the novel camera. This reshaded image is then used as the input to an existing
view synthesis method to relocate the pixels and produce the final novel view
image. We propose to use a neural network to perform reshading and generate a
large set of synthetic input-reshaded pairs to train our network. We
demonstrate that our approach produces plausible novel view images with
realistic moving highlights on a variety of real world scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paliwal_A/0/1/0/all/0/1&quot;&gt;Avinash Paliwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Brandon Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsarov_A/0/1/0/all/0/1&quot;&gt;Andrii Tsarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalantari_N/0/1/0/all/0/1&quot;&gt;Nima Khademi Kalantari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06958">
<title>Comparing the robustness of modern no-reference image- and video-quality metrics to adversarial attacks. (arXiv:2310.06958v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06958</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays neural-network-based image- and video-quality metrics show better
performance compared to traditional methods. However, they also became more
vulnerable to adversarial attacks that increase metrics&apos; scores without
improving visual quality. The existing benchmarks of quality metrics compare
their performance in terms of correlation with subjective quality and
calculation time. However, the adversarial robustness of image-quality metrics
is also an area worth researching. In this paper, we analyse modern metrics&apos;
robustness to different adversarial attacks. We adopted adversarial attacks
from computer vision tasks and compared attacks&apos; efficiency against 15
no-reference image/video-quality metrics. Some metrics showed high resistance
to adversarial attacks which makes their usage in benchmarks safer than
vulnerable metrics. The benchmark accepts new metrics submissions for
researchers who want to make their metrics more robust to attacks or to find
such metrics for their needs. Try our benchmark using pip install
robustness-benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antsiferova_A/0/1/0/all/0/1&quot;&gt;Anastasia Antsiferova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abud_K/0/1/0/all/0/1&quot;&gt;Khaled Abud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gushchin_A/0/1/0/all/0/1&quot;&gt;Aleksandr Gushchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shumitskaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Shumitskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavrushkin_S/0/1/0/all/0/1&quot;&gt;Sergey Lavrushkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vatolin_D/0/1/0/all/0/1&quot;&gt;Dmitriy Vatolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14958">
<title>Learning Real-World Image De-Weathering with Imperfect Supervision. (arXiv:2310.14958v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14958</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world image de-weathering aims at removing various undesirable
weather-related artifacts. Owing to the impossibility of capturing image pairs
concurrently, existing real-world de-weathering datasets often exhibit
inconsistent illumination, position, and textures between the ground-truth
images and the input degraded images, resulting in imperfect supervision. Such
non-ideal supervision negatively affects the training process of learning-based
de-weathering methods. In this work, we attempt to address the problem with a
unified solution for various inconsistencies. Specifically, inspired by
information bottleneck theory, we first develop a Consistent Label Constructor
(CLC) to generate a pseudo-label as consistent as possible with the input
degraded image while removing most weather-related degradations. In particular,
multiple adjacent frames of the current input are also fed into CLC to enhance
the pseudo-label. Then we combine the original imperfect labels and
pseudo-labels to jointly supervise the de-weathering model by the proposed
Information Allocation Strategy (IAS). During testing, only the de-weathering
model is used for inference. Experiments on two real-world de-weathering
datasets show that our method helps existing de-weathering models achieve
better performance. Codes are available at
https://github.&lt;a href=&quot;/abs/com/1180300&quot;&gt;com/1180300&lt;/a&gt;419/imperfect-deweathering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhilu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaohe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chaoyu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaotao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LEI_L/0/1/0/all/0/1&quot;&gt;LEI LEI&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16999">
<title>Trust, but Verify: Robust Image Segmentation using Deep Learning. (arXiv:2310.16999v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16999</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a method for verifying the output of a deep neural network for
medical image segmentation that is robust to several classes of random as well
as worst-case perturbations i.e. adversarial attacks. This method is based on a
general approach recently developed by the authors called &quot;Trust, but Verify&quot;
wherein an auxiliary verification network produces predictions about certain
masked features in the input image using the segmentation as an input. A
well-designed auxiliary network will produce high-quality predictions when the
input segmentations are accurate, but will produce low-quality predictions when
the segmentations are incorrect. Checking the predictions of such a network
with the original image allows us to detect bad segmentations. However, to
ensure the verification method is truly robust, we need a method for checking
the quality of the predictions that does not itself rely on a black-box neural
network. Indeed, we show that previous methods for segmentation evaluation that
do use deep neural regression networks are vulnerable to false negatives i.e.
can inaccurately label bad segmentations as good. We describe the design of a
verification network that avoids such vulnerability and present results to
demonstrate its robustness compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Fahim Ahmed Zaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weiyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonka_M/0/1/0/all/0/1&quot;&gt;Milan Sonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudumbai_R/0/1/0/all/0/1&quot;&gt;Raghuraman Mudumbai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04207">
<title>Deep Hashing via Householder Quantization. (arXiv:2311.04207v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04207</link>
<description rdf:parseType="Literal">&lt;p&gt;Hashing is at the heart of large-scale image similarity search, and recent
methods have been substantially improved through deep learning techniques. Such
algorithms typically learn continuous embeddings of the data. To avoid a
subsequent costly binarization step, a common solution is to employ loss
functions that combine a similarity learning term (to ensure similar images are
grouped to nearby embeddings) and a quantization penalty term (to ensure that
the embedding entries are close to binarized entries, e.g., -1 or 1). Still,
the interaction between these two terms can make learning harder and the
embeddings worse. We propose an alternative quantization strategy that
decomposes the learning problem in two stages: first, perform similarity
learning over the embedding space with no quantization; second, find an optimal
orthogonal transformation of the embeddings so each coordinate of the embedding
is close to its sign, and then quantize the transformed embedding through the
sign function. In the second step, we parametrize orthogonal transformations
using Householder matrices to efficiently leverage stochastic gradient descent.
Since similarity measures are usually invariant under orthogonal
transformations, this quantization strategy comes at no cost in terms of
performance. The resulting algorithm is unsupervised, fast, hyperparameter-free
and can be run on top of any existing deep hashing or metric learning
algorithm. We provide extensive experimental results showing that this approach
leads to state-of-the-art performance on widely used image datasets, and,
unlike other quantization strategies, brings consistent improvements in
performance to existing deep hashing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwengber_L/0/1/0/all/0/1&quot;&gt;Lucas R. Schwengber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Resende_L/0/1/0/all/0/1&quot;&gt;Lucas Resende&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orenstein_P/0/1/0/all/0/1&quot;&gt;Paulo Orenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1&quot;&gt;Roberto I. Oliveira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11383">
<title>A Survey of Emerging Applications of Diffusion Probabilistic Models in MRI. (arXiv:2311.11383v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11383</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion probabilistic models (DPMs) which employ explicit likelihood
characterization and a gradual sampling process to synthesize data, have gained
increasing research interest. Despite their huge computational burdens due to
the large number of steps involved during sampling, DPMs are widely appreciated
in various medical imaging tasks for their high-quality and diversity of
generation. Magnetic resonance imaging (MRI) is an important medical imaging
modality with excellent soft tissue contrast and superb spatial resolution,
which possesses unique opportunities for DPMs. Although there is a recent surge
of studies exploring DPMs in MRI, a survey paper of DPMs specifically designed
for MRI applications is still lacking. This review article aims to help
researchers in the MRI community to grasp the advances of DPMs in different
applications. We first introduce the theory of two dominant kinds of DPMs,
categorized according to whether the diffusion time step is discrete or
continuous, and then provide a comprehensive review of emerging DPMs in MRI,
including reconstruction, image generation, image translation, segmentation,
anomaly detection, and further research topics. Finally, we discuss the general
limitations as well as limitations specific to the MRI tasks of DPMs and point
out potential areas that are worth further exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuheng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Hanxi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiqi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yimin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Haikun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13073">
<title>FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline. (arXiv:2311.13073v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13073</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimedia generation approaches occupy a prominent place in artificial
intelligence research. Text-to-image models achieved high-quality results over
the last few years. However, video synthesis methods recently started to
develop. This paper presents a new two-stage latent diffusion text-to-video
generation architecture based on the text-to-image diffusion model. The first
stage concerns keyframes synthesis to figure the storyline of a video, while
the second one is devoted to interpolation frames generation to make movements
of the scene and objects smooth. We compare several temporal conditioning
approaches for keyframes generation. The results show the advantage of using
separate temporal blocks over temporal layers in terms of metrics reflecting
video generation quality aspects and human preference. The design of our
interpolation model significantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we evaluate different
configurations of MoVQ-based video decoding scheme to improve consistency and
achieve higher PSNR, SSIM, MSE, and LPIPS scores. Finally, we compare our
pipeline with existing solutions and achieve top-2 scores overall and top-1
among open-source solutions: CLIPSIM = 0.2976 and FVD = 433.054. Project page:
https://ai-forever.github.io/kandinsky-video/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arkhipkin_V/0/1/0/all/0/1&quot;&gt;Vladimir Arkhipkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaheen_Z/0/1/0/all/0/1&quot;&gt;Zein Shaheen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilev_V/0/1/0/all/0/1&quot;&gt;Viacheslav Vasilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dakhova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Dakhova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznetsov_A/0/1/0/all/0/1&quot;&gt;Andrey Kuznetsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1&quot;&gt;Denis Dimitrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14521">
<title>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting. (arXiv:2311.14521v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14521</link>
<description rdf:parseType="Literal">&lt;p&gt;3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor&apos;s
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15803">
<title>SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields. (arXiv:2311.15803v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15803</link>
<description rdf:parseType="Literal">&lt;p&gt;In rapidly-evolving domains such as autonomous driving, the use of multiple
sensors with different modalities is crucial to ensure high operational
precision and stability. To correctly exploit the provided information by each
sensor in a single common frame, it is essential for these sensors to be
accurately calibrated. In this paper, we leverage the ability of Neural
Radiance Fields (NeRF) to represent different sensors modalities in a common
volumetric representation to achieve robust and accurate spatio-temporal sensor
calibration. By designing a partitioning approach based on the visible part of
the scene for each sensor, we formulate the calibration problem using only the
overlapping areas. This strategy results in a more robust and accurate
calibration that is less prone to failure. We demonstrate that our approach
works on outdoor urban scenes by validating it on multiple established driving
datasets. Results show that our method is able to get better accuracy and
robustness compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herau_Q/0/1/0/all/0/1&quot;&gt;Quentin Herau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piasco_N/0/1/0/all/0/1&quot;&gt;Nathan Piasco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennehar_M/0/1/0/all/0/1&quot;&gt;Moussab Bennehar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roldao_L/0/1/0/all/0/1&quot;&gt;Luis Rold&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsishkou_D/0/1/0/all/0/1&quot;&gt;Dzmitry Tsishkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Migniot_C/0/1/0/all/0/1&quot;&gt;Cyrille Migniot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1&quot;&gt;Pascal Vasseur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Demonceaux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02464">
<title>SAM-Assisted Remote Sensing Imagery Semantic Segmentation with Object and Boundary Constraints. (arXiv:2312.02464v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02464</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of remote sensing imagery plays a pivotal role in
extracting precise information for diverse down-stream applications. Recent
development of the Segment Anything Model (SAM), an advanced general-purpose
segmentation model, has revolutionized this field, presenting new avenues for
accurate and efficient segmentation. However, SAM is limited to generating
segmentation results without class information. Consequently, the utilization
of such a powerful general vision model for semantic segmentation in remote
sensing images has become a focal point of research. In this paper, we present
a streamlined framework aimed at leveraging the raw output of SAM by exploiting
two novel concepts called SAM-Generated Object (SGO) and SAM-Generated Boundary
(SGB). More specifically, we propose a novel object loss and further introduce
a boundary loss as augmentative components to aid in model optimization in a
general semantic segmentation framework. Taking into account the content
characteristics of SGO, we introduce the concept of object consistency to
leverage segmented regions lacking semantic information. By imposing
constraints on the consistency of predicted values within objects, the object
loss aims to enhance semantic segmentation performance. Furthermore, the
boundary loss capitalizes on the distinctive features of SGB by directing the
model&apos;s attention to the boundary information of the object. Experimental
results on two well-known datasets, namely ISPRS Vaihingen and LoveDA Urban,
demonstrate the effectiveness of our proposed method. The source code for this
work will be accessible at https://github.com/sstary/SSRS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianping Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qianqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_M/0/1/0/all/0/1&quot;&gt;Man-On Pun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bo Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02916">
<title>MIND: Multi-Task Incremental Network Distillation. (arXiv:2312.02916v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02916</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge of pervasive devices that generate dynamic data streams has
underscored the necessity for learning systems to adapt continually to data
distributional shifts. To tackle this challenge, the research community has put
forth a spectrum of methodologies, including the demanding pursuit of
class-incremental learning without replay data. In this study, we present MIND,
a parameter isolation method that aims to significantly enhance the performance
of replay-free solutions and achieve state-of-the-art results on several widely
studied datasets. Our approach introduces two main contributions: two
alternative distillation procedures that significantly improve the efficiency
of MIND increasing the accumulated knowledge of each sub-network, and the
optimization of the BachNorm layers across tasks inside the sub-networks.
Overall, MIND outperforms all the state-of-the-art methods for rehearsal-free
Class-Incremental learning (with an increment in classification accuracy of
approx. +6% on CIFAR-100/10 and +10% on TinyImageNet/10) reaching up to approx.
+40% accuracy in Domain-Incremental scenarios. Moreover, we ablated each
contribution to demonstrate its impact on performance improvement. Our results
showcase the superior performance of MIND indicating its potential for
addressing the challenges posed by Class-incremental and Domain-Incremental
learning in resource-constrained environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonato_J/0/1/0/all/0/1&quot;&gt;Jacopo Bonato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1&quot;&gt;Francesco Pelosin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabetta_L/0/1/0/all/0/1&quot;&gt;Luigi Sabetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolosi_A/0/1/0/all/0/1&quot;&gt;Alessandro Nicolosi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03795">
<title>AnimatableDreamer: Text-Guided Non-rigid 3D Model Generation and Reconstruction with Canonical Score Distillation. (arXiv:2312.03795v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03795</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D model adaptations have advanced static 3D model quality, but
sequential 3D model generation, particularly for animatable objects with large
motions, is still scarce. Our work proposes AnimatableDreamer, a text-to-4D
generation framework capable of generating diverse categories of non-rigid
objects while adhering to the object motions extracted from a monocular video.
At its core, AnimatableDreamer is equipped with our novel optimization design
dubbed Canonical Score Distillation (CSD), which simplifies the generation
dimension from 4D to 3D by denoising over different frames in the time-varying
camera spaces while conducting the distillation process in a unique canonical
space shared per video. Concretely, CSD ensures that score gradients
back-propagate to the canonical space through differentiable warping, hence
guaranteeing the time-consistent generation and maintaining morphological
plausibility across different poses. By lifting the 3D generator to 4D with
warping functions, AnimatableDreamer offers a novel perspective on non-rigid 3D
model generation and reconstruction. Besides, with inductive knowledge from a
multi-view consistent diffusion model, CSD regularizes reconstruction from
novel views, thus cyclically enhancing the generation process. Extensive
experiments demonstrate the capability of our method in generating
high-flexibility text-guided 3D models from the monocular video, while also
showing improved reconstruction performance over typical non-rigid
reconstruction methods. Project page https://AnimatableDreamer.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinzhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junliang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pengkun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Ling Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Kai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04810">
<title>RS-Corrector: Correcting the Racial Stereotypes in Latent Diffusion Models. (arXiv:2312.04810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04810</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-conditioned image generation models have demonstrated an
exceptional capacity to produce diverse and creative imagery with high visual
quality. However, when pre-trained on billion-sized datasets randomly collected
from the Internet, where potential biased human preferences exist, these models
tend to produce images with common and recurring stereotypes, particularly for
certain racial groups. In this paper, we conduct an initial analysis of the
publicly available Stable Diffusion model and its derivatives, highlighting the
presence of racial stereotypes. These models often generate distorted or biased
images for certain racial groups, emphasizing stereotypical characteristics. To
address these issues, we propose a framework called &quot;RS-Corrector&quot;, designed to
establish an anti-stereotypical preference in the latent space and update the
latent code for refined generated results. The correction process occurs during
the inference stage without requiring fine-tuning of the original model.
Extensive empirical evaluations demonstrate that the introduced \themodel
effectively corrects the racial stereotypes of the well-trained Stable
Diffusion model while leaving the original model unchanged.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yueming Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tianxiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04875">
<title>MVDD: Multi-View Depth Diffusion Models. (arXiv:2312.04875v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models have demonstrated outstanding results in 2D image
generation, yet it remains a challenge to replicate its success in 3D shape
generation. In this paper, we propose leveraging multi-view depth, which
represents complex 3D shapes in a 2D data format that is easy to denoise. We
pair this representation with a diffusion model, MVDD, that is capable of
generating high-quality dense point clouds with 20K+ points with fine-grained
details. To enforce 3D consistency in multi-view depth, we introduce an
epipolar line segment attention that conditions the denoising step for a view
on its neighboring views. Additionally, a depth fusion module is incorporated
into diffusion steps to further ensure the alignment of depth maps. When
augmented with surface reconstruction, MVDD can also produce high-quality 3D
meshes. Furthermore, MVDD stands out in other tasks such as depth completion,
and can serve as a 3D prior, significantly boosting many downstream tasks, such
as GAN inversion. State-of-the-art results from extensive experiments
demonstrate MVDD&apos;s excellent ability in 3D shape generation, depth completion,
and its potential as a 3D prior for downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qiangeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_F/0/1/0/all/0/1&quot;&gt;Feitong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1&quot;&gt;Menglei Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shichen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Rohit Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fanello_S/0/1/0/all/0/1&quot;&gt;Sean Fanello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadambi_A/0/1/0/all/0/1&quot;&gt;Achuta Kadambi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06106">
<title>AUGCAL: Improving Sim2Real Adaptation by Uncertainty Calibration on Augmented Synthetic Images. (arXiv:2312.06106v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06106</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic data (SIM) drawn from simulators have emerged as a popular
alternative for training models where acquiring annotated real-world images is
difficult. However, transferring models trained on synthetic images to
real-world applications can be challenging due to appearance disparities. A
commonly employed solution to counter this SIM2REAL gap is unsupervised domain
adaptation, where models are trained using labeled SIM data and unlabeled REAL
data. Mispredictions made by such SIM2REAL adapted models are often associated
with miscalibration - stemming from overconfident predictions on real data. In
this paper, we introduce AUGCAL, a simple training-time patch for unsupervised
adaptation that improves SIM2REAL adapted models by - (1) reducing overall
miscalibration, (2) reducing overconfidence in incorrect predictions and (3)
improving confidence score reliability by better guiding misclassification
detection - all while retaining or improving SIM2REAL performance. Given a base
SIM2REAL adaptation algorithm, at training time, AUGCAL involves replacing
vanilla SIM images with strongly augmented views (AUG intervention) and
additionally optimizing for a training time calibration loss on augmented SIM
predictions (CAL intervention). We motivate AUGCAL using a brief analytical
justification of how to reduce miscalibration on unlabeled REAL data. Through
our experiments, we empirically show the efficacy of AUGCAL across multiple
adaptation methods, backbones, tasks and shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_P/0/1/0/all/0/1&quot;&gt;Prithvijit Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_B/0/1/0/all/0/1&quot;&gt;Bharat Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ecsedi_B/0/1/0/all/0/1&quot;&gt;Boglarka Ecsedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_V/0/1/0/all/0/1&quot;&gt;Viraj Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06914">
<title>Exploring Novel Object Recognition and Spontaneous Location Recognition Machine Learning Analysis Techniques in Alzheimer&apos;s Mice. (arXiv:2312.06914v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06914</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding object recognition patterns in mice is crucial for advancing
behavioral neuroscience and has significant implications for human health,
particularly in the realm of Alzheimer&apos;s research. This study is centered on
the development, application, and evaluation of a state-of-the-art
computational pipeline designed to analyze such behaviors, specifically
focusing on Novel Object Recognition (NOR) and Spontaneous Location Recognition
(SLR) tasks. The pipeline integrates three advanced computational models:
Any-Maze for initial data collection, DeepLabCut for detailed pose estimation,
and Convolutional Neural Networks (CNNs) for nuanced behavioral classification.
Employed across four distinct mouse groups, this pipeline demonstrated high
levels of accuracy and robustness. Despite certain challenges like video
quality limitations and the need for manual calculations, the results affirm
the pipeline&apos;s efficacy and potential for scalability. The study serves as a
proof of concept for a multidimensional computational approach to behavioral
neuroscience, emphasizing the pipeline&apos;s versatility and readiness for future,
more complex analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bafana_S/0/1/0/all/0/1&quot;&gt;Soham Bafana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07879">
<title>CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation. (arXiv:2312.07879v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07879</link>
<description rdf:parseType="Literal">&lt;p&gt;Current text-to-image editing models often encounter challenges with smoothly
manipulating multiple attributes using a single instruction. Taking inspiration
from the Chain-of-Thought prompting technique utilized in language models, we
present an innovative concept known as Chain-of-Instruct Editing (CoIE), which
enhances the capabilities of these models through step-by-step editing using a
series of instructions. In particular, in the context of face manipulation, we
leverage the contextual learning abilities of a pretrained Large Language Model
(LLM), such as GPT-4, to generate a sequence of instructions from the original
input, utilizing a purpose-designed 1-shot template. To further improve the
precision of each editing step, we conduct fine-tuning on the editing models
using our self-constructed instruction-guided face editing dataset,
Instruct-CelebA. And additionally, we incorporate a super-resolution module to
mitigate the adverse effects of editability and quality degradation.
Experimental results across various challenging cases confirm the significant
boost in multi-attribute facial image manipulation using chain-of-instruct
editing. This is evident in enhanced editing success rates, measured by CLIPSim
and Coverage metrics, improved by 17.86% and 85.45% respectively, and
heightened controllability indicated by Preserve L1 and Quality metrics,
improved by 11.58% and 4.93% respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenduo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo-Wen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07937">
<title>BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics. (arXiv:2312.07937v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07937</link>
<description rdf:parseType="Literal">&lt;p&gt;The recently emerging text-to-motion advances have spired numerous attempts
for convenient and interactive human motion generation. Yet, existing methods
are largely limited to generating body motions only without considering the
rich two-hand motions, let alone handling various conditions like body dynamics
or texts. To break the data bottleneck, we propose BOTH57M, a novel multi-modal
dataset for two-hand motion generation. Our dataset includes accurate motion
tracking for the human body and hands and provides pair-wised finger-level hand
annotations and body descriptions. We further provide a strong baseline method,
BOTH2Hands, for the novel task: generating vivid two-hand motions from both
implicit body dynamics and explicit text prompts. We first warm up two parallel
body-to-hand and text-to-hand diffusion models and then utilize the
cross-attention transformer for motion blending. Extensive experiments and
cross-validations demonstrate the effectiveness of our approach and dataset for
generating convincing two-hand motions from the hybrid body-and-textual
conditions. Our dataset and code will be disseminated to the community for
future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Molin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juze Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingya Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08288">
<title>Hybrid Sample Synthesis-based Debiasing of Classifier in Limited Data Setting. (arXiv:2312.08288v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08288</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are known to suffer from the problem of bias, and
researchers have been exploring methods to address this issue. However, most of
these methods require prior knowledge of the bias and are not always practical.
In this paper, we focus on a more practical setting with no prior information
about the bias. Generally, in this setting, there are a large number of
bias-aligned samples that cause the model to produce biased predictions and a
few bias-conflicting samples that do not conform to the bias. If the training
data is limited, the influence of the bias-aligned samples may become even
stronger on the model predictions, and we experimentally demonstrate that
existing debiasing techniques suffer severely in such cases. In this paper, we
examine the effects of unknown bias in small dataset regimes and present a
novel approach to mitigate this issue. The proposed approach directly addresses
the issue of the extremely low occurrence of bias-conflicting samples in
limited data settings through the synthesis of hybrid samples that can be used
to reduce the effect of bias. We perform extensive experiments on several
benchmark datasets and experimentally demonstrate the effectiveness of our
proposed approach in addressing any unknown bias in the presence of limited
data. Specifically, our approach outperforms the vanilla, LfF, LDD, and DebiAN
debiasing methods by absolute margins of 10.39%, 9.08%, 8.07%, and 9.67% when
only 10% of the Corrupted CIFAR-10 Type 1 dataset is available with a
bias-conflicting sample ratio of 0.05.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_P/0/1/0/all/0/1&quot;&gt;Piyush Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_P/0/1/0/all/0/1&quot;&gt;Pratik Mazumder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08488">
<title>PnP for Two-Dimensional Pose Estimation. (arXiv:2312.08488v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08488</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a PnP algorithm for a camera constrained to two-dimensional
movement (applicable, for instance, to many wheeled robotics platforms).
Leveraging this assumption allows performance improvements over 3D PnP
algorithms due to the reduction in search space dimensionality. It also reduces
the incidence of ambiguous pose estimates (as, in most cases, the spurious
solutions fall outside the plane of movement). Our algorithm finds an
approximate solution using geometric criteria and refines its prediction
iteratively. We compare this algorithm to existing 3D PnP algorithms in terms
of accuracy, performance, and robustness to noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Joshua Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08866">
<title>MCANet: Medical Image Segmentation with Multi-Scale Cross-Axis Attention. (arXiv:2312.08866v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08866</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently capturing multi-scale information and building long-range
dependencies among pixels are essential for medical image segmentation because
of the various sizes and shapes of the lesion regions or organs. In this paper,
we present Multi-scale Cross-axis Attention (MCA) to solve the above
challenging issues based on the efficient axial attention. Instead of simply
connecting axial attention along the horizontal and vertical directions
sequentially, we propose to calculate dual cross attentions between two
parallel axial attentions to capture global information better. To process the
significant variations of lesion regions or organs in individual sizes and
shapes, we also use multiple convolutions of strip-shape kernels with different
kernel sizes in each axial attention path to improve the efficiency of the
proposed MCA in encoding spatial information. We build the proposed MCA upon
the MSCAN backbone, yielding our network, termed MCANet. Our MCANet with only
4M+ parameters performs even better than most previous works with heavy
backbones (e.g., Swin Transformer) on four challenging tasks, including skin
lesion segmentation, nuclei segmentation, abdominal multi-organ segmentation,
and polyp segmentation. Code is available at
https://github.com/haoshao-nku/medical_seg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Quansheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hou_Q/0/1/0/all/0/1&quot;&gt;Qibin Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jufeng Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10461">
<title>Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection. (arXiv:2312.10461v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10461</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the proliferation of highly realistic synthetic images, facilitated
through a variety of GANs and Diffusions, has significantly heightened the
susceptibility to misuse. While the primary focus of deepfake detection has
traditionally centered on the design of detection algorithms, an investigative
inquiry into the generator architectures has remained conspicuously absent in
recent years. This paper contributes to this lacuna by rethinking the
architectures of CNN-based generators, thereby establishing a generalized
representation of synthetic artifacts. Our findings illuminate that the
up-sampling operator can, beyond frequency-based artifacts, produce generalized
forgery artifacts. In particular, the local interdependence among image pixels
caused by upsampling operators is significantly demonstrated in synthetic
images generated by GAN or diffusion. Building upon this observation, we
introduce the concept of Neighboring Pixel Relationships(NPR) as a means to
capture and characterize the generalized structural artifacts stemming from
up-sampling operations. A comprehensive analysis is conducted on an open-world
dataset, comprising samples generated by \tft{28 distinct generative models}.
This analysis culminates in the establishment of a novel state-of-the-art
performance, showcasing a remarkable \tft{11.6\%} improvement over existing
methods. The code is available at
https://github.com/chuangchuangtan/NPR-DeepfakeDetection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chuangchuang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1&quot;&gt;Shikui Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1&quot;&gt;Guanghua Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Ping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11057">
<title>DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models. (arXiv:2312.11057v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset sanitization is a widely adopted proactive defense against
poisoning-based backdoor attacks, aimed at filtering out and removing poisoned
samples from training datasets. However, existing methods have shown limited
efficacy in countering the ever-evolving trigger functions, and often leading
to considerable degradation of benign accuracy. In this paper, we propose
DataElixir, a novel sanitization approach tailored to purify poisoned datasets.
We leverage diffusion models to eliminate trigger features and restore benign
features, thereby turning the poisoned samples into benign ones. Specifically,
with multiple iterations of the forward and reverse process, we extract
intermediary images and their predicted labels for each sample in the original
dataset. Then, we identify anomalous samples in terms of the presence of label
transition of the intermediary images, detect the target label by quantifying
distribution discrepancy, select their purified images considering pixel and
feature distance, and determine their ground-truth labels by training a benign
model. Experiments conducted on 9 popular attacks demonstrates that DataElixir
effectively mitigates various complex attacks while exerting minimal impact on
benign accuracy, surpassing the performance of baseline defense methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiachen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1&quot;&gt;Peizhuo Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yibing Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1&quot;&gt;Guozhu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hualong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook. (arXiv:2312.11562v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11841">
<title>MixRT: Mixed Neural Representations For Real-Time NeRF Rendering. (arXiv:2312.11841v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11841</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has emerged as a leading technique for novel
view synthesis, owing to its impressive photorealistic reconstruction and
rendering capability. Nevertheless, achieving real-time NeRF rendering in
large-scale scenes has presented challenges, often leading to the adoption of
either intricate baked mesh representations with a substantial number of
triangles or resource-intensive ray marching in baked representations. We
challenge these conventions, observing that high-quality geometry, represented
by meshes with substantial triangles, is not necessary for achieving
photorealistic rendering quality. Consequently, we propose MixRT, a novel NeRF
representation that includes a low-quality mesh, a view-dependent displacement
map, and a compressed NeRF model. This design effectively harnesses the
capabilities of existing graphics hardware, thus enabling real-time NeRF
rendering on edge devices. Leveraging a highly-optimized WebGL-based rendering
framework, our proposed MixRT attains real-time rendering speeds on edge
devices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),
better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360
datasets), and a smaller storage size (less than 80% compared to
state-of-the-art methods).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chaojian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Bichen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1&quot;&gt;Peter Vajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yingyan/0/1/0/all/0/1&quot;&gt;Yingyan&lt;/a&gt; (Celine)Lin</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12096">
<title>DLCA-Recon: Dynamic Loose Clothing Avatar Reconstruction from Monocular Videos. (arXiv:2312.12096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12096</link>
<description rdf:parseType="Literal">&lt;p&gt;Reconstructing a dynamic human with loose clothing is an important but
difficult task. To address this challenge, we propose a method named DLCA-Recon
to create human avatars from monocular videos. The distance from loose clothing
to the underlying body rapidly changes in every frame when the human freely
moves and acts. Previous methods lack effective geometric initialization and
constraints for guiding the optimization of deformation to explain this
dramatic change, resulting in the discontinuous and incomplete reconstruction
surface. To model the deformation more accurately, we propose to initialize an
estimated 3D clothed human in the canonical space, as it is easier for
deformation fields to learn from the clothed human than from SMPL. With both
representations of explicit mesh and implicit SDF, we utilize the physical
connection information between consecutive frames and propose a dynamic
deformation field (DDF) to optimize deformation fields. DDF accounts for
contributive forces on loose clothing to enhance the interpretability of
deformations and effectively capture the free movement of loose clothing.
Moreover, we propagate SMPL skinning weights to each individual and refine pose
and skinning weights during the optimization to improve skinning
transformation. Based on more reasonable initialization and DDF, we can
simulate real-world physics more accurately. Extensive experiments on public
and our own datasets validate that our method can produce superior results for
humans with loose clothing compared to the SOTA methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1&quot;&gt;Enxu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chunxia Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12143">
<title>Integrating Human Vision Perception in Vision Transformers for Classifying Waste Items. (arXiv:2312.12143v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12143</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose an novel methodology aimed at simulating the
learning phenomenon of nystagmus through the application of differential
blurring on datasets. Nystagmus is a biological phenomenon that influences
human vision throughout life, notably by diminishing head shake from infancy to
adulthood. Leveraging this concept, we address the issue of waste
classification, a pressing global concern. The proposed framework comprises two
modules, with the second module closely resembling the original Vision
Transformer, a state-of-the-art model model in classification tasks. The
primary motivation behind our approach is to enhance the model&apos;s precision and
adaptability, mirroring the real-world conditions that the human visual system
undergoes. This novel methodology surpasses the standard Vision Transformer
model in waste classification tasks, exhibiting an improvement with a margin of
2%. This improvement underscores the potential of our methodology in improving
model precision by drawing inspiration from human vision perception. Further
research in the proposed methodology could yield greater performance results,
and can be extrapolated to other global issues.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Akshat Kishore Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_T/0/1/0/all/0/1&quot;&gt;Tapan Kumar Gandhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12263">
<title>FedDiv: Collaborative Noise Filtering for Federated Learning with Noisy Labels. (arXiv:2312.12263v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12263</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning with noisy labels (F-LNL) aims at seeking an optimal
server model via collaborative distributed learning by aggregating multiple
client models trained with local noisy or clean samples. On the basis of a
federated learning framework, recent advances primarily adopt label noise
filtering to separate clean samples from noisy ones on each client, thereby
mitigating the negative impact of label noise. However, these prior methods do
not learn noise filters by exploiting knowledge across all clients, leading to
sub-optimal and inferior noise filtering performance and thus damaging training
stability. In this paper, we present FedDiv to tackle the challenges of F-LNL.
Specifically, we propose a global noise filter called Federated Noise Filter
for effectively identifying samples with noisy labels on every client, thereby
raising stability during local training sessions. Without sacrificing data
privacy, this is achieved by modeling the global distribution of label noise
across all clients. Then, in an effort to make the global model achieve higher
performance, we introduce a Predictive Consistency based Sampler to identify
more credible local data for local model training, thus preventing noise
memorization and further boosting the training stability. Extensive experiments
on CIFAR-10, CIFAR-100, and Clothing1M demonstrate that \texttt{FedDiv}
achieves superior performance over state-of-the-art F-LNL methods under
different label noise settings for both IID and non-IID data partitions. Source
code is publicly available at https://github.com/lijichang/FLNL-FedDiv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jichang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hui Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12340">
<title>Scalable Geometric Fracture Assembly via Co-creation Space among Assemblers. (arXiv:2312.12340v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12340</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric fracture assembly presents a challenging practical task in
archaeology and 3D computer vision. Previous methods have focused solely on
assembling fragments based on semantic information, which has limited the
quantity of objects that can be effectively assembled. Therefore, there is a
need to develop a scalable framework for geometric fracture assembly without
relying on semantic information. To improve the effectiveness of assembling
geometric fractures without semantic information, we propose a co-creation
space comprising several assemblers capable of gradually and unambiguously
assembling fractures. Additionally, we introduce a novel loss function, i.e.,
the geometric-based collision loss, to address collision issues during the
fracture assembly process and enhance the results. Our framework exhibits
better performance on both PartNet and Breaking Bad datasets compared to
existing state-of-the-art frameworks. Extensive experiments and quantitative
comparisons demonstrate the effectiveness of our proposed framework, which
features linear computational complexity, enhanced abstraction, and improved
generalization. Our code is publicly available at
https://github.com/Ruiyuan-Zhang/CCS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zexi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12436">
<title>A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise. (arXiv:2312.12436v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12436</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge of interest towards Multi-modal Large Language Models (MLLMs),
e.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both
academia and industry. They endow Large Language Models (LLMs) with powerful
capabilities in visual understanding, enabling them to tackle diverse
multi-modal tasks. Very recently, Google released Gemini, its newest and most
capable MLLM built from the ground up for multi-modality. In light of the
superior reasoning capabilities, can Gemini challenge GPT-4V&apos;s leading position
in multi-modal learning? In this paper, we present a preliminary exploration of
Gemini Pro&apos;s visual understanding proficiency, which comprehensively covers
four domains: fundamental perception, advanced cognition, challenging vision
tasks, and various expert capacities. We compare Gemini Pro with the
state-of-the-art GPT-4V to evaluate its upper limits, along with the latest
open-sourced MLLM, Sphinx, which reveals the gap between manual efforts and
black-box systems. The qualitative samples indicate that, while GPT-4V and
Gemini showcase different answering styles and preferences, they can exhibit
comparable visual reasoning capabilities, and Sphinx still trails behind them
concerning domain generalizability. Specifically, GPT-4V tends to elaborate
detailed explanations and intermediate steps, and Gemini prefers to output a
direct and concise answer. The quantitative evaluation on the popular MME
benchmark also demonstrates the potential of Gemini to be a strong challenger
to GPT-4V. Our early investigation of Gemini also observes some common issues
of MLLMs, indicating that there still remains a considerable distance towards
artificial general intelligence. Our project for tracking the progress of MLLM
is released at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chaoyou Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yubo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Longtian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Gaoxiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengdan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peixian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sirui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaohui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Deqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Di Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>