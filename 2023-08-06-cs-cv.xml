<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01499" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01661" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.11233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.12844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.15402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.07901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.09957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.11435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.06267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11363" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01239" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.01318">
<title>Framing image registration as a landmark detection problem for better representation of clinical relevance. (arXiv:2308.01318v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01318</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, registration methods are typically evaluated based on
sub-resolution tracking error differences. In an effort to reinfuse this
evaluation process with clinical relevance, we propose to reframe image
registration as a landmark detection problem. Ideally, landmark-specific
detection thresholds are derived from an inter-rater analysis. To approximate
this costly process, we propose to compute hit rate curves based on the
distribution of errors of a sub-sample inter-rater analysis. Therefore, we
suggest deriving thresholds from the error distribution using the formula:
median + delta * median absolute deviation. The method promises differentiation
of previously indistinguishable registration algorithms and further enables
assessing the clinical significance in algorithm development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Waldmannstetter_D/0/1/0/all/0/1&quot;&gt;Diana Waldmannstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schwarting_J/0/1/0/all/0/1&quot;&gt;Julian Schwarting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Metz_M/0/1/0/all/0/1&quot;&gt;Marie Metz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1&quot;&gt;Spyridon Bakas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baheti_B/0/1/0/all/0/1&quot;&gt;Bhakti Baheti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakrabarty_S/0/1/0/all/0/1&quot;&gt;Satrajit Chakrabarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan S. Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heckemann_R/0/1/0/all/0/1&quot;&gt;Rolf A. Heckemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1&quot;&gt;Marie Piraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern H. Menze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01328">
<title>A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models. (arXiv:2308.01328v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01328</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining lymphoma subtypes is a crucial step for better patients treatment
targeting to potentially increase their survival chances. In this context, the
existing gold standard diagnosis method, which is based on gene expression
technology, is highly expensive and time-consuming making difficult its
accessibility. Although alternative diagnosis methods based on IHC
(immunohistochemistry) technologies exist (recommended by the WHO), they still
suffer from similar limitations and are less accurate. WSI (Whole Slide Image)
analysis by deep learning models showed promising new directions for cancer
diagnosis that would be cheaper and faster than existing alternative methods.
In this work, we propose a vision transformer-based framework for
distinguishing DLBCL (Diffuse Large B-Cell Lymphoma) cancer subtypes from
high-resolution WSIs. To this end, we propose a multi-modal architecture to
train a classifier model from various WSI modalities. We then exploit this
model through a knowledge distillation mechanism for efficiently driving the
learning of a mono-modal classifier. Our experimental study conducted on a
dataset of 157 patients shows the promising performance of our mono-modal
classification model, outperforming six recent methods from the
state-of-the-art dedicated for cancer classification. Moreover, the power-law
curve, estimated on our experimental data, shows that our classification model
requires a reasonable number of additional patients for its training to
potentially reach identical diagnosis accuracy as IHC technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guetarni_B/0/1/0/all/0/1&quot;&gt;Bilel Guetarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Windal_F/0/1/0/all/0/1&quot;&gt;Feryal Windal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Benhabiles_H/0/1/0/all/0/1&quot;&gt;Halim Benhabiles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Petit_M/0/1/0/all/0/1&quot;&gt;Marianne Petit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dubois_R/0/1/0/all/0/1&quot;&gt;Romain Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leteurtre_E/0/1/0/all/0/1&quot;&gt;Emmanuelle Leteurtre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Collard_D/0/1/0/all/0/1&quot;&gt;Dominique Collard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01379">
<title>Computational Long Exposure Mobile Photography. (arXiv:2308.01379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01379</link>
<description rdf:parseType="Literal">&lt;p&gt;Long exposure photography produces stunning imagery, representing moving
elements in a scene with motion-blur. It is generally employed in two
modalities, producing either a foreground or a background blur effect.
Foreground blur images are traditionally captured on a tripod-mounted camera
and portray blurred moving foreground elements, such as silky water or light
trails, over a perfectly sharp background landscape. Background blur images,
also called panning photography, are captured while the camera is tracking a
moving subject, to produce an image of a sharp subject over a background
blurred by relative motion. Both techniques are notoriously challenging and
require additional equipment and advanced skills. In this paper, we describe a
computational burst photography system that operates in a hand-held smartphone
camera app, and achieves these effects fully automatically, at the tap of the
shutter button. Our approach first detects and segments the salient subject. We
track the scene motion over multiple frames and align the images in order to
preserve desired sharpness and to produce aesthetically pleasing motion
streaks. We capture an under-exposed burst and select the subset of input
frames that will produce blur trails of controlled length, regardless of scene
or camera motion velocity. We predict inter-frame motion and synthesize
motion-blur to fill the temporal gaps between the input frames. Finally, we
composite the blurred image with the sharp regular exposure to protect the
sharpness of faces or areas of the scene that are barely moving, and produce a
final high resolution and high dynamic range (HDR) photograph. Our system
democratizes a capability previously reserved to professionals, and makes this
creative style accessible to most casual photographers.
&lt;/p&gt;
&lt;p&gt;More information and supplementary material can be found on our project
webpage: https://motion-mode.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1&quot;&gt;Eric Tabellion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnad_N/0/1/0/all/0/1&quot;&gt;Nikhil Karnad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1&quot;&gt;Noa Glaser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_B/0/1/0/all/0/1&quot;&gt;Ben Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1&quot;&gt;David E. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1&quot;&gt;Yael Pritch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01389">
<title>Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning. (arXiv:2308.01389v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01389</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, autonomous cars are gaining traction due to their numerous
potential applications on battlefields and in resolving a variety of other
real-world challenges. The main goal of our project is to build an autonomous
system using DeepRacer which will follow a specific person (for our project, a
soldier) when they will be moving in any direction. Two main components to
accomplish this project is an optimized Single-Shot Multibox Detection (SSD)
object detection model and a Reinforcement Learning (RL) model. We accomplished
the task using SSD Lite instead of SSD and at the end, compared the results
among SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimental
results show that SSD Lite gives better performance among these three
techniques and exhibits a considerable boost in inference speed (~2-3 times)
without compromising accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_J/0/1/0/all/0/1&quot;&gt;Jumman Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momtaz_M/0/1/0/all/0/1&quot;&gt;Maliha Momtaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01390">
<title>OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models. (arXiv:2308.01390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01390</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce OpenFlamingo, a family of autoregressive vision-language models
ranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to produce
an open-source replication of DeepMind&apos;s Flamingo models. On seven
vision-language datasets, OpenFlamingo models average between 80 - 89% of
corresponding Flamingo performance. This technical report describes our models,
training data, hyperparameters, and evaluation suite. We share our models and
code at https://github.com/mlfoundations/open_flamingo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awadalla_A/0/1/0/all/0/1&quot;&gt;Anas Awadalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_I/0/1/0/all/0/1&quot;&gt;Irena Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1&quot;&gt;Josh Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanafy_Y/0/1/0/all/0/1&quot;&gt;Yusuf Hanafy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wanrong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_K/0/1/0/all/0/1&quot;&gt;Kalyani Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1&quot;&gt;Samir Gadre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sagawa_S/0/1/0/all/0/1&quot;&gt;Shiori Sagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1&quot;&gt;Jenia Jitsev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1&quot;&gt;Simon Kornblith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_P/0/1/0/all/0/1&quot;&gt;Pang Wei Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1&quot;&gt;Gabriel Ilharco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1&quot;&gt;Mitchell Wortsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01412">
<title>Harder synthetic anomalies to improve OoD detection in Medical Images. (arXiv:2308.01412v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01412</link>
<description rdf:parseType="Literal">&lt;p&gt;Our method builds upon previous Medical Out-of-Distribution (MOOD) challenge
winners that empirically show that synthetic local anomalies generated copying
/ interpolating foreign patches are useful to train segmentation networks able
to generalize to unseen types of anomalies. In terms of the synthetic anomaly
generation process, our contributions makes synthetic anomalies more
heterogeneous and challenging by 1) using random shapes instead of squares and
2) smoothing the interpolation edge of anomalies so networks cannot rely on the
high gradient between image - foreign patch to identify anomalies. Our
experiments using the validation set of 2020 MOOD winners show that both
contributions improved substantially the method performance. We used a standard
3D U-Net architecture as segmentation network, trained patch-wise in both brain
and abdominal datasets. Our final challenge submission consisted of 10 U-Nets
trained across 5 data folds with different configurations of the anomaly
generation process. Our method achieved first position in both sample-wise and
pixel-wise tasks in the 2022 edition of the Medical Out-of-Distribution held at
MICCAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marimont_S/0/1/0/all/0/1&quot;&gt;Sergio Naval Marimont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1&quot;&gt;Giacomo Tarroni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01424">
<title>LiDAR View Synthesis for Robust Vehicle Navigation Without Expert Labels. (arXiv:2308.01424v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01424</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for self-driving cars require a diverse training dataset
to safely manage critical driving scenarios on public roads. This includes
having data from divergent trajectories such as the oncoming traffic lane or
sidewalks. Such data would be too dangerous to collect in the real world. Data
augmentation approaches have been proposed to tackle this issue using RGB
images. However, solutions based on LiDAR sensors are scarce. We therefore
propose an approach to synthesize additional LiDAR point clouds from novel
viewpoints without having the need to physically drive at dangerous positions.
The LiDAR view synthesis is done using mesh reconstruction and ray casting. We
train a deep learning model, which takes a LiDAR scan as input and predicts the
future trajectory as output. A waypoint controller is then applied on this
predicted trajectory to determine the throttle and steering labels of the
ego-vehicle. Our method neither requires expert driving labels for the original
nor for the synthesized LiDAR sequence. Instead, we infer labels from LiDAR
odometry. We demonstrate the effectiveness of our approach in a comprehensive
online evaluation and with a comparison to concurrent work. Our results show
the importance of synthesizing additional LiDAR point clouds, particularly in
terms of model robustness. Code and supplementary visualizations are available
at https://jonathsch.github.io/lidar-synthesis/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1&quot;&gt;Jonathan Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_Q/0/1/0/all/0/1&quot;&gt;Qadeer Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01433">
<title>COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography. (arXiv:2308.01433v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01433</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic presented numerous challenges to healthcare systems
worldwide. Given that lung infections are prevalent among COVID-19 patients,
chest Computer Tomography (CT) scans have frequently been utilized as an
alternative method for identifying COVID-19 conditions and various other types
of pulmonary diseases. Deep learning architectures have emerged to automate the
identification of pulmonary disease types by leveraging CT scan slices as
inputs for classification models. This paper introduces COVID-VR, a novel
approach for classifying pulmonary diseases based on volume rendering images of
the lungs captured from multiple angles, thereby providing a comprehensive view
of the entire lung in each image. To assess the effectiveness of our proposal,
we compared it against competing strategies utilizing both private data
obtained from partner hospitals and a publicly available dataset. The results
demonstrate that our approach effectively identifies pulmonary lesions and
performs competitively when compared to slice-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romero_N/0/1/0/all/0/1&quot;&gt;Noemi Maritza L. Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vasconcellos_R/0/1/0/all/0/1&quot;&gt;Ricco Vasconcellos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mendoza_M/0/1/0/all/0/1&quot;&gt;Mariana R. Mendoza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Comba_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o L. D. Comba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01471">
<title>Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving. (arXiv:2308.01471v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01471</link>
<description rdf:parseType="Literal">&lt;p&gt;A self-driving vehicle (SDV) must be able to perceive its surroundings and
predict the future behavior of other traffic participants. Existing works
either perform object detection followed by trajectory forecasting of the
detected objects, or predict dense occupancy and flow grids for the whole
scene. The former poses a safety concern as the number of detections needs to
be kept low for efficiency reasons, sacrificing object recall. The latter is
computationally expensive due to the high-dimensionality of the output grid,
and suffers from the limited receptive field inherent to fully convolutional
networks. Furthermore, both approaches employ many computational resources
predicting areas or objects that might never be queried by the motion planner.
This motivates our unified approach to perception and future prediction that
implicitly represents occupancy and flow over time with a single neural
network. Our method avoids unnecessary computation, as it can be directly
queried by the motion planner at continuous spatio-temporal locations.
Moreover, we design an architecture that overcomes the limited receptive field
of previous explicit occupancy prediction methods by adding an efficient yet
effective global attention mechanism. Through extensive experiments in both
urban and highway settings, we demonstrate that our implicit model outperforms
the current state-of-the-art. For more information, visit the project website:
https://waabi.ai/research/implicito.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agro_B/0/1/0/all/0/1&quot;&gt;Ben Agro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sykora_Q/0/1/0/all/0/1&quot;&gt;Quinlan Sykora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01472">
<title>Reverse Stable Diffusion: What prompt was used to generate this image?. (arXiv:2308.01472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01472</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models such as Stable Diffusion have recently
attracted the interest of many researchers, and inverting the diffusion process
can play an important role in better understanding the generative process and
how to engineer prompts in order to obtain the desired images. To this end, we
introduce the new task of predicting the text prompt given an image generated
by a generative diffusion model. We combine a series of white-box and black-box
models (with and without access to the weights of the diffusion network) to
deal with the proposed task. We propose a novel learning framework comprising
of a joint prompt regression and multi-label vocabulary classification
objective that generates improved prompts. To further improve our method, we
employ a curriculum learning procedure that promotes the learning of
image-prompt pairs with lower labeling noise (i.e. that are better aligned),
and an unsupervised domain-adaptive kernel learning method that uses the
similarities between samples in the source and target domains as extra
features. We conduct experiments on the DiffusionDB data set, predicting text
prompts from images generated by Stable Diffusion. Our novel learning framework
produces excellent results on the aforementioned task, yielding the highest
gains when applied on the white-box model. In addition, we make an interesting
discovery: training a diffusion model on the prompt generation task can make
the model generate images that are much better aligned with the input prompts,
when the model is directly reused for text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croitoru_F/0/1/0/all/0/1&quot;&gt;Florinel-Alin Croitoru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hondru_V/0/1/0/all/0/1&quot;&gt;Vlad Hondru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1&quot;&gt;Radu Tudor Ionescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01477">
<title>HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions. (arXiv:2308.01477v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01477</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the HANDAL dataset for category-level object pose estimation and
affordance prediction. Unlike previous datasets, ours is focused on
robotics-ready manipulable objects that are of the proper size and shape for
functional grasping by robot manipulators, such as pliers, utensils, and
screwdrivers. Our annotation process is streamlined, requiring only a single
off-the-shelf camera and semi-automated processing, allowing us to produce
high-quality 3D annotations without crowd-sourcing. The dataset consists of
308k annotated image frames from 2.2k videos of 212 real-world objects in 17
categories. We focus on hardware and kitchen tool objects to facilitate
research in practical scenarios in which a robot manipulator needs to interact
with the environment beyond simple pushing or indiscriminate grasping. We
outline the usefulness of our dataset for 6-DoF category-level pose+scale
estimation and related tasks. We also provide 3D reconstructed meshes of all
objects, and we outline some of the bottlenecks to be addressed for
democratizing the collection of datasets like this one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_A/0/1/0/all/0/1&quot;&gt;Andrew Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bowen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1&quot;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyree_S/0/1/0/all/0/1&quot;&gt;Stephen Tyree&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jeffrey Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1&quot;&gt;Stan Birchfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01483">
<title>Efficient neural supersampling on a novel gaming dataset. (arXiv:2308.01483v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01483</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time rendering for video games has become increasingly challenging due
to the need for higher resolutions, framerates and photorealism. Supersampling
has emerged as an effective solution to address this challenge. Our work
introduces a novel neural algorithm for supersampling rendered content that is
4 times more efficient than existing methods while maintaining the same level
of accuracy. Additionally, we introduce a new dataset which provides auxiliary
modalities such as motion vectors and depth generated using graphics rendering
features like viewport jittering and mipmap biasing at different resolutions.
We believe that this dataset fills a gap in the current dataset landscape and
can serve as a valuable resource to help measure progress in the field and
advance the state-of-the-art in super-resolution techniques for gaming content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercier_A/0/1/0/all/0/1&quot;&gt;Antoine Mercier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erasmus_R/0/1/0/all/0/1&quot;&gt;Ruan Erasmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savani_Y/0/1/0/all/0/1&quot;&gt;Yashesh Savani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhingra_M/0/1/0/all/0/1&quot;&gt;Manik Dhingra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1&quot;&gt;Fatih Porikli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_G/0/1/0/all/0/1&quot;&gt;Guillaume Berger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01499">
<title>TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations. (arXiv:2308.01499v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01499</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic colored meshes (DCM) are widely used in various applications;
however, these meshes may undergo different processes, such as compression or
transmission, which can distort them and degrade their quality. To facilitate
the development of objective metrics for DCMs and study the influence of
typical distortions on their perception, we create the Tencent - dynamic
colored mesh database (TDMD) containing eight reference DCM objects with six
typical distortions. Using processed video sequences (PVS) derived from the
DCM, we have conducted a large-scale subjective experiment that resulted in 303
distorted DCM samples with mean opinion scores, making the TDMD the largest
available DCM database to our knowledge. This database enabled us to study the
impact of different types of distortion on human perception and offer
recommendations for DCM compression and related tasks. Additionally, we have
evaluated three types of state-of-the-art objective metrics on the TDMD,
including image-based, point-based, and video-based metrics, on the TDMD. Our
experimental results highlight the strengths and weaknesses of each metric, and
we provide suggestions about the selection of metrics in practical DCM
applications. The TDMD will be made publicly available at the following
location: https://multimedia.tencent.com/resources/tdmd.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Joel Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschamps_T/0/1/0/all/0/1&quot;&gt;Timon Deschamps&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01508">
<title>Circumventing Concept Erasure Methods For Text-to-Image Generative Models. (arXiv:2308.01508v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01508</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generative models can produce photo-realistic images for an
extremely broad range of concepts, and their usage has proliferated widely
among the general public. On the flip side, these models have numerous
drawbacks, including their potential to generate images featuring sexually
explicit content, mirror artistic styles without permission, or even
hallucinate (or deepfake) the likenesses of celebrities. Consequently, various
methods have been proposed in order to &quot;erase&quot; sensitive concepts from
text-to-image models. In this work, we examine five recently proposed concept
erasure methods, and show that targeted concepts are not fully excised from any
of these methods. Specifically, we leverage the existence of special learned
word embeddings that can retrieve &quot;erased&quot; concepts from the sanitized models
with no alterations to their weights. Our results highlight the brittleness of
post hoc concept erasure methods, and call into question their use in the
algorithmic toolkit for AI safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_K/0/1/0/all/0/1&quot;&gt;Kelly O. Marshall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01520">
<title>Contrastive Multi-FaceForensics: An End-to-end Bi-grained Contrastive Learning Approach for Multi-face Forgery Detection. (arXiv:2308.01520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;DeepFakes have raised serious societal concerns, leading to a great surge in
detection-based forensics methods in recent years. Face forgery recognition is
the conventional detection method that usually follows a two-phase pipeline: it
extracts the face first and then determines its authenticity by classification.
Since DeepFakes in the wild usually contain multiple faces, using face forgery
detection methods is merely practical as they have to process faces in a
sequel, i.e., only one face is processed at the same time. One straightforward
way to address this issue is to integrate face extraction and forgery detection
in an end-to-end fashion by adapting advanced object detection architectures.
However, as these object detection architectures are designed to capture the
semantic information of different object categories rather than the subtle
forgery traces among the faces, the direct adaptation is far from optimal. In
this paper, we describe a new end-to-end framework, Contrastive
Multi-FaceForensics (COMICS), to enhance multi-face forgery detection. The core
of the proposed framework is a novel bi-grained contrastive learning approach
that explores effective face forgery traces at both the coarse- and
fine-grained levels. Specifically, the coarse-grained level contrastive
learning captures the discriminative features among positive and negative
proposal pairs in multiple scales with the instruction of the proposal
generator, and the fine-grained level contrastive learning captures the
pixel-wise discrepancy between the forged and original areas of the same face
and the pixel-wise content inconsistency between different faces. Extensive
experiments on the OpenForensics dataset demonstrate our method outperforms
other counterparts by a large margin (~18.5%) and shows great potential for
integration into various architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Honggang Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuezun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01521">
<title>PPI-NET: End-to-End Parametric Primitive Inference. (arXiv:2308.01521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01521</link>
<description rdf:parseType="Literal">&lt;p&gt;In engineering applications, line, circle, arc, and point are collectively
referred to as primitives, and they play a crucial role in path planning,
simulation analysis, and manufacturing. When designing CAD models, engineers
typically start by sketching the model&apos;s orthographic view on paper or a
whiteboard and then translate the design intent into a CAD program. Although
this design method is powerful, it often involves challenging and repetitive
tasks, requiring engineers to perform numerous similar operations in each
design. To address this conversion process, we propose an efficient and
accurate end-to-end method that avoids the inefficiency and error accumulation
issues associated with using auto-regressive models to infer parametric
primitives from hand-drawn sketch images. Since our model samples match the
representation format of standard CAD software, they can be imported into CAD
software for solving, editing, and applied to downstream design tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaogang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01525">
<title>VisAlign: Dataset for Measuring the Degree of Alignment between AI and Humans in Visual Perception. (arXiv:2308.01525v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01525</link>
<description rdf:parseType="Literal">&lt;p&gt;AI alignment refers to models acting towards human-intended goals,
preferences, or ethical principles. Given that most large-scale deep learning
models act as black boxes and cannot be manually controlled, analyzing the
similarity between models and humans can be a proxy measure for ensuring AI
safety. In this paper, we focus on the models&apos; visual perception alignment with
humans, further referred to as AI-human visual alignment. Specifically, we
propose a new dataset for measuring AI-human visual alignment in terms of image
classification, a fundamental task in machine perception. In order to evaluate
AI-human visual alignment, a dataset should encompass samples with various
scenarios that may arise in the real world and have gold human perception
labels. Our dataset consists of three groups of samples, namely Must-Act (i.e.,
Must-Classify), Must-Abstain, and Uncertain, based on the quantity and clarity
of visual information in an image and further divided into eight categories.
All samples have a gold human perception label; even Uncertain (severely
blurry) sample labels were obtained via crowd-sourcing. The validity of our
dataset is verified by sampling theory, statistical theories related to survey
design, and experts in the related fields. Using our dataset, we analyze the
visual alignment and reliability of five popular visual perception models and
seven abstention methods. Our code and data is available at
\url{https://github.com/jiyounglee-0523/VisAlign}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jiyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_S/0/1/0/all/0/1&quot;&gt;Seunghyun Won&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1&quot;&gt;James Thorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaeseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_O/0/1/0/all/0/1&quot;&gt;O-Kil Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01526">
<title>Data Augmentation for Human Behavior Analysis in Multi-Person Conversations. (arXiv:2308.01526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01526</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present the solution of our team HFUT-VUT for the
MultiMediate Grand Challenge 2023 at ACM Multimedia 2023. The solution covers
three sub-challenges: bodily behavior recognition, eye contact detection, and
next speaker prediction. We select Swin Transformer as the baseline and exploit
data augmentation strategies to address the above three tasks. Specifically, we
crop the raw video to remove the noise from other parts. At the same time, we
utilize data augmentation to improve the generalization of the model. As a
result, our solution achieves the best results of 0.6262 for bodily behavior
recognition in terms of mean average precision and the accuracy of 0.7771 for
eye contact detection on the corresponding test set. In addition, our approach
also achieves comparable results of 0.5281 for the next speaker prediction in
terms of unweighted average recall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Dan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guoliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01532">
<title>Multimodal Adaptation of CLIP for Few-Shot Action Recognition. (arXiv:2308.01532v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01532</link>
<description rdf:parseType="Literal">&lt;p&gt;Applying large-scale pre-trained visual models like CLIP to few-shot action
recognition tasks can benefit performance and efficiency. Utilizing the
&quot;pre-training, fine-tuning&quot; paradigm makes it possible to avoid training a
network from scratch, which can be time-consuming and resource-intensive.
However, this method has two drawbacks. First, limited labeled samples for
few-shot action recognition necessitate minimizing the number of tunable
parameters to mitigate over-fitting, also leading to inadequate fine-tuning
that increases resource consumption and may disrupt the generalized
representation of models. Second, the video&apos;s extra-temporal dimension
challenges few-shot recognition&apos;s effective temporal modeling, while
pre-trained visual models are usually image models. This paper proposes a novel
method called Multimodal Adaptation of CLIP (MA-CLIP) to address these issues.
It adapts CLIP for few-shot action recognition by adding lightweight adapters,
which can minimize the number of learnable parameters and enable the model to
transfer across different tasks quickly. The adapters we design can combine
information from video-text multimodal sources for task-oriented spatiotemporal
modeling, which is fast, efficient, and has low training costs. Additionally,
based on the attention mechanism, we design a text-guided prototype
construction module that can fully utilize video-text information to enhance
the representation of video prototypes. Our MA-CLIP is plug-and-play, which can
be used in any different few-shot action recognition temporal alignment metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jiazheng Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xiaojun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01536">
<title>MFIM: Megapixel Facial Identity Manipulation. (arXiv:2308.01536v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01536</link>
<description rdf:parseType="Literal">&lt;p&gt;Face swapping is a task that changes a facial identity of a given image to
that of another person. In this work, we propose a novel face-swapping
framework called Megapixel Facial Identity Manipulation (MFIM). The
face-swapping model should achieve two goals. First, it should be able to
generate a high-quality image. We argue that a model which is proficient in
generating a megapixel image can achieve this goal. However, generating a
megapixel image is generally difficult without careful model design. Therefore,
our model exploits pretrained StyleGAN in the manner of GAN-inversion to
effectively generate a megapixel image. Second, it should be able to
effectively transform the identity of a given image. Specifically, it should be
able to actively transform ID attributes (e.g., face shape and eyes) of a given
image into those of another person, while preserving ID-irrelevant attributes
(e.g., pose and expression). To achieve this goal, we exploit 3DMM that can
capture various facial attributes. Specifically, we explicitly supervise our
model to generate a face-swapped image with the desirable attributes using
3DMM. We show that our model achieves state-of-the-art performance through
extensive experiments. Furthermore, we propose a new operation called ID
mixing, which creates a new identity by semantically mixing the identities of
several people. It allows the user to customize the new identity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1&quot;&gt;Sanghyeon Na&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01541">
<title>DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging. (arXiv:2308.01541v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01541</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods are developing rapidly in coded aperture snapshot
spectral imaging (CASSI). The number of parameters and FLOPs of existing
state-of-the-art methods (SOTA) continues to increase, but the reconstruction
accuracy improves slowly. Current methods still face two problems: 1) The
performance of the spatial light modulator (SLM) is not fully developed due to
the limitation of fixed Mask coding. 2) The single input limits the network
performance. In this paper we present a dynamic-mask-based dual camera system,
which consists of an RGB camera and a CASSI system running in parallel. First,
the system learns the spatial feature distribution of the scene based on the
RGB images, then instructs the SLM to encode each scene, and finally sends both
RGB and CASSI images to the network for reconstruction. We further designed the
DMDC-net, which consists of two separate networks, a small-scale CNN-based
dynamic mask network for dynamic adjustment of the mask and a multimodal
reconstruction network for reconstruction using RGB and CASSI measurements.
Extensive experiments on multiple datasets show that our method achieves more
than 9 dB improvement in PSNR over the SOTA.
(https://github.com/caizeyu1992/DMDC)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zeyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Chengqian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Feipeng Da&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01544">
<title>Multimodal Neurons in Pretrained Text-Only Transformers. (arXiv:2308.01544v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01544</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models demonstrate remarkable capacity to generalize representations
learned in one modality to downstream tasks in other modalities. Can we trace
this ability to individual neurons? We study the case where a frozen text
transformer is augmented with vision using a self-supervised visual encoder and
a single linear projection learned on an image-to-text task. Outputs of the
projection layer are not immediately decodable into language describing image
content; instead, we find that translation between modalities occurs deeper
within the transformer. We introduce a procedure for identifying &quot;multimodal
neurons&quot; that convert visual representations into corresponding text, and
decoding the concepts they inject into the model&apos;s residual stream. In a series
of experiments, we show that multimodal neurons operate on specific visual
concepts across inputs, and have a systematic causal effect on image
captioning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1&quot;&gt;Sarah Schwettmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1&quot;&gt;Neil Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01547">
<title>Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation. (arXiv:2308.01547v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01547</link>
<description rdf:parseType="Literal">&lt;p&gt;We generalize the class vectors found in neural networks to linear subspaces
(i.e.~points in the Grassmann manifold) and show that the Grassmann Class
Representation (GCR) enables the simultaneous improvement in accuracy and
feature transferability. In GCR, each class is a subspace and the logit is
defined as the norm of the projection of a feature onto the class subspace. We
integrate Riemannian SGD into deep learning frameworks such that class
subspaces in a Grassmannian are jointly optimized with the rest model
parameters. Compared to the vector form, the representative capability of
subspaces is more powerful. We show that on ImageNet-1K, the top-1 error of
ResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced by 5.6%, 4.5%, 3.0% and
3.5%, respectively. Subspaces also provide freedom for features to vary and we
observed that the intra-class feature variability grows when the subspace
dimension increases. Consequently, we found the quality of GCR features is
better for downstream tasks. For ResNet50-D, the average linear transfer
accuracy across 6 datasets improves from 77.98% to 79.70% compared to the
strong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4%
and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results,
we believe that more applications could benefit from the Grassmann class
representation. Code is released at https://github.com/innerlee/GCR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wayne Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01568">
<title>MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior. (arXiv:2308.01568v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01568</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, many deep learning-based methods have been proposed to
tackle the problem of optical flow estimation and achieved promising results.
However, they hardly consider that most videos are compressed and thus ignore
the pre-computed information in compressed video streams. Motion vectors, one
of the compression information, record the motion of the video frames. They can
be directly extracted from the compression code stream without computational
cost and serve as a solid prior for optical flow estimation. Therefore, we
propose an optical flow model, MVFlow, which uses motion vectors to improve the
speed and accuracy of optical flow estimation for compressed videos. In detail,
MVFlow includes a key Motion-Vector Converting Module, which ensures that the
motion vectors can be transformed into the same domain of optical flow and then
be utilized fully by the flow estimation module. Meanwhile, we construct four
optical flow datasets for compressed videos containing frames and motion
vectors in pairs. The experimental results demonstrate the superiority of our
proposed MVFlow, which can reduce the AEPE by 1.09 compared to existing models
or save 52% time to achieve similar accuracy to existing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shili Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xuhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ruian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01587">
<title>Consistency Regularization for Generalizable Source-free Domain Adaptation. (arXiv:2308.01587v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01587</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation (SFDA) aims to adapt a well-trained source
model to an unlabelled target domain without accessing the source dataset,
making it applicable in a variety of real-world scenarios. Existing SFDA
methods ONLY assess their adapted models on the target training set, neglecting
the data from unseen but identically distributed testing sets. This oversight
leads to overfitting issues and constrains the model&apos;s generalization ability.
In this paper, we propose a consistency regularization framework to develop a
more generalizable SFDA method, which simultaneously boosts model performance
on both target training and testing datasets. Our method leverages soft
pseudo-labels generated from weakly augmented images to supervise strongly
augmented images, facilitating the model training process and enhancing the
generalization ability of the adapted model. To leverage more potentially
useful supervision, we present a sampling-based pseudo-label selection
strategy, taking samples with severer domain shift into consideration.
Moreover, global-oriented calibration methods are introduced to exploit global
class distribution and feature cluster information, further improving the
adaptation process. Extensive experiments demonstrate our method achieves
state-of-the-art performance on several SFDA benchmarks, and exhibits
robustness on unseen testing datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Longxiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chunming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01594">
<title>Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models. (arXiv:2308.01594v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01594</link>
<description rdf:parseType="Literal">&lt;p&gt;Electron microscopy (EM) images exhibit anisotropic axial resolution due to
the characteristics inherent to the imaging modality, presenting challenges in
analysis and downstream tasks.In this paper, we propose a diffusion-model-based
framework that overcomes the limitations of requiring reference data or prior
knowledge about the degradation process. Our approach utilizes 2D diffusion
models to consistently reconstruct 3D volumes and is well-suited for highly
downsampled data. Extensive experiments conducted on two public datasets
demonstrate the robustness and superiority of leveraging the generative prior
compared to supervised learning methods. Additionally, we demonstrate our
method&apos;s feasibility for self-supervised reconstruction, which can restore a
single anisotropic volume without any training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyungryun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_W/0/1/0/all/0/1&quot;&gt;Won-Ki Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01604">
<title>IndoHerb: Indonesia Medicinal Plants Recognition using Transfer Learning and Deep Learning. (arXiv:2308.01604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01604</link>
<description rdf:parseType="Literal">&lt;p&gt;Herbal plants are nutritious plants that can be used as an alternative to
traditional disease healing. In Indonesia there are various types of herbal
plants. But with the development of the times, the existence of herbal plants
as traditional medicines began to be forgotten so that not everyone could
recognize them. Having the ability to identify herbal plants can have many
positive impacts. However, there is a problem where identifying plants can take
a long time because it requires in-depth knowledge and careful examination of
plant criteria. So that the application of computer vision can help identify
herbal plants. Previously, research had been conducted on the introduction of
herbal plants from Vietnam using several algorithms, but from these research
the accuracy was not high enough. Therefore, this study intends to implement
transfer learning from the Convolutional Neural Network (CNN) algorithm to
classify types of herbal plants from Indonesia. This research was conducted by
collecting image data of herbal plants from Indonesia independently through the
Google Images search engine. After that, it will go through the data
preprocessing, classification using the transfer learning method from CNN, and
analysis will be carried out. The CNN transfer learning models used are
ResNet34, DenseNet121, and VGG11_bn. Based on the test results of the three
models, it was found that DenseNet121 was the model with the highest accuracy,
which was 87.4%. In addition, testing was also carried out using the scratch
model and obtained an accuracy of 43.53%. The Hyperparameter configuration used
in this test is the ExponentialLR scheduler with a gamma value of 0.9; learning
rate 0.001; Cross Entropy Loss function; Adam optimizer; and the number of
epochs is 50. Indonesia Medicinal Plant Dataset can be accessed at the
following link https://github.com/Salmanim20/indo_medicinal_plant
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musyaffa_M/0/1/0/all/0/1&quot;&gt;Muhammad Salman Ikrar Musyaffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yudistira_N/0/1/0/all/0/1&quot;&gt;Novanto Yudistira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Muhammad Arif Rahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01613">
<title>Real-time Light Estimation and Neural Soft Shadows for AR Indoor Scenarios. (arXiv:2308.01613v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01613</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a pipeline for realistic embedding of virtual objects into footage
of indoor scenes with focus on real-time AR applications. Our pipeline consists
of two main components: A light estimator and a neural soft shadow texture
generator. Our light estimation is based on deep neural nets and determines the
main light direction, light color, ambient color and an opacity parameter for
the shadow texture. Our neural soft shadow method encodes object-based
realistic soft shadows as light direction dependent textures in a small MLP. We
show that our pipeline can be used to integrate objects into AR scenes in a new
level of realism in real-time. Our models are small enough to run on current
mobile devices. We achieve runtimes of 9ms for light estimation and 5ms for
neural shadows on an iPhone 11 Pro.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_A/0/1/0/all/0/1&quot;&gt;Alexander Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwanecke_U/0/1/0/all/0/1&quot;&gt;Ulrich Schwanecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schomer_E/0/1/0/all/0/1&quot;&gt;Elmar Sch&amp;#xf6;mer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01614">
<title>Assessing Systematic Weaknesses of DNNs using Counterfactuals. (arXiv:2308.01614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01614</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of DNNs into safety-critical applications, testing
approaches for such models have gained more attention. A current direction is
the search for and identification of systematic weaknesses that put safety
assumptions based on average performance values at risk. Such weaknesses can
take on the form of (semantically coherent) subsets or areas in the input space
where a DNN performs systematically worse than its expected average. However,
it is non-trivial to attribute the reason for such observed low performances to
the specific semantic features that describe the subset. For instance,
inhomogeneities within the data w.r.t. other (non-considered) attributes might
distort results. However, taking into account all (available) attributes and
their interaction is often computationally highly expensive. Inspired by
counterfactual explanations, we propose an effective and computationally cheap
algorithm to validate the semantic attribution of existing subsets, i.e., to
check whether the identified attribute is likely to have caused the degraded
performance. We demonstrate this approach on an example from the autonomous
driving domain using highly annotated simulated data, where we show for a
semantic segmentation model that (i) performance differences among the
different pedestrian assets exist, but (ii) only in some cases is the asset
type itself the reason for this reduction in the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gannamaneni_S/0/1/0/all/0/1&quot;&gt;Sujan Sai Gannamaneni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1&quot;&gt;Michael Mock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akila_M/0/1/0/all/0/1&quot;&gt;Maram Akila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01618">
<title>A Survey on Deep Learning-based Spatio-temporal Action Detection. (arXiv:2308.01618v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01618</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatio-temporal action detection (STAD) aims to classify the actions present
in a video and localize them in space and time. It has become a particularly
active area of research in computer vision because of its explosively emerging
real-world applications, such as autonomous driving, visual surveillance,
entertainment, etc. Many efforts have been devoted in recent years to building
a robust and effective framework for STAD. This paper provides a comprehensive
review of the state-of-the-art deep learning-based methods for STAD. Firstly, a
taxonomy is developed to organize these methods. Next, the linking algorithms,
which aim to associate the frame- or clip-level detection results together to
form action tubes, are reviewed. Then, the commonly used benchmark datasets and
evaluation metrics are introduced, and the performance of state-of-the-art
models is compared. At last, this paper is concluded, and a set of potential
research directions of STAD are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fanwei Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuntao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01621">
<title>A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01621</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on image
classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hang Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bing Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01622">
<title>ReIDTrack: Multi-Object Track and Segmentation Without Motion. (arXiv:2308.01622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01622</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS)
methods mainly follow the tracking-by-detection paradigm. Transformer-based
end-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannot
achieve a new state-of-the-art (SOTA) performance in major MOT and MOTS
benchmarks. Detection and association are two main modules of the
tracking-by-detection paradigm. Association techniques mainly depend on the
combination of motion and appearance information. As deep learning has been
recently developed, the performance of the detection and appearance model is
rapidly improved. These trends made us consider whether we can achieve SOTA
based on only high-performance detection and appearance model. Our paper mainly
focuses on exploring this direction based on CBNetV2 with Swin-B as a detection
model and MoCo-v2 as a self-supervised appearance model. Motion information and
IoU mapping were removed during the association. Our method wins 1st place on
the MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. We
hope our simple and effective method can give some insights to the MOT and MOTS
research community. Source code will be released under this git repository
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bingchuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twombly_C/0/1/0/all/0/1&quot;&gt;Christopher Walter Twombly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01626">
<title>Interleaving GANs with knowledge graphs to support design creativity for book covers. (arXiv:2308.01626v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01626</link>
<description rdf:parseType="Literal">&lt;p&gt;An attractive book cover is important for the success of a book. In this
paper, we apply Generative Adversarial Networks (GANs) to the book covers
domain, using different methods for training in order to obtain better
generated images. We interleave GANs with knowledge graphs to alter the input
title to obtain multiple possible options for any given title, which are then
used as an augmented input to the generator. Finally, we use the discriminator
obtained during the training phase to select the best images generated with new
titles. Our method performed better at generating book covers than previous
attempts, and the knowledge graph gives better options to the book author or
editor compared to using GANs alone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motogna_A/0/1/0/all/0/1&quot;&gt;Alexandru Motogna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groza_A/0/1/0/all/0/1&quot;&gt;Adrian Groza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01630">
<title>Erasure-based Interaction Network for RGBT Video Object Detection and A Unified Benchmark. (arXiv:2308.01630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01630</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, many breakthroughs are made in the field of Video Object Detection
(VOD), but the performance is still limited due to the imaging limitations of
RGB sensors in adverse illumination conditions. To alleviate this issue, this
work introduces a new computer vision task called RGB-thermal (RGBT) VOD by
introducing the thermal modality that is insensitive to adverse illumination
conditions. To promote the research and development of RGBT VOD, we design a
novel Erasure-based Interaction Network (EINet) and establish a comprehensive
benchmark dataset (VT-VOD50) for this task. Traditional VOD methods often
leverage temporal information by using many auxiliary frames, and thus have
large computational burden. Considering that thermal images exhibit less noise
than RGB ones, we develop a negative activation function that is used to erase
the noise of RGB features with the help of thermal image features. Furthermore,
with the benefits from thermal images, we rely only on a small temporal window
to model the spatio-temporal information to greatly improve efficiency while
maintaining detection accuracy.
&lt;/p&gt;
&lt;p&gt;VT-VOD50 dataset consists of 50 pairs of challenging RGBT video sequences
with complex backgrounds, various objects and different illuminations, which
are collected in real traffic scenarios. Extensive experiments on VT-VOD50
dataset demonstrate the effectiveness and efficiency of our proposed method
against existing mainstream VOD methods. The code of EINet and the dataset will
be released to the public for free academic usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qishun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongshun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kunpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01634">
<title>Disentangling Multi-view Representations Beyond Inductive Bias. (arXiv:2308.01634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01634</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-view (or -modality) representation learning aims to understand the
relationships between different view representations. Existing methods
disentangle multi-view representations into consistent and view-specific
representations by introducing strong inductive biases, which can limit their
generalization ability. In this paper, we propose a novel multi-view
representation disentangling method that aims to go beyond inductive biases,
ensuring both interpretability and generalizability of the resulting
representations. Our method is based on the observation that discovering
multi-view consistency in advance can determine the disentangling information
boundary, leading to a decoupled learning objective. We also found that the
consistency can be easily extracted by maximizing the transformation invariance
and clustering consistency between views. These observations drive us to
propose a two-stage framework. In the first stage, we obtain multi-view
consistency by training a consistent encoder to produce semantically-consistent
representations across views as well as their corresponding pseudo-labels. In
the second stage, we disentangle specificity from comprehensive representations
by minimizing the upper bound of mutual information between consistent and
comprehensive representations. Finally, we reconstruct the original data by
concatenating pseudo-labels and view-specific representations. Our experiments
on four multi-view datasets demonstrate that our proposed method outperforms 12
comparison methods in terms of clustering and classification performance. The
visualization results also show that the extracted consistency and specificity
are compact and interpretable. Our code can be found at
\url{https://github.com/Guanzhou-Ke/DMRIB}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chenyang/0/1/0/all/0/1&quot;&gt;Chenyang&lt;/a&gt;, Xu, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shengfeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01639">
<title>Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection. (arXiv:2308.01639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01639</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrocardiogram (ECG) is a widely used diagnostic tool for detecting heart
conditions. Rare cardiac diseases may be underdiagnosed using traditional ECG
analysis, considering that no training dataset can exhaust all possible cardiac
disorders. This paper proposes using anomaly detection to identify any
unhealthy status, with normal ECGs solely for training. However, detecting
anomalies in ECG can be challenging due to significant inter-individual
differences and anomalies present in both global rhythm and local morphology.
To address this challenge, this paper introduces a novel multi-scale
cross-restoration framework for ECG anomaly detection and localization that
considers both local and global ECG characteristics. The proposed framework
employs a two-branch autoencoder to facilitate multi-scale feature learning
through a masking and restoration process, with one branch focusing on global
features from the entire ECG and the other on local features from
heartbeat-level details, mimicking the diagnostic process of cardiologists.
Anomalies are identified by their high restoration errors. To evaluate the
performance on a large number of individuals, this paper introduces a new
challenging benchmark with signal point-level ground truths annotated by
experienced cardiologists. The proposed method demonstrates state-of-the-art
performance on this benchmark and two other well-known ECG datasets. The
benchmark dataset and source code are available at:
\url{https://github.com/MediaBrain-SJTU/ECGAD}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1&quot;&gt;Aofan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoqin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01655">
<title>DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models. (arXiv:2308.01655v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01655</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent data-driven image colorization methods have enabled automatic or
reference-based colorization, while still suffering from unsatisfactory and
inaccurate object-level color control. To address these issues, we propose a
new method called DiffColor that leverages the power of pre-trained diffusion
models to recover vivid colors conditioned on a prompt text, without any
additional inputs. DiffColor mainly contains two stages: colorization with
generative color prior and in-context controllable colorization. Specifically,
we first fine-tune a pre-trained text-to-image model to generate colorized
images using a CLIP-based contrastive loss. Then we try to obtain an optimized
text embedding aligning the colorized image and the text prompt, and a
fine-tuned diffusion model enabling high-quality image reconstruction. Our
method can produce vivid and diverse colors with a few iterations, and keep the
structure and background intact while having colors well-aligned with the
target language guidance. Moreover, our method allows for in-context
colorization, i.e., producing different colorization results by modifying
prompt texts without any fine-tuning, and can achieve object-level controllable
colorization results. Extensive experiments and user studies demonstrate that
DiffColor outperforms previous works in terms of visual quality, color
fidelity, and diversity of colorization options.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianxin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Peng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongju Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01661">
<title>BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout. (arXiv:2308.01661v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01661</link>
<description rdf:parseType="Literal">&lt;p&gt;Using synthesized images to boost the performance of perception models is a
long-standing research challenge in computer vision. It becomes more eminent in
visual-centric autonomous driving systems with multi-view cameras as some
long-tail scenarios can never be collected. Guided by the BEV segmentation
layouts, the existing generative networks seem to synthesize photo-realistic
street-view images when evaluated solely on scene-level metrics. However, once
zoom-in, they usually fail to produce accurate foreground and background
details such as heading. To this end, we propose a two-stage generative method,
dubbed BEVControl, that can generate accurate foreground and background
contents. In contrast to segmentation-like input, it also supports sketch style
input, which is more flexible for humans to edit. In addition, we propose a
comprehensive multi-level evaluation protocol to fairly compare the quality of
the generated scene, foreground object, and background geometry. Our extensive
experiments show that our BEVControl surpasses the state-of-the-art method,
BEVGen, by a significant margin, from 5.89 to 26.80 on foreground segmentation
mIoU. In addition, we show that using images generated by BEVControl to train
the downstream perception model, it achieves on average 1.29 improvement in NDS
score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kairui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_E/0/1/0/all/0/1&quot;&gt;Enhui Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jibin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Di Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01686">
<title>LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment. (arXiv:2308.01686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;3D panoptic segmentation is a challenging perception task that requires both
semantic segmentation and instance segmentation. In this task, we notice that
images could provide rich texture, color, and discriminative information, which
can complement LiDAR data for evident performance improvement, but their fusion
remains a challenging problem. To this end, we propose LCPS, the first
LiDAR-Camera Panoptic Segmentation network. In our approach, we conduct
LiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation Pixel
Alignment (ACPA) module that calibrates the coordinate misalignment caused by
asynchronous problems between sensors; 2) a Semantic-Aware Region Alignment
(SARA) module that extends the one-to-one point-pixel mapping to one-to-many
semantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module that
integrates both geometric and semantic fusion information for the entire point
cloud. Our fusion strategy improves about 6.9% PQ performance over the
LiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitative
experiments further demonstrate the effectiveness of our novel framework. The
code will be released at https://github.com/zhangzw12319/lcps.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Ran Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lizhuang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01698">
<title>Balanced Destruction-Reconstruction Dynamics for Memory-replay Class Incremental Learning. (arXiv:2308.01698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01698</link>
<description rdf:parseType="Literal">&lt;p&gt;Class incremental learning (CIL) aims to incrementally update a trained model
with the new classes of samples (plasticity) while retaining previously learned
ability (stability). To address the most challenging issue in this goal, i.e.,
catastrophic forgetting, the mainstream paradigm is memory-replay CIL, which
consolidates old knowledge by replaying a small number of old classes of
samples saved in the memory. Despite effectiveness, the inherent
destruction-reconstruction dynamics in memory-replay CIL are an intrinsic
limitation: if the old knowledge is severely destructed, it will be quite hard
to reconstruct the lossless counterpart. Our theoretical analysis shows that
the destruction of old knowledge can be effectively alleviated by balancing the
contribution of samples from the current phase and those saved in the memory.
Motivated by this theoretical finding, we propose a novel Balanced
Destruction-Reconstruction module (BDR) for memory-replay CIL, which can
achieve better knowledge reconstruction by reducing the degree of maximal
destruction of old knowledge. Specifically, to achieve a better balance between
old knowledge and new classes, the proposed BDR module takes into account two
factors: the variance in training status across different classes and the
quantity imbalance of samples from the current phase and memory. By dynamically
manipulating the gradient during training based on these factors, BDR can
effectively alleviate knowledge destruction and improve knowledge
reconstruction. Extensive experiments on a range of CIL benchmarks have shown
that as a lightweight plug-and-play module, BDR can significantly improve the
performance of existing state-of-the-art methods with good generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiangchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1&quot;&gt;Feng Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01700">
<title>Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition. (arXiv:2308.01700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01700</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection could be defined as an optimization problem and solved by
bio-inspired algorithms. Bees Algorithm (BA) shows decent performance in
feature selection optimization tasks. On the other hand, Local Phase
Quantization (LPQ) is a frequency domain feature which has excellent
performance on Depth images. Here, after extracting LPQ features out of RGB
(colour) and Depth images from the Iranian Kinect Face Database (IKFDB), the
Bees feature selection algorithm applies to select the desired number of
features for final classification tasks. IKFDB is recorded with Kinect sensor
V.2 and contains colour and depth images for facial and facial
micro-expressions recognition purposes. Here five facial expressions of Anger,
Joy, Surprise, Disgust and Fear are used for final validation. The proposed
Bees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCA
LPQ, Lasso LPQ, and just LPQ features for classification tasks with Support
Vector Machines (SVM), K-Nearest Neighbourhood (KNN), Shallow Neural Network
and Ensemble Subspace KNN. Returned results, show a decent performance of the
proposed algorithm (99 % accuracy) in comparison with others.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Muhammad Hossein Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilanloo_A/0/1/0/all/0/1&quot;&gt;Atiye Ilanloo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01721">
<title>Weakly Supervised 3D Instance Segmentation without Instance-level Annotations. (arXiv:2308.01721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01721</link>
<description rdf:parseType="Literal">&lt;p&gt;3D semantic scene understanding tasks have achieved great success with the
emergence of deep learning, but often require a huge amount of manually
annotated training data. To alleviate the annotation cost, we propose the first
weakly-supervised 3D instance segmentation method that only requires
categorical semantic labels as supervision, and we do not need instance-level
labels. The required semantic annotations can be either dense or extreme sparse
(e.g. 0.02% of total points). Even without having any instance-related
ground-truth, we design an approach to break point clouds into raw fragments
and find the most confident samples for learning instance centroids.
Furthermore, we construct a recomposed dataset using pseudo instances, which is
used to learn our defined multilevel shape-aware objectness signal. An
asymmetrical object inference algorithm is followed to process core points and
boundary points with different strategies, and generate high-quality pseudo
instance labels to guide iterative training. Experiments demonstrate that our
method can achieve comparable results with recent fully supervised methods. By
generating pseudo instance labels from categorical semantic labels, our
designed approach can also assist existing methods for learning 3D instance
segmentation at reduced annotation cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shichao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01731">
<title>Quantification of Predictive Uncertainty via Inference-Time Sampling. (arXiv:2308.01731v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01731</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive variability due to data ambiguities has typically been addressed
via construction of dedicated models with built-in probabilistic capabilities
that are trained to predict uncertainty estimates as variables of interest.
These approaches require distinct architectural components and training
mechanisms, may include restrictive assumptions and exhibit overconfidence,
i.e., high confidence in imprecise predictions. In this work, we propose a
post-hoc sampling strategy for estimating predictive uncertainty accounting for
data ambiguity. The method can generate different plausible outputs for a given
input and does not assume parametric forms of predictive distributions. It is
architecture agnostic and can be applied to any feed-forward deterministic
network without changes to the architecture or training procedure. Experiments
on regression tasks on imaging and non-imaging input data show the method&apos;s
ability to generate diverse and multi-modal predictive distributions, and a
desirable correlation of the estimated uncertainty with the prediction error.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tothova_K/0/1/0/all/0/1&quot;&gt;Katar&amp;#xed;na T&amp;#xf3;thov&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ladicky_L/0/1/0/all/0/1&quot;&gt;&amp;#x13d;ubor Ladick&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thul_D/0/1/0/all/0/1&quot;&gt;Daniel Thul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1&quot;&gt;Ender Konukoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01738">
<title>Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution. (arXiv:2308.01738v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01738</link>
<description rdf:parseType="Literal">&lt;p&gt;Visibility in hazy nighttime scenes is frequently reduced by multiple
factors, including low light, intense glow, light scattering, and the presence
of multicolored light sources. Existing nighttime dehazing methods often
struggle with handling glow or low-light conditions, resulting in either
excessively dark visuals or unsuppressed glow outputs. In this paper, we
enhance the visibility from a single nighttime haze image by suppressing glow
and enhancing low-light regions. To handle glow effects, our framework learns
from the rendered glow pairs. Specifically, a light source aware network is
proposed to detect light sources of night images, followed by the APSF (Angular
Point Spread Function)-guided glow rendering. Our framework is then trained on
the rendered images, resulting in glow suppression. Moreover, we utilize
gradient-adaptive convolution, to capture edges and textures in hazy scenes. By
leveraging extracted edges and textures, we enhance the contrast of the scene
without losing important structural details. To boost low-light intensity, our
network learns an attention map, then adjusted by gamma correction. This
attention has high values on low-light regions and low values on haze and glow
regions. Extensive evaluation on real nighttime haze images, demonstrates the
effectiveness of our method. Our experiments demonstrate that our method
achieves a PSNR of 30.72dB, outperforming state-of-the-art methods by 14$\%$ on
GTA5 nighttime haze dataset. Our data and code is available at:
\url{https://github.com/jinyeying/nighttime_dehaze}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yeying Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Beibei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Wending Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1&quot;&gt;Robby T. Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01746">
<title>Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants. (arXiv:2308.01746v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01746</link>
<description rdf:parseType="Literal">&lt;p&gt;How to enable learnability for new classes while keeping the capability well
on old classes has been a crucial challenge for class incremental learning.
Beyond the normal case, long-tail class incremental learning and few-shot class
incremental learning are also proposed to consider the data imbalance and data
scarcity, respectively, which are common in real-world implementations and
further exacerbate the well-known problem of catastrophic forgetting. Existing
methods are specifically proposed for one of the three tasks. In this paper, we
offer a unified solution to the misalignment dilemma in the three tasks.
Concretely, we propose neural collapse terminus that is a fixed structure with
the maximal equiangular inter-class separation for the whole label space. It
serves as a consistent target throughout the incremental training to avoid
dividing the feature space incrementally. For CIL and LTCIL, we further propose
a prototype evolving scheme to drive the backbone features into our neural
collapse terminus smoothly. Our method also works for FSCIL with only minor
adaptations. Theoretical analysis indicates that our method holds the neural
collapse optimality in an incremental fashion regardless of data imbalance or
data scarcity. We also design a generalized case where we do not know the total
number of classes and whether the data distribution is normal, long-tail, or
few-shot for each coming session, to test the generalizability of our method.
Extensive experiments with multiple datasets are conducted to demonstrate the
effectiveness of our unified solution to all the three tasks and the
generalized case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangtai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianlong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lefei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01760">
<title>NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&amp;E-Stained Histological Images. (arXiv:2308.01760v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01760</link>
<description rdf:parseType="Literal">&lt;p&gt;In computational pathology, automatic nuclei instance segmentation plays an
essential role in whole slide image analysis. While many computerized
approaches have been proposed for this task, supervised deep learning (DL)
methods have shown superior segmentation performances compared to classical
machine learning and image processing techniques. However, these models need
fully annotated datasets for training which is challenging to acquire,
especially in the medical domain. In this work, we release one of the biggest
fully manually annotated datasets of nuclei in Hematoxylin and Eosin
(H&amp;amp;E)-stained histological images, called NuInsSeg. This dataset contains 665
image patches with more than 30,000 manually segmented nuclei from 31 human and
mouse organs. Moreover, for the first time, we provide additional ambiguous
area masks for the entire dataset. These vague areas represent the parts of the
images where precise and deterministic manual annotations are impossible, even
for human experts. The dataset and detailed step-by-step instructions to
generate related segmentation masks are publicly available at
https://www.kaggle.com/datasets/ipateam/nuinsseg and
https://github.com/masih4/NuInsSeg, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1&quot;&gt;Amirreza Mahbod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Polak_C/0/1/0/all/0/1&quot;&gt;Christine Polak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feldmann_K/0/1/0/all/0/1&quot;&gt;Katharina Feldmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_R/0/1/0/all/0/1&quot;&gt;Rumsha Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gelles_K/0/1/0/all/0/1&quot;&gt;Katharina Gelles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorffner_G/0/1/0/all/0/1&quot;&gt;Georg Dorffner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Woitek_R/0/1/0/all/0/1&quot;&gt;Ramona Woitek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hatamikia_S/0/1/0/all/0/1&quot;&gt;Sepideh Hatamikia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1&quot;&gt;Isabella Ellinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01766">
<title>PoissonNet: Resolution-Agnostic 3D Shape Reconstruction using Fourier Neural Operators. (arXiv:2308.01766v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01766</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PoissonNet, an architecture for shape reconstruction that
addresses the challenge of recovering 3D shapes from points. Traditional deep
neural networks face challenges with common 3D shape discretization techniques
due to their computational complexity at higher resolutions. To overcome this,
we leverage Fourier Neural Operators (FNOs) to solve the Poisson equation and
reconstruct a mesh from oriented point cloud measurements. PoissonNet exhibits
two main advantages. First, it enables efficient training on low-resolution
data while achieving comparable performance at high-resolution evaluation,
thanks to the resolution-agnostic nature of FNOs. This feature allows for
one-shot super-resolution. Second, our method surpasses existing approaches in
reconstruction quality while being differentiable. Overall, our proposed method
not only improves upon the limitations of classical deep neural networks in
shape reconstruction but also achieves superior results in terms of
reconstruction quality, running time, and resolution flexibility. Furthermore,
we demonstrate that the Poisson surface reconstruction problem is well-posed in
the limit case by showing a universal approximation theorem for the solution
operator of the Poisson equation with distributional data utilizing the Fourier
Neuronal Operator, which provides a theoretical foundation for our numerical
results. The code to reproduce the experiments is available on:
\url{https://github.com/arsenal9971/PoissonNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrade_Loarca_H/0/1/0/all/0/1&quot;&gt;Hector Andrade-Loarca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bacho_A/0/1/0/all/0/1&quot;&gt;Aras Bacho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hege_J/0/1/0/all/0/1&quot;&gt;Julius Hege&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1&quot;&gt;Gitta Kutyniok&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01768">
<title>A Novel Tensor Decomposition of arbitrary order based on Block Convolution with Reflective Boundary Conditions for Multi-Dimensional Data Analysis. (arXiv:2308.01768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01768</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor decompositions are powerful tools for analyzing multi-dimensional data
in their original format. Besides tensor decompositions like Tucker and CP,
Tensor SVD (t-SVD) which is based on the t-product of tensors is another
extension of SVD to tensors that recently developed and has found numerous
applications in analyzing high dimensional data. This paper offers a new
insight into the t-Product and shows that this product is a block convolution
of two tensors with periodic boundary conditions. Based on this viewpoint, we
propose a new tensor-tensor product called the $\star_c{}\text{-Product}$ based
on Block convolution with reflective boundary conditions. Using a tensor
framework, this product can be easily extended to tensors of arbitrary order.
Additionally, we introduce a tensor decomposition based on our
$\star_c{}\text{-Product}$ for arbitrary order tensors. Compared to t-SVD, our
new decomposition has lower complexity, and experiments show that it yields
higher-quality results in applications such as classification and compression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molavi_M/0/1/0/all/0/1&quot;&gt;Mahdi Molavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezghi_M/0/1/0/all/0/1&quot;&gt;Mansoor Rezghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_T/0/1/0/all/0/1&quot;&gt;Tayyebeh Saeedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01769">
<title>Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN. (arXiv:2308.01769v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01769</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating nuclei in microscopy images for the training of neural networks is
a laborious task that requires expert knowledge and suffers from inter- and
intra-rater variability, especially in fluorescence microscopy. Generative
networks such as CycleGAN can inverse the process and generate synthetic
microscopy images for a given mask, thereby building a synthetic dataset.
However, past works report content inconsistencies between the mask and
generated image, partially due to CycleGAN minimizing its loss by hiding
shortcut information for the image reconstruction in high frequencies rather
than encoding the desired image content and learning the target task. In this
work, we propose to remove the hidden shortcut information, called
steganography, from generated images by employing a low pass filtering based on
the DCT. We show that this increases coherence between generated images and
cycled masks and evaluate synthetic datasets on a downstream nuclei
segmentation task. Here we achieve an improvement of 5.4 percentage points in
the F1-score compared to a vanilla CycleGAN. Integrating advanced
regularization techniques into the CycleGAN architecture may help mitigate
steganography-related issues and produce more accurate synthetic datasets for
nuclei segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utz_J/0/1/0/all/0/1&quot;&gt;Jonas Utz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weise_T/0/1/0/all/0/1&quot;&gt;Tobias Weise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlereth_M/0/1/0/all/0/1&quot;&gt;Maja Schlereth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Fabian Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thies_M/0/1/0/all/0/1&quot;&gt;Mareike Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uderhardt_S/0/1/0/all/0/1&quot;&gt;Stefan Uderhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Breininger_K/0/1/0/all/0/1&quot;&gt;Katharina Breininger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01771">
<title>Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment. (arXiv:2308.01771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01771</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigated the potential of end-to-end deep learning tools as a
more effective substitute for FEM in predicting stress-strain fields within 2D
cross sections of arterial wall. We first proposed a U-Net based fully
convolutional neural network (CNN) to predict the von Mises stress and strain
distribution based on the spatial arrangement of calcification within arterial
wall cross-sections. Further, we developed a conditional generative adversarial
network (cGAN) to enhance, particularly from the perceptual perspective, the
prediction accuracy of stress and strain field maps for arterial walls with
various calcification quantities and spatial configurations. On top of U-Net
and cGAN, we also proposed their ensemble approaches, respectively, to further
improve the prediction accuracy of field maps. Our dataset, consisting of input
and output images, was generated by implementing boundary conditions and
extracting stress-strain field maps. The trained U-Net models can accurately
predict von Mises stress and strain fields, with structural similarity index
scores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 for
stress and strain, respectively, on a reserved test set. Meanwhile, the cGAN
models in a combination of ensemble and transfer learning techniques
demonstrate high accuracy in predicting von Mises stress and strain fields, as
evidenced by SSIM scores of 0.890 for stress and 0.803 for strain.
Additionally, mean squared errors of 0.008 for stress and 0.017 for strain
further support the model&apos;s performance on a designated test set. Overall, this
study developed a surrogate model for finite element analysis, which can
accurately and efficiently predict stress-strain fields of arterial walls
regardless of complex geometries and boundary conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shokrollahi1_Y/0/1/0/all/0/1&quot;&gt;Yasin Shokrollahi1&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong1_P/0/1/0/all/0/1&quot;&gt;Pengfei Dong1&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_L/0/1/0/all/0/1&quot;&gt;Linxia Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01779">
<title>Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport. (arXiv:2308.01779v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01779</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised image segmentation has recently attracted increasing
research attentions, aiming to avoid the expensive pixel-wise labeling. In this
paper, we present an effective method, namely Point2Mask, to achieve
high-quality panoptic prediction using only a single random point annotation
per target for training. Specifically, we formulate the panoptic pseudo-mask
generation as an Optimal Transport (OT) problem, where each ground-truth (gt)
point label and pixel sample are defined as the label supplier and consumer,
respectively. The transportation cost is calculated by the introduced
task-oriented maps, which focus on the category-wise and instance-wise
differences among the various thing and stuff targets. Furthermore, a
centroid-based scheme is proposed to set the accurate unit number for each gt
point supplier. Hence, the pseudo-mask generation is converted into finding the
optimal transport plan at a globally minimal transportation cost, which can be
solved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC and
COCO demonstrate the promising performance of our proposed Point2Mask approach
to point-supervised panoptic segmentation. Source code is available at:
https://github.com/LiWentomng/Point2Mask.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wentong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuqian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jianke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01804">
<title>QUEST: Query Stream for Vehicle-Infrastructure Cooperative Perception. (arXiv:2308.01804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01804</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative perception can effectively enhance individual perception
performance by providing additional viewpoint and expanding the sensing field.
Existing cooperation paradigms are either interpretable (result cooperation) or
flexible (feature cooperation). In this paper, we propose the concept of query
cooperation to enable interpretable instance-level flexible feature
interaction. To specifically explain the concept, we propose a cooperative
perception framework, termed QUEST, which let query stream flow among agents.
The cross-agent queries are interacted via fusion for co-aware instances and
complementation for individual unaware instances. Taking camera-based
vehicle-infrastructure perception as a typical practical application scene, the
experimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate the
effectiveness of QUEST and further reveal the advantage of the query
cooperation paradigm on transmission flexibility and robustness to packet
dropout. We hope our work can further facilitate the cross-agent representation
interaction for better cooperative perception in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Siqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haibao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenxian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jirui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zaiqing Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01810">
<title>An End-to-end Food Portion Estimation Framework Based on Shape Reconstruction from Monocular Image. (arXiv:2308.01810v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01810</link>
<description rdf:parseType="Literal">&lt;p&gt;Dietary assessment is a key contributor to monitoring health status. Existing
self-report methods are tedious and time-consuming with substantial biases and
errors. Image-based food portion estimation aims to estimate food energy values
directly from food images, showing great potential for automated dietary
assessment solutions. Existing image-based methods either use a single-view
image or incorporate multi-view images and depth information to estimate the
food energy, which either has limited performance or creates user burdens. In
this paper, we propose an end-to-end deep learning framework for food energy
estimation from a monocular image through 3D shape reconstruction. We leverage
a generative model to reconstruct the voxel representation of the food object
from the input image to recover the missing 3D information. Our method is
evaluated on a publicly available food image dataset Nutrition5k, resulting a
Mean Absolute Error (MAE) of 40.05 kCal and Mean Absolute Percentage Error
(MAPE) of 11.47% for food energy estimation. Our method uses RGB image as the
only input at the inference stage and achieves competitive results compared to
the existing method requiring both RGB and depth information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zeman Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinod_G/0/1/0/all/0/1&quot;&gt;Gautham Vinod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiangpeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengqing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01813">
<title>Deep Neural Networks Fused with Textures for Image Classification. (arXiv:2308.01813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained image classification (FGIC) is a challenging task in computer
vision for due to small visual differences among inter-subcategories, but,
large intra-class variations. Deep learning methods have achieved remarkable
success in solving FGIC. In this paper, we propose a fusion approach to address
FGIC by combining global texture with local patch-based information. The first
pipeline extracts deep features from various fixed-size non-overlapping patches
and encodes features by sequential modelling using the long short-term memory
(LSTM). Another path computes image-level textures at multiple scales using the
local binary patterns (LBP). The advantages of both streams are integrated to
represent an efficient feature vector for image classification. The method is
tested on eight datasets representing the human faces, skin lesions, food
dishes, marine lives, etc. using four standard backbone CNNs. Our method has
attained better classification accuracy over existing methods with notable
margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1&quot;&gt;Asish Bera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1&quot;&gt;Debotosh Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasipuri_M/0/1/0/all/0/1&quot;&gt;Mita Nasipuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01839">
<title>Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data. (arXiv:2308.01839v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2308.01839</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-cell data integration can provide a comprehensive molecular view of
cells, and many algorithms have been developed to remove unwanted technical or
biological variations and integrate heterogeneous single-cell datasets. Despite
their wide usage, existing methods suffer from several fundamental limitations.
In particular, we lack a rigorous statistical test for whether two
high-dimensional single-cell datasets are alignable (and therefore should even
be aligned). Moreover, popular methods can substantially distort the data
during alignment, making the aligned data and downstream analysis difficult to
interpret. To overcome these limitations, we present a spectral manifold
alignment and inference (SMAI) framework, which enables principled and
interpretable alignability testing and structure-preserving integration of
single-cell data. SMAI provides a statistical test to robustly determine the
alignability between datasets to avoid misleading inference, and is justified
by high-dimensional statistical theory. On a diverse range of real and
simulated benchmark datasets, it outperforms commonly used alignment methods.
Moreover, we show that SMAI improves various downstream analyses such as
identification of differentially expressed genes and imputation of single-cell
spatial transcriptomics, providing further biological insights. SMAI&apos;s
interpretability also enables quantification and a deeper understanding of the
sources of technical confounders in single-cell data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Rong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sun_E/0/1/0/all/0/1&quot;&gt;Eric D. Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Donoho_D/0/1/0/all/0/1&quot;&gt;David Donoho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01850">
<title>Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling. (arXiv:2308.01850v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01850</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-motion generation has gained increasing attention, but most existing
methods are limited to generating short-term motions that correspond to a
single sentence describing a single action. However, when a text stream
describes a sequence of continuous motions, the generated motions corresponding
to each sentence may not be coherently linked. Existing long-term motion
generation methods face two main issues. Firstly, they cannot directly generate
coherent motions and require additional operations such as interpolation to
process the generated actions. Secondly, they generate subsequent actions in an
autoregressive manner without considering the influence of future actions on
previous ones. To address these issues, we propose a novel approach that
utilizes a past-conditioned diffusion model with two optional coherent sampling
methods: Past Inpainting Sampling and Compositional Transition Sampling. Past
Inpainting Sampling completes subsequent motions by treating previous motions
as conditions, while Compositional Transition Sampling models the distribution
of the transition as the composition of two adjacent motions guided by
different text prompts. Our experimental results demonstrate that our proposed
method is capable of generating compositional and coherent long-term 3D human
motions controlled by a user-instructed long text stream. The code is available
at
\href{https://github.com/yangzhao1230/PCMDM}{https://github.com/yangzhao1230/PCMDM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01854">
<title>Reconstructing Three-Dimensional Models of Interacting Humans. (arXiv:2308.01854v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01854</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding 3d human interactions is fundamental for fine-grained scene
analysis and behavioural modeling. However, most of the existing models predict
incorrect, lifeless 3d estimates, that miss the subtle human contact
aspects--the essence of the event--and are of little use for detailed
behavioral understanding. This paper addresses such issues with several
contributions: (1) we introduce models for interaction signature estimation
(ISP) encompassing contact detection, segmentation, and 3d contact signature
prediction; (2) we show how such components can be leveraged to ensure contact
consistency during 3d reconstruction; (3) we construct several large datasets
for learning and evaluating 3d contact prediction and reconstruction methods;
specifically, we introduce CHI3D, a lab-based accurate 3d motion capture
dataset with 631 sequences containing $2,525$ contact events, $728,664$ ground
truth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with
$14,081$ processed pairs of people, and $81,233$ facet-level surface
correspondences. Finally, (4) we propose methodology for recovering the
ground-truth pose and shape of interacting people in a controlled setup and (5)
annotate all 3d interaction motions in CHI3D with textual descriptions. Motion
data in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) is
made available for research purposes at \url{https://ci3d.imar.ro}, together
with an evaluation server and a public benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieraru_M/0/1/0/all/0/1&quot;&gt;Mihai Fieraru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1&quot;&gt;Mihai Zanfir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oneata_E/0/1/0/all/0/1&quot;&gt;Elisabeta Oneata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popa_A/0/1/0/all/0/1&quot;&gt;Alin-Ionut Popa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olaru_V/0/1/0/all/0/1&quot;&gt;Vlad Olaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1&quot;&gt;Cristian Sminchisescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01867">
<title>MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01867</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,
DPU), deploying deep learning models on edge devices with fixed-point hardware
is still challenging due to complex model quantization and conversion. Existing
model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and
Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,
only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep
learning models cannot be easily quantized for diverse fixed-point hardwares,
mainly due to slightly different quantization requirements. In this paper, we
envision a new type of model quantization approach called MRQ (model
re-quantization), which takes existing quantized models and quickly transforms
the models to meet different quantization requirements (e.g., asymmetric -&amp;gt;
symmetric, non-power-of-2 scale -&amp;gt; power-of-2 scale). Re-quantization is much
simpler than quantizing from scratch because it avoids costly re-training and
provides support for multiple quantization schemes simultaneously. To minimize
re-quantization error, we developed a new set of re-quantization algorithms
including weight correction and rounding error folding. We have demonstrated
that MobileNetV2 QAT model [7] can be quickly re-quantized into two different
quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less
than 0.64 units of accuracy loss. We believe our work is the first to leverage
this concept of re-quantization for model quantization and models obtained from
the re-quantization process have been successfully deployed on NNA in the Echo
Show devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manohara_M/0/1/0/all/0/1&quot;&gt;Manasa Manohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayal_S/0/1/0/all/0/1&quot;&gt;Sankalp Dayal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1&quot;&gt;Tarqi Afzal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1&quot;&gt;Rahul Bakshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kahkuen Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01888">
<title>FROD: Robust Object Detection for Free. (arXiv:2308.01888v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01888</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection is a vital task in computer vision and has become an
integral component of numerous critical systems. However, state-of-the-art
object detectors, similar to their classification counterparts, are susceptible
to small adversarial perturbations that can significantly alter their normal
behavior. Unlike classification, the robustness of object detectors has not
been thoroughly explored. In this work, we take the initial step towards
bridging the gap between the robustness of classification and object detection
by leveraging adversarially trained classification models. Merely utilizing
adversarially trained models as backbones for object detection does not result
in robustness. We propose effective modifications to the classification-based
backbone to instill robustness in object detection without incurring any
computational overhead. To further enhance the robustness achieved by the
proposed modified backbone, we introduce two lightweight components: imitation
loss and delayed adversarial training. Extensive experiments on the MS-COCO and
Pascal VOC datasets are conducted to demonstrate the effectiveness of our
proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad/0/1/0/all/0/1&quot;&gt;Muhammad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awais/0/1/0/all/0/1&quot;&gt;Awais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiming/0/1/0/all/0/1&quot;&gt;Weiming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang/0/1/0/all/0/1&quot;&gt;Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lingjuan/0/1/0/all/0/1&quot;&gt;Lingjuan&lt;/a&gt;, Lyu, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung-Ho/0/1/0/all/0/1&quot;&gt;Sung-Ho&lt;/a&gt;, Bae</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01890">
<title>DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations. (arXiv:2308.01890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01890</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-label image recognition in the low-label regime is a task of great
challenge and practical significance. Previous works have focused on learning
the alignment between textual and visual spaces to compensate for limited image
labels, yet may suffer from reduced accuracy due to the scarcity of
high-quality multi-label annotations. In this research, we leverage the
powerful alignment between textual and visual features pretrained with millions
of auxiliary image-text pairs. We introduce an efficient and effective
framework called Evidence-guided Dual Context Optimization (DualCoOp++), which
serves as a unified approach for addressing partial-label and zero-shot
multi-label recognition. In DualCoOp++ we separately encode evidential,
positive, and negative contexts for target classes as parametric components of
the linguistic input (i.e., prompts). The evidential context aims to discover
all the related visual content for the target class, and serves as guidance to
aggregate positive and negative contexts from the spatial domain of the image,
enabling better distinguishment between similar categories. Additionally, we
introduce a Winner-Take-All module that promotes inter-class interaction during
training, while avoiding the need for extra parameters and costs. As DualCoOp++
imposes minimal additional learnable overhead on the pretrained vision-language
framework, it enables rapid adaptation to multi-label recognition tasks with
limited annotations and even unseen classes. Experiments on standard
multi-label recognition benchmarks across two challenging low-label settings
demonstrate the superior performance of our approach compared to
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Ping Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Ximeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1&quot;&gt;Stan Sclaroff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01898">
<title>UniSim: A Neural Closed-Loop Sensor Simulator. (arXiv:2308.01898v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01898</link>
<description rdf:parseType="Literal">&lt;p&gt;Rigorously testing autonomy systems is essential for making safe self-driving
vehicles (SDV) a reality. It requires one to generate safety critical scenarios
beyond what can be collected safely in the world, as many scenarios happen
rarely on public roads. To accurately evaluate performance, we need to test the
SDV on these scenarios in closed-loop, where the SDV and other actors interact
with each other at each timestep. Previously recorded driving logs provide a
rich resource to build these new scenarios from, but for closed loop
evaluation, we need to modify the sensor data based on the new scene
configuration and the SDV&apos;s decisions, as actors might be added or removed and
the trajectories of existing actors and the SDV will differ from the original
log. In this paper, we present UniSim, a neural sensor simulator that takes a
single recorded log captured by a sensor-equipped vehicle and converts it into
a realistic closed-loop multi-sensor simulation. UniSim builds neural feature
grids to reconstruct both the static background and dynamic actors in the
scene, and composites them together to simulate LiDAR and camera data at new
viewpoints, with actors added or removed and at new placements. To better
handle extrapolated views, we incorporate learnable priors for dynamic objects,
and leverage a convolutional network to complete unseen regions. Our
experiments show UniSim can simulate realistic sensor data with small domain
gap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation of
an autonomy system on safety-critical scenarios as if it were in the real
world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ze Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manivasagam_S/0/1/0/all/0/1&quot;&gt;Sivabalan Manivasagam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei-Chiu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01904">
<title>DETR Doesn&apos;t Need Multi-Scale or Locality Design. (arXiv:2308.01904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01904</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an improved DETR detector that maintains a &quot;plain&quot;
nature: using a single-scale feature map and global cross-attention
calculations without specific locality constraints, in contrast to previous
leading DETR-based detectors that reintroduce architectural inductive biases of
multi-scale and locality into the decoder. We show that two simple technologies
are surprisingly effective within a plain design to compensate for the lack of
multi-scale feature maps and locality constraints. The first is a box-to-pixel
relative position bias (BoxRPB) term added to the cross-attention formulation,
which well guides each query to attend to the corresponding object region while
also providing encoding flexibility. The second is masked image modeling
(MIM)-based backbone pre-training which helps learn representation with
fine-grained localization ability and proves crucial for remedying dependencies
on the multi-scale feature maps. By incorporating these technologies and recent
advancements in training and problem formation, the improved &quot;plain&quot; DETR
showed exceptional improvements over the original DETR detector. By leveraging
the Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using a
Swin-L backbone, which is highly competitive with state-of-the-art detectors
which all heavily rely on multi-scale feature maps and region-based feature
extraction. Code is available at https://github.com/impiga/Plain-DETR .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yutong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01905">
<title>Revisiting Deformable Convolution for Depth Completion. (arXiv:2308.01905v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;Depth completion, which aims to generate high-quality dense depth maps from
sparse depth maps, has attracted increasing attention in recent years. Previous
work usually employs RGB images as guidance, and introduces iterative spatial
propagation to refine estimated coarse depth maps. However, most of the
propagation refinement methods require several iterations and suffer from a
fixed receptive field, which may contain irrelevant and useless information
with very sparse input. In this paper, we address these two challenges
simultaneously by revisiting the idea of deformable convolution. We propose an
effective architecture that leverages deformable kernel convolution as a
single-pass refinement module, and empirically demonstrate its superiority. To
better understand the function of deformable convolution and exploit it for
depth completion, we further systematically investigate a variety of
representative strategies. Our study reveals that, different from prior work,
deformable convolution needs to be applied on an estimated depth map with a
relatively high density for better performance. We evaluate our model on the
large-scale KITTI dataset and achieve state-of-the-art level performance in
both accuracy and inference speed. Our code is available at
https://github.com/AlexSunNik/ReDC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinglong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1&quot;&gt;Jean Ponce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01907">
<title>The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World. (arXiv:2308.01907v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01907</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the All-Seeing (AS) project: a large-scale data and model for
recognizing and understanding everything in the open world. Using a scalable
data engine that incorporates human feedback and efficient models in the loop,
we create a new dataset (AS-1B) with over 1 billion regions annotated with
semantic tags, question-answering pairs, and detailed captions. It covers a
wide range of 3.5 million common and rare concepts in the real world, and has
132.2 billion tokens that describe the concepts and their attributes.
Leveraging this new dataset, we develop the All-Seeing model (ASM), a unified
framework for panoptic visual recognition and understanding. The model is
trained with open-ended language prompts and locations, which allows it to
generalize to various vision and language tasks with remarkable zero-shot
performance, including region-text retrieval, region recognition, captioning,
and question-answering. We hope that this project can serve as a foundation for
vision-language artificial general intelligence research. Models and the
dataset shall be released at https://github.com/OpenGVLab/All-Seeing, and demo
can be seen at https://huggingface.co/spaces/OpenGVLab/all-seeing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Min Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhenhang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Linjie Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yushi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.11233">
<title>ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation. (arXiv:2011.11233v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.11233</link>
<description rdf:parseType="Literal">&lt;p&gt;Albeit being a prevalent architecture searching approach, differentiable
architecture search (DARTS) is largely hindered by its substantial memory cost
since the entire supernet resides in the memory. This is where the single-path
DARTS comes in, which only chooses a single-path submodel at each step. While
being memory-friendly, it also comes with low computational costs. Nonetheless,
we discover a critical issue of single-path DARTS that has not been primarily
noticed. Namely, it also suffers from severe performance collapse since too
many parameter-free operations like skip connections are derived, just like
DARTS does. In this paper, we propose a new algorithm called RObustifying
Memory-Efficient NAS (ROME) to give a cure. First, we disentangle the topology
search from the operation search to make searching and evaluation consistent.
We then adopt Gumbel-Top2 reparameterization and gradient accumulation to
robustify the unwieldy bi-level optimization. We verify ROME extensively across
15 benchmarks to demonstrate its effectiveness and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yuda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhexi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.12844">
<title>Reconstructing Pruned Filters using Cheap Spatial Transformations. (arXiv:2110.12844v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.12844</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an efficient alternative to the convolutional layer using cheap
spatial transformations. This construction exploits an inherent spatial
redundancy of the learned convolutional filters to enable a much greater
parameter efficiency, while maintaining the top-end accuracy of their dense
counter-parts. Training these networks is modelled as a generalised pruning
problem, whereby the pruned filters are replaced with cheap transformations
from the set of non-pruned filters. We provide an efficient implementation of
the proposed layer, followed by two natural extensions to avoid excessive
feature compression and to improve the expressivity of the transformed
features. We show that these networks can achieve comparable or improved
performance to state-of-the-art pruning models across both the CIFAR-10 and
ImageNet-1K datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.15402">
<title>Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship. (arXiv:2112.15402v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.15402</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning is a promising machine learning paradigm to learn new
tasks while retaining previously learned knowledge over streaming training
data. Till now, rehearsal-based methods, keeping a small part of data from old
tasks as a memory buffer, have shown good performance in mitigating
catastrophic forgetting for previously learned knowledge. However, most of
these methods typically treat each new task equally, which may not adequately
consider the relationship or similarity between old and new tasks. Furthermore,
these methods commonly neglect sample importance in the continual training
process and result in sub-optimal performance on certain tasks. To address this
challenging problem, we propose Relational Experience Replay (RER), a bi-level
learning framework, to adaptively tune task-wise relationships and sample
importance within each task to achieve a better `stability&apos; and `plasticity&apos;
trade-off. As such, the proposed method is capable of accumulating new
knowledge while consolidating previously learned old knowledge during continual
learning. Extensive experiments conducted on three publicly available datasets
(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed method
can consistently improve the performance of all baselines and surpass current
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quanziang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renzhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuexiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1&quot;&gt;Deyu Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01615">
<title>Lawin Transformer: Improving New-Era Vision Backbones with Multi-Scale Representations for Semantic Segmentation. (arXiv:2201.01615v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01615</link>
<description rdf:parseType="Literal">&lt;p&gt;The multi-level aggregation (MLA) module has emerged as a critical component
for advancing new-era vision back-bones in semantic segmentation. In this
paper, we propose Lawin (large window) Transformer, a novel MLA architecture
that creatively utilizes multi-scale feature maps from the vision backbone. At
the core of Lawin Transformer is the Lawin attention, a newly designed window
attention mechanism capable of querying much larger context windows than local
windows. We focus on studying the efficient and simplistic application of the
large-window paradigm, allowing for flexible regulation of the ratio of large
context to query and capturing multi-scale representations. We validate the
effectiveness of Lawin Transformer on Cityscapes and ADE20K, consistently
demonstrating great superiority to widely-used MLA modules when combined with
new-era vision backbones. The code is available at
https://github.com/yan-hao-tian/lawin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haotian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.07901">
<title>Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition. (arXiv:2202.07901v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.07901</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal representation learning learns a shared embedding between two or
more modalities to improve performance in a given task compared to using only
one of the modalities. Cross-modal representation learning from different data
types -- such as images and time-series data (e.g., audio or text data) --
requires a deep metric learning loss that minimizes the distance between the
modality embeddings. In this paper, we propose to use the contrastive or
triplet loss, which uses positive and negative identities to create sample
pairs with different labels, for cross-modal representation learning between
image and time-series modalities (CMR-IS). By adapting the triplet loss for
cross-modal representation learning, higher accuracy in the main (time-series
classification) task can be achieved by exploiting additional information of
the auxiliary (image classification) task. We present a triplet loss with a
dynamic margin for single label and sequence-to-sequence classification tasks.
We perform extensive evaluations on synthetic image and time-series data, and
on data for offline handwriting recognition (HWR) and on online HWR from
sensor-enhanced pens for classifying written words. Our experiments show an
improved classification accuracy, faster convergence, and better
generalizability due to an improved cross-modal representation. Furthermore,
the more suitable generalizability leads to a better adaptability between
writers for online HWR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_F/0/1/0/all/0/1&quot;&gt;Felix Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heublein_L/0/1/0/all/0/1&quot;&gt;Lucas Heublein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutschler_C/0/1/0/all/0/1&quot;&gt;Christopher Mutschler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.09957">
<title>Enhancement of Novel View Synthesis Using Omnidirectional Image Completion. (arXiv:2203.09957v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.09957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a method for synthesizing novel views from a single
360-degree RGB-D image based on the neural radiance field (NeRF) . Prior
studies relied on the neighborhood interpolation capability of multi-layer
perceptrons to complete missing regions caused by occlusion and zooming, which
leads to artifacts. In the method proposed in this study, the input image is
reprojected to 360-degree RGB images at other camera positions, the missing
regions of the reprojected images are completed by a 2D image generative model,
and the completed images are utilized to train the NeRF. Because multiple
completed images contain inconsistencies in 3D, we introduce a method to learn
the NeRF model using a subset of completed images that cover the target scene
with less overlap of completed regions. The selection of such a subset of
images can be attributed to the maximum weight independent set problem, which
is solved through simulated annealing. Experiments demonstrated that the
proposed method can synthesize plausible novel views while preserving the
features of the scene for both artificial and real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1&quot;&gt;Takayuki Hara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00874">
<title>S$^2$Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning. (arXiv:2208.00874v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00874</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent efforts in accurate 3D annotations in hand and object
datasets, there still exist gaps in 3D hand and object reconstructions.
Existing works leverage contact maps to refine inaccurate hand-object pose
estimations and generate grasps given object models. However, they require
explicit 3D supervision which is seldom available and therefore, are limited to
constrained settings, e.g., where thermal cameras observe residual heat left on
manipulated objects. In this paper, we propose a novel semi-supervised
framework that allows us to learn contact from monocular images. Specifically,
we leverage visual and geometric consistency constraints in large-scale
datasets for generating pseudo-labels in semi-supervised learning and propose
an efficient graph-based network to infer contact. Our semi-supervised learning
framework achieves a favourable improvement over the existing supervised
learning methods trained on data with `limited&apos; annotations. Notably, our
proposed model is able to achieve superior results with less than half the
network parameters and memory access cost when compared with the commonly-used
PointNet-based approach. We show benefits from using a contact map that rules
hand-object interactions to produce more accurate reconstructions. We further
demonstrate that training with pseudo-labels can extend contact map estimations
to out-of-domain objects and generalise better across multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tse_T/0/1/0/all/0/1&quot;&gt;Tze Ho Elden Tse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongqun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1&quot;&gt;Ales Leonardis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyung Jin Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.11435">
<title>Bidirectional Contrastive Split Learning for Visual Question Answering. (arXiv:2208.11435v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.11435</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Question Answering (VQA) based on multi-modal data facilitates
real-life applications such as home robots and medical diagnoses. One
significant challenge is to devise a robust decentralized learning framework
for various client models where centralized data collection is refrained due to
confidentiality concerns. This work aims to tackle privacy-preserving VQA by
decoupling a multi-modal model into representation modules and a contrastive
module and leveraging inter-module gradients sharing and inter-client weight
sharing. To this end, we propose Bidirectional Contrastive Split Learning
(BiCSL) to train a global multi-modal model on the entire data distribution of
decentralized clients. We employ the contrastive loss that enables a more
efficient self-supervised learning of decentralized modules. Comprehensive
experiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,
demonstrating the effectiveness of the proposed method. Furthermore, we inspect
BiCSL&apos;s robustness against a dual-key backdoor attack on VQA. Consequently,
BiCSL shows much better robustness to the multi-modal adversarial attack
compared to the centralized learning method, which provides a promising
approach to decentralized multi-modal learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1&quot;&gt;Hideya Ochiai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13061">
<title>A Masked Face Classification Benchmark on Low-Resolution Surveillance Images. (arXiv:2211.13061v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13061</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel image dataset focused on tiny faces wearing face masks for
mask classification purposes, dubbed Small Face MASK (SF-MASK), composed of a
collection made from 20k low-resolution images exported from diverse and
heterogeneous datasets, ranging from 7 x 7 to 64 x 64 pixel resolution. An
accurate visualization of this collection, through counting grids, made it
possible to highlight gaps in the variety of poses assumed by the heads of the
pedestrians. In particular, faces filmed by very high cameras, in which the
facial features appear strongly skewed, are absent. To address this structural
deficiency, we produced a set of synthetic images which resulted in a
satisfactory covering of the intra-class variance. Furthermore, a small
subsample of 1701 images contains badly worn face masks, opening to multi-class
classification challenges. Experiments on SF-MASK focus on face mask
classification using several classifiers. Results show that the richness of
SF-MASK (real + synthetic images) leads all of the tested classifiers to
perform better than exploiting comparative face mask datasets, on a fixed 1077
images testing set. Dataset and evaluation code are publicly available here:
https://github.com/HumaticsLAB/sf-mask
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunico_F/0/1/0/all/0/1&quot;&gt;Federico Cunico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toaiari_A/0/1/0/all/0/1&quot;&gt;Andrea Toaiari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1&quot;&gt;Marco Cristani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13755">
<title>TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network. (arXiv:2211.13755v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13755</link>
<description rdf:parseType="Literal">&lt;p&gt;We present TemporalStereo, a coarse-to-fine stereo matching network that is
highly efficient, and able to effectively exploit the past geometry and context
information to boost matching accuracy. Our network leverages sparse cost
volume and proves to be effective when a single stereo pair is given. However,
its peculiar ability to use spatio-temporal information across stereo sequences
allows TemporalStereo to alleviate problems such as occlusions and reflective
regions while enjoying high efficiency also in this latter case. Notably, our
model -- trained once with stereo videos -- can run in both single-pair and
temporal modes seamlessly. Experiments show that our network relying on camera
motion is robust even to dynamic objects when running on videos. We validate
TemporalStereo through extensive experiments on synthetic (SceneFlow,
TartanAir) and real (KITTI 2012, KITTI 2015) datasets. Our model achieves
state-of-the-art performance on any of these datasets. Code is available at
\url{https://github.com/youmi-zym/TemporalStereo.git}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youmin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1&quot;&gt;Matteo Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1&quot;&gt;Stefano Mattoccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14512">
<title>Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation. (arXiv:2211.14512v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14512</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation models classify pixels into a set of known
(``in-distribution&apos;&apos;) visual classes. When deployed in an open world, the
reliability of these models depends on their ability not only to classify
in-distribution pixels but also to detect out-of-distribution (OoD) pixels.
Historically, the poor OoD detection performance of these models has motivated
the design of methods based on model re-training using synthetic training
images that include OoD visual objects. Although successful, these re-trained
methods have two issues: 1) their in-distribution segmentation accuracy may
drop during re-training, and 2) their OoD detection accuracy does not
generalise well to new contexts (e.g., country surroundings) outside the
training set (e.g., city surroundings). In this paper, we mitigate these issues
with: (i) a new residual pattern learning (RPL) module that assists the
segmentation model to detect OoD pixels without affecting the inlier
segmentation performance; and (ii) a novel context-robust contrastive learning
(CoroCL) that enforces RPL to robustly detect OoD pixels among various
contexts. Our approach improves by around 10\% FPR and 7\% AuPRC the previous
state-of-the-art in Fishyscapes, Segment-Me-If-You-Can, and RoadAnomaly
datasets. Our code is available at: https://github.com/yyliu01/RPL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Choubo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1&quot;&gt;Vasileios Belagiannis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02796">
<title>DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model. (arXiv:2212.02796v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02796</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the development of 2D keypoint detectors, monocular 3D human pose
estimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkable
improvements. Still, monocular 3D HPE is a challenging problem due to the
inherent depth ambiguities and occlusions. To handle this problem, many
previous works exploit temporal information to mitigate such difficulties.
However, there are many real-world applications where frame sequences are not
accessible. This paper focuses on reconstructing a 3D pose from a single 2D
keypoint detection. Rather than exploiting temporal information, we alleviate
the depth ambiguity by generating multiple 3D pose candidates which can be
mapped to an identical 2D keypoint. We build a novel diffusion-based framework
to effectively sample diverse 3D poses from an off-the-shelf 2D detector. By
considering the correlation between human joints by replacing the conventional
denoising U-Net with graph convolutional network, our approach accomplishes
further performance improvements. We evaluate our method on the widely adopted
Human3.6M and HumanEva-I datasets. Comprehensive experiments are conducted to
prove the efficacy of the proposed method, and they confirm that our model
outperforms state-of-the-art multi-hypothesis 3D HPE methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongjun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1&quot;&gt;Dongseok Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;H. Jin Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04385">
<title>BEVBert: Multimodal Map Pre-training for Language-guided Navigation. (arXiv:2212.04385v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04385</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-training has shown promising results on the
vision-and-language navigation (VLN) task. However, most existing pre-training
methods employ discrete panoramas to learn visual-textual associations. This
requires the model to implicitly correlate incomplete, duplicate observations
within the panoramas, which may impair an agent&apos;s spatial understanding. Thus,
we propose a new map-based pre-training paradigm that is spatial-aware for use
in VLN. Concretely, we build a local metric map to explicitly aggregate
incomplete observations and remove duplicates, while modeling navigation
dependency in a global topological map. This hybrid design can balance the
demand of VLN for both short-term reasoning and long-term planning. Then, based
on the hybrid map, we devise a pre-training framework to learn a multimodal map
representation, which enhances spatial-aware cross-modal reasoning thereby
facilitating the language-guided navigation goal. Extensive experiments
demonstrate the effectiveness of the map-based pre-training route for VLN, and
the proposed method achieves state-of-the-art on four VLN benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_D/0/1/0/all/0/1&quot;&gt;Dong An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuankai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06458">
<title>HS-Diffusion: Semantic-Mixing Diffusion for Head Swapping. (arXiv:2212.06458v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06458</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-based head swapping task aims to stitch a source head to another source
body flawlessly. This seldom-studied task faces two major challenges: 1)
Preserving the head and body from various sources while generating a seamless
transition region. 2) No paired head swapping dataset and benchmark so far. In
this paper, we propose a semantic-mixing diffusion model for head swapping
(HS-Diffusion) which consists of a latent diffusion model (LDM) and a semantic
layout generator. We blend the semantic layouts of source head and source body,
and then inpaint the transition region by the semantic layout generator,
achieving a coarse-grained head swapping. Semantic-mixing LDM can further
implement a fine-grained head swapping with the inpainted layout as condition
by a progressive fusion process, while preserving head and body with
high-quality reconstruction. To this end, we propose a semantic calibration
strategy for natural inpainting and a neck alignment for geometric realism.
Importantly, we construct a new image-based head swapping benchmark and design
two tailor-designed metrics (Mask-FID and Focal-FID). Extensive experiments
demonstrate the superiority of our framework. The code will be available:
https://github.com/qinghew/HS-Diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qinghe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lijie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1&quot;&gt;Miao Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bing Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08583">
<title>Semi-Siamese Network for Robust Change Detection Across Different Domains with Applications to 3D Printing. (arXiv:2212.08583v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08583</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic defect detection for 3D printing processes, which shares many
characteristics with change detection problems, is a vital step for quality
control of 3D printed products. However, there are some critical challenges in
the current state of practice. First, existing methods for computer
vision-based process monitoring typically work well only under specific camera
viewpoints and lighting situations, requiring expensive pre-processing,
alignment, and camera setups. Second, many defect detection techniques are
specific to pre-defined defect patterns and/or print schematics. In this work,
we approach the defect detection problem using a novel Semi-Siamese deep
learning model that directly compares a reference schematic of the desired
print and a camera image of the achieved print. The model then solves an image
segmentation problem, precisely identifying the locations of defects of
different types with respect to the reference schematic. Our model is designed
to enable comparison of heterogeneous images from different domains while being
robust against perturbations in the imaging setup such as different camera
angles and illumination. Crucially, we show that our simple architecture, which
is easy to pre-train for enhanced performance on new datasets, outperforms more
complex state-of-the-art approaches based on generative adversarial networks
and transformers. Using our model, defect localization predictions can be made
in less than half a second per layer using a standard MacBook Pro while
achieving an F1-score of more than 0.9, demonstrating the efficacy of using our
method for in-situ defect detection in 3D printing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yushuo Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadwick_E/0/1/0/all/0/1&quot;&gt;Ethan Chadwick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Anson W. K. Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13726">
<title>A Clustering-guided Contrastive Fusion for Multi-view Representation Learning. (arXiv:2212.13726v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13726</link>
<description rdf:parseType="Literal">&lt;p&gt;The past two decades have seen increasingly rapid advances in the field of
multi-view representation learning due to it extracting useful information from
diverse domains to facilitate the development of multi-view applications.
However, the community faces two challenges: i) how to learn robust
representations from a large amount of unlabeled data to against noise or
incomplete views setting, and ii) how to balance view consistency and
complementary for various downstream tasks. To this end, we utilize a deep
fusion network to fuse view-specific representations into the view-common
representation, extracting high-level semantics for obtaining robust
representation. In addition, we employ a clustering task to guide the fusion
network to prevent it from leading to trivial solutions. For balancing
consistency and complementary, then, we design an asymmetrical contrastive
strategy that aligns the view-common representation and each view-specific
representation. These modules are incorporated into a unified method known as
CLustering-guided cOntrastiVE fusioN (CLOVEN). We quantitatively and
qualitatively evaluate the proposed method on five datasets, demonstrating that
CLOVEN outperforms 11 competitive multi-view learning methods in clustering and
classification. In the incomplete view scenario, our proposed method resists
noise interference better than those of our competitors. Furthermore, the
visualization analysis shows that CLOVEN can preserve the intrinsic structure
of view-specific representation while also improving the compactness of
view-commom representation. Our source code will be available soon at
https://github.com/guanzhou-ke/cloven.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_G/0/1/0/all/0/1&quot;&gt;Guoqing Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.06267">
<title>Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models. (arXiv:2301.06267v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.06267</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to quickly learn a new task with minimal instruction - known as
few-shot learning - is a central aspect of intelligent agents. Classical
few-shot benchmarks make use of few-shot samples from a single modality, but
such samples may not be sufficient to characterize an entire concept class. In
contrast, humans use cross-modal information to learn new concepts efficiently.
In this work, we demonstrate that one can indeed build a better ${\bf visual}$
dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them
bark. To do so, we exploit the fact that recent multimodal foundation models
such as CLIP are inherently cross-modal, mapping different modalities to the
same representation space. Specifically, we propose a simple cross-modal
adaptation approach that learns from few-shot examples spanning different
modalities. By repurposing class names as additional one-shot training samples,
we achieve SOTA results with an embarrassingly simple linear classifier for
vision-language adaptation. Furthermore, we show that our approach can benefit
existing methods such as prefix tuning, adapters, and classifier ensembling.
Finally, to explore other modalities beyond vision and language, we construct
the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal
training to improve the performance of both image and audio classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiqiu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Samuel Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13869">
<title>EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography. (arXiv:2302.13869v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13869</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces the Efficient Decoupled Masked Autoencoder (EDMAE), a
novel self-supervised method for recognizing standard views in pediatric
echocardiography. EDMAE introduces a new proxy task based on the
encoder-decoder structure. The EDMAE encoder is composed of a teacher and a
student encoder. The teacher encoder extracts the potential representation of
the masked image blocks, while the student encoder extracts the potential
representation of the visible image blocks. The loss is calculated between the
feature maps output by the two encoders to ensure consistency in the latent
representations they extract. EDMAE uses pure convolution operations instead of
the ViT structure in the MAE encoder. This improves training efficiency and
convergence speed. EDMAE is pre-trained on a large-scale private dataset of
pediatric echocardiography using self-supervised learning, and then fine-tuned
for standard view recognition. The proposed method achieves high classification
accuracy in 27 standard views of pediatric echocardiography. To further verify
the effectiveness of the proposed method, the authors perform another
downstream task of cardiac ultrasound segmentation on the public dataset CAMUS.
The experimental results demonstrate that the proposed method outperforms some
popular supervised and recent self-supervised methods, and is more competitive
on different downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiman Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoxiang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tongtong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiajun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Menghan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiangang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05118">
<title>SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model. (arXiv:2303.05118v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05118</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of continual learning is to improve the performance of recognition
models in learning sequentially arrived data. Although most existing works are
established on the premise of learning from scratch, growing efforts have been
devoted to incorporating the benefits of pre-training. However, how to
adaptively exploit the pre-trained knowledge for each incremental task while
maintaining its generalizability remains an open question. In this work, we
present an extensive analysis for continual learning on a pre-trained model
(CLPM), and attribute the key challenge to a progressive overfitting problem.
Observing that selectively reducing the learning rate can almost resolve this
issue in the representation layer, we propose a simple but extremely effective
approach named Slow Learner with Classifier Alignment (SLCA), which further
improves the classification layer by modeling the class-wise distributions and
aligning the classification layers in a post-hoc fashion. Across a variety of
scenarios, our proposal provides substantial improvements for CLPM (e.g., up to
49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, Split
CUB-200 and Split Cars-196, respectively), and thus outperforms
state-of-the-art approaches by a large margin. Based on such a strong baseline,
critical factors and promising directions are analyzed in-depth to facilitate
subsequent research. Code has been made available at:
https://github.com/GengDavid/SLCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1&quot;&gt;Guoliang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06388">
<title>FAC: 3D Representation Learning via Foreground Aware Feature Contrast. (arXiv:2303.06388v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06388</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has recently demonstrated great potential for
unsupervised pre-training in 3D scene understanding tasks. However, most
existing work randomly selects point features as anchors while building
contrast, leading to a clear bias toward background points that often dominate
in 3D scenes. Also, object awareness and foreground-to-background
discrimination are neglected, making contrastive learning less effective. To
tackle these issues, we propose a general foreground-aware feature contrast
(FAC) framework to learn more effective point cloud representations in
pre-training. FAC consists of two novel contrast designs to construct more
effective and informative contrast pairs. The first is building positive pairs
within the same foreground segment where points tend to have the same
semantics. The second is that we prevent over-discrimination between 3D
segments/objects and encourage foreground-to-background distinctions at the
segment level with adaptive feature learning in a Siamese correspondence
network, which adaptively learns feature correlations within and across point
cloud views effectively. Visualization with point activation maps shows that
our contrast pairs capture clear correspondences among foreground regions
during pre-training. Quantitative experiments also show that FAC achieves
superior knowledge transfer and data efficiency in various downstream 3D
semantic segmentation and object detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kangcheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1&quot;&gt;Aoran Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Ling Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08757">
<title>CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke. (arXiv:2303.08757v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08757</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and fast prediction methods for ischemic areas comprised of dead
tissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)
patients are of significant clinical interest. They play an essential role in
improving diagnosis and treatment planning. Computed Tomography (CT) scan is
one of the primary modalities for early assessment in patients with suspected
AIS. CT Perfusion (CTP) is often used as a primary assessment to determine
stroke location, severity, and volume of ischemic lesions. Current automatic
segmentation methods for CTP mostly use already processed 3D parametric maps
conventionally used for clinical interpretation by radiologists as input.
Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time
input, where the spatial information over the volume is ignored. In addition,
these methods are only interested in segmenting core regions, while predicting
penumbra can be essential for treatment planning. This paper investigates
different methods to utilize the entire 4D CTP as input to fully exploit the
spatio-temporal information, leading us to propose a novel 4D convolution
layer. Our comprehensive experiments on a local dataset of 152 patients divided
into three groups show that our proposed models generate more precise results
than other methods explored. Adopting the proposed 4D mJ-Net, a Dice
Coefficient of 0.53 and 0.23 is achieved for segmenting penumbra and core
areas, respectively. The code is available on
https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1&quot;&gt;Luca Tomasetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1&quot;&gt;Kjersti Engan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollesli_L/0/1/0/all/0/1&quot;&gt;Liv Jorunn H&amp;#xf8;llesli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurz_K/0/1/0/all/0/1&quot;&gt;Kathinka D&amp;#xe6;hli Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanmohammadi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Khanmohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10741">
<title>Computer Vision Estimation of Emotion Reaction Intensity in the Wild. (arXiv:2303.10741v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10741</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotions play an essential role in human communication. Developing computer
vision models for automatic recognition of emotion expression can aid in a
variety of domains, including robotics, digital behavioral healthcare, and
media analytics. There are three types of emotional representations which are
traditionally modeled in affective computing research: Action Units, Valence
Arousal (VA), and Categorical Emotions. As part of an effort to move beyond
these representations towards more fine-grained labels, we describe our
submission to the newly introduced Emotional Reaction Intensity (ERI)
Estimation challenge in the 5th competition for Affective Behavior Analysis
in-the-Wild (ABAW). We developed four deep neural networks trained in the
visual domain and a multimodal model trained with both visual and audio
features to predict emotion reaction intensity. Our best performing model on
the Hume-Reaction dataset achieved an average Pearson correlation coefficient
of 0.4080 on the test set using a pre-trained ResNet50 model. This work
provides a first step towards the development of production-grade models which
predict emotion reaction intensities rather than discrete emotion categories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yang Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kargarandehkordi_A/0/1/0/all/0/1&quot;&gt;Ali Kargarandehkordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1&quot;&gt;Onur Cezmi Mutlu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surabhi_S/0/1/0/all/0/1&quot;&gt;Saimourya Surabhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Honarmand_M/0/1/0/all/0/1&quot;&gt;Mohammadmahdi Honarmand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1&quot;&gt;Dennis Paul Wall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1&quot;&gt;Peter Washington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11098">
<title>A closer look at the training dynamics of knowledge distillation. (arXiv:2303.11098v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11098</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we revisit the efficacy of knowledge distillation as a function
matching and metric learning problem. In doing so we verify three important
design decisions, namely the normalisation, soft maximum function, and
projection layers as key ingredients. We theoretically show that the projector
implicitly encodes information on past examples, enabling relational gradients
for the student. We then show that the normalisation of representations is
tightly coupled with the training dynamics of this projector, which can have a
large impact on the students performance. Finally, we show that a simple soft
maximum function can be used to address any significant capacity gap problems.
Experimental results on various benchmark datasets demonstrate that using these
insights can lead to superior or comparable performance to state-of-the-art
knowledge distillation techniques, despite being much more computationally
efficient. In particular, we obtain these results across image classification
(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficult
distillation objectives, such as training data efficient transformers, whereby
we attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miles_R/0/1/0/all/0/1&quot;&gt;Roy Miles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1&quot;&gt;Krystian Mikolajczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12360">
<title>Automatically Predict Material Properties with Microscopic Image Example Polymer Compatibility. (arXiv:2303.12360v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12360</link>
<description rdf:parseType="Literal">&lt;p&gt;Many material properties are manifested in the morphological appearance and
characterized with microscopic image, such as scanning electron microscopy
(SEM). Polymer miscibility is a key physical quantity of polymer material and
commonly and intuitively judged by SEM images. However, human observation and
judgement for the images is time-consuming, labor-intensive and hard to be
quantified. Computer image recognition with machine learning method can make up
the defects of artificial judging, giving accurate and quantitative judgement.
We achieve automatic miscibility recognition utilizing convolution neural
network and transfer learning method, and the model obtains up to 94% accuracy.
We also put forward a quantitative criterion for polymer miscibility with this
model. The proposed method can be widely applied to the quantitative
characterization of the microstructure and properties of various materials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhi Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Ruixin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jinying Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Changshui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12678">
<title>Uni-Fusion: Universal Continuous Mapping. (arXiv:2303.12678v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12678</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Uni-Fusion, a universal continuous mapping framework for surfaces,
surface properties (color, infrared, etc.) and more (latent features in CLIP
embedding space, etc.). We propose the first universal implicit encoding model
that supports encoding of both geometry and different types of properties (RGB,
infrared, features, etc.) without requiring any training. Based on this, our
framework divides the point cloud into regular grid voxels and generates a
latent feature in each voxel to form a Latent Implicit Map (LIM) for geometries
and arbitrary properties. Then, by fusing a local LIM frame-wisely into a
global LIM, an incremental reconstruction is achieved. Encoded with
corresponding types of data, our Latent Implicit Map is capable of generating
continuous surfaces, surface property fields, surface feature fields, and all
other possible options. To demonstrate the capabilities of our model, we
implement three applications: (1) incremental reconstruction for surfaces and
color (2) 2D-to-3D transfer of fabricated properties (3) open-vocabulary scene
understanding by creating a text CLIP feature field on surfaces. We evaluate
Uni-Fusion by comparing it in corresponding applications, from which Uni-Fusion
shows high-flexibility in various applications while performing best or being
competitive. The project page of Uni-Fusion is available at
https://jarrome.github.io/Uni-Fusion/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yijun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuechter_A/0/1/0/all/0/1&quot;&gt;Andreas Nuechter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16739">
<title>Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimization. (arXiv:2303.16739v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16739</link>
<description rdf:parseType="Literal">&lt;p&gt;Actively planning sensor views during object reconstruction is crucial for
autonomous mobile robots. An effective method should be able to strike a
balance between accuracy and efficiency. In this paper, we propose a seamless
integration of the emerging implicit representation with the active
reconstruction task. We build an implicit occupancy field as our geometry
proxy. While training, the prior object bounding box is utilized as auxiliary
information to generate clean and detailed reconstructions. To evaluate view
uncertainty, we employ a sampling-based approach that directly extracts entropy
from the reconstructed occupancy probability field as our measure of view
information gain. This eliminates the need for additional uncertainty maps or
learning. Unlike previous methods that compare view uncertainty within a finite
set of candidates, we aim to find the next-best-view (NBV) on a continuous
manifold. Leveraging the differentiability of the implicit representation, the
NBV can be optimized directly by maximizing the view uncertainty using gradient
descent. It significantly enhances the method&apos;s adaptability to different
scenarios. Simulation and real-world experiments demonstrate that our approach
effectively improves reconstruction accuracy and efficiency of view planning in
active reconstruction tasks. The proposed system will open source at
https://github.com/HITSZ-NRSL/ActiveImplicitRecon.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1&quot;&gt;Dongyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_F/0/1/0/all/0/1&quot;&gt;Fengyu Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoyao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1&quot;&gt;Mengmeng Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09347">
<title>Dual Stage Stylization Modulation for Domain Generalized Semantic Segmentation. (arXiv:2304.09347v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09347</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining sufficient labeled data for training deep models is often
challenging in real-life applications. To address this issue, we propose a
novel solution for single-source domain generalized semantic segmentation.
Recent approaches have explored data diversity enhancement using hallucination
techniques. However, excessive hallucination can degrade performance,
particularly for imbalanced datasets. As shown in our experiments, minority
classes are more susceptible to performance reduction due to hallucination
compared to majority classes. To tackle this challenge, we introduce a
dual-stage Feature Transform (dFT) layer within the Adversarial Semantic
Hallucination+ (ASH+) framework. The ASH+ framework performs a dual-stage
manipulation of hallucination strength. By leveraging semantic information for
each pixel, our approach adaptively adjusts the pixel-wise hallucination
strength, thus providing fine-grained control over hallucination. We validate
the effectiveness of our proposed method through comprehensive experiments on
publicly available semantic segmentation benchmark datasets (Cityscapes and
SYNTHIA). Quantitative and qualitative comparisons demonstrate that our
approach is competitive with state-of-the-art methods for the Cityscapes
dataset and surpasses existing solutions for the SYNTHIA dataset. Code for our
framework will be made readily available to the research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1&quot;&gt;Gabriel Tjio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Ping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1&quot;&gt;Chee-Keong Kwoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00679">
<title>Enhanced Multi-level Features for Very High Resolution Remote Sensing Scene Classification. (arXiv:2305.00679v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00679</link>
<description rdf:parseType="Literal">&lt;p&gt;Very high-resolution (VHR) remote sensing (RS) scene classification is a
challenging task due to the higher inter-class similarity and intra-class
variability problems. Recently, the existing deep learning (DL)-based methods
have shown great promise in VHR RS scene classification. However, they still
provide an unstable classification performance. To address such a problem, we,
in this letter, propose a novel DL-based approach. For this, we devise an
enhanced VHR attention module (EAM), followed by the atrous spatial pyramid
pooling (ASPP) and global average pooling (GAP). This procedure imparts the
enhanced features from the corresponding level. Then, the multi-level feature
fusion is performed. Experimental results on two widely-used VHR RS datasets
show that the proposed approach yields a competitive and stable/robust
classification performance with the least standard deviation of 0.001. Further,
the highest overall accuracies on the AID and the NWPU datasets are 95.39% and
93.04%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitaula_C/0/1/0/all/0/1&quot;&gt;Chiranjibi Sitaula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+KC_S/0/1/0/all/0/1&quot;&gt;Sumesh KC&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_J/0/1/0/all/0/1&quot;&gt;Jagannath Aryal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable generalisation of neural network softmax
classification trained with cross-entropy loss, referred to as variational
classification (VC). Our approach offers a novel probabilistic perspective on
the highly familiar softmax classification model, to which it relates similarly
to how variational and traditional autoencoders relate. We derive a training
objective based on the evidence lower bound (ELBO) that is non-trivial to
optimize, and therefore propose an adversarial approach to maximise it. We show
that VC addresses an inherent inconsistency within softmax classification,
whilst also allowing more flexible choices of prior distributions in the latent
space in place of implicit assumptions revealed within off-the-shelf softmax
classifiers. Empirical evaluation on image and text classification datasets
demonstrates that variational classification maintains prediction accuracy
while improving other desirable properties such as calibration and adversarial
robustness, particularly under distribution shift and low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12726">
<title>Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach. (arXiv:2305.12726v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12726</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of in-the-wild videos has greatly expanded the Video
Quality Assessment (VQA) problem. Unlike early definitions that usually focus
on limited distortion types, VQA on in-the-wild videos is especially
challenging as it could be affected by complicated factors, including various
distortions and diverse contents. Though subjective studies have collected
overall quality scores for these videos, how the abstract quality scores relate
with specific factors is still obscure, hindering VQA methods from more
concrete quality evaluations (e.g. sharpness of a video). To solve this
problem, we collect over two million opinions on 4,543 in-the-wild videos on 13
dimensions of quality-related factors, including in-capture authentic
distortions (e.g. motion blur, noise, flicker), errors introduced by
compression and transmission, and higher-level experiences on semantic contents
and aesthetic issues (e.g. composition, camera trajectory), to establish the
multi-dimensional Maxwell database. Specifically, we ask the subjects to label
among a positive, a negative, and a neutral choice for each dimension. These
explanation-level opinions allow us to measure the relationships between
specific quality factors and abstract subjective quality ratings, and to
benchmark different categories of VQA algorithms on each dimension, so as to
more comprehensively analyze their strengths and weaknesses. Furthermore, we
propose the MaxVQA, a language-prompted VQA approach that modifies
vision-language foundation model CLIP to better capture important quality
issues as observed in our analyses. The MaxVQA can jointly evaluate various
specific quality factors and final quality scores with state-of-the-art
accuracy on all dimensions, and superb generalization ability on existing
datasets. Code and data available at https://github.com/VQAssessment/MaxVQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Erli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1&quot;&gt;Liang Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jingwen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Annan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenxiu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qiong Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weisi Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13501">
<title>LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. (arXiv:2305.13501v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13501</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapidly evolving fields of e-commerce and metaverse continue to seek
innovative approaches to enhance the consumer experience. At the same time,
recent advancements in the development of diffusion models have enabled
generative networks to create remarkably realistic images. In this context,
image-based virtual try-on, which consists in generating a novel image of a
target model wearing a given in-shop garment, has yet to capitalize on the
potential of these powerful generative solutions. This work introduces
LaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for the
Virtual Try-ON task. The proposed architecture relies on a latent diffusion
model extended with a novel additional autoencoder module that exploits
learnable skip connections to enhance the generation process preserving the
model&apos;s characteristics. To effectively maintain the texture and details of the
in-shop garment, we propose a textual inversion component that can map the
visual features of the garment to the CLIP token embedding space and thus
generate a set of pseudo-word token embeddings capable of conditioning the
generation process. Experimental results on Dress Code and VITON-HD datasets
demonstrate that our approach outperforms the competitors by a consistent
margin, achieving a significant milestone for the task. Source code and trained
models are publicly available at: https://github.com/miccunifi/ladi-vton.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morelli_D/0/1/0/all/0/1&quot;&gt;Davide Morelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldrati_A/0/1/0/all/0/1&quot;&gt;Alberto Baldrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartella_G/0/1/0/all/0/1&quot;&gt;Giuseppe Cartella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1&quot;&gt;Marcella Cornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14579">
<title>Real-Time Idling Vehicles Detection using Combined Audio-Visual Deep Learning. (arXiv:2305.14579v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14579</link>
<description rdf:parseType="Literal">&lt;p&gt;Combustion vehicle emissions contribute to poor air quality and release
greenhouse gases into the atmosphere, and vehicle pollution has been associated
with numerous adverse health effects. Roadways with extensive waiting and/or
passenger drop off, such as schools and hospital drop-off zones, can result in
high incidence and density of idling vehicles. This can produce micro-climates
of increased vehicle pollution. Thus, the detection of idling vehicles can be
helpful in monitoring and responding to unnecessary idling and be integrated
into real-time or off-line systems to address the resulting pollution. In this
paper we present a real-time, dynamic vehicle idling detection algorithm. The
proposed idle detection algorithm and notification rely on an algorithm to
detect these idling vehicles. The proposed method relies on a multi-sensor,
audio-visual, machine-learning workflow to detect idling vehicles visually
under three conditions: moving, static with the engine on, and static with the
engine off. The visual vehicle motion detector is built in the first stage, and
then a contrastive-learning-based latent space is trained for classifying
static vehicle engine sound. We test our system in real-time at a hospital
drop-off point in Salt Lake City. This in-situ dataset was collected and
annotated, and it includes vehicles of varying models and types. The
experiments show that the method can detect engine switching on or off
instantly and achieves 71.02 average precision (AP) for idle detections and
91.06 for engine off detections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangin_T/0/1/0/all/0/1&quot;&gt;Tristalee Mangin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Surojit Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanchard_E/0/1/0/all/0/1&quot;&gt;Evan Blanchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dillon Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poppe_H/0/1/0/all/0/1&quot;&gt;Henry Poppe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Searle_N/0/1/0/all/0/1&quot;&gt;Nathan Searle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_O/0/1/0/all/0/1&quot;&gt;Ouk Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_K/0/1/0/all/0/1&quot;&gt;Kerry Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitaker_R/0/1/0/all/0/1&quot;&gt;Ross Whitaker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18905">
<title>atTRACTive: Semi-automatic white matter tract segmentation using active learning. (arXiv:2305.18905v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18905</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately identifying white matter tracts in medical images is essential for
various applications, including surgery planning and tract-specific analysis.
Supervised machine learning models have reached state-of-the-art solving this
task automatically. However, these models are primarily trained on healthy
subjects and struggle with strong anatomical aberrations, e.g. caused by brain
tumors. This limitation makes them unsuitable for tasks such as preoperative
planning, wherefore time-consuming and challenging manual delineation of the
target tract is typically employed. We propose semi-automatic entropy-based
active learning for quick and intuitive segmentation of white matter tracts
from whole-brain tractography consisting of millions of streamlines. The method
is evaluated on 21 openly available healthy subjects from the Human Connectome
Project and an internal dataset of ten neurosurgical cases. With only a few
annotations, the proposed approach enables segmenting tracts on tumor cases
comparable to healthy subjects (dice=0.71), while the performance of automatic
methods, like TractSeg dropped substantially (dice=0.34) in comparison to
healthy subjects. The method is implemented as a prototype named atTRACTive in
the freely available software MITK Diffusion. Manual experiments on tumor data
showed higher efficiency due to lower segmentation times compared to
traditional ROI-based segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peretzke_R/0/1/0/all/0/1&quot;&gt;Robin Peretzke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bohn_J/0/1/0/all/0/1&quot;&gt;Jonas Bohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirchhoff_Y/0/1/0/all/0/1&quot;&gt;Yannick Kirchhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Saikat Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oberli_Palma_S/0/1/0/all/0/1&quot;&gt;Sabrina Oberli-Palma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Becker_D/0/1/0/all/0/1&quot;&gt;Daniela Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lenga_P/0/1/0/all/0/1&quot;&gt;Pavlina Lenga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Neher_P/0/1/0/all/0/1&quot;&gt;Peter Neher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05357">
<title>Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models. (arXiv:2306.05357v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05357</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06038">
<title>WindowNet: Learnable Windows for Chest X-ray Classification. (arXiv:2306.06038v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06038</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-ray (CXR) images are commonly compressed to a lower resolution and
bit depth to reduce their size, potentially altering subtle diagnostic
features.
&lt;/p&gt;
&lt;p&gt;Radiologists use windowing operations to enhance image contrast, but the
impact of such operations on CXR classification performance is unclear.
&lt;/p&gt;
&lt;p&gt;In this study, we show that windowing can improve CXR classification
performance, and propose WindowNet, a model that learns optimal window
settings.
&lt;/p&gt;
&lt;p&gt;We first investigate the impact of bit-depth on classification performance
and find that a higher bit-depth (12-bit) leads to improved performance.
&lt;/p&gt;
&lt;p&gt;We then evaluate different windowing settings and show that training with a
distinct window generally improves pathology-wise classification performance.
&lt;/p&gt;
&lt;p&gt;Finally, we propose and evaluate WindowNet, a model that learns optimal
window settings, and show that it significantly improves performance compared
to the baseline model without windowing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wollek_A/0/1/0/all/0/1&quot;&gt;Alessandro Wollek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hyska_S/0/1/0/all/0/1&quot;&gt;Sardi Hyska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sabel_B/0/1/0/all/0/1&quot;&gt;Bastian Sabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ingrisch_M/0/1/0/all/0/1&quot;&gt;Michael Ingrisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06051">
<title>Higher Chest X-ray Resolution Improves Classification Performance. (arXiv:2306.06051v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06051</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models for image classification are often trained at a
resolution of 224 x 224 pixels for historical and efficiency reasons. However,
chest X-rays are acquired at a much higher resolution to display subtle
pathologies. This study investigates the effect of training resolution on chest
X-ray classification performance, using the chest X-ray 14 dataset. The results
show that training with a higher image resolution, specifically 1024 x 1024
pixels, results in the best overall classification performance with a mean AUC
of 84.2 % compared to 82.7 % when trained with 256 x 256 pixel images.
Additionally, comparison of bounding boxes and GradCAM saliency maps suggest
that low resolutions, such as 256 x 256 pixels, are insufficient for
identifying small pathologies and force the model to use spurious
discriminating features. Our code is publicly available at
https://gitlab.lrz.de/IP/cxr-resolution
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wollek_A/0/1/0/all/0/1&quot;&gt;Alessandro Wollek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyska_S/0/1/0/all/0/1&quot;&gt;Sardi Hyska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabel_B/0/1/0/all/0/1&quot;&gt;Bastian Sabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingrisch_M/0/1/0/all/0/1&quot;&gt;Michael Ingrisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11363">
<title>Masked Diffusion Models Are Fast and Privacy-Aware Learners. (arXiv:2306.11363v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11363</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as the \emph{de-facto} technique for image
generation, yet they entail significant computational overhead, hindering the
technique&apos;s broader application in the research community. We propose a
prior-based denoising training framework, the first to incorporate the
pre-train and fine-tune paradigm into the diffusion model training process,
which substantially improves training efficiency and shows potential in
facilitating various downstream tasks. Our approach centers on masking a high
proportion (e.g., up to 90\%) of the input image and employing masked denoising
score matching to denoise the visible areas, thereby guiding the diffusion
model to learn more salient features from training data as prior knowledge. By
utilizing masked learning in a pre-training stage, we efficiently train the
ViT-based diffusion model on CelebA-HQ $256 \times 256$ in the pixel space,
achieving a 4x acceleration and enhancing the quality of generated images
compared to denoising diffusion probabilistic model (DDPM). Moreover, our
masked pre-training technique can be universally applied to various diffusion
models that directly generate images in the pixel space, aiding in the learning
of pre-trained models with superior generalizability. For instance, a diffusion
model pre-trained on VGGFace2 attains a 46\% quality improvement through
fine-tuning with merely 10\% data from a different distribution. Moreover, our
method shows the potential to serve as a training paradigm for enhancing the
privacy protection capabilities of diffusion models. Our code is available at
\url{https://github.com/jiachenlei/maskdm}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1&quot;&gt;Jiachen Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Peng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Z/0/1/0/all/0/1&quot;&gt;Zhongjie Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kui Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04577">
<title>AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System. (arXiv:2307.04577v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04577</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-based teleoperation offers the possibility to endow robots with
human-level intelligence to physically interact with the environment, while
only requiring low-cost camera sensors. However, current vision-based
teleoperation systems are designed and engineered towards a particular robot
model and deploy environment, which scales poorly as the pool of the robot
models expands and the variety of the operating environment increases. In this
paper, we propose AnyTeleop, a unified and general teleoperation system to
support multiple different arms, hands, realities, and camera configurations
within a single system. Although being designed to provide great flexibility to
the choice of simulators and real hardware, our system can still achieve great
performance. For real-world experiments, AnyTeleop can outperform a previous
system that was designed for a specific robot hardware with a higher success
rate, using the same robot. For teleoperation in simulation, AnyTeleop leads to
better imitation learning performance, compared with a previous system that is
particularly designed for that simulator. Project page: &lt;a href=&quot;http://anyteleop.com/.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Binghao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1&quot;&gt;Karl Van Wyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1&quot;&gt;Dieter Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07928">
<title>Reinforced Disentanglement for Face Swapping without Skip Connection. (arXiv:2307.07928v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;The SOTA face swap models still suffer the problem of either target identity
(i.e., shape) being leaked or the target non-identity attributes (i.e.,
background, hair) failing to be fully preserved in the final results. We show
that this insufficient disentanglement is caused by two flawed designs that
were commonly adopted in prior models: (1) counting on only one compressed
encoder to represent both the semantic-level non-identity facial
attributes(i.e., pose) and the pixel-level non-facial region details, which is
contradictory to satisfy at the same time; (2) highly relying on long
skip-connections between the encoder and the final generator, leaking a certain
amount of target face identity into the result. To fix them, we introduce a new
face swap framework called &apos;WSC-swap&apos; that gets rid of skip connections and
uses two target encoders to respectively capture the pixel-level non-facial
region attributes and the semantic non-identity attributes in the face region.
To further reinforce the disentanglement learning for the target encoder, we
employ both identity removal loss via adversarial training (i.e., GAN) and the
non-identity preservation loss via prior 3DMM models like [11]. Extensive
experiments on both FaceForensics++ and CelebA-HQ show that our results
significantly outperform previous works on a rich set of metrics, including one
novel metric for measuring identity consistency that was completely neglected
before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaohang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1&quot;&gt;Pengfei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Heung-Yeung Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10123">
<title>Two Approaches to Supervised Image Segmentation. (arXiv:2307.10123v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;Though performed almost effortlessly by humans, segmenting 2D gray-scale or
color images in terms of regions of interest (e.g.~background, objects, or
portions of objects) constitutes one of the greatest challenges in science and
technology as a consequence of the involved dimensionality reduction(3D to 2D),
noise, reflections, shades, and occlusions, among many other possible effects.
While a large number of interesting related approaches have been suggested
along the last decades, it was mainly thanks to the recent development of deep
learning that more effective and general solutions have been obtained,
currently constituting the basic comparison reference for this type of
operation. Also developed recently, a multiset-based methodology has been
described that is capable of encouraging image segmentation performance while
combining spatial accuracy, stability, and robustness while requiring little
computational resources (hardware and/or training and recognition time). The
interesting features of the multiset neurons methodology mostly follow from the
enhanced selectivity and sensitivity, as well as good robustness to data
perturbations and outliers, allowed by the coincidence similarity index on
which the multiset approach to supervised image segmentation is based. After
describing the deep learning and multiset neurons approaches, the present work
develops two comparison experiments between them which are primarily aimed at
illustrating their respective main interesting features when applied to the
adopted specific type of data and parameter configurations. While the deep
learning approach confirmed its potential for performing image segmentation,
the alternative multiset methodology allowed for enhanced accuracy while
requiring little computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benatti_A/0/1/0/all/0/1&quot;&gt;Alexandre Benatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1&quot;&gt;Luciano da F. Costa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11067">
<title>CNOS: A Strong Baseline for CAD-based Novel Object Segmentation. (arXiv:2307.11067v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11067</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a simple three-stage approach to segment unseen objects in RGB
images using their CAD models. Leveraging recent powerful foundation models,
DINOv2 and Segment Anything, we create descriptors and generate proposals,
including binary masks for a given input RGB image. By matching proposals with
reference descriptors created from CAD models, we achieve precise object ID
assignment along with modal masks. We experimentally demonstrate that our
method achieves state-of-the-art results in CAD-based novel object
segmentation, surpassing existing approaches on the seven core datasets of the
BOP challenge by 19.8% AP using the same BOP evaluation protocol. Our source
code is available at https://github.com/nv-nguyen/cnos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van Nguyen Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodan_T/0/1/0/all/0/1&quot;&gt;Tomas Hodan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ponimatkin_G/0/1/0/all/0/1&quot;&gt;Georgy Ponimatkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1&quot;&gt;Thibault Groueix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1&quot;&gt;Vincent Lepetit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13539">
<title>Model Calibration in Dense Classification with Adaptive Label Perturbation. (arXiv:2307.13539v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13539</link>
<description rdf:parseType="Literal">&lt;p&gt;For safety-related applications, it is crucial to produce trustworthy deep
neural networks whose prediction is associated with confidence that can
represent the likelihood of correctness for subsequent decision-making.
Existing dense binary classification models are prone to being over-confident.
To improve model calibration, we propose Adaptive Stochastic Label Perturbation
(ASLP) which learns a unique label perturbation level for each training image.
ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,
which unifies label perturbation processes including stochastic approaches
(like DisturbLabel), and label smoothing, to correct calibration while
maintaining classification rates. ASLP follows Maximum Entropy Inference of
classic statistical mechanics to maximise prediction entropy with respect to
missing information. It performs this while: (1) preserving classification
accuracy on known data as a conservative solution, or (2) specifically improves
model calibration degree by minimising the gap between the prediction accuracy
and expected confidence of the target training label. Extensive results
demonstrate that ASLP can significantly improve calibration degrees of dense
binary classification models on both in-distribution and out-of-distribution
data. The code is available on https://github.com/Carlisle-Liu/ASLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Changkun Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1&quot;&gt;Ruikai Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14073">
<title>VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet. (arXiv:2307.14073v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14073</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion models like StableDiffusion have achieved impressive
image generation results. However, the generation process of such diffusion
models is uncontrollable, which makes it hard to generate videos with
continuous and consistent content. In this work, by using the diffusion model
with ControlNet, we proposed a new motion-guided video-to-video translation
framework called VideoControlNet to generate various videos based on the given
prompts and the condition from the input video. Inspired by the video codecs
that use motion information for reducing temporal redundancy, our framework
uses motion information to prevent the regeneration of the redundant areas for
content consistency. Specifically, we generate the first frame (i.e., the
I-frame) by using the diffusion model with ControlNet. Then we generate other
key frames (i.e., the P-frame) based on the previous I/P-frame by using our
newly proposed motion-guided P-frame generation (MgPG) method, in which the
P-frames are generated based on the motion information and the occlusion areas
are inpainted by using the diffusion model. Finally, the rest frames (i.e., the
B-frame) are generated by using our motion-guided B-frame interpolation (MgBI)
module. Our experiments demonstrate that our proposed VideoControlNet inherits
the generation capability of the pre-trained large diffusion model and extends
the image diffusion model to the video diffusion model by using motion
information. More results are provided at our project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14701">
<title>MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images. (arXiv:2307.14701v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14701</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Out-of-Distribution (OOD) detection consists in identifying
anomalous regions in images leveraging only models trained on images of healthy
anatomy. An established approach is to tokenize images and model the
distribution of tokens with Auto-Regressive (AR) models. AR models are used to
1) identify anomalous tokens and 2) in-paint anomalous representations with
in-distribution tokens. However, AR models are slow at inference time and prone
to error accumulation issues which negatively affect OOD detection performance.
Our novel method, MIM-OOD, overcomes both speed and error accumulation issues
by replacing the AR model with two task-specific networks: 1) a transformer
optimized to identify anomalous tokens and 2) a transformer optimized to
in-paint anomalous tokens using masked image modelling (MIM). Our experiments
with brain MRI anomalies show that MIM-OOD substantially outperforms AR models
(DICE 0.458 vs 0.301) while achieving a nearly 25x speedup (9.5s vs 244s).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marimont_S/0/1/0/all/0/1&quot;&gt;Sergio Naval Marimont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siomos_V/0/1/0/all/0/1&quot;&gt;Vasilis Siomos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarroni_G/0/1/0/all/0/1&quot;&gt;Giacomo Tarroni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15989">
<title>Freespace Optical Flow Modeling for Automated Driving. (arXiv:2307.15989v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15989</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical flow and disparity are two informative visual features for autonomous
driving perception. They have been used for a variety of applications, such as
obstacle and lane detection. The concept of &quot;U-V-Disparity&quot; has been widely
explored in the literature, while its counterpart in optical flow has received
relatively little attention. Traditional motion analysis algorithms estimate
optical flow by matching correspondences between two successive video frames,
which limits the full utilization of environmental information and geometric
constraints. Therefore, we propose a novel strategy to model optical flow in
the collision-free space (also referred to as drivable area or simply
freespace) for intelligent vehicles, with the full utilization of geometry
information in a 3D driving environment. We provide explicit representations of
optical flow and deduce the quadratic relationship between the optical flow
component and the vertical coordinate. Through extensive experiments on several
public datasets, we demonstrate the high accuracy and robustness of our model.
Additionally, our proposed freespace optical flow model boasts a diverse array
of applications within the realm of automated driving, providing a geometric
constraint in freespace detection, vehicle localization, and more. We have made
our source code publicly available at https://mias.group/FSOF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jiayuan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rui Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16680">
<title>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey. (arXiv:2307.16680v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16680</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models and large language models have emerged as leading-edge
generative models and have sparked a revolutionary impact on various aspects of
human life. However, the practical implementation of these models has also
exposed inherent risks, highlighting their dual nature and raising concerns
regarding their trustworthiness. Despite the abundance of literature on this
subject, a comprehensive survey specifically delving into the intersection of
large-scale generative models and their trustworthiness remains largely absent.
To bridge this gap, This paper investigates both the long-standing and emerging
threats associated with these models across four fundamental dimensions:
privacy, security, fairness, and responsibility. In this way, we construct an
extensive map outlining the trustworthiness of these models, while also
providing practical recommendations and identifying future directions. These
efforts are crucial for promoting the trustworthy deployment of these models,
ultimately benefiting society as a whole.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00471">
<title>A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography. (arXiv:2308.00471v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00471</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographic
imaging technique that first needs intravenously administration of an iodinated
contrast medium; then, it collects both a low-energy image, comparable to
standard mammography, and a high-energy image. The two scans are then combined
to get a recombined image showing contrast enhancement. Despite CESM diagnostic
advantages for breast cancer diagnosis, the use of contrast medium can cause
side effects, and CESM also beams patients with a higher radiation dose
compared to standard mammography. To address these limitations this work
proposes to use deep generative models for virtual contrast enhancement on
CESM, aiming to make the CESM contrast-free as well as to reduce the radiation
dose. Our deep networks, consisting of an autoencoder and two Generative
Adversarial Networks, the Pix2Pix, and the CycleGAN, generate synthetic
recombined images solely from low-energy images. We perform an extensive
quantitative and qualitative analysis of the model&apos;s performance, also
exploiting radiologists&apos; assessments, on a novel CESM dataset that includes
1138 images that, as a further contribution of this work, we make publicly
available. The results show that CycleGAN is the most promising deep network to
generate synthetic recombined images, highlighting the potential of artificial
intelligence techniques for virtual contrast enhancement in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rofena_A/0/1/0/all/0/1&quot;&gt;Aurora Rofena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guarrasi_V/0/1/0/all/0/1&quot;&gt;Valerio Guarrasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sarli_M/0/1/0/all/0/1&quot;&gt;Marina Sarli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piccolo_C/0/1/0/all/0/1&quot;&gt;Claudia Lucia Piccolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sammarra_M/0/1/0/all/0/1&quot;&gt;Matteo Sammarra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zobel_B/0/1/0/all/0/1&quot;&gt;Bruno Beomonte Zobel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soda_P/0/1/0/all/0/1&quot;&gt;Paolo Soda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00692">
<title>LISA: Reasoning Segmentation via Large Language Model. (arXiv:2308.00692v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00692</link>
<description rdf:parseType="Literal">&lt;p&gt;Although perception systems have made remarkable advancements in recent
years, they still rely on explicit human instruction to identify the target
objects or categories before executing visual recognition tasks. Such systems
lack the ability to actively reason and comprehend implicit user intentions. In
this work, we propose a new segmentation task -- reasoning segmentation. The
task is designed to output a segmentation mask given a complex and implicit
query text. Furthermore, we establish a benchmark comprising over one thousand
image-instruction pairs, incorporating intricate reasoning and world knowledge
for evaluation purposes. Finally, we present LISA: large Language Instructed
Segmentation Assistant, which inherits the language generation capabilities of
the multi-modal Large Language Model (LLM) while also possessing the ability to
produce segmentation masks. We expand the original vocabulary with a &amp;lt;SEG&amp;gt;
token and propose the embedding-as-mask paradigm to unlock the segmentation
capability. Remarkably, LISA can handle cases involving: 1) complex reasoning;
2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,
it demonstrates robust zero-shot capability when trained exclusively on
reasoning-free datasets. In addition, fine-tuning the model with merely 239
reasoning segmentation image-instruction pairs results in further performance
enhancement. Experiments show our method not only unlocks new reasoning
segmentation capabilities but also proves effective in both complex reasoning
segmentation and standard referring segmentation tasks. Code, models, and demo
are at https://github.com/dvlab-research/LISA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1&quot;&gt;Xin Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01006">
<title>FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving. (arXiv:2308.01006v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01006</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a multi-modality multi-task neural network toward accurate and
robust performance is a de-facto standard in perception task of autonomous
driving. However, leveraging such data from multiple sensors to jointly
optimize the prediction and planning tasks remains largely unexplored. In this
paper, we present FusionAD, to the best of our knowledge, the first unified
framework that fuse the information from two most critical sensors, camera and
LiDAR, goes beyond perception task. Concretely, we first build a transformer
based multi-modality fusion network to effectively produce fusion based
features. In constrast to camera-based end-to-end method UniAD, we then
establish a fusion aided modality-aware prediction and status-aware planning
modules, dubbed FMSPnP that take advantages of multi-modality features. We
conduct extensive experiments on commonly used benchmark nuScenes dataset, our
FusionAD achieves state-of-the-art performance and surpassing baselines on
average 15% on perception tasks like detection and tracking, 10% on occupancy
prediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE score
and reduces the collision rate from 0.31% to only 0.12%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tengju Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_W/0/1/0/all/0/1&quot;&gt;Wei Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chunyong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shikun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lingping Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Ke Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wencong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weibo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kaicheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01239">
<title>CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion. (arXiv:2308.01239v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01239</link>
<description rdf:parseType="Literal">&lt;p&gt;The U-shaped architecture has emerged as a crucial paradigm in the design of
medical image segmentation networks. However, due to the inherent local
limitations of convolution, a fully convolutional segmentation network with
U-shaped architecture struggles to effectively extract global context
information, which is vital for the precise localization of lesions. While
hybrid architectures combining CNNs and Transformers can address these issues,
their application in real medical scenarios is limited due to the computational
resource constraints imposed by the environment and edge devices. In addition,
the convolutional inductive bias in lightweight networks adeptly fits the
scarce medical data, which is lacking in the Transformer based network. In
order to extract global context information while taking advantage of the
inductive bias, we propose CMUNeXt, an efficient fully convolutional
lightweight medical image segmentation network, which enables fast and accurate
auxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel and
inverted bottleneck design to thoroughly mix distant spatial and location
information, efficiently extracting global context information. We also
introduce the Skip-Fusion block, designed to enable smooth skip-connections and
ensure ample feature fusion. Experimental results on multiple medical image
datasets demonstrate that CMUNeXt outperforms existing heavyweight and
lightweight medical image segmentation networks in terms of segmentation
performance, while offering a faster inference speed, lighter weights, and a
reduced computational cost. The code is available at
https://github.com/FengheTan9/CMUNeXt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fenghe Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jianrui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingtao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_C/0/1/0/all/0/1&quot;&gt;Chunping Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>