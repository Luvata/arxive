<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01573" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.10629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03449" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01240" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.01463">
<title>Creating Trustworthy LLMs: Dealing with Hallucinations in Healthcare AI. (arXiv:2311.01463v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01463</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have proliferated across multiple domains in as short
period of time. There is however hesitation in the medical and healthcare
domain towards their adoption because of issues like factuality, coherence, and
hallucinations. Give the high stakes nature of healthcare, many researchers
have even cautioned against its usage until these issues are resolved. The key
to the implementation and deployment of LLMs in healthcare is to make these
models trustworthy, transparent (as much possible) and explainable. In this
paper we describe the key elements in creating reliable, trustworthy, and
unbiased models as a necessary condition for their adoption in healthcare.
Specifically we focus on the quantification, validation, and mitigation of
hallucinations in the context in healthcare. Lastly, we discuss how the future
of LLMs in healthcare may look like.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_M/0/1/0/all/0/1&quot;&gt;Muhammad Aurangzeb Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaramis_I/0/1/0/all/0/1&quot;&gt;Ilker Yaramis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Taposh Dutta Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01473">
<title>Adversarial Examples in the Physical World: A Survey. (arXiv:2311.01473v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have demonstrated high vulnerability to
adversarial examples. Besides the attacks in the digital world, the practical
implications of adversarial examples in the physical world present significant
challenges and safety concerns. However, current research on physical
adversarial examples (PAEs) lacks a comprehensive understanding of their unique
characteristics, leading to limited significance and understanding. In this
paper, we address this gap by thoroughly examining the characteristics of PAEs
within a practical workflow encompassing training, manufacturing, and
re-sampling processes. By analyzing the links between physical adversarial
attacks, we identify manufacturing and re-sampling as the primary sources of
distinct attributes and particularities in PAEs. Leveraging this knowledge, we
develop a comprehensive analysis and classification framework for PAEs based on
their specific characteristics, covering over 100 studies on physical-world
adversarial examples. Furthermore, we investigate defense strategies against
PAEs and identify open challenges and opportunities for future research. We aim
to provide a fresh, thorough, and systematic understanding of PAEs, thereby
promoting the development of robust adversarial learning and its application in
open-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiakai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donghua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01475">
<title>Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts. (arXiv:2311.01475v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01475</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised image segmentation aims at grouping different semantic patterns
in an image without the use of human annotation. Similarly, image clustering
searches for groupings of images based on their semantic content without
supervision. Classically, both problems have captivated researchers as they
drew from sound mathematical concepts to produce concrete applications. With
the emergence of deep learning, the scientific community turned its attention
to complex neural network-based solvers that achieved impressive results in
those domains but rarely leveraged the advances made by classical methods. In
this work, we propose a patch-based unsupervised image segmentation strategy
that bridges advances in unsupervised feature extraction from deep clustering
methods with the algorithmic help of classical graph-based methods. We show
that a simple convolutional neural network, trained to classify image patches
and iteratively regularized using graph cuts, naturally leads to a
state-of-the-art fully-convolutional unsupervised pixel-level segmenter.
Furthermore, we demonstrate that this is the ideal setting for leveraging the
patch-level pairwise features generated by vision transformer models. Our
results on real image data demonstrate the effectiveness of our proposed
methodology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasserman_I/0/1/0/all/0/1&quot;&gt;Isaac Wasserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_J/0/1/0/all/0/1&quot;&gt;Jeova Farias Sales Rocha Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01477">
<title>FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models. (arXiv:2311.01477v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01477</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FAITHSCORE (Faithfulness to Atomic Image Facts Score), a
reference-free and fine-grained evaluation metric that measures the
faithfulness of the generated free-form answers from large vision-language
models (LVLMs). The FAITHSCORE evaluation first identifies sub-sentences
containing descriptive statements that need to be verified, then extracts a
comprehensive list of atomic facts from these sub-sentences, and finally
conducts consistency verification between fine-grained atomic facts and the
input image. Meta-evaluation demonstrates that our metric highly correlates
with human judgments of faithfulness. We collect two benchmark datasets (i.e.
LLaVA-1k and MSCOCO-Cap) for evaluating LVLMs instruction-following
hallucinations. We measure hallucinations in state-of-the-art LVLMs with
FAITHSCORE on the datasets. Results reveal that current systems are prone to
generate hallucinated content unfaithful to the image, which leaves room for
future improvements. Further, we find that current LVLMs despite doing well on
color and counting, still struggle with long answers, relations, and multiple
objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Liqiang Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruosen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunmo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1&quot;&gt;Mengzhao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinya Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01478">
<title>Adversary ML Resilience in Autonomous Driving Through Human Centered Perception Mechanisms. (arXiv:2311.01478v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01478</link>
<description rdf:parseType="Literal">&lt;p&gt;Physical adversarial attacks on road signs are continuously exploiting
vulnerabilities in modern day autonomous vehicles (AVs) and impeding their
ability to correctly classify what type of road sign they encounter. Current
models cannot generalize input data well, resulting in overfitting or
underfitting. In overfitting, the model memorizes the input data but cannot
generalize to new scenarios. In underfitting, the model does not learn enough
of the input data to accurately classify these road signs. This paper explores
the resilience of autonomous driving systems against three main physical
adversarial attacks (tape, graffiti, illumination), specifically targeting
object classifiers. Several machine learning models were developed and
evaluated on two distinct datasets: road signs (stop signs, speed limit signs,
traffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons,
circles, squares, and triangles). The study compared algorithm performance
under different conditions, including clean and adversarial training and
testing on these datasets. To build robustness against attacks, defense
techniques like adversarial training and transfer learning were implemented.
Results demonstrated transfer learning models played a crucial role in
performance by allowing knowledge gained from shape training to improve
generalizability of road sign classification, despite the datasets being
completely different. The paper suggests future research directions, including
human-in-the-loop validation, security analysis, real-world testing, and
explainable AI for transparency. This study aims to contribute to improving
security and robustness of object classifiers in autonomous vehicles and
mitigating adversarial example impacts on driving systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Aakriti Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01487">
<title>What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning. (arXiv:2311.01487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01487</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual instruction tuning is an essential approach to improving the zero-shot
generalization capability of Multi-modal Large Language Models (MLLMs). A surge
of visual instruction datasets with various focuses and characteristics have
been proposed recently, enabling MLLMs to achieve surprising results on
evaluation benchmarks. To develop more capable MLLMs, in this paper, we aim to
investigate a more fundamental question: ``what makes for good visual
instructions?&apos;&apos;. By conducting a comprehensive empirical study, we find that
instructions focused on complex visual reasoning tasks are particularly
effective in improving the performance of MLLMs on evaluation benchmarks.
Building upon this finding, we design a systematic approach to automatically
creating high-quality complex visual reasoning instructions. Our approach
employs a synthesis-complication-reformulation paradigm, leveraging multiple
stages to gradually increase the complexity of the instructions while
guaranteeing quality. Based on this approach, we create the synthetic visual
reasoning instruction dataset consisting of 32K examples, namely ComVint, and
fine-tune four MLLMs on it. Experimental results demonstrate that our dataset
consistently enhances the performance of all the compared MLLMs, e.g.,
improving the performance of MiniGPT-4 and BLIP-2 on MME-Cognition by 32.6% and
28.8%, respectively. Our code and data are publicly available at the link:
https://github.com/RUCAIBox/ComVint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yifan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hangyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1&quot;&gt;Mingchen Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Ruihua Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01520">
<title>4D-Former: Multimodal 4D Panoptic Segmentation. (arXiv:2311.01520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;4D panoptic segmentation is a challenging but practically useful task that
requires every point in a LiDAR point-cloud sequence to be assigned a semantic
class label, and individual objects to be segmented and tracked over time.
Existing approaches utilize only LiDAR inputs which convey limited information
in regions with point sparsity. This problem can, however, be mitigated by
utilizing RGB camera images which offer appearance-based information that can
reinforce the geometry-based LiDAR features. Motivated by this, we propose
4D-Former: a novel method for 4D panoptic segmentation which leverages both
LiDAR and image modalities, and predicts semantic masks as well as temporally
consistent object masks for the input point-cloud sequence. We encode semantic
classes and objects using a set of concise queries which absorb feature
information from both data modalities. Additionally, we propose a learned
mechanism to associate object tracks over time which reasons over both
appearance and spatial location. We apply 4D-Former to the nuScenes and
SemanticKITTI datasets where it achieves state-of-the-art results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athar_A/0/1/0/all/0/1&quot;&gt;Ali Athar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;Enxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01556">
<title>MemorySeg: Online LiDAR Semantic Segmentation with a Latent Memory. (arXiv:2311.01556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01556</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation of LiDAR point clouds has been widely studied in recent
years, with most existing methods focusing on tackling this task using a single
scan of the environment. However, leveraging the temporal stream of
observations can provide very rich contextual information on regions of the
scene with poor visibility (e.g., occlusions) or sparse observations (e.g., at
long range), and can help reduce redundant computation frame after frame. In
this paper, we tackle the challenge of exploiting the information from the past
frames to improve the predictions of the current frame in an online fashion. To
address this challenge, we propose a novel framework for semantic segmentation
of a temporal sequence of LiDAR point clouds that utilizes a memory network to
store, update and retrieve past information. Our framework also includes a
regularizer that penalizes prediction variations in the neighborhood of the
point cloud. Prior works have attempted to incorporate memory in range view
representations for semantic segmentation, but these methods fail to handle
occlusions and the range view representation of the scene changes drastically
as agents nearby move. Our proposed framework overcomes these limitations by
building a sparse 3D latent representation of the surroundings. We evaluate our
method on SemanticKITTI, nuScenes, and PandaSet. Our experiments demonstrate
the effectiveness of the proposed framework compared to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1&quot;&gt;Enxu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01563">
<title>Assist Is Just as Important as the Goal: Image Resurfacing to Aid Model&apos;s Robust Prediction. (arXiv:2311.01563v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01563</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial patches threaten visual AI models in the real world. The number
of patches in a patch attack is variable and determines the attack&apos;s potency in
a specific environment. Most existing defenses assume a single patch in the
scene, and the multiple patch scenarios are shown to overcome them. This paper
presents a model-agnostic defense against patch attacks based on total
variation for image resurfacing (TVR). The TVR is an image-cleansing method
that processes images to remove probable adversarial regions. TVR can be
utilized solely or augmented with a defended model, providing multi-level
security for robust prediction. TVR nullifies the influence of patches in a
single image scan with no prior assumption on the number of patches in the
scene. We validate TVR on the ImageNet-Patch benchmark dataset and with
real-world physical objects, demonstrating its ability to mitigate patch
attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Abhijith Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munz_P/0/1/0/all/0/1&quot;&gt;Phil Munz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayan_A/0/1/0/all/0/1&quot;&gt;Apurva Narayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01567">
<title>Exploring the Hyperparameter Space of Image Diffusion Models for Echocardiogram Generation. (arXiv:2311.01567v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01567</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an extensive hyperparameter search on Image Diffusion
Models for Echocardiogram generation. The objective is to establish
foundational benchmarks and provide guidelines within the realm of ultrasound
image and video generation. This study builds over the latest advancements,
including cutting-edge model architectures and training methodologies. We also
examine the distribution shift between real and generated samples and consider
potential solutions, crucial to train efficient models on generated data. We
determine an Optimal FID score of $0.88$ for our research problem and achieve
an FID of $2.60$. This work is aimed at contributing valuable insights and
serving as a reference for further developments in the specialized field of
ultrasound image and video generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1&quot;&gt;Hadrien Reynaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1&quot;&gt;Bernhard Kainz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01570">
<title>Sequential Subset Matching for Dataset Distillation. (arXiv:2311.01570v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01570</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset distillation is a newly emerging task that synthesizes a small-size
dataset used in training deep neural networks (DNNs) for reducing data storage
and model training costs. The synthetic datasets are expected to capture the
essence of the knowledge contained in real-world datasets such that the former
yields a similar performance as the latter. Recent advancements in distillation
methods have produced notable improvements in generating synthetic datasets.
However, current state-of-the-art methods treat the entire synthetic dataset as
a unified entity and optimize each synthetic instance equally. This static
optimization approach may lead to performance degradation in dataset
distillation. Specifically, we argue that static optimization can give rise to
a coupling issue within the synthetic data, particularly when a larger amount
of synthetic data is being optimized. This coupling issue, in turn, leads to
the failure of the distilled dataset to extract the high-level features learned
by the deep neural network (DNN) in the latter epochs.
&lt;/p&gt;
&lt;p&gt;In this study, we propose a new dataset distillation strategy called
Sequential Subset Matching (SeqMatch), which tackles this problem by adaptively
optimizing the synthetic data to encourage sequential acquisition of knowledge
during dataset distillation. Our analysis indicates that SeqMatch effectively
addresses the coupling issue by sequentially generating the synthetic
instances, thereby enhancing its performance significantly. Our proposed
SeqMatch outperforms state-of-the-art methods in various datasets, including
SVNH, CIFAR-10, CIFAR-100, and Tiny ImageNet. Our code is available at
https://github.com/shqii1j/seqmatch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jiawei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1&quot;&gt;Qin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01573">
<title>Improving Fairness using Vision-Language Driven Image Augmentation. (arXiv:2311.01573v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01573</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness is crucial when training a deep-learning discriminative model,
especially in the facial domain. Models tend to correlate specific
characteristics (such as age and skin color) with unrelated attributes
(downstream tasks), resulting in biases which do not correspond to reality. It
is common knowledge that these correlations are present in the data and are
then transferred to the models during training. This paper proposes a method to
mitigate these correlations to improve fairness. To do so, we learn
interpretable and meaningful paths lying in the semantic space of a pre-trained
diffusion model (DiffAE) -- such paths being supervised by contrastive text
dipoles. That is, we learn to edit protected characteristics (age and skin
color). These paths are then applied to augment images to improve the fairness
of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on
several downstream tasks with age and skin color as protected characteristics.
As a proxy for fairness, we compute the difference in accuracy with respect to
the protected characteristics. Quantitative results show how the augmented
images help the model improve the overall accuracy, the aforementioned metric,
and the disparity of equal opportunity. Code is available at:
https://github.com/Moreno98/Vision-Language-Bias-Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DInca_M/0/1/0/all/0/1&quot;&gt;Moreno D&amp;#x27;Inc&amp;#xe0;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1&quot;&gt;Christos Tzelepis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1&quot;&gt;Ioannis Patras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01574">
<title>Improving Lesion Segmentation in FDG-18 Whole-Body PET/CT scans using Multilabel approach: AutoPET II challenge. (arXiv:2311.01574v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01574</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic segmentation of lesions in FDG-18 Whole Body (WB) PET/CT scans
using deep learning models is instrumental for determining treatment response,
optimizing dosimetry, and advancing theranostic applications in oncology.
However, the presence of organs with elevated radiotracer uptake, such as the
liver, spleen, brain, and bladder, often leads to challenges, as these regions
are often misidentified as lesions by deep learning models. To address this
issue, we propose a novel approach of segmenting both organs and lesions,
aiming to enhance the performance of automatic lesion segmentation methods. In
this study, we assessed the effectiveness of our proposed method using the
AutoPET II challenge dataset, which comprises 1014 subjects. We evaluated the
impact of inclusion of additional labels and data in the segmentation
performance of the model. In addition to the expert-annotated lesion labels, we
introduced eight additional labels for organs, including the liver, kidneys,
urinary bladder, spleen, lung, brain, heart, and stomach. These labels were
integrated into the dataset, and a 3D UNET model was trained within the nnUNet
framework. Our results demonstrate that our method achieved the top ranking in
the held-out test dataset, underscoring the potential of this approach to
significantly improve lesion segmentation accuracy in FDG-18 Whole-Body PET/CT
scans, ultimately benefiting cancer patients and advancing clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Murugesan_G/0/1/0/all/0/1&quot;&gt;Gowtham Krishnan Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McCrumb_D/0/1/0/all/0/1&quot;&gt;Diana McCrumb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brunner_E/0/1/0/all/0/1&quot;&gt;Eric Brunner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_J/0/1/0/all/0/1&quot;&gt;Jithendra Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soni_R/0/1/0/all/0/1&quot;&gt;Rahul Soni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Grigorash_V/0/1/0/all/0/1&quot;&gt;Vasily Grigorash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moore_S/0/1/0/all/0/1&quot;&gt;Stephen Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oss_J/0/1/0/all/0/1&quot;&gt;Jeff Van Oss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01617">
<title>Look-Ahead Selective Plasticity for Continual Learning of Visual Tasks. (arXiv:2311.01617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01617</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive representation learning has emerged as a promising technique for
continual learning as it can learn representations that are robust to
catastrophic forgetting and generalize well to unseen future tasks. Previous
work in continual learning has addressed forgetting by using previous task data
and trained models. Inspired by event models created and updated in the brain,
we propose a new mechanism that takes place during task boundaries, i.e., when
one task finishes and another starts. By observing the redundancy-inducing
ability of contrastive loss on the output of a neural network, our method
leverages the first few samples of the new task to identify and retain
parameters contributing most to the transfer ability of the neural network,
freeing up the remaining parts of the network to learn new features. We
evaluate the proposed methods on benchmark computer vision datasets including
CIFAR10 and TinyImagenet and demonstrate state-of-the-art performance in the
task-incremental, class-incremental, and domain-incremental continual learning
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meshkinnejad_R/0/1/0/all/0/1&quot;&gt;Rouzbeh Meshkinnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lizotte_D/0/1/0/all/0/1&quot;&gt;Daniel Lizotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohsenzadeh_Y/0/1/0/all/0/1&quot;&gt;Yalda Mohsenzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01619">
<title>InsPLAD: A Dataset and Benchmark for Power Line Asset Inspection in UAV Images. (arXiv:2311.01619v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01619</link>
<description rdf:parseType="Literal">&lt;p&gt;Power line maintenance and inspection are essential to avoid power supply
interruptions, reducing its high social and financial impacts yearly.
Automating power line visual inspections remains a relevant open problem for
the industry due to the lack of public real-world datasets of power line
components and their various defects to foster new research. This paper
introduces InsPLAD, a Power Line Asset Inspection Dataset and Benchmark
containing 10,607 high-resolution Unmanned Aerial Vehicles colour images. The
dataset contains seventeen unique power line assets captured from real-world
operating power lines. Additionally, five of those assets present six defects:
four of which are corrosion, one is a broken component, and one is a bird&apos;s
nest presence. All assets were labelled according to their condition, whether
normal or the defect name found on an image level. We thoroughly evaluate
state-of-the-art and popular methods for three image-level computer vision
tasks covered by InsPLAD: object detection, through the AP metric; defect
classification, through Balanced Accuracy; and anomaly detection, through the
AUROC metric. InsPLAD offers various vision challenges from uncontrolled
environments, such as multi-scale objects, multi-size class instances, multiple
objects per image, intra-class variation, cluttered background, distinct
point-of-views, perspective distortion, occlusion, and varied lighting
conditions. To the best of our knowledge, InsPLAD is the first large real-world
dataset and benchmark for power line asset inspection with multiple components
and defects for various computer vision tasks, with a potential impact to
improve state-of-the-art methods in the field. It will be publicly available in
its integrity on a repository with a thorough description. It can be found at
https://github.com/andreluizbvs/InsPLAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Luiz Buarque Vieira e Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felix_H/0/1/0/all/0/1&quot;&gt;Heitor de Castro Felix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoes_F/0/1/0/all/0/1&quot;&gt;Franscisco Paulo Magalh&amp;#xe3;es Sim&amp;#xf5;es&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1&quot;&gt;Veronica Teichrieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_M/0/1/0/all/0/1&quot;&gt;Michel Mozinho dos Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1&quot;&gt;Hemir Santiago&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sgotti_V/0/1/0/all/0/1&quot;&gt;Virginia Sgotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_H/0/1/0/all/0/1&quot;&gt;Henrique Lott Neto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01620">
<title>ACQUIRED: A Dataset for Answering Counterfactual Questions In Real-Life Videos. (arXiv:2311.01620v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01620</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal counterfactual reasoning is a vital yet challenging ability for AI
systems. It involves predicting the outcomes of hypothetical circumstances
based on vision and language inputs, which enables AI models to learn from
failures and explore hypothetical scenarios. Despite its importance, there are
only a few datasets targeting the counterfactual reasoning abilities of
multimodal models. Among them, they only cover reasoning over synthetic
environments or specific types of events (e.g. traffic collisions), making them
hard to reliably benchmark the model generalization ability in diverse
real-world scenarios and reasoning dimensions. To overcome these limitations,
we develop a video question answering dataset, ACQUIRED: it consists of 3.9K
annotated videos, encompassing a wide range of event types and incorporating
both first and third-person viewpoints, which ensures a focus on real-world
diversity. In addition, each video is annotated with questions that span three
distinct dimensions of reasoning, including physical, social, and temporal,
which can comprehensively evaluate the model counterfactual abilities along
multiple aspects. We benchmark our dataset against several state-of-the-art
language-only and multimodal models and experimental results demonstrate a
significant performance gap (&amp;gt;13%) between models and humans. The findings
suggest that multimodal counterfactual reasoning remains an open challenge and
ACQUIRED is a comprehensive and reliable benchmark for inspiring future
research in this direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Te-Lin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zi-Yi Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yu Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_N/0/1/0/all/0/1&quot;&gt;Nischal Reddy Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freedman_M/0/1/0/all/0/1&quot;&gt;Marjorie Freedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weischedel_R/0/1/0/all/0/1&quot;&gt;Ralph M. Weischedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Nanyun Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01623">
<title>VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Video analytics is widely used in contemporary systems and services. At the
forefront of video analytics are video queries that users develop to find
objects of particular interest. Building upon the insight that video objects
(e.g., human, animals, cars, etc.), the center of video analytics, are similar
in spirit to objects modeled by traditional object-oriented languages, we
propose to develop an object-oriented approach to video analytics. This
approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant
with constructs that make it easy for users to express video objects and their
interactions$\unicode{x2015}$as well as an extensible backend that can
automatically construct and optimize pipelines based on video objects. We have
implemented and open-sourced VQPy, which has been productized in Cisco as part
of its DeepVision framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhenting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hanchen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Pengzhan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padmanabhan_A/0/1/0/all/0/1&quot;&gt;Arthi Padmanabhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latapie_H/0/1/0/all/0/1&quot;&gt;Hugo Latapie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Harry Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01624">
<title>Attention based Dual-Branch Complex Feature Fusion Network for Hyperspectral Image Classification. (arXiv:2311.01624v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01624</link>
<description rdf:parseType="Literal">&lt;p&gt;This research work presents a novel dual-branch model for hyperspectral image
classification that combines two streams: one for processing standard
hyperspectral patches using Real-Valued Neural Network (RVNN) and the other for
processing their corresponding Fourier transforms using Complex-Valued Neural
Network (CVNN). The proposed model is evaluated on the Pavia University and
Salinas datasets. Results show that the proposed model outperforms
state-of-the-art methods in terms of overall accuracy, average accuracy, and
Kappa. Through the incorporation of Fourier transforms in the second stream,
the model is able to extract frequency information, which complements the
spatial information extracted by the first stream. The combination of these two
streams improves the overall performance of the model. Furthermore, to enhance
the model performance, the Squeeze and Excitation (SE) mechanism has been
utilized. Experimental evidence show that SE block improves the models overall
accuracy by almost 1\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alkhatib_M/0/1/0/all/0/1&quot;&gt;Mohammed Q. Alkhatib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Saad_M/0/1/0/all/0/1&quot;&gt;Mina Al-Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aburaed_N/0/1/0/all/0/1&quot;&gt;Nour Aburaed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zitouni_M/0/1/0/all/0/1&quot;&gt;M. Sami Zitouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahmad_H/0/1/0/all/0/1&quot;&gt;Hussain Al Ahmad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01646">
<title>SemiGPC: Distribution-Aware Label Refinement for Imbalanced Semi-Supervised Learning Using Gaussian Processes. (arXiv:2311.01646v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01646</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce SemiGPC, a distribution-aware label refinement
strategy based on Gaussian Processes where the predictions of the model are
derived from the labels posterior distribution. Differently from other
buffer-based semi-supervised methods such as CoMatch and SimMatch, our SemiGPC
includes a normalization term that addresses imbalances in the global data
distribution while maintaining local sensitivity. This explicit control allows
SemiGPC to be more robust to confirmation bias especially under class
imbalance. We show that SemiGPC improves performance when paired with different
Semi-Supervised methods such as FixMatch, ReMixMatch, SimMatch and FreeMatch
and different pre-training strategies including MSN and Dino. We also show that
SemiGPC achieves state of the art results under different degrees of class
imbalance on standard CIFAR10-LT/CIFAR100-LT especially in the low data-regime.
Using SemiGPC also results in about 2% avg.accuracy increase compared to a new
competitive baseline on the more challenging benchmarks SemiAves, SemiCUB,
SemiFungi and Semi-iNat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemkhenter_A/0/1/0/all/0/1&quot;&gt;Abdelhak Lemkhenter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Manchen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1&quot;&gt;Luca Zancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_G/0/1/0/all/0/1&quot;&gt;Gurumurthy Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_P/0/1/0/all/0/1&quot;&gt;Paolo Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1&quot;&gt;Davide Modolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01651">
<title>Keypoint Description by Symmetry Assessment -- Applications in Biometrics. (arXiv:2311.01651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01651</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a model-based feature extractor to describe neighborhoods around
keypoints by finite expansion, estimating the spatially varying orientation by
harmonic functions. The iso-curves of such functions are highly symmetric
w.r.t. the origin (a keypoint) and the estimated parameters have well defined
geometric interpretations. The origin is also a unique singularity of all
harmonic functions, helping to determine the location of a keypoint precisely,
whereas the functions describe the object shape of the neighborhood. This is
novel and complementary to traditional texture features which describe
texture-shape properties i.e. they are purposively invariant to translation
(within a texture). We report on experiments of verification and identification
of keypoints in forensic fingerprints by using publicly available data (NIST
SD27) and discuss the results in comparison to other studies. These support our
conclusions that the novel features can equip single cores or single minutia
with a significant verification power at 19% EER, and an identification power
of 24-78% for ranks of 1-20. Additionally, we report verification results of
periocular biometrics using near-infrared images, reaching an EER performance
of 13%, which is comparable to the state of the art. More importantly, fusion
of two systems, our and texture features (Gabor), result in a measurable
performance improvement. We report reduction of the EER to 9%, supporting the
view that the novel features capture relevant visual information, which
traditional texture features do not.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikaelyan_A/0/1/0/all/0/1&quot;&gt;Anna Mikaelyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01653">
<title>INeAT: Iterative Neural Adaptive Tomography. (arXiv:2311.01653v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01653</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography (CT) with its remarkable capability for three-dimensional
imaging from multiple projections, enjoys a broad range of applications in
clinical diagnosis, scientific observation, and industrial detection. Neural
Adaptive Tomography (NeAT) is a recently proposed 3D rendering method based on
neural radiance field for CT, and it demonstrates superior performance compared
to traditional methods. However, it still faces challenges when dealing with
the substantial perturbations and pose shifts encountered in CT scanning
processes. Here, we propose a neural rendering method for CT reconstruction,
named Iterative Neural Adaptive Tomography (INeAT), which incorporates
iterative posture optimization to effectively counteract the influence of
posture perturbations in data, particularly in cases involving significant
posture variations. Through the implementation of a posture feedback
optimization strategy, INeAT iteratively refines the posture corresponding to
the input images based on the reconstructed 3D volume. We demonstrate that
INeAT achieves artifact-suppressed and resolution-enhanced reconstruction in
scenarios with significant pose disturbances. Furthermore, we show that our
INeAT maintains comparable reconstruction performance to stable-state
acquisitions even using data from unstable-state acquisitions, which
significantly reduces the time required for CT scanning and relaxes the
stringent requirements on imaging hardware systems, underscoring its immense
potential for applications in short-time and low-cost CT technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_B/0/1/0/all/0/1&quot;&gt;Bo Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_C/0/1/0/all/0/1&quot;&gt;Changqing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zihan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01655">
<title>Detecting Spurious Correlations via Robust Visual Concepts in Real and AI-Generated Image Classification. (arXiv:2311.01655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.01655</link>
<description rdf:parseType="Literal">&lt;p&gt;Often machine learning models tend to automatically learn associations
present in the training data without questioning their validity or
appropriateness. This undesirable property is the root cause of the
manifestation of spurious correlations, which render models unreliable and
prone to failure in the presence of distribution shifts. Research shows that
most methods attempting to remedy spurious correlations are only effective for
a model&apos;s known spurious associations. Current spurious correlation detection
algorithms either rely on extensive human annotations or are too restrictive in
their formulation. Moreover, they rely on strict definitions of visual
artifacts that may not apply to data produced by generative models, as they are
known to hallucinate contents that do not conform to standard specifications.
In this work, we introduce a general-purpose method that efficiently detects
potential spurious correlations, and requires significantly less human
interference in comparison to the prior art. Additionally, the proposed method
provides intuitive explanations while eliminating the need for pixel-level
annotations. We demonstrate the proposed method&apos;s tolerance to the peculiarity
of AI-generated images, which is a considerably challenging task, one where
most of the existing methods fall short. Consequently, our method is also
suitable for detecting spurious correlations that may propagate to downstream
applications originating from generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dammu_P/0/1/0/all/0/1&quot;&gt;Preetam Prabhu Srikar Dammu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chirag Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01659">
<title>Efficient Cloud Pipelines for Neural Radiance Fields. (arXiv:2311.01659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01659</link>
<description rdf:parseType="Literal">&lt;p&gt;Since their introduction in 2020, Neural Radiance Fields (NeRFs) have taken
the computer vision community by storm. They provide a multi-view
representation of a scene or object that is ideal for eXtended Reality (XR)
applications and for creative endeavors such as virtual production, as well as
change detection operations in geospatial analytics. The computational cost of
these generative AI models is quite high, however, and the construction of
cloud pipelines to generate NeRFs is neccesary to realize their potential in
client applications. In this paper, we present pipelines on a high performance
academic computing cluster and compare it with a pipeline implemented on
Microsoft Azure. Along the way, we describe some uses of NeRFs in enabling
novel user interaction scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacoby_D/0/1/0/all/0/1&quot;&gt;Derek Jacoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Donglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribas_W/0/1/0/all/0/1&quot;&gt;Weder Ribas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_V/0/1/0/all/0/1&quot;&gt;Vishwanath Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1&quot;&gt;Mengdi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blois_E/0/1/0/all/0/1&quot;&gt;Emma De Blois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coady_Y/0/1/0/all/0/1&quot;&gt;Yvonne Coady&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01673">
<title>Content Significance Distribution of Sub-Text Blocks in Articles and Its Application to Article-Organization Assessment. (arXiv:2311.01673v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01673</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore how to capture the significance of a sub-text block in an article
and how it may be used for text mining tasks. A sub-text block is a
sub-sequence of sentences in the article. We formulate the notion of content
significance distribution (CSD) of sub-text blocks, referred to as CSD of the
first kind and denoted by CSD-1. In particular, we leverage Hugging Face&apos;s
SentenceTransformer to generate contextual sentence embeddings, and use
MoverScore over text embeddings to measure how similar a sub-text block is to
the entire text. To overcome the exponential blowup on the number of sub-text
blocks, we present an approximation algorithm and show that the approximated
CSD-1 is almost identical to the exact CSD-1. Under this approximation, we show
that the average and median CSD-1&apos;s for news, scholarly research, argument, and
narrative articles share the same pattern. We also show that under a certain
linear transformation, the complement of the cumulative distribution function
of the beta distribution with certain values of $\alpha$ and $\beta$ resembles
a CSD-1 curve. We then use CSD-1&apos;s to extract linguistic features to train an
SVC classifier for assessing how well an article is organized. Through
experiments, we show that this method achieves high accuracy for assessing
student essays. Moreover, we study CSD of sentence locations, referred to as
CSD of the second kind and denoted by CSD-2, and show that average CSD-2&apos;s for
different types of articles possess distinctive patterns, which either conform
common perceptions of article structures or provide rectification with minor
deviation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01676">
<title>MineSegSAT: An automated system to evaluate mining disturbed area extents from Sentinel-2 imagery. (arXiv:2311.01676v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01676</link>
<description rdf:parseType="Literal">&lt;p&gt;Assessing the environmental impact of the mineral extraction industry plays a
critical role in understanding and mitigating the ecological consequences of
extractive activities. This paper presents MineSegSAT, a model that presents a
novel approach to predicting environmentally impacted areas of mineral
extraction sites using the SegFormer deep learning segmentation architecture
trained on Sentinel-2 data. The data was collected from non-overlapping regions
over Western Canada in 2021 containing areas of land that have been
environmentally impacted by mining activities that were identified from
high-resolution satellite imagery in 2021. The SegFormer architecture, a
state-of-the-art semantic segmentation framework, is employed to leverage its
advanced spatial understanding capabilities for accurate land cover
classification. We investigate the efficacy of loss functions including Dice,
Tversky, and Lovasz loss respectively. The trained model was utilized for
inference over the test region in the ensuing year to identify potential areas
of expansion or contraction over these same periods. The Sentinel-2 data is
made available on Amazon Web Services through a collaboration with Earth Daily
Analytics which provides corrected and tiled analytics-ready data on the AWS
platform. The model and ongoing API to access the data on AWS allow the
creation of an automated tool to monitor the extent of disturbed areas
surrounding known mining sites to ensure compliance with their environmental
impact goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDonald_E/0/1/0/all/0/1&quot;&gt;Ezra MacDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacoby_D/0/1/0/all/0/1&quot;&gt;Derek Jacoby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coady_Y/0/1/0/all/0/1&quot;&gt;Yvonne Coady&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01682">
<title>Flow-Based Feature Fusion for Vehicle-Infrastructure Cooperative 3D Object Detection. (arXiv:2311.01682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01682</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperatively utilizing both ego-vehicle and infrastructure sensor data can
significantly enhance autonomous driving perception abilities. However, the
uncertain temporal asynchrony and limited communication conditions can lead to
fusion misalignment and constrain the exploitation of infrastructure data. To
address these issues in vehicle-infrastructure cooperative 3D (VIC3D) object
detection, we propose the Feature Flow Net (FFNet), a novel cooperative
detection framework. FFNet is a flow-based feature fusion framework that uses a
feature flow prediction module to predict future features and compensate for
asynchrony. Instead of transmitting feature maps extracted from still-images,
FFNet transmits feature flow, leveraging the temporal coherence of sequential
infrastructure frames. Furthermore, we introduce a self-supervised training
approach that enables FFNet to generate feature flow with feature prediction
ability from raw infrastructure sequences. Experimental results demonstrate
that our proposed method outperforms existing cooperative detection methods
while only requiring about 1/100 of the transmission cost of raw data and
covers all latency in one model on the DAIR-V2X dataset. The code is available
at
\href{https://github.com/haibao-yu/FFNet-VIC3D}{https://github.com/haibao-yu/FFNet-VIC3D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haibao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yingjuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jilei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Z/0/1/0/all/0/1&quot;&gt;Zaiqing Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01686">
<title>Disentangled Representation Learning with Transmitted Information Bottleneck. (arXiv:2311.01686v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01686</link>
<description rdf:parseType="Literal">&lt;p&gt;Encoding only the task-related information from the raw data, \ie,
disentangled representation learning, can greatly contribute to the robustness
and generalizability of models. Although significant advances have been made by
regularizing the information in representations with information theory, two
major challenges remain: 1) the representation compression inevitably leads to
performance drop; 2) the disentanglement constraints on representations are in
complicated optimization. To these issues, we introduce Bayesian networks with
transmitted information to formulate the interaction among input and
representations during disentanglement. Building upon this framework, we
propose \textbf{DisTIB} (\textbf{T}ransmitted \textbf{I}nformation
\textbf{B}ottleneck for \textbf{Dis}entangled representation learning), a novel
objective that navigates the balance between information compression and
preservation. We employ variational inference to derive a tractable estimation
for DisTIB. This estimation can be simply optimized via standard gradient
descent with a reparameterization trick. Moreover, we theoretically prove that
DisTIB can achieve optimal disentanglement, underscoring its superior efficacy.
To solidify our claims, we conduct extensive experiments on various downstream
tasks to demonstrate the appealing efficacy of DisTIB and validate our
theoretical analyses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1&quot;&gt;Zhuohang Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minnan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chengyou Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jihong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01696">
<title>Universal Perturbation-based Secret Key-Controlled Data Hiding. (arXiv:2311.01696v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.01696</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are demonstrated to be vulnerable to universal
perturbation, a single quasi-perceptible perturbation that can deceive the DNN
on most images. However, the previous works are focused on using universal
perturbation to perform adversarial attacks, while the potential usability of
universal perturbation as data carriers in data hiding is less explored,
especially for the key-controlled data hiding method. In this paper, we propose
a novel universal perturbation-based secret key-controlled data-hiding method,
realizing data hiding with a single universal perturbation and data decoding
with the secret key-controlled decoder. Specifically, we optimize a single
universal perturbation, which serves as a data carrier that can hide multiple
secret images and be added to most cover images. Then, we devise a secret
key-controlled decoder to extract different secret images from the single
container image constructed by the universal perturbation by using different
secret keys. Moreover, a suppress loss function is proposed to prevent the
secret image from leakage. Furthermore, we adopt a robust module to boost the
decoder&apos;s capability against corruption. Finally, A co-joint optimization
strategy is proposed to find the optimal universal perturbation and decoder.
Extensive experiments are conducted on different datasets to demonstrate the
effectiveness of the proposed method. Additionally, the physical test performed
on platforms (e.g., WeChat and Twitter) verifies the usability of the proposed
method in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donghua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tingsong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01702">
<title>Medical Image Segmentation with Domain Adaptation: A Survey. (arXiv:2311.01702v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01702</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) has shown remarkable success in various medical imaging
data analysis applications. However, it remains challenging for DL models to
achieve good generalization, especially when the training and testing datasets
are collected at sites with different scanners, due to domain shift caused by
differences in data distributions. Domain adaptation has emerged as an
effective means to address this challenge by mitigating domain gaps in medical
imaging applications. In this review, we specifically focus on domain
adaptation approaches for DL-based medical image segmentation. We first present
the motivation and background knowledge underlying domain adaptations, then
provide a comprehensive review of domain adaptation applications in medical
image segmentations, and finally discuss the challenges, limitations, and
future research trends in the field to promote the methodology development of
domain adaptation in the context of medical image segmentation. Our goal was to
provide researchers with up-to-date references on the applications of domain
adaptation in medical image segmentation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuemeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yong Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01703">
<title>Taking a PEEK into YOLOv5 for Satellite Component Recognition via Entropy-based Visual Explanations. (arXiv:2311.01703v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01703</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating risk of collisions and the accumulation of space debris in Low
Earth Orbit (LEO) has reached critical concern due to the ever increasing
number of spacecraft. Addressing this crisis, especially in dealing with
non-cooperative and unidentified space debris, is of paramount importance. This
paper contributes to efforts in enabling autonomous swarms of small chaser
satellites for target geometry determination and safe flight trajectory
planning for proximity operations in LEO. Our research explores on-orbit use of
the You Only Look Once v5 (YOLOv5) object detection model trained to detect
satellite components. While this model has shown promise, its inherent lack of
interpretability hinders human understanding, a critical aspect of validating
algorithms for use in safety-critical missions. To analyze the decision
processes, we introduce Probabilistic Explanations for Entropic Knowledge
extraction (PEEK), a method that utilizes information theoretic analysis of the
latent representations within the hidden layers of the model. Through both
synthetic in hardware-in-the-loop experiments, PEEK illuminates the
decision-making processes of the model, helping identify its strengths,
limitations and biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meni_M/0/1/0/all/0/1&quot;&gt;Mackenzie J. Meni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendrakar_T/0/1/0/all/0/1&quot;&gt;Trupti Mahendrakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raney_O/0/1/0/all/0/1&quot;&gt;Olivia D. M. Raney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1&quot;&gt;Ryan T. White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayo_M/0/1/0/all/0/1&quot;&gt;Michael L. Mayo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilkiewicz_K/0/1/0/all/0/1&quot;&gt;Kevin Pilkiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01714">
<title>EXIM: A Hybrid Explicit-Implicit Representation for Text-Guided 3D Shape Generation. (arXiv:2311.01714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01714</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new text-guided technique for generating 3D shapes. The
technique leverages a hybrid 3D shape representation, namely EXIM, combining
the strengths of explicit and implicit representations. Specifically, the
explicit stage controls the topology of the generated 3D shapes and enables
local modifications, whereas the implicit stage refines the shape and paints it
with plausible colors. Also, the hybrid approach separates the shape and color
and generates color conditioned on shape to ensure shape-color consistency.
Unlike the existing state-of-the-art methods, we achieve high-fidelity shape
generation from natural-language descriptions without the need for
time-consuming per-shape optimization or reliance on human-annotated texts
during training or test-time optimization. Further, we demonstrate the
applicability of our approach to generate indoor scenes with consistent styles
using text-induced 3D shapes. Through extensive experiments, we demonstrate the
compelling quality of our results and the high coherency of our generated
shapes with the input texts, surpassing the performance of existing methods by
a significant margin. Codes and models are released at
https://github.com/liuzhengzhe/EXIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengzhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jingyu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_K/0/1/0/all/0/1&quot;&gt;Ka-Hei Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chi-Wing Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01723">
<title>Towards Calibrated Robust Fine-Tuning of Vision-Language Models. (arXiv:2311.01723v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01723</link>
<description rdf:parseType="Literal">&lt;p&gt;While fine-tuning unleashes the potential of a pre-trained model to a
specific task, it trades off the model&apos;s generalization capability on
out-of-distribution (OOD) datasets. To mitigate this, robust fine-tuning aims
to ensure performance on OOD datasets as well as an in-distribution (ID)
dataset for which the model is being tuned. However, another criterion for
reliable machine learning (ML), confidence calibration, has been overlooked
despite its increasing demand for real-world high-stakes ML applications (e.g.,
autonomous driving and medical diagnosis). For the first time, we raise
concerns about the calibration of fine-tuned vision-language models (VLMs)
under distribution shift by showing that naive fine-tuning and even
state-of-the-art robust fine-tuning methods hurt the calibration of pre-trained
VLMs, especially on OOD datasets. To address this, we provide a simple
approach, called a calibrated robust fine-tuning (CaRot) that incentivizes the
calibration and robustness on both ID and OOD datasets. Empirical results on
ImageNet-1K distribution shift evaluation verify the effectiveness of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Mijoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junhyeok Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_E/0/1/0/all/0/1&quot;&gt;Euiseog Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01731">
<title>Capturing Local and Global Features in Medical Images by Using Ensemble CNN-Transformer. (arXiv:2311.01731v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01731</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a groundbreaking classification model called the
Controllable Ensemble Transformer and CNN (CETC) for the analysis of medical
images. The CETC model combines the powerful capabilities of convolutional
neural networks (CNNs) and transformers to effectively capture both local and
global features present in medical images. The model architecture comprises
three main components: a convolutional encoder block (CEB), a
transposed-convolutional decoder block (TDB), and a transformer classification
block (TCB). The CEB is responsible for capturing multi-local features at
different scales and draws upon components from VGGNet, ResNet, and MobileNet
as backbones. By leveraging this combination, the CEB is able to effectively
detect and encode local features. The TDB, on the other hand, consists of
sub-decoders that decode and sum the captured features using ensemble
coefficients. This enables the model to efficiently integrate the information
from multiple scales. Finally, the TCB utilizes the SwT backbone and a
specially designed prediction head to capture global features, ensuring a
comprehensive understanding of the entire image. The paper provides detailed
information on the experimental setup and implementation, including the use of
transfer learning, data preprocessing techniques, and training settings. The
CETC model is trained and evaluated using two publicly available COVID-19
datasets. Remarkably, the model outperforms existing state-of-the-art models
across various evaluation metrics. The experimental results clearly demonstrate
the superiority of the CETC model, emphasizing its potential for accurately and
efficiently analyzing medical images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaleybar_J/0/1/0/all/0/1&quot;&gt;Javad Mirzapour Kaleybar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saadat_H/0/1/0/all/0/1&quot;&gt;Hooman Saadat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khaloo_H/0/1/0/all/0/1&quot;&gt;Hooman Khaloo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01734">
<title>MixCon3D: Synergizing Multi-View and Cross-Modal Contrastive Learning for Enhancing 3D Representation. (arXiv:2311.01734v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01734</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as a promising paradigm for 3D open-world
understanding, jointly with text, image, and point cloud. In this paper, we
introduce MixCon3D, which combines the complementary information between 2D
images and 3D point clouds to enhance contrastive learning. With the further
integration of multi-view 2D images, MixCon3D enhances the traditional
tri-modal representation by offering a more accurate and comprehensive
depiction of real-world 3D objects and bolstering text alignment. Additionally,
we pioneer the first thorough investigation of various training recipes for the
3D contrastive learning paradigm, building a solid baseline with improved
performance. Extensive experiments conducted on three representative benchmarks
reveal that our method renders significant improvement over the baseline,
surpassing the previous state-of-the-art performance on the challenging
1,156-category Objaverse-LVIS dataset by 5.7%. We further showcase the
effectiveness of our approach in more applications, including text-to-3D
retrieval and point cloud captioning. The code is available at
https://github.com/UCSC-VLAA/MixCon3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yipeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-Shi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuyin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01744">
<title>Data-Centric Long-Tailed Image Recognition. (arXiv:2311.01744v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01744</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of the long-tail scenario, models exhibit a strong demand for
high-quality data. Data-centric approaches aim to enhance both the quantity and
quality of data to improve model performance. Among these approaches,
information augmentation has been progressively introduced as a crucial
category. It achieves a balance in model performance by augmenting the richness
and quantity of samples in the tail classes. However, there is currently a lack
of research into the underlying mechanisms explaining the effectiveness of
information augmentation methods. Consequently, the utilization of information
augmentation in long-tail recognition tasks relies heavily on empirical and
intricate fine-tuning. This work makes two primary contributions. Firstly, we
approach the problem from the perspectives of feature diversity and
distribution shift, introducing the concept of Feature Diversity Gain (FDG) to
elucidate why information augmentation is effective. We find that the
performance of information augmentation can be explained by FDG, and its
performance peaks when FDG achieves an appropriate balance. Experimental
results demonstrate that by using FDG to select augmented data, we can further
enhance model performance without the need for any modifications to the model&apos;s
architecture. Thus, data-centric approaches hold significant potential in the
field of long-tail recognition, beyond the development of new model structures.
Furthermore, we systematically introduce the core components and fundamental
tasks of a data-centric long-tail learning framework for the first time. These
core components guide the implementation and deployment of the system, while
the corresponding fundamental tasks refine and expand the research area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yanbiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Puhua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01755">
<title>Towards a Unified Transformer-based Framework for Scene Graph Generation and Human-object Interaction Detection. (arXiv:2311.01755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01755</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene graph generation (SGG) and human-object interaction (HOI) detection are
two important visual tasks aiming at localising and recognising relationships
between objects, and interactions between humans and objects, respectively.
&lt;/p&gt;
&lt;p&gt;Prevailing works treat these tasks as distinct tasks, leading to the
development of task-specific models tailored to individual datasets. However,
we posit that the presence of visual relationships can furnish crucial
contextual and intricate relational cues that significantly augment the
inference of human-object interactions. This motivates us to think if there is
a natural intrinsic relationship between the two tasks, where scene graphs can
serve as a source for inferring human-object interactions. In light of this, we
introduce SG2HOI+, a unified one-step model based on the Transformer
architecture. Our approach employs two interactive hierarchical Transformers to
seamlessly unify the tasks of SGG and HOI detection. Concretely, we initiate a
relation Transformer tasked with generating relation triples from a suite of
visual features. Subsequently, we employ another transformer-based decoder to
predict human-object interactions based on the generated relation triples. A
comprehensive series of experiments conducted across established benchmark
datasets including Visual Genome, V-COCO, and HICO-DET demonstrates the
compelling performance of our SG2HOI+ model in comparison to prevalent
one-stage SGG models. Remarkably, our approach achieves competitive performance
when compared to state-of-the-art HOI methods. Additionally, we observe that
our SG2HOI+ jointly trained on both SGG and HOI tasks in an end-to-end manner
yields substantial improvements for both tasks compared to individualized
training paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan-Fang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01766">
<title>Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation. (arXiv:2311.01766v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.01766</link>
<description rdf:parseType="Literal">&lt;p&gt;Mis- and disinformation online have become a major societal problem as major
sources of online harms of different kinds. One common form of mis- and
disinformation is out-of-context (OOC) information, where different pieces of
information are falsely associated, e.g., a real image combined with a false
textual caption or a misleading textual description. Although some past studies
have attempted to defend against OOC mis- and disinformation through external
evidence, they tend to disregard the role of different pieces of evidence with
different stances. Motivated by the intuition that the stance of evidence
represents a bias towards different detection results, we propose a stance
extraction network (SEN) that can extract the stances of different pieces of
multi-modal evidence in a unified framework. Moreover, we introduce a
support-refutation score calculated based on the co-occurrence relations of
named entities into the textual SEN. Extensive experiments on a public
large-scale dataset demonstrated that our proposed method outperformed the
state-of-the-art baselines, with the best model achieving a performance gain of
3.2% in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1&quot;&gt;Weidong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shujun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01770">
<title>Modeling the Uncertainty with Maximum Discrepant Students for Semi-supervised 2D Pose Estimation. (arXiv:2311.01770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01770</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised pose estimation is a practically challenging task for
computer vision. Although numerous excellent semi-supervised classification
methods have emerged, these methods typically use confidence to evaluate the
quality of pseudo-labels, which is difficult to achieve in pose estimation
tasks. For example, in pose estimation, confidence represents only the
possibility that a position of the heatmap is a keypoint, not the quality of
that prediction. In this paper, we propose a simple yet efficient framework to
estimate the quality of pseudo-labels in semi-supervised pose estimation tasks
from the perspective of modeling the uncertainty of the pseudo-labels.
Concretely, under the dual mean-teacher framework, we construct the two maximum
discrepant students (MDSs) to effectively push two teachers to generate
different decision boundaries for the same sample. Moreover, we create multiple
uncertainties to assess the quality of the pseudo-labels. Experimental results
demonstrate that our method improves the performance of semi-supervised pose
estimation on three datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Junbiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01773">
<title>PDF: Point Diffusion Implicit Function for Large-scale Scene Neural Representation. (arXiv:2311.01773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01773</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in implicit neural representations have achieved impressive
results by sampling and fusing individual points along sampling rays in the
sampling space. However, due to the explosively growing sampling space, finely
representing and synthesizing detailed textures remains a challenge for
unbounded large-scale outdoor scenes. To alleviate the dilemma of using
individual points to perceive the entire colossal space, we explore learning
the surface distribution of the scene to provide structural priors and reduce
the samplable space and propose a Point Diffusion implicit Function, PDF, for
large-scale scene neural representation. The core of our method is a
large-scale point cloud super-resolution diffusion module that enhances the
sparse point cloud reconstructed from several training images into a dense
point cloud as an explicit prior. Then in the rendering stage, only sampling
points with prior points within the sampling radius are retained. That is, the
sampling space is reduced from the unbounded space to the scene surface.
Meanwhile, to fill in the background of the scene that cannot be provided by
point clouds, the region sampling based on Mip-NeRF 360 is employed to model
the background representation. Expensive experiments have demonstrated the
effectiveness of our method for large-scale scene novel view synthesis, which
outperforms relevant state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yuhan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fukun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiayuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chongshan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+YU_G/0/1/0/all/0/1&quot;&gt;Gang YU&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01777">
<title>CheX-Nomaly: Segmenting Lung Abnormalities from Chest Radiographs using Machine Learning. (arXiv:2311.01777v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01777</link>
<description rdf:parseType="Literal">&lt;p&gt;The global challenge in chest radiograph X-ray (CXR) abnormalities often
being misdiagnosed is primarily associated with perceptual errors, where
healthcare providers struggle to accurately identify the location of
abnormalities, rather than misclassification errors. We currently address this
problem through disease-specific segmentation models. Unfortunately, these
models cannot be released in the field due to their lack of generalizability
across all thoracic diseases. A binary model tends to perform poorly when it
encounters a disease that isn&apos;t represented in the dataset. We present
CheX-nomaly: a binary localization U-net model that leverages transfer learning
techniques with the incorporation of an innovative contrastive learning
approach. Trained on the VinDr-CXR dataset, which encompasses 14 distinct
diseases in addition to &apos;no finding&apos; cases, my model achieves generalizability
across these 14 diseases and others it has not seen before. We show that we can
significantly improve the generalizability of an abnormality localization model
by incorporating a contrastive learning method and dissociating the bounding
boxes with its disease class. We also introduce a new loss technique to apply
to enhance the U-nets performance on bounding box segmentation. By introducing
CheX-nomaly, we offer a promising solution to enhance the precision of chest
disease diagnosis, with a specific focus on reducing the significant number of
perceptual errors in healthcare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sanskriti Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01782">
<title>Generating Unbiased Pseudo-labels via a Theoretically Guaranteed Chebyshev Constraint to Unify Semi-supervised Classification and Regression. (arXiv:2311.01782v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01782</link>
<description rdf:parseType="Literal">&lt;p&gt;Both semi-supervised classification and regression are practically
challenging tasks for computer vision. However, semi-supervised classification
methods are barely applied to regression tasks. Because the threshold-to-pseudo
label process (T2L) in classification uses confidence to determine the quality
of label. It is successful for classification tasks but inefficient for
regression tasks. In nature, regression also requires unbiased methods to
generate high-quality labels. On the other hand, T2L for classification often
fails if the confidence is generated by a biased method. To address this issue,
in this paper, we propose a theoretically guaranteed constraint for generating
unbiased labels based on Chebyshev&apos;s inequality, combining multiple predictions
to generate superior quality labels from several inferior ones. In terms of
high-quality labels, the unbiased method naturally avoids the drawback of T2L.
Specially, we propose an Unbiased Pseudo-labels network (UBPL network) with
multiple branches to combine multiple predictions as pseudo-labels, where a
Feature Decorrelation loss (FD loss) is proposed based on Chebyshev constraint.
In principle, our method can be used for both classification and regression and
can be easily extended to any semi-supervised framework, e.g. Mean Teacher,
FixMatch, DualPose. Our approach achieves superior performance over SOTAs on
the pose estimation datasets Mouse, FLIC and LSP, as well as the classification
datasets CIFAR10/100 and SVHN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1&quot;&gt;Junbiao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01804">
<title>inkn&apos;hue: Enhancing Manga Colorization from Multiple Priors with Alignment Multi-Encoder VAE. (arXiv:2311.01804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01804</link>
<description rdf:parseType="Literal">&lt;p&gt;Manga, a form of Japanese comics and distinct visual storytelling, has
captivated readers worldwide. Traditionally presented in black and white,
manga&apos;s appeal lies in its ability to convey complex narratives and emotions
through intricate line art and shading. Yet, the desire to experience manga in
vibrant colors has sparked the pursuit of manga colorization, a task of
paramount significance for artists. However, existing methods, originally
designed for line art and sketches, face challenges when applied to manga.
These methods often fall short in achieving the desired results, leading to the
need for specialized manga-specific solutions. Existing approaches frequently
rely on a single training step or extensive manual artist intervention, which
can yield less satisfactory outcomes. To address these challenges, we propose a
specialized framework for manga colorization. Leveraging established models for
shading and vibrant coloring, our approach aligns both using a multi-encoder
VAE. This structured workflow ensures clear and colorful results, with the
option to incorporate reference images and manual hints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiramahapokee_T/0/1/0/all/0/1&quot;&gt;Tawin Jiramahapokee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01811">
<title>DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating high-quality and person-generic visual dubbing remains a
challenge. Recent innovation has seen the advent of a two-stage paradigm,
decoupling the rendering and lip synchronization process facilitated by
intermediate representation as a conduit. Still, previous methodologies rely on
rough landmarks or are confined to a single speaker, thus limiting their
performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We
first craft the Diffusion auto-encoder by an inpainting renderer incorporating
a mask to delineate editable zones and unaltered regions. This allows for
seamless filling of the lower-face region while preserving the remaining parts.
Throughout our experiments, we encountered several challenges. Primarily, the
semantic encoder lacks robustness, constricting its ability to capture
high-level features. Besides, the modeling ignored facial positioning, causing
mouth or nose jitters across frames. To tackle these issues, we employ
versatile strategies, including data augmentation and supplementary eye
guidance. Moreover, we encapsulated a conformer-based reference encoder and
motion generator fortified by a cross-attention mechanism. This enables our
model to learn person-specific textures with varying references and reduces
reliance on paired audio-visual data. Our rigorous experiments comprehensively
highlight that our ground-breaking approach outpaces existing methods with
considerable margins and delivers seamless, intelligible videos in
person-generic and multilingual scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chenpeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shuai Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01813">
<title>FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation. (arXiv:2311.01813v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01813</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, open-domain text-to-video (T2V) generation models have made
remarkable progress. However, the promising results are mainly shown by the
qualitative cases of generated videos, while the quantitative evaluation of T2V
models still faces two critical problems. Firstly, existing studies lack
fine-grained evaluation of T2V models on different categories of text prompts.
Although some benchmarks have categorized the prompts, their categorization
either only focuses on a single aspect or fails to consider the temporal
information in video generation. Secondly, it is unclear whether the automatic
evaluation metrics are consistent with human standards. To address these
problems, we propose FETV, a benchmark for Fine-grained Evaluation of
Text-to-Video generation. FETV is multi-aspect, categorizing the prompts based
on three orthogonal aspects: the major content, the attributes to control and
the prompt complexity. FETV is also temporal-aware, which introduces several
temporal categories tailored for video generation. Based on FETV, we conduct
comprehensive manual evaluations of four representative T2V models, revealing
their pros and cons on different categories of prompts from different aspects.
We also extend FETV as a testbed to evaluate the reliability of automatic T2V
metrics. The multi-aspect categorization of FETV enables fine-grained analysis
of the metrics&apos; reliability in different scenarios. We find that existing
automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human
evaluation. To address this problem, we explore several solutions to improve
CLIPScore and FVD, and develop two automatic metrics that exhibit significant
higher correlation with humans than existing metrics. Benchmark page:
https://github.com/llyx97/FETV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanxin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1&quot;&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1&quot;&gt;Rundong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sishuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Lu Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01815">
<title>Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields. (arXiv:2311.01815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01815</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods based on Neural Radiance Fields (NeRF) significantly lack the
capacity to quantify uncertainty in their predictions, particularly on the
unseen space including the occluded and outside scene content. This limitation
hinders their extensive applications in robotics, where the reliability of
model predictions has to be considered for tasks such as robotic exploration
and planning in unknown environments. To address this, we propose a novel
approach to estimate a 3D Uncertainty Field based on the learned incomplete
scene geometry, which explicitly identifies these unseen regions. By
considering the accumulated transmittance along each camera ray, our
Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for
rays directly casting towards occluded or outside the scene content. To
quantify the uncertainty on the learned surface, we model a stochastic radiance
field. Our experiments demonstrate that our approach is the only one that can
explicitly reason about high uncertainty both on 3D unseen regions and its
involved 2D rendered pixels, compared with recent methods. Furthermore, we
illustrate that our designed uncertainty field is ideally suited for real-world
robotics tasks, such as next-best-view selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianxiong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1&quot;&gt;Ruijie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1&quot;&gt;Adria Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01823">
<title>Multi-LiDAR Localization and Mapping Pipeline for Urban Autonomous Driving. (arXiv:2311.01823v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01823</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles require accurate and robust localization and mapping
algorithms to navigate safely and reliably in urban environments. We present a
novel sensor fusion-based pipeline for offline mapping and online localization
based on LiDAR sensors. The proposed approach leverages four LiDAR sensors.
Mapping and localization algorithms are based on the KISS-ICP, enabling
real-time performance and high accuracy. We introduce an approach to generate
semantic maps for driving tasks such as path planning. The presented pipeline
is integrated into the ROS 2 based Autoware software stack, providing a robust
and flexible environment for autonomous driving applications. We show that our
pipeline outperforms state-of-the-art approaches for a given research vehicle
and real-world autonomous driving application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sauerbeck_F/0/1/0/all/0/1&quot;&gt;Florian Sauerbeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulmer_D/0/1/0/all/0/1&quot;&gt;Dominik Kulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pielmeier_M/0/1/0/all/0/1&quot;&gt;Markus Pielmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitenstern_M/0/1/0/all/0/1&quot;&gt;Maximilian Leitenstern&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_C/0/1/0/all/0/1&quot;&gt;Christoph Wei&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betz_J/0/1/0/all/0/1&quot;&gt;Johannes Betz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01851">
<title>Holistic Representation Learning for Multitask Trajectory Anomaly Detection. (arXiv:2311.01851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01851</link>
<description rdf:parseType="Literal">&lt;p&gt;Video anomaly detection deals with the recognition of abnormal events in
videos. Apart from the visual signal, video anomaly detection has also been
addressed with the use of skeleton sequences. We propose a holistic
representation of skeleton trajectories to learn expected motions across
segments at different times. Our approach uses multitask learning to
reconstruct any continuous unobserved temporal segment of the trajectory
allowing the extrapolation of past or future segments and the interpolation of
in-between segments. We use an end-to-end attention-based encoder-decoder. We
encode temporally occluded trajectories, jointly learn latent representations
of the occluded segments, and reconstruct trajectories based on expected
motions across different temporal segments. Extensive experiments on three
trajectory-based video anomaly detection datasets show the advantages and
effectiveness of our approach with state-of-the-art results on anomaly
detection in skeleton trajectories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stergiou_A/0/1/0/all/0/1&quot;&gt;Alexandros Stergiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerdt_B/0/1/0/all/0/1&quot;&gt;Brent De Weerdt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1&quot;&gt;Nikos Deligiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01854">
<title>An Ensemble Machine Learning Approach for Screening Covid-19 based on Urine Parameters. (arXiv:2311.01854v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01854</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid spread of COVID-19 and the emergence of new variants underscore the
importance of effective screening measures. Rapid diagnosis and subsequent
quarantine of infected individuals can prevent further spread of the virus in
society. While PCR tests are the gold standard for COVID-19 diagnosis, they are
costly and time-consuming. In contrast, urine test strips are an inexpensive,
non-invasive, and rapidly obtainable screening method that can provide
important information about a patient&apos;s health status. In this study, we
collected a new dataset and used the RGB (Red Green Blue) color space of urine
test strips parameters to detect the health status of individuals. To improve
the accuracy of our model, we converted the RGB space to 10 additional color
spaces. After evaluating four different machine learning models, we proposed a
new ensemble model based on a multi-layer perceptron neural network. Although
the initial results were not strong, we were able to improve the model&apos;s
screening performance for COVID-19 by removing uncertain regions of the model
space. Ultimately, our model achieved a screening accuracy of 80% based on
urine parameters. Our results suggest that urine test strips can be a useful
tool for COVID-19 screening, particularly in resource-constrained settings
where PCR testing may not be feasible. Further research is needed to validate
our findings and explore the potential role of urine test strips in COVID-19
diagnosis and management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moayedi_B/0/1/0/all/0/1&quot;&gt;Behzad Moayedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Keramatfar_A/0/1/0/all/0/1&quot;&gt;Abdalsamad Keramatfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goldani_M/0/1/0/all/0/1&quot;&gt;Mohammad Hadi Goldani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fallahi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Fallahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jahangirisisakht_A/0/1/0/all/0/1&quot;&gt;Alborz Jahangirisisakht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saboori_M/0/1/0/all/0/1&quot;&gt;Mohammad Saboori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+badiei_L/0/1/0/all/0/1&quot;&gt;Leyla badiei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01886">
<title>Bridging the Gap between Multi-focus and Multi-modal: A Focused Integration Framework for Multi-modal Image Fusion. (arXiv:2311.01886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01886</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal image fusion (MMIF) integrates valuable information from
different modality images into a fused one. However, the fusion of multiple
visible images with different focal regions and infrared images is a
unprecedented challenge in real MMIF applications. This is because of the
limited depth of the focus of visible optical lenses, which impedes the
simultaneous capture of the focal information within the same scene. To address
this issue, in this paper, we propose a MMIF framework for joint focused
integration and modalities information extraction. Specifically, a
semi-sparsity-based smoothing filter is introduced to decompose the images into
structure and texture components. Subsequently, a novel multi-scale operator is
proposed to fuse the texture components, capable of detecting significant
information by considering the pixel focus attributes and relevant data from
various modal images. Additionally, to achieve an effective capture of scene
luminance and reasonable contrast maintenance, we consider the distribution of
energy information in the structural components in terms of multi-directional
frequency variance and information entropy. Extensive experiments on existing
MMIF datasets, as well as the object detection and depth estimation tasks,
consistently demonstrate that the proposed algorithm can surpass the
state-of-the-art methods in visual perception and quantitative evaluation. The
code is available at https://github.com/ixilai/MFIF-MMIF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xilai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaosong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiaoqi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Haishu Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01894">
<title>Simulation of acquisition shifts in T2 Flair MR images to stress test AI segmentation networks. (arXiv:2311.01894v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01894</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: To provide a simulation framework for routine neuroimaging test
data, which allows for &quot;stress testing&quot; of deep segmentation networks against
acquisition shifts that commonly occur in clinical practice for T2 weighted
(T2w) fluid attenuated inversion recovery (FLAIR) Magnetic Resonance Imaging
(MRI) protocols.
&lt;/p&gt;
&lt;p&gt;Approach: The approach simulates &quot;acquisition shift derivatives&quot; of MR images
based on MR signal equations. Experiments comprise the validation of the
simulated images by real MR scans and example stress tests on state-of-the-art
MS lesion segmentation networks to explore a generic model function to describe
the F1 score in dependence of the contrast-affecting sequence parameters echo
time (TE) and inversion time (TI).
&lt;/p&gt;
&lt;p&gt;Results: The differences between real and simulated images range up to 19 %
in gray and white matter for extreme parameter settings. For the segmentation
networks under test the F1 score dependency on TE and TI can be well described
by quadratic model functions (R^2 &amp;gt; 0.9). The coefficients of the model
functions indicate that changes of TE have more influence on the model
performance than TI.
&lt;/p&gt;
&lt;p&gt;Conclusions: We show that these deviations are in the range of values as may
be caused by erroneous or individual differences of relaxation times as
described by literature. The coefficients of the F1 model function allow for
quantitative comparison of the influences of TE and TI. Limitations arise
mainly from tissues with the low baseline signal (like CSF) and when the
protocol contains contrast-affecting measures that cannot be modelled due to
missing information in the DICOM header.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Posselt_C/0/1/0/all/0/1&quot;&gt;Christiane Posselt&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avci_M/0/1/0/all/0/1&quot;&gt;Mehmet Yigit Avci&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yigitsoy_M/0/1/0/all/0/1&quot;&gt;Mehmet Yigitsoy&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schunke_P/0/1/0/all/0/1&quot;&gt;Patrick Sch&amp;#xfc;nke&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kolbitsch_C/0/1/0/all/0/1&quot;&gt;Christoph Kolbitsch&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaffter_T/0/1/0/all/0/1&quot;&gt;Tobias Sch&amp;#xe4;ffter&lt;/a&gt; (3 and 4), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Remmele_S/0/1/0/all/0/1&quot;&gt;Stefanie Remmele&lt;/a&gt; (1) ((1) University of Applied Sciences, Faculty of Electrical and Industrial Engineering, Am Lurzenhof 1, Landshut, Germany, (2) deepc GmbH, Blumenstrasse 28, 80331 Munich, Germany, (3) Physikalisch Technische Bundesanstalt, Abbestrasse 2-12, 10587 Berlin, Germany, (4) Technical University of Berlin, Department of Medical Engineering, Dovestrasse 6, Berlin, Germany)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01905">
<title>From Chaos to Calibration: A Geometric Mutual Information Approach to Target-Free Camera LiDAR Extrinsic Calibration. (arXiv:2311.01905v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01905</link>
<description rdf:parseType="Literal">&lt;p&gt;Sensor fusion is vital for the safe and robust operation of autonomous
vehicles. Accurate extrinsic sensor to sensor calibration is necessary to
accurately fuse multiple sensor&apos;s data in a common spatial reference frame. In
this paper, we propose a target free extrinsic calibration algorithm that
requires no ground truth training data, artificially constrained motion
trajectories, hand engineered features or offline optimization and that is
accurate, precise and extremely robust to initialization error.
&lt;/p&gt;
&lt;p&gt;Most current research on online camera-LiDAR extrinsic calibration requires
ground truth training data which is impossible to capture at scale. We revisit
analytical mutual information based methods first proposed in 2012 and
demonstrate that geometric features provide a robust information metric for
camera-LiDAR extrinsic calibration. We demonstrate our proposed improvement
using the KITTI and KITTI-360 fisheye data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borer_J/0/1/0/all/0/1&quot;&gt;Jack Borer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschirner_J/0/1/0/all/0/1&quot;&gt;Jeremy Tschirner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olsner_F/0/1/0/all/0/1&quot;&gt;Florian &amp;#xd6;lsner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milz_S/0/1/0/all/0/1&quot;&gt;Stefan Milz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01908">
<title>LLM-driven Multimodal Target Volume Contouring in Radiation Oncology. (arXiv:2311.01908v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01908</link>
<description rdf:parseType="Literal">&lt;p&gt;Target volume contouring for radiation therapy is considered significantly
more challenging than the normal organ segmentation tasks as it necessitates
the utilization of both image and text-based clinical information. Inspired by
the recent advancement of large language models (LLMs) that can facilitate the
integration of the textural information and images, here we present a novel
LLM-driven multi-modal AI that utilizes the clinical text information and is
applicable to the challenging task of target volume contouring for radiation
therapy, and validate it within the context of breast cancer radiation therapy
target volume contouring. Using external validation and data-insufficient
environments, which attributes highly conducive to real-world applications, we
demonstrate that the proposed model exhibits markedly improved performance
compared to conventional vision-only AI models, particularly exhibiting robust
generalization performance and data-efficiency. To our best knowledge, this is
the first LLM-driven multimodal AI model that integrates the clinical text
information into target volume delineation for radiation oncology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Yujin Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangjoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hwa Kyung Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Sung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01912">
<title>End-to-End assessment of AR-assisted neurosurgery systems. (arXiv:2311.01912v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.01912</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented Reality (AR) has emerged as a significant advancement in surgical
procedures, offering a solution to the challenges posed by traditional
neuronavigation methods. These conventional techniques often necessitate
surgeons to split their focus between the surgical site and a separate monitor
that displays guiding images. Over the years, many systems have been developed
to register and track the hologram at the targeted locations, each employed its
own evaluation technique. On the other hand, hologram displacement measurement
is not a straightforward task because of various factors such as occlusion,
Vengence-Accomodation Conflict, and unstable holograms in space. In this study,
we explore and classify different techniques for assessing an AR-assisted
neurosurgery system and propose a new technique to systematize the assessment
procedure. Moreover, we conduct a deeper investigation to assess surgeon error
in the pre- and intra-operative phases of the surgery based on the respective
feedback given. We found that although the system can undergo registration and
tracking errors, physical feedback can significantly reduce the error caused by
hologram displacement. However, the lack of visual feedback on the hologram
does not have a significant effect on the user 3D perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagheri_M/0/1/0/all/0/1&quot;&gt;Mahdi Bagheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piri_F/0/1/0/all/0/1&quot;&gt;Farhad Piri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Digale_H/0/1/0/all/0/1&quot;&gt;Hadi Digale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattarzadeh_S/0/1/0/all/0/1&quot;&gt;Saem Sattarzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01916">
<title>Contrast-Agnostic Groupwise Registration by Robust PCA for Quantitative Cardiac MRI. (arXiv:2311.01916v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01916</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantitative cardiac magnetic resonance imaging (MRI) is an increasingly
important diagnostic tool for cardiovascular diseases. Yet, co-registration of
all baseline images within the quantitative MRI sequence is essential for the
accuracy and precision of quantitative maps. However, co-registering all
baseline images from a quantitative cardiac MRI sequence remains a nontrivial
task because of the simultaneous changes in intensity and contrast, in
combination with cardiac and respiratory motion. To address the challenge, we
propose a novel motion correction framework based on robust principle component
analysis (rPCA) that decomposes quantitative cardiac MRI into low-rank and
sparse components, and we integrate the groupwise CNN-based registration
backbone within the rPCA framework. The low-rank component of rPCA corresponds
to the quantitative mapping (i.e. limited degree of freedom in variation),
while the sparse component corresponds to the residual motion, making it easier
to formulate and solve the groupwise registration problem. We evaluated our
proposed method on cardiac T1 mapping by the modified Look-Locker inversion
recovery (MOLLI) sequence, both before and after the Gadolinium contrast agent
administration. Our experiments showed that our method effectively improved
registration performance over baseline methods without introducing rPCA, and
reduced quantitative mapping error in both in-domain (pre-contrast MOLLI) and
out-of-domain (post-contrast MOLLI) inference. The proposed rPCA framework is
generic and can be integrated with other registration backbones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yidong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gemert_J/0/1/0/all/0/1&quot;&gt;Jan van Gemert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_Q/0/1/0/all/0/1&quot;&gt;Qian Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01929">
<title>ProS: Facial Omni-Representation Learning via Prototype-based Self-Distillation. (arXiv:2311.01929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach, called Prototype-based
Self-Distillation (ProS), for unsupervised face representation learning. The
existing supervised methods heavily rely on a large amount of annotated
training facial data, which poses challenges in terms of data collection and
privacy concerns. To address these issues, we propose ProS, which leverages a
vast collection of unlabeled face images to learn a comprehensive facial
omni-representation. In particular, ProS consists of two vision-transformers
(teacher and student models) that are trained with different augmented images
(cropping, blurring, coloring, etc.). Besides, we build a face-aware retrieval
system along with augmentations to obtain the curated images comprising
predominantly facial areas. To enhance the discrimination of learned features,
we introduce a prototype-based matching loss that aligns the similarity
distributions between features (teacher or student) and a set of learnable
prototypes. After pre-training, the teacher vision transformer serves as a
backbone for downstream tasks, including attribute estimation, expression
recognition, and landmark alignment, achieved through simple fine-tuning with
additional layers. Extensive experiments demonstrate that our method achieves
state-of-the-art performance on various tasks, both in full and few-shot
settings. Furthermore, we investigate pre-training with synthetic face images,
and ProS exhibits promising performance in this scenario as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xing Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yiyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01961">
<title>Assessing Fidelity in XAI post-hoc techniques: A Comparative Study with Ground Truth Explanations Datasets. (arXiv:2311.01961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01961</link>
<description rdf:parseType="Literal">&lt;p&gt;The evaluation of the fidelity of eXplainable Artificial Intelligence (XAI)
methods to their underlying models is a challenging task, primarily due to the
absence of a ground truth for explanations. However, assessing fidelity is a
necessary step for ensuring a correct XAI methodology. In this study, we
conduct a fair and objective comparison of the current state-of-the-art XAI
methods by introducing three novel image datasets with reliable ground truth
for explanations. The primary objective of this comparison is to identify
methods with low fidelity and eliminate them from further research, thereby
promoting the development of more trustworthy and effective XAI techniques. Our
results demonstrate that XAI methods based on the backpropagation of output
information to input yield higher accuracy and reliability compared to methods
relying on sensitivity analysis or Class Activation Maps (CAM). However, the
backpropagation method tends to generate more noisy saliency maps. These
findings have significant implications for the advancement of XAI methods,
enabling the elimination of erroneous explanations and fostering the
development of more robust and reliable XAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1&quot;&gt;M. Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1&quot;&gt;A. Jaume-i-Cap&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_Alcover_G/0/1/0/all/0/1&quot;&gt;G. Moy&amp;#xe0;-Alcover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01966">
<title>Depth-guided Free-space Segmentation for a Mobile Robot. (arXiv:2311.01966v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.01966</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate indoor free-space segmentation is a challenging task due to the
complexity and the dynamic nature that indoor environments exhibit. We propose
an indoors free-space segmentation method that associates large depth values
with navigable regions. Our method leverages an unsupervised masking technique
that, using positive instances, generates segmentation labels based on textural
homogeneity and depth uniformity. Moreover, we generate superpixels
corresponding to areas of higher depth and align them with features extracted
from a Dense Prediction Transformer (DPT). Using the estimated free-space masks
and the DPT feature representation, a SegFormer model is fine-tuned on our
custom-collected indoor dataset. Our experiments demonstrate sufficient
performance in intricate scenarios characterized by cluttered obstacles and
challenging identification of free space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sevastopoulos_C/0/1/0/all/0/1&quot;&gt;Christos Sevastopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_J/0/1/0/all/0/1&quot;&gt;Joey Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantopoulos_S/0/1/0/all/0/1&quot;&gt;Stasinos Konstantopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkaletsis_V/0/1/0/all/0/1&quot;&gt;Vangelis Karkaletsis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1&quot;&gt;Fillia Makedon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01984">
<title>Optimal Image Transport on Sparse Dictionaries. (arXiv:2311.01984v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we derive a novel optimal image transport algorithm over
sparse dictionaries by taking advantage of Sparse Representation (SR) and
Optimal Transport (OT). Concisely, we design a unified optimization framework
in which the individual image features (color, textures, styles, etc.) are
encoded using sparse representation compactly, and an optimal transport plan is
then inferred between two learned dictionaries in accordance with the encoding
process. This paradigm gives rise to a simple but effective way for
simultaneous image representation and transformation, which is also empirically
solvable because of the moderate size of sparse coding and optimal transport
sub-problems. We demonstrate its versatility and many benefits to different
image-to-image translation tasks, in particular image color transform and
artistic style transfer, and show the plausible results for photo-realistic
transferred effects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junqing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haihui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiermann_A/0/1/0/all/0/1&quot;&gt;Andreas Weiermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1&quot;&gt;Michael Ruzhansky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01989">
<title>Leveraging Large-Scale Pretrained Vision Foundation Models for Label-Efficient 3D Point Cloud Segmentation. (arXiv:2311.01989v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.01989</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, large-scale pre-trained models such as Segment-Anything Model (SAM)
and Contrastive Language-Image Pre-training (CLIP) have demonstrated remarkable
success and revolutionized the field of computer vision. These foundation
vision models effectively capture knowledge from a large-scale broad data with
their vast model parameters, enabling them to perform zero-shot segmentation on
previously unseen data without additional training. While they showcase
competence in 2D tasks, their potential for enhancing 3D scene understanding
remains relatively unexplored. To this end, we present a novel framework that
adapts various foundational models for the 3D point cloud segmentation task.
Our approach involves making initial predictions of 2D semantic masks using
different large vision models. We then project these mask predictions from
various frames of RGB-D video sequences into 3D space. To generate robust 3D
semantic pseudo labels, we introduce a semantic label fusion strategy that
effectively combines all the results via voting. We examine diverse scenarios,
like zero-shot learning and limited guidance from sparse 2D point labels, to
assess the pros and cons of different vision foundation models. Our approach is
experimented on ScanNet dataset for 3D indoor scenes, and the results
demonstrate the effectiveness of adopting general 2D foundation models on
solving 3D point cloud segmentation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shichao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01996">
<title>Detection of keratoconus Diseases using deep Learning. (arXiv:2311.01996v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.01996</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the most serious corneal disorders, keratoconus is difficult to
diagnose in its early stages and can result in blindness. This illness, which
often appears in the second decade of life, affects people of all sexes and
races. Convolutional neural networks (CNNs), one of the deep learning
approaches, have recently come to light as particularly promising tools for the
accurate and timely diagnosis of keratoconus. The purpose of this study was to
evaluate how well different D-CNN models identified keratoconus-related
diseases. To be more precise, we compared five different CNN-based deep
learning architectures (DenseNet201, InceptionV3, MobileNetV2, VGG19,
Xception). In our comprehensive experimental analysis, the DenseNet201-based
model performed very well in keratoconus disease identification in our
extensive experimental research. This model outperformed its D-CNN equivalents,
with an astounding accuracy rate of 89.14% in three crucial classes:
Keratoconus, Normal, and Suspect. The results demonstrate not only the
stability and robustness of the model but also its practical usefulness in
real-world applications for accurate and dependable keratoconus identification.
In addition, D-CNN DenseNet201 performs extraordinarily well in terms of
precision, recall rates, and F1 scores in addition to accuracy. These measures
validate the model&apos;s usefulness as an effective diagnostic tool by highlighting
its capacity to reliably detect instances of keratoconus and to reduce false
positives and negatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haque_A/0/1/0/all/0/1&quot;&gt;AKM Enzam-Ul Haque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rabbany_G/0/1/0/all/0/1&quot;&gt;Golam Rabbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siam_M/0/1/0/all/0/1&quot;&gt;Md. Siam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02003">
<title>A Structured Pruning Algorithm for Model-based Deep Learning. (arXiv:2311.02003v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a growing interest in model-based deep learning (MBDL) for solving
imaging inverse problems. MBDL networks can be seen as iterative algorithms
that estimate the desired image using a physical measurement model and a
learned image prior specified using a convolutional neural net (CNNs). The
iterative nature of MBDL networks increases the test-time computational
complexity, which limits their applicability in certain large-scale
applications. We address this issue by presenting structured pruning algorithm
for model-based deep learning (SPADE) as the first structured pruning algorithm
for MBDL networks. SPADE reduces the computational complexity of CNNs used
within MBDL networks by pruning its non-essential weights. We propose three
distinct strategies to fine-tune the pruned MBDL networks to minimize the
performance loss. Each fine-tuning strategy has a unique benefit that depends
on the presence of a pre-trained model and a high-quality ground truth. We
validate SPADE on two distinct inverse problems, namely compressed sensing MRI
and image super-resolution. Our results highlight that MBDL models pruned by
SPADE can achieve substantial speed up in testing time while maintaining
competitive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chicago Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Weijie Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_Z/0/1/0/all/0/1&quot;&gt;Zihao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhixin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1&quot;&gt;Ulugbek S. Kamilov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02007">
<title>Towards Unsupervised Object Detection From LiDAR Point Clouds. (arXiv:2311.02007v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02007</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of unsupervised object detection from 3D
point clouds in self-driving scenes. We present a simple yet effective method
that exploits (i) point clustering in near-range areas where the point clouds
are dense, (ii) temporal consistency to filter out noisy unsupervised
detections, (iii) translation equivariance of CNNs to extend the auto-labels to
long range, and (iv) self-supervision for improving on its own. Our approach,
OYSTER (Object Discovery via Spatio-Temporal Refinement), does not impose
constraints on data collection (such as repeated traversals of the same
location), is able to detect objects in a zero-shot manner without supervised
finetuning (even in sparse, distant regions), and continues to self-improve
given more rounds of iterative self-training. To better measure model
performance in self-driving scenarios, we propose a new planning-centric
perception metric based on distance-to-collision. We demonstrate that our
unsupervised object detector significantly outperforms unsupervised baselines
on PandaSet and Argoverse 2 Sensor dataset, showing promise that
self-supervision combined with object priors can enable object discovery in the
wild. For more information, visit the project website:
https://waabi.ai/research/oyster
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lunjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Anqi Joyce Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casas_S/0/1/0/all/0/1&quot;&gt;Sergio Casas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengye Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1&quot;&gt;Raquel Urtasun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02018">
<title>Active Reasoning in an Open-World Environment. (arXiv:2311.02018v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.02018</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in vision-language learning have achieved notable success on
complete-information question-answering datasets through the integration of
extensive world knowledge. Yet, most models operate passively, responding to
questions based on pre-stored knowledge. In stark contrast, humans possess the
ability to actively explore, accumulate, and reason using both newfound and
existing information to tackle incomplete-information questions. In response to
this gap, we introduce $Conan$, an interactive open-world environment devised
for the assessment of active reasoning. $Conan$ facilitates active exploration
and promotes multi-round abductive inference, reminiscent of rich, open-world
settings like Minecraft. Diverging from previous works that lean primarily on
single-round deduction via instruction following, $Conan$ compels agents to
actively interact with their surroundings, amalgamating new evidence with prior
knowledge to elucidate events from incomplete observations. Our analysis on
$Conan$ underscores the shortcomings of contemporary state-of-the-art models in
active exploration and understanding complex scenarios. Additionally, we
explore Abduction from Deduction, where agents harness Bayesian rules to recast
the challenge of abduction as a deductive process. Through $Conan$, we aim to
galvanize advancements in active reasoning and set the stage for the next
generation of artificial intelligence agents adept at dynamically engaging in
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Manjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guangyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02044">
<title>Occlusion-Aware 2D and 3D Centerline Detection for Urban Driving via Automatic Label Generation. (arXiv:2311.02044v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02044</link>
<description rdf:parseType="Literal">&lt;p&gt;This research work seeks to explore and identify strategies that can
determine road topology information in 2D and 3D under highly dynamic urban
driving scenarios. To facilitate this exploration, we introduce a substantial
dataset comprising nearly one million automatically labeled data frames. A key
contribution of our research lies in developing an automatic label-generation
process and an occlusion handling strategy. This strategy is designed to model
a wide range of occlusion scenarios, from mild disruptions to severe blockages.
Furthermore, we present a comprehensive ablation study wherein multiple
centerline detection methods are developed and evaluated. This analysis not
only benchmarks the performance of various approaches but also provides
valuable insights into the interpretability of these methods. Finally, we
demonstrate the practicality of our methods and assess their adaptability
across different sensor configurations, highlighting their versatility and
relevance in real-world scenarios. Our dataset and experimental models are
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_D/0/1/0/all/0/1&quot;&gt;David Paz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranganatha_N/0/1/0/all/0/1&quot;&gt;Narayanan E. Ranganatha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_S/0/1/0/all/0/1&quot;&gt;Srinidhi K. Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunchao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_H/0/1/0/all/0/1&quot;&gt;Henrik I. Christensen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02058">
<title>LOTUS: Continual Imitation Learning for Robot Manipulation Through Unsupervised Skill Discovery. (arXiv:2311.02058v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.02058</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce LOTUS, a continual imitation learning algorithm that empowers a
physical robot to continuously and efficiently learn to solve new manipulation
tasks throughout its lifespan. The core idea behind LOTUS is constructing an
ever-growing skill library from a sequence of new tasks with a small number of
human demonstrations. LOTUS starts with a continual skill discovery process
using an open-vocabulary vision model, which extracts skills as recurring
patterns presented in unsegmented demonstrations. Continual skill discovery
updates existing skills to avoid catastrophic forgetting of previous tasks and
adds new skills to solve novel tasks. LOTUS trains a meta-controller that
flexibly composes various skills to tackle vision-based manipulation tasks in
the lifelong learning process. Our comprehensive experiments show that LOTUS
outperforms state-of-the-art baselines by over 11% in success rate, showing its
superior knowledge transfer ability compared to prior methods. More results and
videos can be found on the project website:
https://ut-austin-rpl.github.io/Lotus/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Weikang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yifeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rutav Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02072">
<title>Learning Historical Status Prompt for Accurate and Robust Visual Tracking. (arXiv:2311.02072v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02072</link>
<description rdf:parseType="Literal">&lt;p&gt;Most trackers perform template and search region similarity matching to find
the most similar object to the template during tracking. However, they struggle
to make prediction when the target appearance changes due to the limited
historical information introduced by roughly cropping the current search region
based on the predicted result of previous frame. In this paper, we identify
that the central impediment to improving the performance of existing trackers
is the incapacity to integrate abundant and effective historical information.
To address this issue, we propose a Historical Information Prompter (HIP) to
enhance the provision of historical information. We also build HIPTrack upon
HIP module. HIP is a plug-and-play module that make full use of search region
features to introduce historical appearance information. It also incorporates
historical position information by constructing refined mask of the target. HIP
is a lightweight module to generate historical information prompts. By
integrating historical information prompts, HIPTrack significantly enhances the
tracking performance without the need to retrain the backbone. Experimental
results demonstrate that our method outperforms all state-of-the-art approaches
on LaSOT, LaSOT ext, GOT10k and NfS. Futhermore, HIP module exhibits strong
generality and can be seamlessly integrated into trackers to improve tracking
performance. The source code and models will be released for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wenrui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02077">
<title>EmerNeRF: Emergent Spatial-Temporal Scene Decomposition via Self-Supervision. (arXiv:2311.02077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.02077</link>
<description rdf:parseType="Literal">&lt;p&gt;We present EmerNeRF, a simple yet powerful approach for learning
spatial-temporal representations of dynamic driving scenes. Grounded in neural
fields, EmerNeRF simultaneously captures scene geometry, appearance, motion,
and semantics via self-bootstrapping. EmerNeRF hinges upon two core components:
First, it stratifies scenes into static and dynamic fields. This decomposition
emerges purely from self-supervision, enabling our model to learn from general,
in-the-wild data sources. Second, EmerNeRF parameterizes an induced flow field
from the dynamic field and uses this flow field to further aggregate
multi-frame features, amplifying the rendering precision of dynamic objects.
Coupling these three fields (static, dynamic, and flow) enables EmerNeRF to
represent highly-dynamic scenes self-sufficiently, without relying on ground
truth object annotations or pre-trained models for dynamic object segmentation
or optical flow estimation. Our method achieves state-of-the-art performance in
sensor simulation, significantly outperforming previous methods when
reconstructing static (+2.93 PSNR) and dynamic (+3.70 PSNR) scenes. In
addition, to bolster EmerNeRF&apos;s semantic generalization, we lift 2D visual
foundation model features into 4D space-time and address a general positional
bias in modern Transformers, significantly boosting 3D perception performance
(e.g., 37.50% relative improvement in occupancy prediction accuracy on
average). Finally, we construct a diverse and challenging 120-sequence dataset
to benchmark neural fields under extreme and highly-dynamic settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiawei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1&quot;&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1&quot;&gt;Or Litany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xinshuo Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Wook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_T/0/1/0/all/0/1&quot;&gt;Tong Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Danfei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1&quot;&gt;Sanja Fidler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.10629">
<title>CSLNSpeech: solving extended speech separation problem with the help of Chinese sign language. (arXiv:2007.10629v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2007.10629</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous audio-visual speech separation methods use the synchronization of
the speaker&apos;s facial movement and speech in the video to supervise the speech
separation in a self-supervised way. In this paper, we propose a model to solve
the speech separation problem assisted by both face and sign language, which we
call the extended speech separation problem. We design a general deep learning
network for learning the combination of three modalities, audio, face, and sign
language information, for better solving the speech separation problem. To
train the model, we introduce a large-scale dataset named the Chinese Sign
Language News Speech (CSLNSpeech) dataset, in which three modalities of audio,
face, and sign language coexist. Experiment results show that the proposed
model has better performance and robustness than the usual audio-visual system.
Besides, sign language modality can also be used alone to supervise speech
separation tasks, and the introduction of sign language is helpful for
hearing-impaired people to learn and communicate. Last, our model is a general
speech separation framework and can achieve very competitive separation
performance on two open-source audio-visual datasets. The code is available at
https://github.com/iveveive/SLNSpeech
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiasong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taotao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanman Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Youyong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Senhadji_L/0/1/0/all/0/1&quot;&gt;Lotfi Senhadji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shu_H/0/1/0/all/0/1&quot;&gt;Huazhong Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01466">
<title>Recognition of Unseen Bird Species by Learning from Field Guides. (arXiv:2206.01466v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01466</link>
<description rdf:parseType="Literal">&lt;p&gt;We exploit field guides to learn bird species recognition, in particular
zero-shot recognition of unseen species. Illustrations contained in field
guides deliberately focus on discriminative properties of each species, and can
serve as side information to transfer knowledge from seen to unseen bird
species. We study two approaches: (1) a contrastive encoding of illustrations,
which can be fed into standard zero-shot learning schemes; and (2) a novel
method that leverages the fact that illustrations are also images and as such
structurally more similar to photographs than other kinds of side information.
Our results show that illustrations from field guides, which are readily
available for a wide range of species, are indeed a competitive source of side
information for zero-shot learning. On a subset of the iNaturalist2021 dataset
with 749 seen and 739 unseen species, we obtain a classification accuracy of
unseen bird species of $12\%$ @top-1 and $38\%$ @top-10, which shows the
potential of field guides for challenging real-world scenarios with many
species. Our code is available at https://github.com/ac-rodriguez/zsl_billow
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s C. Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1&quot;&gt;Stefano D&amp;#x27;Aronco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daudt_R/0/1/0/all/0/1&quot;&gt;Rodrigo Caye Daudt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1&quot;&gt;Jan D. Wegner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02205">
<title>Clustered Saliency Prediction. (arXiv:2207.02205v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02205</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new method for image salience prediction, Clustered Saliency
Prediction. This method divides subjects into clusters based on their personal
features and their known saliency maps, and generates an image salience model
conditioned on the cluster label. We test our approach on a public dataset of
personalized saliency maps and cluster the subjects using selected importance
weights for personal feature factors. We propose the Multi-Domain Saliency
Translation model which uses image stimuli and universal saliency maps to
predict saliency maps for each cluster. For obtaining universal saliency maps,
we applied various state-of-the-art methods, DeepGaze IIE, ML-Net and SalGAN,
and compared their effectiveness in our system. We show that our Clustered
Saliency Prediction technique outperforms the universal saliency prediction
models. Also, we demonstrate the effectiveness of our clustering method by
comparing the results of Clustered Saliency Prediction using clusters obtained
by our algorithm with some baseline methods. Finally, we propose an approach to
assign new people to their most appropriate cluster and prove its usefulness in
the experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherkati_R/0/1/0/all/0/1&quot;&gt;Rezvan Sherkati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;James J. Clark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04873">
<title>Multimodal Prototype-Enhanced Network for Few-Shot Action Recognition. (arXiv:2212.04873v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04873</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods for few-shot action recognition mainly fall into the metric
learning framework following ProtoNet, which demonstrates the importance of
prototypes. Although they achieve relatively good performance, the effect of
multimodal information is ignored, e.g. label texts. In this work, we propose a
novel MultimOdal PRototype-ENhanced Network (MORN), which uses the semantic
information of label texts as multimodal information to enhance prototypes. A
CLIP visual encoder and a frozen CLIP text encoder are introduced to obtain
features with good multimodal initialization. Then in the visual flow, visual
prototypes are computed by a Temporal-Relational CrossTransformer (TRX) module
for example. In the text flow, a semantic-enhanced (SE) module and an inflating
operation are used to obtain text prototypes. The final multimodal prototypes
are then computed by a multimodal prototype-enhanced (MPE) module. Besides, we
define a PRototype SImilarity DiffErence (PRIDE) to evaluate the quality of
prototypes, which is used to verify our improvement on the prototype level and
effectiveness of MORN. We conduct extensive experiments on four popular
datasets, and MORN achieves state-of-the-art results on HMDB51, UCF101,
Kinetics and SSv2. When plugging PRIDE into the training stage, the performance
can be further improved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_X/0/1/0/all/0/1&quot;&gt;Xinzhe Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yatai Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yujiu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01182">
<title>PMT-IQA: Progressive Multi-task Learning for Blind Image Quality Assessment. (arXiv:2301.01182v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01182</link>
<description rdf:parseType="Literal">&lt;p&gt;Blind image quality assessment (BIQA) remains challenging due to the
diversity of distortion and image content variation, which complicate the
distortion patterns crossing different scales and aggravate the difficulty of
the regression problem for BIQA. However, existing BIQA methods often fail to
consider multi-scale distortion patterns and image content, and little research
has been done on learning strategies to make the regression model produce
better performance. In this paper, we propose a simple yet effective
Progressive Multi-Task Image Quality Assessment (PMT-IQA) model, which contains
a multi-scale feature extraction module (MS) and a progressive multi-task
learning module (PMT), to help the model learn complex distortion patterns and
better optimize the regression issue to align with the law of human learning
process from easy to hard. To verify the effectiveness of the proposed PMT-IQA
model, we conduct experiments on four widely used public datasets, and the
experimental results indicate that the performance of PMT-IQA is superior to
the comparison approaches, and both MS and PMT modules improve the model&apos;s
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Qingyi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_N/0/1/0/all/0/1&quot;&gt;Ning Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qingge_L/0/1/0/all/0/1&quot;&gt;Letu Qingge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Pei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08880">
<title>A Large-scale Film Style Dataset for Learning Multi-frequency Driven Film Enhancement. (arXiv:2301.08880v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08880</link>
<description rdf:parseType="Literal">&lt;p&gt;Film, a classic image style, is culturally significant to the whole
photographic industry since it marks the birth of photography. However, film
photography is time-consuming and expensive, necessitating a more efficient
method for collecting film-style photographs. Numerous datasets that have
emerged in the field of image enhancement so far are not film-specific. In
order to facilitate film-based image stylization research, we construct
FilmSet, a large-scale and high-quality film style dataset. Our dataset
includes three different film types and more than 5000 in-the-wild high
resolution images. Inspired by the features of FilmSet images, we propose a
novel framework called FilmNet based on Laplacian Pyramid for stylizing images
across frequency bands and achieving film style outcomes. Experiments reveal
that the performance of our model is superior than state-of-the-art techniques.
The link of code and data is \url{https://github.com/CXH-Research/FilmNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zinuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xuhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1&quot;&gt;Chi-Man Pun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10625">
<title>Navigating the Pitfalls of Active Learning Evaluation: A Systematic Framework for Meaningful Performance Assessment. (arXiv:2301.10625v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10625</link>
<description rdf:parseType="Literal">&lt;p&gt;Active Learning (AL) aims to reduce the labeling burden by interactively
selecting the most informative samples from a pool of unlabeled data. While
there has been extensive research on improving AL query methods in recent
years, some studies have questioned the effectiveness of AL compared to
emerging paradigms such as semi-supervised (Semi-SL) and self-supervised
learning (Self-SL), or a simple optimization of classifier configurations.
Thus, today&apos;s AL literature presents an inconsistent and contradictory
landscape, leaving practitioners uncertain about whether and how to use AL in
their tasks. In this work, we make the case that this inconsistency arises from
a lack of systematic and realistic evaluation of AL methods. Specifically, we
identify five key pitfalls in the current literature that reflect the delicate
considerations required for AL evaluation. Further, we present an evaluation
framework that overcomes these pitfalls and thus enables meaningful statements
about the performance of AL methods. To demonstrate the relevance of our
protocol, we present a large-scale empirical study and benchmark for image
classification spanning various data sets, query methods, AL settings, and
training paradigms. Our findings clarify the inconsistent picture in the
literature and enable us to give hands-on recommendations for practitioners.
The benchmark is hosted at https://github.com/IML-DKFZ/realistic-al .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luth_C/0/1/0/all/0/1&quot;&gt;Carsten T. L&amp;#xfc;th&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bungert_T/0/1/0/all/0/1&quot;&gt;Till J. Bungert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1&quot;&gt;Lukas Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12247">
<title>SEGA: Instructing Text-to-Image Models using Semantic Guidance. (arXiv:2301.12247v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have recently received a lot of interest for
their astonishing ability to produce high-fidelity images from text only.
However, achieving one-shot generation that aligns with the user&apos;s intent is
nearly impossible, yet small changes to the input prompt often result in very
different images. This leaves the user with little semantic control. To put the
user in control, we show how to interact with the diffusion process to flexibly
steer it along semantic directions. This semantic guidance (SEGA) generalizes
to any generative architecture using classifier-free guidance. More
importantly, it allows for subtle and extensive edits, changes in composition
and style, as well as optimizing the overall artistic conception. We
demonstrate SEGA&apos;s effectiveness on both latent and pixel-based diffusion
models such as Stable Diffusion, Paella, and DeepFloyd-IF using a variety of
tasks, thus providing strong evidence for its versatility, flexibility, and
improvements over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10834">
<title>Object-Centric Slot Diffusion. (arXiv:2303.10834v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10834</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of transformer-based image generative models in
object-centric learning highlights the importance of powerful image generators
for handling complex scenes. However, despite the high expressiveness of
diffusion models in image generation, their integration into object-centric
learning remains largely unexplored in this domain. In this paper, we explore
the feasibility and potential of integrating diffusion models into
object-centric learning and investigate the pros and cons of this approach. We
introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes:
it is the first object-centric learning model to replace conventional slot
decoders with a latent diffusion model conditioned on object slots, and it is
also the first unsupervised compositional conditional diffusion model that
operates without the need for supervised annotations like text. Through
experiments on various object-centric tasks, including the first application of
the FFHQ dataset in this field, we demonstrate that LSD significantly
outperforms state-of-the-art transformer-based decoders, particularly in more
complex scenes, and exhibits superior unsupervised compositional generation
quality. In addition, we conduct a preliminary investigation into the
integration of pre-trained diffusion models in LSD and demonstrate its
effectiveness in real-world image segmentation and generation. Project page is
available at https://latentslotdiffusion.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jindong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1&quot;&gt;Fei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gautam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungjin Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03689">
<title>COLA: A Benchmark for Compositional Text-to-image Retrieval. (arXiv:2305.03689v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03689</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional reasoning is a hallmark of human visual intelligence. Yet,
despite the size of large vision-language models, they struggle to represent
simple compositions by combining objects with their attributes. To measure this
lack of compositional capability, we design Cola, a text-to-image retrieval
benchmark to Compose Objects Localized with Attributes. To solve Cola, a model
must retrieve images with the correct configuration of attributes and objects
and avoid choosing a distractor image with the same objects and attributes but
in the wrong configuration. Cola contains about 1.2k composed queries of 168
objects and 197 attributes on around 30K images. Our human evaluation finds
that Cola is 83.33% accurate, similar to contemporary compositionality
benchmarks. Using Cola as a testbed, we explore empirical modeling designs to
adapt pre-trained vision-language models to reason compositionally. We explore
6 adaptation strategies on 2 seminal vision-language models, using
compositionality-centric test benchmarks - Cola and CREPE. We find the optimal
adaptation strategy is to train a multi-modal attention layer that jointly
attends over the frozen pre-trained image and language features. Surprisingly,
training multimodal layers on CLIP performs better than tuning a larger FLAVA
model with already pre-trained multimodal layers. Furthermore, our adaptation
strategy improves CLIP and FLAVA to comparable levels, suggesting that training
multimodal layers using contrastive attribute-object data is key, as opposed to
using them pre-trained. Lastly, we show that Cola is harder than a closely
related contemporary benchmark, CREPE, since simpler fine-tuning strategies
without multimodal layers suffice on CREPE but not on Cola. However, we still
see a significant gap between our best adaptation and human accuracy,
suggesting considerable room for further research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1&quot;&gt;Arijit Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1&quot;&gt;Filip Radenovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Abhimanyu Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08946">
<title>Image Matching by Bare Homography. (arXiv:2305.08946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
&lt;/p&gt;
&lt;p&gt;Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
&lt;/p&gt;
&lt;p&gt;The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
&lt;/p&gt;
&lt;p&gt;In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1&quot;&gt;Fabio Bellavia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11092">
<title>Universal Domain Adaptation from Foundation Models: A Baseline Study. (arXiv:2305.11092v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11092</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models (e.g., CLIP or DINOv2) have shown their impressive learning
and transfer capabilities in a wide range of visual tasks, by training on a
large corpus of data and adapting to specific downstream tasks. It is, however,
interesting that foundation models have not been fully explored for universal
domain adaptation (UniDA), which is to learn models using labeled data in a
source domain and unlabeled data in a target one, such that the learned models
can successfully adapt to the target data. In this paper, we make comprehensive
empirical studies of state-of-the-art UniDA methods using foundation models. We
first observe that, unlike fine-tuning from ImageNet pre-trained models, as
previous methods do, fine-tuning from foundation models yields significantly
poorer results, sometimes even worse than training from scratch. While freezing
the backbones, we demonstrate that although the foundation models greatly
improve the performance of the baseline method that trains the models on the
source data alone, existing UniDA methods generally fail to improve over the
baseline. This suggests that new research efforts are very necessary for UniDA
using foundation models. Based on these findings, we introduce \textit{CLIP
distillation}, a parameter-free method specifically designed to distill target
knowledge from CLIP models. The core of our \textit{CLIP distillation} lies in
a self-calibration technique for automatic temperature scaling, a feature that
significantly enhances the baseline&apos;s out-class detection capability. Although
simple, our method outperforms previous approaches in most benchmark tasks,
excelling in evaluation metrics including H-score/H$^3$-score and the newly
proposed universal classification rate (UCR) metric. We hope that our
investigation and the proposed simple framework can serve as a strong baseline
to facilitate future studies in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Bin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1&quot;&gt;Kui Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13484">
<title>Flover: A Temporal Fusion Framework for Efficient Autoregressive Model Parallel Inference. (arXiv:2305.13484v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13484</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoregressive models, despite their commendable performance in a myriad of
generative tasks, face challenges stemming from their inherently sequential
structure. Inference on these models, by design, harnesses a temporal
dependency, where the current token&apos;s probability distribution is conditioned
on preceding tokens. This inherent characteristic severely impedes
computational efficiency during inference as a typical inference request can
require more than thousands of tokens, where generating each token requires a
load of entire model weights, making the inference more memory-bound. The large
overhead becomes profound in real deployment where requests arrive randomly,
necessitating various generation lengths. Existing solutions, such as dynamic
batching and concurrent instances, introduce significant response delays and
bandwidth contention, falling short of achieving optimal latency and
throughput. To address these shortcomings, we propose Flover -- a temporal
fusion framework for efficiently inferring multiple requests in parallel. We
deconstruct the general generation pipeline into pre-processing and token
generation, and equip the framework with a dedicated work scheduler for fusing
the generation process temporally across all requests. By orchestrating the
token-level parallelism, Flover exhibits optimal hardware efficiency and
significantly spares the system resources. By further employing a fast buffer
reordering algorithm that allows memory eviction of finished tasks, it brings
over 11x inference speedup on GPT and 16x on LLAMA compared to the cutting-edge
solutions provided by NVIDIA FasterTransformer. Crucially, by leveraging the
advanced tensor parallel technique, Flover proves efficacious across diverse
computational landscapes, from single-GPU setups to distributed scenarios,
thereby offering robust performance optimization that adapts to variable use
cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jinghan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alnaasan_N/0/1/0/all/0/1&quot;&gt;Nawras Alnaasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafi_A/0/1/0/all/0/1&quot;&gt;Aamir Shafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramoni_H/0/1/0/all/0/1&quot;&gt;Hari Subramoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+K%2E_D/0/1/0/all/0/1&quot;&gt;Dhabaleswar K.&lt;/a&gt; (DK) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda/0/1/0/all/0/1&quot;&gt;Panda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14022">
<title>Realistic Noise Synthesis with Diffusion Models. (arXiv:2305.14022v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14022</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep image denoising models often rely on large amount of training data for
the high quality performance. However, it is challenging to obtain sufficient
amount of data under real-world scenarios for the supervised training. As such,
synthesizing realistic noise becomes an important solution. However, existing
techniques have limitations in modeling complex noise distributions, resulting
in residual noise and edge artifacts in denoising methods relying on synthetic
data. To overcome these challenges, we propose a novel method that synthesizes
realistic noise using diffusion models, namely Realistic Noise Synthesize
Diffusor (RNSD). In particular, the proposed time-aware controlling module can
simulate various environmental conditions under given camera settings. RNSD can
incorporate guided multiscale content, such that more realistic noise with
spatial correlations can be generated at multiple frequencies. In addition, we
construct an inversion mechanism to predict the unknown camera setting, which
enables the extension of RNSD to datasets without setting information.
Extensive experiments demonstrate that our RNSD method significantly
outperforms the existing methods not only in the synthesized noise under
multiple realism metrics, but also in the single image denoising performances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Mingyan Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Ting Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Bing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14637">
<title>Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering. (arXiv:2305.14637v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14637</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated reverse engineering of HTML/CSS code from UI screenshots is an
important yet challenging problem with broad applications in website
development and design. In this paper, we propose a novel vision-code
transformer (ViCT) composed of a vision encoder processing the screenshots and
a language decoder to generate the code. They are initialized by pre-trained
models such as ViT/DiT and GPT-2/LLaMA but aligning the two modalities requires
end-to-end finetuning, which aims to minimize the visual discrepancy between
the code-rendered webpage and the original screenshot. However, the rendering
is non-differentiable and causes costly overhead. We address this problem by
actor-critic fine-tuning where a visual critic without rendering (ViCR) is
developed to predict visual discrepancy given the original and generated code.
To train and evaluate our models, we created two synthetic datasets of varying
complexity, with over 75,000 unique (code, screenshot) pairs. We evaluate the
UI-to-Code performance using a combination of automated metrics such as MSE,
BLEU, IoU, and a novel htmlBLEU score. ViCT outperforms a strong baseline model
DiT-GPT2, improving IoU from 0.64 to 0.79 and lowering MSE from 12.25 to 9.02.
With much lower computational cost, it can achieve comparable performance as
when using a larger decoder such as LLaMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soselia_D/0/1/0/all/0/1&quot;&gt;Davit Soselia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1&quot;&gt;Khalid Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16397">
<title>Are Diffusion Models Vision-And-Language Reasoners?. (arXiv:2305.16397v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16397</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image generation models have recently shown immense
qualitative success using denoising diffusion processes. However, unlike
discriminative vision-and-language models, it is a non-trivial task to subject
these diffusion-based generative models to automatic fine-grained quantitative
evaluation of high-level phenomena such as compositionality. Towards this goal,
we perform two innovations. First, we transform diffusion-based models (in our
case, Stable Diffusion) for any image-text matching (ITM) task using a novel
method called DiffusionITM. Second, we introduce the Generative-Discriminative
Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language
tasks, bias evaluation and detailed analysis. We find that Stable Diffusion +
DiffusionITM is competitive on many tasks and outperforms CLIP on compositional
tasks like like CLEVR and Winoground. We further boost its compositional
performance with a transfer setup by fine-tuning on MS-COCO while retaining
generative capabilities. We also measure the stereotypical bias in diffusion
models, and find that Stable Diffusion 2.1 is, for the most part, less biased
than Stable Diffusion 1.5. Overall, our results point in an exciting direction
bringing discriminative and generative model evaluation closer. We will release
code and benchmark setup soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krojer_B/0/1/0/all/0/1&quot;&gt;Benno Krojer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poole_Dayan_E/0/1/0/all/0/1&quot;&gt;Elinor Poole-Dayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voleti_V/0/1/0/all/0/1&quot;&gt;Vikram Voleti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1&quot;&gt;Christopher Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18381">
<title>Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18381</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-efficient learning has drawn significant attention, especially given the
current trend of large multi-modal models, where dataset distillation can be an
effective solution. However, the dataset distillation process itself is still
very inefficient. In this work, we model the distillation problem with
reference to information transport. Observing that severe data redundancy
exists in dataset distillation, we argue to put more emphasis on the utility of
the training samples. We propose a family of methods to exploit the most
valuable samples, which is validated by our comprehensive analysis of the
optimal data selection. The new strategy significantly reduces the training
cost and extends a variety of existing distillation algorithms to larger and
more diversified datasets, e.g., in some cases only 0.04% training data is
sufficient for comparable distillation performance. Moreover, our strategy
consistently enhances the performance, which may open up new analyses on the
dynamics of distillation and networks. Our method is able to extend the
distillation algorithms to much larger-scale datasets and more heterogeneous
datasets, e.g., ImageNet-1K and Kinetics-400. Our code is available on
https://github.com/silicx/GoldFromOres.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kaitong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00265">
<title>Doubly Robust Self-Training. (arXiv:2306.00265v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00265</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-training is an important technique for solving semi-supervised learning
problems. It leverages unlabeled data by generating pseudo-labels and combining
them with a limited labeled dataset for training. The effectiveness of
self-training heavily relies on the accuracy of these pseudo-labels. In this
paper, we introduce doubly robust self-training, a novel semi-supervised
algorithm that provably balances between two extremes. When the pseudo-labels
are entirely incorrect, our method reduces to a training process solely using
labeled data. Conversely, when the pseudo-labels are completely accurate, our
method transforms into a training process utilizing all pseudo-labeled data and
labeled data, thus increasing the effective sample size. Through empirical
evaluations on both the ImageNet dataset for image classification and the
nuScenes autonomous driving dataset for 3D object detection, we demonstrate the
superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobson_P/0/1/0/all/0/1&quot;&gt;Philip Jacobson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1&quot;&gt;Michael Jordan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03449">
<title>Universal Semi-supervised Model Adaptation via Collaborative Consistency Training. (arXiv:2307.03449v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03449</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a realistic and challenging domain adaptation
problem called Universal Semi-supervised Model Adaptation (USMA), which i)
requires only a pre-trained source model, ii) allows the source and target
domain to have different label sets, i.e., they share a common label set and
hold their own private label set, and iii) requires only a few labeled samples
in each class of the target domain. To address USMA, we propose a collaborative
consistency training framework that regularizes the prediction consistency
between two models, i.e., a pre-trained source model and its variant
pre-trained with target data only, and combines their complementary strengths
to learn a more powerful model. The rationale of our framework stems from the
observation that the source model performs better on common categories than the
target-only model, while on target-private categories, the target-only model
performs better. We also propose a two-perspective, i.e., sample-wise and
class-wise, consistency regularization to improve the training. Experimental
results demonstrate the effectiveness of our method on several benchmark
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yushuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11957">
<title>High-performance real-world optical computing trained by in situ model-free optimization. (arXiv:2307.11957v3 [physics.optics] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11957</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical computing systems can provide high-speed and low-energy data
processing but face deficiencies in computationally demanding training and
simulation-to-reality gap. We propose a model-free solution for lightweight in
situ optimization of optical computing systems based on the score gradient
estimation algorithm. This approach treats the system as a black box and
back-propagates loss directly to the optical weights&apos; probabilistic
distributions, hence circumventing the need for computation-heavy and biased
system simulation. We demonstrate a superior classification accuracy on the
MNIST and FMNIST datasets through experiments on a single-layer diffractive
optical computing system. Furthermore, we show its potential for image-free and
high-speed cell analysis. The inherent simplicity of our proposed method,
combined with its low demand for computational resources, expedites the
transition of optical computing from laboratory demonstrations to real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guangyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shu_X/0/1/0/all/0/1&quot;&gt;Xin Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05993">
<title>Image-based Geolocalization by Ground-to-2.5D Map Matching. (arXiv:2308.05993v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05993</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the image-based geolocalization problem, aiming to localize
ground-view query images on cartographic maps. Current methods often utilize
cross-view localization techniques to match ground-view query images with 2D
maps. However, the performance of these methods is unsatisfactory due to
significant cross-view appearance differences. In this paper, we lift
cross-view matching to a 2.5D space, where heights of structures (e.g., trees
and buildings) provide geometric information to guide the cross-view matching.
We propose a new approach to learning representative embeddings from
multi-modal data. Specifically, we establish a projection relationship between
2.5D space and 2D aerial-view space. The projection is further used to combine
multi-modal features from the 2.5D and 2D maps using an effective
pixel-to-point fusion method. By encoding crucial geometric cues, our method
learns discriminative location embeddings for matching panoramic images and
maps. Additionally, we construct the first large-scale ground-to-2.5D map
geolocalization dataset to validate our method and facilitate future research.
Both single-image based and route based localization experiments are conducted
to test our method. Extensive experiments demonstrate that the proposed method
achieves significantly higher localization accuracy and faster convergence than
previous 2D map-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiran Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1&quot;&gt;Andrew Calway&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13612">
<title>Is Deep Learning Network Necessary for Image Generation?. (arXiv:2308.13612v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13612</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, images are considered samples from a high-dimensional distribution,
and deep learning has become almost synonymous with image generation. However,
is a deep learning network truly necessary for image generation? In this paper,
we investigate the possibility of image generation without using a deep
learning network, motivated by validating the assumption that images follow a
high-dimensional distribution. Since images are assumed to be samples from such
a distribution, we utilize the Gaussian Mixture Model (GMM) to describe it. In
particular, we employ a recent distribution learning technique named as
Monte-Carlo Marginalization to capture the parameters of the GMM based on image
samples. Moreover, we also use the Singular Value Decomposition (SVD) for
dimensionality reduction to decrease computational complexity. During our
evaluation experiment, we first attempt to model the distribution of image
samples directly to verify the assumption that images truly follow a
distribution. We then use the SVD for dimensionality reduction. The principal
components, rather than raw image data, are used for distribution learning.
Compared to methods relying on deep learning networks, our approach is more
explainable, and its performance is promising. Experiments show that our images
have a lower FID value compared to those generated by variational
auto-encoders, demonstrating the feasibility of image generation without deep
learning networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenqiu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanfang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Anup Basu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16150">
<title>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI. (arXiv:2308.16150v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16150</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised anomaly segmentation aims to detect patterns that are distinct
from any patterns processed during training, commonly called abnormal or
out-of-distribution patterns, without providing any associated manual
segmentations. Since anomalies during deployment can lead to model failure,
detecting the anomaly can enhance the reliability of models, which is valuable
in high-risk domains like medical imaging. This paper introduces Masked
Modality Cycles with Conditional Diffusion (MMCCD), a method that enables
segmentation of anomalies across diverse patterns in multimodal MRI. The method
is based on two fundamental ideas. First, we propose the use of cyclic modality
translation as a mechanism for enabling abnormality detection.
Image-translation models learn tissue-specific modality mappings, which are
characteristic of tissue physiology. Thus, these learned mappings fail to
translate tissues or image patterns that have never been encountered during
training, and the error enables their segmentation. Furthermore, we combine
image translation with a masked conditional diffusion model, which attempts to
`imagine&apos; what tissue exists under a masked area, further exposing unknown
patterns as the generative model fails to recreate them. We evaluate our method
on a proxy task by training on healthy-looking slices of BraTS2021
multi-modality MRIs and testing on slices with tumors. We show that our method
compares favorably to previous unsupervised approaches based on image
reconstruction and denoising with autoencoders and diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Ziyun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anthony_H/0/1/0/all/0/1&quot;&gt;Harry Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Felix Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamnitsas_K/0/1/0/all/0/1&quot;&gt;Konstantinos Kamnitsas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04579">
<title>EGOFALLS: A visual-audio dataset and benchmark for fall detection using egocentric cameras. (arXiv:2309.04579v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04579</link>
<description rdf:parseType="Literal">&lt;p&gt;Falls are significant and often fatal for vulnerable populations such as the
elderly. Previous works have addressed the detection of falls by relying on
data capture by a single sensor, images or accelerometers. In this work, we
rely on multimodal descriptors extracted from videos captured by egocentric
cameras. Our proposed method includes a late decision fusion layer that builds
on top of the extracted descriptors. Furthermore, we collect a new dataset on
which we assess our proposed approach. We believe this is the first public
dataset of its kind. The dataset comprises 10,948 video samples by 14 subjects.
We conducted ablation experiments to assess the performance of individual
feature extractors, fusion of visual information, and fusion of both visual and
audio information. Moreover, we experimented with internal and external
cross-validation. Our results demonstrate that the fusion of audio and visual
information through late decision fusion improves detection performance, making
it a promising tool for fall prevention and mitigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueyi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06884">
<title>Autoencoder-Based Visual Anomaly Localization for Manufacturing Quality Control. (arXiv:2309.06884v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06884</link>
<description rdf:parseType="Literal">&lt;p&gt;Manufacturing industries require efficient and voluminous production of
high-quality finished goods. In the context of Industry 4.0, visual anomaly
detection poses an optimistic solution for automatically controlled product
quality with high precision. In general, automation based on computer vision is
a promising solution to prevent bottlenecks at the product quality checkpoint.
We considered recent advancements in machine learning to improve visual defect
localization, but challenges persist in obtaining a balanced feature set and
database of the wide variety of defects occurring in the production line.
Hence, this paper proposes a defect localizing autoencoder with unsupervised
class selection by clustering with k-means the features extracted from a
pre-trained VGG16 network. Moreover, the selected classes of defects are
augmented with natural wild textures to simulate artificial defects. The study
demonstrates the effectiveness of the defect localizing autoencoder with
unsupervised class selection for improving defect detection in manufacturing
industries. The proposed methodology shows promising results with precise and
accurate localization of quality defects on melamine-faced boards for the
furniture industry. Incorporating artificial defects into the training data
shows significant potential for practical implementation in real-world quality
control scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1&quot;&gt;Devang Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klarmann_N/0/1/0/all/0/1&quot;&gt;Noah Klarmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09599">
<title>MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential Deep Learning. (arXiv:2309.09599v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09599</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in deep learning-based 3D object detection necessitate the
availability of large-scale datasets. However, this requirement introduces the
challenge of manual annotation, which is often both burdensome and
time-consuming. To tackle this issue, the literature has seen the emergence of
several weakly supervised frameworks for 3D object detection which can
automatically generate pseudo labels for unlabeled data. Nevertheless, these
generated pseudo labels contain noise and are not as accurate as those labeled
by humans. In this paper, we present the first approach that addresses the
inherent ambiguities present in pseudo labels by introducing an Evidential Deep
Learning (EDL) based uncertainty estimation framework. Specifically, we propose
MEDL-U, an EDL framework based on MTrans, which not only generates pseudo
labels but also quantifies the associated uncertainties. However, applying EDL
to 3D object detection presents three primary challenges: (1) relatively lower
pseudolabel quality in comparison to other autolabelers; (2) excessively high
evidential uncertainty estimates; and (3) lack of clear interpretability and
effective utilization of uncertainties for downstream tasks. We tackle these
issues through the introduction of an uncertainty-aware IoU-based loss, an
evidence-aware multi-task loss function, and the implementation of a
post-processing stage for uncertainty refinement. Our experimental results
demonstrate that probabilistic detectors trained using the outputs of MEDL-U
surpass deterministic detectors trained using outputs from previous 3D
annotators on the KITTI val set for all difficulty levels. Moreover, MEDL-U
achieves state-of-the-art results on the KITTI official test set compared to
existing 3D automatic annotators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paat_H/0/1/0/all/0/1&quot;&gt;Helbert Paat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1&quot;&gt;Qing Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Weilong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14072">
<title>BoIR: Box-Supervised Instance Representation for Multi-Person Pose Estimation. (arXiv:2309.14072v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14072</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-stage multi-person human pose estimation (MPPE) methods have shown
great performance improvements, but existing methods fail to disentangle
features by individual instances under crowded scenes. In this paper, we
propose a bounding box-level instance representation learning called BoIR,
which simultaneously solves instance detection, instance disentanglement, and
instance-keypoint association problems. Our new instance embedding loss
provides a learning signal on the entire area of the image with bounding box
annotations, achieving globally consistent and disentangled instance
representation. Our method exploits multi-task learning of bottom-up keypoint
estimation, bounding box regression, and contrastive instance embedding
learning, without additional computational cost during inference. BoIR is
effective for crowded scenes, outperforming state-of-the-art on COCO val (0.8
AP), COCO test-dev (0.5 AP), CrowdPose (4.9 AP), and OCHuman (3.5 AP). Code
will be available at https://github.com/uyoung-jeong/BoIR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_U/0/1/0/all/0/1&quot;&gt;Uyoung Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seungryul Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15411">
<title>3D Multiple Object Tracking on Autonomous Driving: A Literature Review. (arXiv:2309.15411v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15411</link>
<description rdf:parseType="Literal">&lt;p&gt;3D multi-object tracking (3D MOT) stands as a pivotal domain within
autonomous driving, experiencing a surge in scholarly interest and commercial
promise over recent years. Despite its paramount significance, 3D MOT confronts
a myriad of formidable challenges, encompassing abrupt alterations in object
appearances, pervasive occlusion, the presence of diminutive targets, data
sparsity, missed detections, and the unpredictable initiation and termination
of object motion trajectories. Countless methodologies have emerged to grapple
with these issues, yet 3D MOT endures as a formidable problem that warrants
further exploration. This paper undertakes a comprehensive examination,
assessment, and synthesis of the research landscape in this domain, remaining
attuned to the latest developments in 3D MOT while suggesting prospective
avenues for future investigation. Our exploration commences with a systematic
exposition of key facets of 3D MOT and its associated domains, including
problem delineation, classification, methodological approaches, fundamental
principles, and empirical investigations. Subsequently, we categorize these
methodologies into distinct groups, dissecting each group meticulously with
regard to its challenges, underlying rationale, progress, merits, and demerits.
Furthermore, we present a concise recapitulation of experimental metrics and
offer an overview of prevalent datasets, facilitating a quantitative comparison
for a more intuitive assessment. Lastly, our deliberations culminate in a
discussion of the prevailing research landscape, highlighting extant challenges
and charting possible directions for 3D MOT research. We present a structured
and lucid road-map to guide forthcoming endeavors in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xin Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03940">
<title>Hard View Selection for Contrastive Learning. (arXiv:2310.03940v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03940</link>
<description rdf:parseType="Literal">&lt;p&gt;Many Contrastive Learning (CL) methods train their models to be invariant to
different &quot;views&quot; of an image input for which a good data augmentation pipeline
is crucial. While considerable efforts were directed towards improving pre-text
tasks, architectures, or robustness (e.g., Siamese networks or teacher-softmax
centering), the majority of these methods remain strongly reliant on the random
sampling of operations within the image augmentation pipeline, such as the
random resized crop or color distortion operation. In this paper, we argue that
the role of the view generation and its effect on performance has so far
received insufficient attention. To address this, we propose an easy,
learning-free, yet powerful Hard View Selection (HVS) strategy designed to
extend the random view generation to expose the pretrained model to harder
samples during CL training. It encompasses the following iterative steps: 1)
randomly sample multiple views and create pairs of two views, 2) run forward
passes for each view pair on the currently trained model, 3) adversarially
select the pair yielding the worst loss, and 4) run the backward pass with the
selected pair. In our empirical analysis we show that under the hood, HVS
increases task difficulty by controlling the Intersection over Union of views
during pretraining. With only 300-epoch pretraining, HVS is able to closely
rival the 800-epoch DINO baseline which remains very favorable even when
factoring in the slowdown induced by the additional forwards of HVS.
Additionally, HVS consistently achieves accuracy improvements on ImageNet
between 0.4% and 1.9% on linear evaluation and similar improvements on transfer
tasks across multiple CL methods, such as DINO, SimSiam, and SimCLR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rapant_I/0/1/0/all/0/1&quot;&gt;Ivo Rapant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05058">
<title>Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading. (arXiv:2310.05058v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05058</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for speaker adaptation in lip
reading, motivated by two observations. Firstly, a speaker&apos;s own
characteristics can always be portrayed well by his/her few facial images or
even a single image with shallow networks, while the fine-grained dynamic
features associated with speech content expressed by the talking face always
need deep sequential networks to represent accurately. Therefore, we treat the
shallow and deep layers differently for speaker adaptive lip reading. Secondly,
we observe that a speaker&apos;s unique characteristics ( e.g. prominent oral cavity
and mandible) have varied effects on lip reading performance for different
words and pronunciations, necessitating adaptive enhancement or suppression of
the features for robust lip reading. Based on these two observations, we
propose to take advantage of the speaker&apos;s own characteristics to automatically
learn separable hidden unit contributions with different targets for shallow
layers and deep layers respectively. For shallow layers where features related
to the speaker&apos;s characteristics are stronger than the speech content related
features, we introduce speaker-adaptive features to learn for enhancing the
speech content features. For deep layers where both the speaker&apos;s features and
the speech content features are all expressed well, we introduce the
speaker-adaptive features to learn for suppressing the speech content
irrelevant noise for robust lip reading. Our approach consistently outperforms
existing methods, as confirmed by comprehensive analysis and comparison across
different settings. Besides the evaluation on the popular LRW-ID and GRID
datasets, we also release a new dataset for evaluation, CAS-VSR-S68h, to
further assess the performance in an extreme setting where just a few speakers
are available but the speech content covers a large and diversified range.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Songtao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xilin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05812">
<title>Provably Convergent Data-Driven Convex-Nonconvex Regularization. (arXiv:2310.05812v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05812</link>
<description rdf:parseType="Literal">&lt;p&gt;An emerging new paradigm for solving inverse problems is via the use of deep
learning to learn a regularizer from data. This leads to high-quality results,
but often at the cost of provable guarantees. In this work, we show how
well-posedness and convergent regularization arises within the convex-nonconvex
(CNC) framework for inverse problems. We introduce a novel input weakly convex
neural network (IWCNN) construction to adapt the method of learned adversarial
regularization to the CNC framework. Empirically we show that our method
overcomes numerical issues of previous adversarial methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1&quot;&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budd_J/0/1/0/all/0/1&quot;&gt;Jeremy Budd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Subhadip Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1&quot;&gt;Carola-Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07223">
<title>Deep Learning for blind spectral unmixing of LULC classes with MODIS multispectral time series and ancillary data. (arXiv:2310.07223v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07223</link>
<description rdf:parseType="Literal">&lt;p&gt;Remotely sensed data are dominated by mixed Land Use and Land Cover (LULC)
types. Spectral unmixing is a technique to extract information from mixed
pixels into their constituent LULC types and corresponding abundance fractions.
Traditionally, solving this task has relied on either classical methods that
require prior knowledge of endmembers or machine learning methods that avoid
explicit endmembers calculation, also known as blind spectral unmixing (BSU).
Most BSU studies based on Deep Learning (DL) focus on one time-step
hyperspectral or multispectral data. To our knowledge, here we provide the
first study on BSU of LULC classes using MODIS multispectral time series, in
presence of missing data, with end-to-end DL models. We further boost the
performance of a Long-Short Term Memory (LSTM)-based model by incorporating
geographic plus topographic (geo-topographic) and climatic ancillary
information. Our experiments show that combining spectral-temporal input data
together with geo-topographic and climatic information substantially improves
the abundance estimation of LULC classes in mixed pixels. To carry out this
study, we built a new labeled dataset of the region of Andalusia (Spain) with
monthly multispectral time series of pixels for the year 2013 from MODIS at
460m resolution, for two hierarchical levels of LULC classes, named Andalusia
MultiSpectral MultiTemporal Unmixing (Andalusia-MSMTU). This dataset provides,
at the pixel level, a multispectral time series plus ancillary information
annotated with the abundance of each LULC class inside each pixel. The dataset
(https://zenodo.org/record/7752348##.ZBmkkezMLdo) and code
(https://github.com/jrodriguezortega/MSMTU) are available to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Ortega_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Rodr&amp;#xed;guez-Ortega&lt;/a&gt; (1 and 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaldi_R/0/1/0/all/0/1&quot;&gt;Rohaifa Khaldi&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcaraz_Segura_D/0/1/0/all/0/1&quot;&gt;Domingo Alcaraz-Segura&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabik_S/0/1/0/all/0/1&quot;&gt;Siham Tabik&lt;/a&gt; (1) ((1) Department of Computer Science and Artificial Intelligence, DaSCI, University of Granada, Granada, Spain, (2) LifeWatch-ERIC ICT Core, Seville, Spain, (3) Department of Botany, Faculty of Science, University of Granada, Granada, Spain)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08854">
<title>Rank-DETR for High Quality Object Detection. (arXiv:2310.08854v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08854</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern detection transformers (DETRs) use a set of object queries to predict
a list of bounding boxes, sort them by their classification confidence scores,
and select the top-ranked predictions as the final detection results for the
given input image. A highly performant object detector requires accurate
ranking for the bounding box predictions. For DETR-based detectors, the
top-ranked bounding boxes suffer from less accurate localization quality due to
the misalignment between classification scores and localization accuracy, thus
impeding the construction of high-quality detectors. In this work, we introduce
a simple and highly performant DETR-based object detector by proposing a series
of rank-oriented designs, combinedly called Rank-DETR. Our key contributions
include: (i) a rank-oriented architecture design that can prompt positive
predictions and suppress the negative ones to ensure lower false positive
rates, as well as (ii) a rank-oriented loss function and matching cost design
that prioritizes predictions of more accurate localization accuracy during
ranking to boost the AP under high IoU thresholds. We apply our method to
improve the recent SOTA methods (e.g., H-DETR and DINO-DETR) and report strong
COCO object detection results when using different backbones such as
ResNet-$50$, Swin-T, and Swin-L, demonstrating the effectiveness of our
approach. Code is available at \url{https://github.com/LeapLabTHU/Rank-DETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yifan Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Weicong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yiduo Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yukang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16717">
<title>Rebuild City Buildings from Off-Nadir Aerial Images with Offset-Building Model (OBM). (arXiv:2310.16717v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16717</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate measurement of the offset from roof-to-footprint in
very-high-resolution remote sensing imagery is crucial for urban information
extraction tasks. With the help of deep learning, existing methods typically
rely on two-stage CNN models to extract regions of interest on building feature
maps. At the first stage, a Region Proposal Network (RPN) is applied to extract
thousands of ROIs (Region of Interests) which will post-imported into a
Region-based Convolutional Neural Networks (RCNN) to extract wanted
information. However, because of inflexible RPN, these methods often lack
effective user interaction, encounter difficulties in instance correspondence,
and struggle to keep up with the advancements in general artificial
intelligence. This paper introduces an interactive Transformer model combined
with a prompt encoder to precisely extract building segmentation as well as the
offset vectors from roofs to footprints. In our model, a powerful module,
namely ROAM, was tailored for common problems in predicting roof-to-footprint
offsets. We tested our model&apos;s feasibility on the publicly available BONAI
dataset, achieving a significant reduction in Prompt-Instance-Level offset
errors ranging from 14.6% to 16.3%. Additionally, we developed a Distance-NMS
algorithm tailored for large-scale building offsets, significantly enhancing
the accuracy of predicted building offset angles and lengths in a
straightforward and efficient manner. To further validate the model&apos;s
robustness, we created a new test set using 0.5m remote sensing imagery from
Huizhou, China, for inference testing. Our code, training methods, and the
updated dataset will be accessable at https://github.com/likaiucas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yupeng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yunlong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Diyou Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Junxian Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17419">
<title>AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors. (arXiv:2310.17419v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17419</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models can create remarkably photorealistic fake images while
raising concerns about misinformation and copyright infringement, known as
deepfake threats. Deepfake detection technique is developed to distinguish
between real and fake images, where the existing methods typically train
classifiers in the image domain or various feature domains. However, the
generalizability of deepfake detection against emerging and more advanced
generative models remains challenging. In this paper, inspired by the zero-shot
advantages of Vision-Language Models (VLMs), we propose a novel approach using
VLMs (e.g. InstructBLIP) and prompt tuning techniques to improve the deepfake
detection accuracy over unseen data. We formulate deepfake detection as a
visual question answering problem, and tune soft prompts for InstructBLIP to
distinguish a query image is real or fake. We conduct full-spectrum experiments
on datasets from 3 held-in and 13 held-out generative models, covering modern
text-to-image generation, image editing and image attacks. Results demonstrate
that (1) the deepfake detection accuracy can be significantly and consistently
improved (from 54.6% to 91.31%, in average accuracy over unseen data) using
pretrained vision-language models with prompt tuning; (2) our superior
performance is at less cost of trainable parameters, resulting in an effective
and efficient solution for deepfake detection. Code and models can be found at
https://github.com/nctu-eva-lab/AntifakePrompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;You-Ming Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chen Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1&quot;&gt;Wei-Chen Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1&quot;&gt;Ning Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18961">
<title>AnomalyCLIP: Object-agnostic Prompt Learning for Zero-shot Anomaly Detection. (arXiv:2310.18961v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18961</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot anomaly detection (ZSAD) requires detection models trained using
auxiliary data to detect anomalies without any training sample in a target
dataset. It is a crucial task when training data is not accessible due to
various concerns, \eg, data privacy, yet it is challenging since the models
need to generalize to anomalies across different domains where the appearance
of foreground objects, abnormal regions, and background features, such as
defects/tumors on different products/organs, can vary significantly. Recently
large pre-trained vision-language models (VLMs), such as CLIP, have
demonstrated strong zero-shot recognition ability in various vision tasks,
including anomaly detection. However, their ZSAD performance is weak since the
VLMs focus more on modeling the class semantics of the foreground objects
rather than the abnormality/normality in the images. In this paper we introduce
a novel approach, namely AnomalyCLIP, to adapt CLIP for accurate ZSAD across
different domains. The key insight of AnomalyCLIP is to learn object-agnostic
text prompts that capture generic normality and abnormality in an image
regardless of its foreground objects. This allows our model to focus on the
abnormal image regions rather than the object semantics, enabling generalized
normality and abnormality recognition on diverse types of objects. Large-scale
experiments on 17 real-world anomaly detection datasets show that AnomalyCLIP
achieves superior zero-shot performance of detecting and segmenting anomalies
in datasets of highly diverse class semantics from various defect inspection
and medical imaging domains. Code will be made available at
https://github.com/zqhang/AnomalyCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shibo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiming Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19180">
<title>JEN-1 Composer: A Unified Framework for High-Fidelity Multi-Track Music Generation. (arXiv:2310.19180v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19180</link>
<description rdf:parseType="Literal">&lt;p&gt;With rapid advances in generative artificial intelligence, the text-to-music
synthesis task has emerged as a promising direction for music generation from
scratch. However, finer-grained control over multi-track generation remains an
open challenge. Existing models exhibit strong raw generation capability but
lack the flexibility to compose separate tracks and combine them in a
controllable manner, differing from typical workflows of human composers. To
address this issue, we propose JEN-1 Composer, a unified framework to
efficiently model marginal, conditional, and joint distributions over
multi-track music via a single model. JEN-1 Composer framework exhibits the
capacity to seamlessly incorporate any diffusion-based music generation system,
\textit{e.g.} Jen-1, enhancing its capacity for versatile multi-track music
generation. We introduce a curriculum training strategy aimed at incrementally
instructing the model in the transition from single-track generation to the
flexible generation of multi-track combinations. During the inference, users
have the ability to iteratively produce and choose music tracks that meet their
preferences, subsequently creating an entire musical composition incrementally
following the proposed Human-AI co-composition workflow. Quantitative and
qualitative assessments demonstrate state-of-the-art performance in
controllable and high-fidelity multi-track music synthesis. The proposed JEN-1
Composer represents a significant advance toward interactive AI-facilitated
music creation and composition. Demos will be available at
https://www.jenmusic.ai/audio-demos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yao Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peike Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Boyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Alex Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20243">
<title>Contrast-agent-induced deterministic component of CT-density in the abdominal aorta during routine angiography: proof of concept study. (arXiv:2310.20243v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20243</link>
<description rdf:parseType="Literal">&lt;p&gt;Background and objective: CTA is a gold standard of preoperative diagnosis of
abdominal aorta and typically used for geometric-only characteristic
extraction. We assume that a model describing the dynamic behavior of the
contrast agent in the vessel can be developed from the data of routine CTA
studies, allowing the procedure to be investigated and optimized without the
need for additional perfusion CT studies. Obtained spatial distribution of CA
can be valuable for both increasing the diagnostic value of a particular study
and improving the CT data processing tools. Methods: In accordance with the
Beer-Lambert law and the absence of chemical interaction between blood and CA,
we postulated the existence of a deterministic CA-induced component in the CT
signal density. The proposed model, having a double-sigmoid structure, contains
six coefficients relevant to the properties of hemodynamics. To validate the
model, expert segmentation was performed using the 3D Slicer application for
the CTA data obtained from publicly available source. The model was fitted to
the data using the non-linear least square method with Levenberg-Marquardt
optimization. Results: We analyzed 594 CTA images (4 studies with median size
of 144 slices, IQR [134; 158.5]; 1:1 normal:pathology balance). Goodness-of-fit
was proved by Wilcox test (p-value &amp;gt; 0.05 for all cases). The proposed model
correctly simulated normal blood flow and hemodynamics disturbances caused by
local abnormalities (aneurysm, thrombus and arterial branching). Conclusions:
Proposed approach can be useful for personalized CA modeling of vessels,
improvement of CTA image processing and preparation of synthetic CT training
data for artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kodenko_M/0/1/0/all/0/1&quot;&gt;Maria R. Kodenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilev_Y/0/1/0/all/0/1&quot;&gt;Yuriy A. Vasilev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulberg_N/0/1/0/all/0/1&quot;&gt;Nicholas S. Kulberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samorodov_A/0/1/0/all/0/1&quot;&gt;Andrey V. Samorodov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vladzimirskyy_A/0/1/0/all/0/1&quot;&gt;Anton V. Vladzimirskyy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omelyanskaya_O/0/1/0/all/0/1&quot;&gt;Olga V. Omelyanskaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reshetnikov_R/0/1/0/all/0/1&quot;&gt;Roman V. Reshetnikov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20271">
<title>From Denoising Training to Test-Time Adaptation: Enhancing Domain Generalization for Medical Image Segmentation. (arXiv:2310.20271v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20271</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical image segmentation, domain generalization poses a significant
challenge due to domain shifts caused by variations in data acquisition devices
and other factors. These shifts are particularly pronounced in the most common
scenario, which involves only single-source domain data due to privacy
concerns. To address this, we draw inspiration from the self-supervised
learning paradigm that effectively discourages overfitting to the source
domain. We propose the Denoising Y-Net (DeY-Net), a novel approach
incorporating an auxiliary denoising decoder into the basic U-Net architecture.
The auxiliary decoder aims to perform denoising training, augmenting the
domain-invariant representation that facilitates domain generalization.
Furthermore, this paradigm provides the potential to utilize unlabeled data.
Building upon denoising training, we propose Denoising Test Time Adaptation
(DeTTA) that further: (i) adapts the model to the target domain in a
sample-wise manner, and (ii) adapts to the noise-corrupted input. Extensive
experiments conducted on widely-adopted liver segmentation benchmarks
demonstrate significant domain generalization improvements over our baseline
and state-of-the-art results compared to other methods. Code is available at
https://github.com/WenRuxue/DeTTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_R/0/1/0/all/0/1&quot;&gt;Ruxue Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hangjie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1&quot;&gt;Wenbo Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yaoyao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20381">
<title>A Comprehensive Study of GPT-4V&apos;s Multimodal Capabilities in Medical Imaging. (arXiv:2310.20381v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20381</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive evaluation of GPT-4V&apos;s capabilities
across diverse medical imaging tasks, including Radiology Report Generation,
Medical Visual Question Answering (VQA), and Visual Grounding. While prior
efforts have explored GPT-4V&apos;s performance in medical image anaylsis, to the
best of our knowledge, our study represents the first quantitative evaluation
on publicly available benchmarks. Our findings highlight GPT-4V&apos;s potential in
generating descriptive reports for chest X-ray images, particularly when guided
by well-structured prompts. Meanwhile, its performance on the MIMIC-CXR dataset
benchmark reveals areas for improvement in certain evaluation metrics, such as
CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in
distinguishing between question types but falls short of the VQA-RAD benchmark
in terms of accuracy. Furthermore, our analysis finds the limitations of
conventional evaluation metrics like the BLEU score, advocating for the
development of more semantically robust assessment methods. In the field of
Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding
boxes, but its precision is lacking, especially in identifying specific medical
organs and signs. Our evaluation underscores the significant potential of
GPT-4V in the medical imaging domain, while also emphasizing the need for
targeted refinements to fully unlock its capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xinyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Leyang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20550">
<title>CapsFusion: Rethinking Image-Text Data at Scale. (arXiv:2310.20550v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20550</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multimodal models demonstrate remarkable generalist ability to perform
diverse multimodal tasks in a zero-shot manner. Large-scale web-based
image-text pairs contribute fundamentally to this success, but suffer from
excessive noise. Recent studies use alternative captions synthesized by
captioning models and have achieved notable benchmark performance. However, our
experiments reveal significant Scalability Deficiency and World Knowledge Loss
issues in models trained with synthetic captions, which have been largely
obscured by their initial benchmark success. Upon closer examination, we
identify the root cause as the overly-simplified language structure and lack of
knowledge details in existing synthetic captions. To provide higher-quality and
more scalable multimodal pretraining data, we propose CapsFusion, an advanced
framework that leverages large language models to consolidate and refine
information from both web-based image-text pairs and synthetic captions.
Extensive experiments show that CapsFusion captions exhibit remarkable
all-round superiority over existing captions in terms of model performance
(e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample
efficiency (requiring 11-16 times less computation than baselines), world
knowledge depth, and scalability. These effectiveness, efficiency and
scalability advantages position CapsFusion as a promising candidate for future
scaling of LMM training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qiying Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Quan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yufeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yue Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01057">
<title>Ultra-Efficient On-Device Object Detection on AI-Integrated Smart Glasses with TinyissimoYOLO. (arXiv:2311.01057v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01057</link>
<description rdf:parseType="Literal">&lt;p&gt;Smart glasses are rapidly gaining advanced functionality thanks to
cutting-edge computing technologies, accelerated hardware architectures, and
tiny AI algorithms. Integrating AI into smart glasses featuring a small form
factor and limited battery capacity is still challenging when targeting
full-day usage for a satisfactory user experience. This paper illustrates the
design and implementation of tiny machine-learning algorithms exploiting novel
low-power processors to enable prolonged continuous operation in smart glasses.
We explore the energy- and latency-efficient of smart glasses in the case of
real-time object detection. To this goal, we designed a smart glasses prototype
as a research platform featuring two microcontrollers, including a novel
milliwatt-power RISC-V parallel processor with a hardware accelerator for
visual AI, and a Bluetooth low-power module for communication. The smart
glasses integrate power cycling mechanisms, including image and audio sensing
interfaces. Furthermore, we developed a family of novel tiny deep-learning
models based on YOLO with sub-million parameters customized for
microcontroller-based inference dubbed TinyissimoYOLO v1.3, v5, and v8, aiming
at benchmarking object detection with smart glasses for energy and latency.
Evaluations on the prototype of the smart glasses demonstrate TinyissimoYOLO&apos;s
17ms inference latency and 1.59mJ energy consumption per inference while
ensuring acceptable detection accuracy. Further evaluation reveals an
end-to-end latency from image capturing to the algorithm&apos;s prediction of 56ms
or equivalently 18 fps, with a total power consumption of 62.9mW, equivalent to
a 9.3 hours of continuous run time on a 154mAh battery. These results
outperform MCUNet (TinyNAS+TinyEngine), which runs a simpler task (image
classification) at just 7.3 fps per second.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moosmann_J/0/1/0/all/0/1&quot;&gt;Julian Moosmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayer_P/0/1/0/all/0/1&quot;&gt;Philipp Mayer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benini_L/0/1/0/all/0/1&quot;&gt;Luca Benini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01240">
<title>FacadeNet: Conditional Facade Synthesis via Selective Editing. (arXiv:2311.01240v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01240</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FacadeNet, a deep learning approach for synthesizing building
facade images from diverse viewpoints. Our method employs a conditional GAN,
taking a single view of a facade along with the desired viewpoint information
and generates an image of the facade from the distinct viewpoint. To precisely
modify view-dependent elements like windows and doors while preserving the
structure of view-independent components such as walls, we introduce a
selective editing module. This module leverages image embeddings extracted from
a pre-trained vision transformer. Our experiments demonstrated state-of-the-art
performance on building facade generation, surpassing alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgiou_Y/0/1/0/all/0/1&quot;&gt;Yiangos Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loizou_M/0/1/0/all/0/1&quot;&gt;Marios Loizou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1&quot;&gt;Tom Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averkiou_M/0/1/0/all/0/1&quot;&gt;Melinos Averkiou&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>