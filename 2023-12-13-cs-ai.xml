<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-11T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.00326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.03170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.08642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.08644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.08564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.03810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.15320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13191" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09936" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.05256">
<title>Holistic Evaluation of GPT-4V for Biomedical Imaging. (arXiv:2312.05256v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.05256</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a large-scale evaluation probing GPT-4V&apos;s
capabilities and limitations for biomedical image analysis. GPT-4V represents a
breakthrough in artificial general intelligence (AGI) for computer vision, with
applications in the biomedical domain. We assess GPT-4V&apos;s performance across 16
medical imaging categories, including radiology, oncology, ophthalmology,
pathology, and more. Tasks include modality recognition, anatomy localization,
disease diagnosis, report generation, and lesion detection. The extensive
experiments provide insights into GPT-4V&apos;s strengths and weaknesses. Results
show GPT-4V&apos;s proficiency in modality and anatomy recognition but difficulty
with disease diagnosis and localization. GPT-4V excels at diagnostic report
generation, indicating strong image captioning skills. While promising for
biomedical imaging AI, GPT-4V requires further enhancement and validation
before clinical deployment. We emphasize responsible development and testing
for trustworthy integration of biomedical AGI. This rigorous evaluation of
GPT-4V on diverse medical images advances understanding of multimodal large
language models (LLMs) and guides future work toward impactful healthcare
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hanqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_T/0/1/0/all/0/1&quot;&gt;Tianyang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yutong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shu_P/0/1/0/all/0/1&quot;&gt;Peng Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yanjun Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Junjie Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_P/0/1/0/all/0/1&quot;&gt;Peixin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhenxiang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaochen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yaonai Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Haixing Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zewei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Liheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiang_N/0/1/0/all/0/1&quot;&gt;Ning Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ge_B/0/1/0/all/0/1&quot;&gt;Bao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xintao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dajiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05257">
<title>Autonomous Port Navigation With Ranging Sensors Using Model-Based Reinforcement Learning. (arXiv:2312.05257v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.05257</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous shipping has recently gained much interest in the research
community. However, little research focuses on inland - and port navigation,
even though this is identified by countries such as Belgium and the Netherlands
as an essential step towards a sustainable future. These environments pose
unique challenges, since they can contain dynamic obstacles that do not
broadcast their location, such as small vessels, kayaks or buoys. Therefore,
this research proposes a navigational algorithm which can navigate an inland
vessel in a wide variety of complex port scenarios using ranging sensors to
observe the environment. The proposed methodology is based on a machine
learning approach that has recently set benchmark results in various domains:
model-based reinforcement learning. By randomizing the port environments during
training, the trained model can navigate in scenarios that it never encountered
during training. Furthermore, results show that our approach outperforms the
commonly used dynamic window approach and a benchmark model-free reinforcement
learning algorithm. This work is therefore a significant step towards vessels
that can navigate autonomously in complex port scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herremans_S/0/1/0/all/0/1&quot;&gt;Siemen Herremans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1&quot;&gt;Ali Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Troch_A/0/1/0/all/0/1&quot;&gt;Arne Troch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravijts_I/0/1/0/all/0/1&quot;&gt;Ian Ravijts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vangeneugden_M/0/1/0/all/0/1&quot;&gt;Maarten Vangeneugden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mercelis_S/0/1/0/all/0/1&quot;&gt;Siegfried Mercelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellinckx_P/0/1/0/all/0/1&quot;&gt;Peter Hellinckx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05259">
<title>Optimizing the Passenger Flow for Airport Security Check. (arXiv:2312.05259v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05259</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the necessary security for the airport and flight, passengers are
required to have strict security check before getting aboard. However, there
are frequent complaints of wasting huge amount of time while waiting for the
security check. This paper presents a potential solution aimed at optimizing
gate setup procedures specifically tailored for Chicago OHare International
Airport. By referring to queueing theory and performing Monte Carlo
simulations, we propose an approach to significantly diminish the average
waiting time to a more manageable level. Additionally, our study meticulously
examines and identifies the influential factors contributing to this
optimization, providing a comprehensive understanding of their impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaotian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chaoyu Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05265">
<title>Multimodal Group Emotion Recognition In-the-wild Using Privacy-Compliant Features. (arXiv:2312.05265v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05265</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores privacy-compliant group-level emotion recognition
&apos;&apos;in-the-wild&apos;&apos; within the EmotiW Challenge 2023. Group-level emotion
recognition can be useful in many fields including social robotics,
conversational agents, e-coaching and learning analytics. This research imposes
itself using only global features avoiding individual ones, i.e. all features
that can be used to identify or track people in videos (facial landmarks, body
poses, audio diarization, etc.). The proposed multimodal model is composed of a
video and an audio branches with a cross-attention between modalities. The
video branch is based on a fine-tuned ViT architecture. The audio branch
extracts Mel-spectrograms and feed them through CNN blocks into a transformer
encoder. Our training paradigm includes a generated synthetic dataset to
increase the sensitivity of our model on facial expression within the image in
a data-driven way. The extensive experiments show the significance of our
methodology. Our privacy-compliant proposal performs fairly on the EmotiW
challenge, with 79.24% and 75.13% of accuracy respectively on validation and
test set for the best models. Noticeably, our findings highlight that it is
possible to reach this accuracy level with privacy-compliant features using
only 5 frames uniformly distributed on the video.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augusma_A/0/1/0/all/0/1&quot;&gt;Anderson Augusma&lt;/a&gt; (M-PSI, SVH), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaufreydaz_D/0/1/0/all/0/1&quot;&gt;Dominique Vaufreydaz&lt;/a&gt; (M-PSI), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letue_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;d&amp;#xe9;rique Letu&amp;#xe9;&lt;/a&gt; (SVH)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05273">
<title>Evaluating Zero-Shot Scoring for In Vitro Antibody Binding Prediction with Experimental Validation. (arXiv:2312.05273v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2312.05273</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of therapeutic antibodies relies on their ability to selectively
bind antigens. AI-based antibody design protocols have shown promise in
generating epitope-specific designs. Many of these protocols use an inverse
folding step to generate diverse sequences given a backbone structure. Due to
prohibitive screening costs, it is key to identify candidate sequences likely
to bind in vitro. Here, we compare the efficacy of 8 common scoring paradigms
based on open-source models to classify antibody designs as binders or
non-binders. We evaluate these approaches on a novel surface plasmon resonance
(SPR) dataset, spanning 5 antigens. Our results show that existing methods
struggle to detect binders, and performance is highly variable across antigens.
We find that metrics computed on flexibly docked antibody-antigen complexes are
more robust, and ensembles scores are more consistent than individual metrics.
We provide experimental insight to analyze current scoring techniques,
highlighting that the development of robust, zero-shot filters is an important
research gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nori_D/0/1/0/all/0/1&quot;&gt;Divya Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mathis_S/0/1/0/all/0/1&quot;&gt;Simon V. Mathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shanehsazzadeh_A/0/1/0/all/0/1&quot;&gt;Amir Shanehsazzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05275">
<title>Exploring the Limits of ChatGPT in Software Security Applications. (arXiv:2312.05275v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.05275</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have undergone rapid evolution and achieved
remarkable results in recent times. OpenAI&apos;s ChatGPT, backed by GPT-3.5 or
GPT-4, has gained instant popularity due to its strong capability across a wide
range of tasks, including natural language tasks, coding, mathematics, and
engaging conversations. However, the impacts and limits of such LLMs in system
security domain are less explored. In this paper, we delve into the limits of
LLMs (i.e., ChatGPT) in seven software security applications including
vulnerability detection/repair, debugging, debloating, decompilation, patching,
root cause analysis, symbolic execution, and fuzzing. Our exploration reveals
that ChatGPT not only excels at generating code, which is the conventional
application of language models, but also demonstrates strong capability in
understanding user-provided commands in natural languages, reasoning about
control and data flows within programs, generating complex data structures, and
even decompiling assembly code. Notably, GPT-4 showcases significant
improvements over GPT-3.5 in most security tasks. Also, certain limitations of
ChatGPT in security-related tasks are identified, such as its constrained
ability to process long code contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangzhou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajaj_A/0/1/0/all/0/1&quot;&gt;Ati Priya Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_T/0/1/0/all/0/1&quot;&gt;Tiffany Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ning Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu &amp;quot;Fish&amp;quot; Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05276">
<title>Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation. (arXiv:2312.05276v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05276</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, the rapid development of mobile economy has promoted the
flourishing of online marketing campaigns, whose success greatly hinges on the
efficient matching between user preferences and desired marketing campaigns
where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG)
could serve as the critical &quot;bridge&quot; for preference propagation. In this paper,
we seek to carefully prompt a Large Language Model (LLM) with domain-level
knowledge as a better marketing-oriented knowledge miner for marketing-oriented
knowledge graph construction, which is however non-trivial, suffering from
several inevitable issues in real-world marketing scenarios, i.e.,
uncontrollable relation generation of LLMs,insufficient prompting ability of a
single prompt, the unaffordable deployment cost of LLMs. To this end, we
propose PAIR, a novel Progressive prompting Augmented mIning fRamework for
harvesting marketing-oriented knowledge graph with LLMs. In particular, we
reduce the pure relation generation to an LLM based adaptive relation filtering
process through the knowledge-empowered prompting technique. Next, we steer
LLMs for entity expansion with progressive prompting augmentation,followed by a
reliable aggregation with comprehensive consideration of both self-consistency
and semantic relatedness. In terms of online serving, we specialize in a small
and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality
corpus provided by a strong teacher-LLM. Extensive experiments and practical
applications in audience targeting verify the effectiveness of the proposed
(Light)PAIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chunjing Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Binbin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yue Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guannan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05291">
<title>GlitchBench: Can large multimodal models detect video game glitches?. (arXiv:2312.05291v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05291</link>
<description rdf:parseType="Literal">&lt;p&gt;Large multimodal models (LMMs) have evolved from large language models (LLMs)
to integrate multiple input modalities, such as visual inputs. This integration
augments the capacity of LLMs for tasks requiring visual comprehension and
reasoning. However, the extent and limitations of their enhanced abilities are
not fully understood, especially when it comes to real-world tasks. To address
this gap, we introduce GlitchBench, a novel benchmark derived from video game
quality assurance tasks, to test and evaluate the reasoning capabilities of
LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios
from video games and aims to challenge both the visual and linguistic reasoning
powers of LMMs in detecting and interpreting out-of-the-ordinary events. We
evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents
a new challenge for these models. Code and data are available at:
https://glitchbench.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taesiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Taesiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tianjun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezemer_C/0/1/0/all/0/1&quot;&gt;Cor-Paul Bezemer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05328">
<title>Bad Students Make Great Teachers:Active Learning Accelerates Large-Scale Visual Understanding. (arXiv:2312.05328v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05328</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for accelerating large-scale pre-training with online
data selection policies. For the first time, we demonstrate that model-based
data selection can reduce the total computation needed to reach the performance
of models trained with uniform sampling. The key insight which enables this
&quot;compute-positive&quot; regime is that small models provide good proxies for the
loss of much larger models, such that computation spent on scoring data can be
drastically scaled down without diminishing the efficiency gains afforded to
the learner. These data selection policies also strongly generalize across
datasets and tasks, opening an avenue for further amortizing the overhead of
data scoring by re-using off-the-shelf models and training sequences. Our
methods, ClassAct and ActiveCLIP, require 46% and 51% fewer training updates
and up to 25% less total computation when training visual classifiers on JFT
and multimodal models on ALIGN, respectively. Finally, our paradigm seamlessly
applies to the curation of large-scale image-text datasets, yielding a new
state-of-the-art in several multimodal transfer tasks and pre-training regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_T/0/1/0/all/0/1&quot;&gt;Talfan Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1&quot;&gt;Shreya Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merzic_H/0/1/0/all/0/1&quot;&gt;Hamza Merzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarz_J/0/1/0/all/0/1&quot;&gt;Jonathan Schwarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1&quot;&gt;Olivier J. Henaff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05337">
<title>Artificial Neural Nets and the Representation of Human Concepts. (arXiv:2312.05337v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05337</link>
<description rdf:parseType="Literal">&lt;p&gt;What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiesleben_T/0/1/0/all/0/1&quot;&gt;Timo Freiesleben&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05349">
<title>PixLore: A Dataset-driven Approach to Rich Image Captioning. (arXiv:2312.05349v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05349</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of vision-language integration, generating detailed image
captions poses a significant challenge due to the lack of a curated and rich
dataset. This study introduces PixLore, a novel method that leverages Querying
Transformers through the fine-tuning of the BLIP-2 model using the LoRa method
on a standard commercial GPU. Our approach, which involves training on a
carefully assembled dataset from state-of-the-art Computer Vision models
combined and augmented by ChatGPT, addresses the question of whether intricate
image understanding can be achieved with an ensemble of smaller-scale models.
Comparative evaluations against major models such as GPT-4 and Google Bard
demonstrate that PixLore-2.7B, despite having considerably fewer parameters, is
rated higher than the existing State-of-the-Art models in over half of the
assessments. This research not only presents a groundbreaking approach but also
highlights the importance of well-curated datasets in enhancing the performance
of smaller models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonilla_D/0/1/0/all/0/1&quot;&gt;Diego Bonilla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05361">
<title>Emergence and Function of Abstract Representations in Self-Supervised Transformers. (arXiv:2312.05361v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05361</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence relies in part on our brains&apos; ability to create abstract
mental models that succinctly capture the hidden blueprint of our reality. Such
abstract world models notably allow us to rapidly navigate novel situations by
generalizing prior knowledge, a trait deep learning systems have historically
struggled to replicate. However, the recent shift from supervised to
self-supervised objectives, combined with expressive transformer-based
architectures, have yielded powerful foundation models that appear to learn
versatile representations that can support a wide range of downstream tasks.
This promising development raises the intriguing possibility of such models
developing in silico abstract world models. We test this hypothesis by studying
the inner workings of small-scale transformers trained to reconstruct partially
masked visual scenes generated from a simple blueprint. We show that the
network develops intermediate abstract representations, or abstractions, that
encode all semantic features of the dataset. These abstractions manifest as
low-dimensional manifolds where the embeddings of semantically related tokens
transiently converge, thus allowing for the generalization of downstream
computations. Using precise manipulation experiments, we demonstrate that
abstractions are central to the network&apos;s decision-making process. Our research
also suggests that these abstractions are compositionally structured,
exhibiting features like contextual independence and part-whole relationships
that mirror the compositional nature of the dataset. Finally, we introduce a
Language-Enhanced Architecture (LEA) designed to encourage the network to
articulate its computations. We find that LEA develops an abstraction-centric
language that can be easily interpreted, allowing us to more readily access and
steer the network&apos;s decision-making process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferry_Q/0/1/0/all/0/1&quot;&gt;Quentin RV. Ferry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1&quot;&gt;Joshua Ching&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawai_T/0/1/0/all/0/1&quot;&gt;Takashi Kawai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05368">
<title>Toward Scalable and Transparent Multimodal Analytics to Study Standard Medical Procedures: Linking Hand Movement, Proximity, and Gaze Data. (arXiv:2312.05368v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05368</link>
<description rdf:parseType="Literal">&lt;p&gt;This study employed multimodal learning analytics (MMLA) to analyze
behavioral dynamics during the ABCDE procedure in nursing education, focusing
on gaze entropy, hand movement velocities, and proximity measures. Utilizing
accelerometers and eye-tracking techniques, behaviorgrams were generated to
depict various procedural phases. Results identified four primary phases
characterized by distinct patterns of visual attention, hand movements, and
proximity to the patient or instruments. The findings suggest that MMLA can
offer valuable insights into procedural competence in medical education. This
research underscores the potential of MMLA to provide detailed, objective
evaluations of clinical procedures and their inherent complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heilala_V/0/1/0/all/0/1&quot;&gt;Ville Heilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehesvuori_S/0/1/0/all/0/1&quot;&gt;Sami Lehesvuori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamalainen_R/0/1/0/all/0/1&quot;&gt;Raija H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkkainen_T/0/1/0/all/0/1&quot;&gt;Tommi K&amp;#xe4;rkk&amp;#xe4;inen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05379">
<title>Exploring Parity Challenges in Reinforcement Learning through Curriculum Learning with Noisy Labels. (arXiv:2312.05379v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into applying reinforcement learning (RL) in strategy
games, particularly those characterized by parity challenges, as seen in
specific positions of Go and Chess and a broader range of impartial games. We
propose a simulated learning process, structured within a curriculum learning
framework and augmented with noisy labels, to mirror the intricacies of
self-play learning scenarios. This approach thoroughly analyses how neural
networks (NNs) adapt and evolve from elementary to increasingly complex game
positions. Our empirical research indicates that even minimal label noise can
significantly impede NNs&apos; ability to discern effective strategies, a difficulty
that intensifies with the growing complexity of the game positions. These
findings underscore the urgent need for advanced methodologies in RL training,
specifically tailored to counter the obstacles imposed by noisy evaluations.
The development of such methodologies is crucial not only for enhancing NN
proficiency in strategy games with significant parity elements but also for
broadening the resilience and efficiency of RL systems across diverse and
complex environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1&quot;&gt;Soren Riis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05392">
<title>The logic of NTQR evaluations of noisy AI agents: Complete postulates and logically consistent error correlations. (arXiv:2312.05392v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05392</link>
<description rdf:parseType="Literal">&lt;p&gt;In his &quot;ship of state&quot; allegory (\textit{Republic}, Book VI, 488) Plato poses
a question -- how can a crew of sailors presumed to know little about the art
of navigation recognize the true pilot among them? The allegory argues that a
simple majority voting procedure cannot safely determine who is most qualified
to pilot a ship when the voting members are ignorant or biased. We formalize
Plato&apos;s concerns by considering the problem in AI safety of monitoring noisy AI
agents in unsupervised settings. An algorithm evaluating AI agents using
unlabeled data would be subject to the evaluation dilemma - how would we know
the evaluation algorithm was correct itself? This endless validation chain can
be avoided by considering purely algebraic functions of the observed responses.
We can construct complete postulates than can prove or disprove the logical
consistency of any grading algorithm. A complete set of postulates exists
whenever we are evaluating $N$ experts that took $T$ tests with $Q$ questions
with $R$ responses each. We discuss evaluating binary classifiers that have
taken a single test - the $(N,T=1,Q,R=2)$ tests. We show how some of the
postulates have been previously identified in the ML literature but not
recognized as such - the \textbf{agreement equations} of Platanios. The
complete postulates for pair correlated binary classifiers are considered and
we show how it allows for error correlations to be quickly calculated. An
algebraic evaluator based on the assumption that the ensemble is error
independent is compared with grading by majority voting on evaluations using
the \uciadult and and \texttt{two-norm} datasets. Throughout, we demonstrate
how the formalism of logical consistency via algebraic postulates of evaluation
can help increase the safety of machines using AI algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrada_Emmanuel_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Corrada-Emmanuel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05404">
<title>Disentangled Latent Representation Learning for Tackling the Confounding M-Bias Problem in Causal Inference. (arXiv:2312.05404v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05404</link>
<description rdf:parseType="Literal">&lt;p&gt;In causal inference, it is a fundamental task to estimate the causal effect
from observational data. However, latent confounders pose major challenges in
causal inference in observational data, for example, confounding bias and
M-bias. Recent data-driven causal effect estimators tackle the confounding bias
problem via balanced representation learning, but assume no M-bias in the
system, thus they fail to handle the M-bias. In this paper, we identify a
challenging and unsolved problem caused by a variable that leads to confounding
bias and M-bias simultaneously. To address this problem with co-occurring
M-bias and confounding bias, we propose a novel Disentangled Latent
Representation learning framework for learning latent representations from
proxy variables for unbiased Causal effect Estimation (DLRCE) from
observational data. Specifically, DLRCE learns three sets of latent
representations from the measured proxy variables to adjust for the confounding
bias and M-bias. Extensive experiments on both synthetic and three real-world
datasets demonstrate that DLRCE significantly outperforms the state-of-the-art
estimators in the case of the presence of both confounding bias and M-bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Debo Cheng&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yang Xie&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Xu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jixue Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zaiwen Feng&lt;/a&gt; (2) ((1) UniSA STEM, University of South Australia, Adelaide, Australia and (2) College of Informatics, Huazhong Agricultural University, Wuhan, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05409">
<title>Large-scale Training of Foundation Models for Wearable Biosignals. (arXiv:2312.05409v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05409</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracking biosignals is crucial for monitoring wellness and preempting the
development of severe medical conditions. Today, wearable devices can
conveniently record various biosignals, creating the opportunity to monitor
health status without disruption to one&apos;s daily routine. Despite widespread use
of wearable devices and existing digital biomarkers, the absence of curated
data with annotated medical labels hinders the development of new biomarkers to
measure common health conditions. In fact, medical datasets are usually small
in comparison to other domains, which is an obstacle for developing neural
network models for biosignals. To address this challenge, we have employed
self-supervised learning using the unlabeled sensor data collected under
informed consent from the large longitudinal Apple Heart and Movement Study
(AHMS) to train foundation models for two common biosignals:
photoplethysmography (PPG) and electrocardiogram (ECG) recorded on Apple Watch.
We curated PPG and ECG datasets from AHMS that include data from ~141K
participants spanning ~3 years. Our self-supervised learning framework includes
participant level positive pair selection, stochastic augmentation module and a
regularized contrastive loss optimized with momentum training, and generalizes
well to both PPG and ECG modalities. We show that the pre-trained foundation
models readily encode information regarding participants&apos; demographics and
health conditions. To the best of our knowledge, this is the first study that
builds foundation models using large-scale PPG and ECG data collected via
wearable consumer devices $\unicode{x2013}$ prior works have commonly used
smaller-size datasets collected in clinical and experimental settings. We
believe PPG and ECG foundation models can enhance future wearable devices by
reducing the reliance on labeled data and hold the potential to help the users
improve their health.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbaspourazad_S/0/1/0/all/0/1&quot;&gt;Salar Abbaspourazad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elachqar_O/0/1/0/all/0/1&quot;&gt;Oussama Elachqar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_A/0/1/0/all/0/1&quot;&gt;Andrew C. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emrani_S/0/1/0/all/0/1&quot;&gt;Saba Emrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nallasamy_U/0/1/0/all/0/1&quot;&gt;Udhyakumar Nallasamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_I/0/1/0/all/0/1&quot;&gt;Ian Shapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05431">
<title>Efficient Quantization Strategies for Latent Diffusion Models. (arXiv:2312.05431v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05431</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent Diffusion Models (LDMs) capture the dynamic evolution of latent
variables over time, blending patterns and multimodality in a generative
system. Despite the proficiency of LDM in various applications, such as
text-to-image generation, facilitated by robust text encoders and a variational
autoencoder, the critical need to deploy large generative models on edge
devices compels a search for more compact yet effective alternatives. Post
Training Quantization (PTQ), a method to compress the operational size of deep
learning models, encounters challenges when applied to LDM due to temporal and
structural complexities. This study proposes a quantization strategy that
efficiently quantize LDMs, leveraging Signal-to-Quantization-Noise Ratio (SQNR)
as a pivotal metric for evaluation. By treating the quantization discrepancy as
relative noise and identifying sensitive part(s) of a model, we propose an
efficient quantization approach encompassing both global and local strategies.
The global quantization process mitigates relative quantization noise by
initiating higher-precision quantization on sensitive blocks, while local
treatments address specific challenges in quantization-sensitive and
time-sensitive modules. The outcomes of our experiments reveal that the
implementation of both global and local treatments yields a highly efficient
and effective Post Training Quantization (PTQ) of LDMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuewei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peizhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongbo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05433">
<title>Stochastic Directly-Follows Process Discovery Using Grammatical Inference. (arXiv:2312.05433v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05433</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting with a collection of traces generated by process executions, process
discovery is the task of constructing a simple model that describes the
process, where simplicity is often measured in terms of model size. The
challenge of process discovery is that the process of interest is unknown, and
that while the input traces constitute positive examples of process executions,
no negative examples are available. Many commercial tools discover
Directly-Follows Graphs, in which nodes represent the observable actions of the
process, and directed arcs indicate execution order possibilities over the
actions. We propose a new approach for discovering sound Directly-Follows
Graphs that is grounded in grammatical inference over the input traces. To
promote the discovery of small graphs that also describe the process accurately
we design and evaluate a genetic algorithm that supports the convergence of the
inference parameters to the areas that lead to the discovery of interesting
models. Experiments over real-world datasets confirm that our new approach can
construct smaller models that represent the input traces and their frequencies
more accurately than the state-of-the-art technique. Reasoning over the
frequencies of encoded traces also becomes possible, due to the stochastic
semantics of the action graphs we propose, which, for the first time, are
interpreted as models that describe the stochastic languages of action traces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkhammash_H/0/1/0/all/0/1&quot;&gt;Hanan Alkhammash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyvyanyy_A/0/1/0/all/0/1&quot;&gt;Artem Polyvyanyy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moffat_A/0/1/0/all/0/1&quot;&gt;Alistair Moffat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05437">
<title>Rate-Distortion-Perception Theory for Semantic Communication. (arXiv:2312.05437v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2312.05437</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic communication has attracted significant interest recently due to its
capability to meet the fast growing demand on user-defined and human-oriented
communication services such as holographic communications, eXtended reality
(XR), and human-to-machine interactions. Unfortunately, recent study suggests
that the traditional Shannon information theory, focusing mainly on delivering
semantic-agnostic symbols, will not be sufficient to investigate the
semantic-level perceptual quality of the recovered messages at the receiver. In
this paper, we study the achievable data rate of semantic communication under
the symbol distortion and semantic perception constraints. Motivated by the
fact that the semantic information generally involves rich intrinsic knowledge
that cannot always be directly observed by the encoder, we consider a semantic
information source that can only be indirectly sensed by the encoder. Both
encoder and decoder can access to various types of side information that may be
closely related to the user&apos;s communication preference. We derive the
achievable region that characterizes the tradeoff among the data rate, symbol
distortion, and semantic perception, which is then theoretically proved to be
achievable by a stochastic coding scheme. We derive a closed-form achievable
rate for binary semantic information source under any given distortion and
perception constraints. We observe that there exists cases that the receiver
can directly infer the semantic information source satisfying certain
distortion and perception constraints without requiring any data communication
from the transmitter. Experimental results based on the image semantic source
signal have been presented to verify our theoretical observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Jingxuan Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guangming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05440">
<title>Consistency Models for Scalable and Fast Simulation-Based Inference. (arXiv:2312.05440v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05440</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation-based inference (SBI) is constantly in search of more expressive
algorithms for accurately inferring the parameters of complex models from noisy
data. We present consistency models for neural posterior estimation (CMPE), a
new free-form conditional sampler for scalable, fast, and amortized SBI with
generative neural networks. CMPE combines the advantages of normalizing flows
and flow matching methods into a single generative architecture: It essentially
distills a continuous probability flow and enables rapid few-shot inference
with an unconstrained architecture that can be tailored to the structure of the
estimation problem. Our empirical evaluation demonstrates that CMPE not only
outperforms current state-of-the-art algorithms on three hard low-dimensional
problems, but also achieves competitive performance in a high-dimensional
Bayesian denoising experiment and in estimating a computationally demanding
multi-scale model of tumor spheroid growth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1&quot;&gt;Marvin Schmitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pratz_V/0/1/0/all/0/1&quot;&gt;Valentin Pratz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1&quot;&gt;Ullrich K&amp;#xf6;the&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1&quot;&gt;Paul-Christian B&amp;#xfc;rkner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1&quot;&gt;Stefan T Radev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05454">
<title>Model Evaluation for Domain Identification of Unknown Classes in Open-World Recognition: A Proposal. (arXiv:2312.05454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05454</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-World Recognition (OWR) is an emerging field that makes a machine
learning model competent in rejecting the unknowns, managing them, and
incrementally adding novel samples to the base knowledge. However, this broad
objective is not practical for an agent that works on a specific task. Not all
rejected samples will be used for learning continually in the future. Some
novel images in the open environment may not belong to the domain of interest.
Hence, identifying the unknown in the domain of interest is essential for a
machine learning model to learn merely the important samples. In this study, we
propose an evaluation protocol for estimating a model&apos;s capability in
separating unknown in-domain (ID) and unknown out-of-domain (OOD). We evaluated
using three approaches with an unknown domain and demonstrated the possibility
of identifying the domain of interest using the pre-trained parameters through
traditional transfer learning, Automated Machine Learning (AutoML), and Nearest
Class Mean (NCM) classifier with First Integer Neighbor Clustering Hierarchy
(FINCH). We experimented with five different domains: garbage, food, dogs,
plants, and birds. The results show that all approaches can be used as an
initial baseline yielding a good accuracy. In addition, a Balanced Accuracy
(BACCU) score from a pre-trained model indicates a tendency to excel in one or
more domains of interest. We observed that MobileNetV3 yielded the highest
BACCU score for the garbage domain and surpassed complex models such as the
transformer network. Meanwhile, our results also suggest that a strong
representation in the pre-trained model is important for identifying unknown
classes in the same domain. This study could open the bridge toward open-world
recognition in domain-specific tasks where the relevancy of the unknown classes
is vital.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfarisy_G/0/1/0/all/0/1&quot;&gt;Gusti Ahmad Fanshuri Alfarisy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_O/0/1/0/all/0/1&quot;&gt;Owais Ahmed Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_O/0/1/0/all/0/1&quot;&gt;Ong Wee Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05461">
<title>STREAMLINE: An Automated Machine Learning Pipeline for Biomedicine Applied to Examine the Utility of Photography-Based Phenotypes for OSA Prediction Across International Sleep Centers. (arXiv:2312.05461v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05461</link>
<description rdf:parseType="Literal">&lt;p&gt;While machine learning (ML) includes a valuable array of tools for analyzing
biomedical data, significant time and expertise is required to assemble
effective, rigorous, and unbiased pipelines. Automated ML (AutoML) tools seek
to facilitate ML application by automating a subset of analysis pipeline
elements. In this study we develop and validate a Simple, Transparent,
End-to-end Automated Machine Learning Pipeline (STREAMLINE) and apply it to
investigate the added utility of photography-based phenotypes for predicting
obstructive sleep apnea (OSA); a common and underdiagnosed condition associated
with a variety of health, economic, and safety consequences. STREAMLINE is
designed to tackle biomedical binary classification tasks while adhering to
best practices and accommodating complexity, scalability, reproducibility,
customization, and model interpretation. Benchmarking analyses validated the
efficacy of STREAMLINE across data simulations with increasingly complex
patterns of association. Then we applied STREAMLINE to evaluate the utility of
demographics (DEM), self-reported comorbidities (DX), symptoms (SYM), and
photography-based craniofacial (CF) and intraoral (IO) anatomy measures in
predicting any OSA or moderate/severe OSA using 3,111 participants from Sleep
Apnea Global Interdisciplinary Consortium (SAGIC). OSA analyses identified a
significant increase in ROC-AUC when adding CF to DEM+DX+SYM to predict
moderate/severe OSA. A consistent but non-significant increase in PRC-AUC was
observed with the addition of each subsequent feature set to predict any OSA,
with CF and IO yielding minimal improvements. Application of STREAMLINE to OSA
data suggests that CF features provide additional value in predicting
moderate/severe OSA, but neither CF nor IO features meaningfully improved the
prediction of any OSA beyond established demographics, comorbidity and symptom
characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urbanowicz_R/0/1/0/all/0/1&quot;&gt;Ryan J. Urbanowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandhey_H/0/1/0/all/0/1&quot;&gt;Harsh Bandhey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keenan_B/0/1/0/all/0/1&quot;&gt;Brendan T. Keenan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maislin_G/0/1/0/all/0/1&quot;&gt;Greg Maislin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sy Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mowery_D/0/1/0/all/0/1&quot;&gt;Danielle L. Mowery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lynch_S/0/1/0/all/0/1&quot;&gt;Shannon M. Lynch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzotti_D/0/1/0/all/0/1&quot;&gt;Diego R. Mazzotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1&quot;&gt;Fang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Yun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penzel_T/0/1/0/all/0/1&quot;&gt;Thomas Penzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tufik_S/0/1/0/all/0/1&quot;&gt;Sergio Tufik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bittencourt_L/0/1/0/all/0/1&quot;&gt;Lia Bittencourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gislason_T/0/1/0/all/0/1&quot;&gt;Thorarinn Gislason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chazal_P/0/1/0/all/0/1&quot;&gt;Philip de Chazal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1&quot;&gt;Bhajan Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McArdle_N/0/1/0/all/0/1&quot;&gt;Nigel McArdle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Ning-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pack_A/0/1/0/all/0/1&quot;&gt;Allan Pack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwab_R/0/1/0/all/0/1&quot;&gt;Richard J. Schwab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cistulli_P/0/1/0/all/0/1&quot;&gt;Peter A. Cistulli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magalang_U/0/1/0/all/0/1&quot;&gt;Ulysses J. Magalang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05468">
<title>Image and Data Mining in Reticular Chemistry Using GPT-4V. (arXiv:2312.05468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05468</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of artificial intelligence into scientific research has
reached a new pinnacle with GPT-4V, a large language model featuring enhanced
vision capabilities, accessible through ChatGPT or an API. This study
demonstrates the remarkable ability of GPT-4V to navigate and obtain complex
data for metal-organic frameworks, especially from graphical sources. Our
approach involved an automated process of converting 346 scholarly articles
into 6240 images, which represents a benchmark dataset in this task, followed
by deploying GPT-4V to categorize and analyze these images using natural
language prompts. This methodology enabled GPT-4V to accurately identify and
interpret key plots integral to MOF characterization, such as nitrogen
isotherms, PXRD patterns, and TGA curves, among others, with accuracy and
recall above 93%. The model&apos;s proficiency in extracting critical information
from these plots not only underscores its capability in data mining but also
highlights its potential in aiding the creation of comprehensive digital
databases for reticular chemistry. In addition, the extracted nitrogen isotherm
data from the selected literature allowed for a comparison between theoretical
and experimental porosity values for over 200 compounds, highlighting certain
discrepancies and underscoring the importance of integrating computational and
experimental data. This work highlights the potential of AI in accelerating
scientific discovery and innovation, bridging the gap between computational
tools and experimental research, and paving the way for more efficient,
inclusive, and comprehensive scientific inquiry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhiling Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiguo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1&quot;&gt;Omar Khattab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampal_N/0/1/0/all/0/1&quot;&gt;Nakul Rampal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1&quot;&gt;Matei A. Zaharia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borgs_C/0/1/0/all/0/1&quot;&gt;Christian Borgs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chayes_J/0/1/0/all/0/1&quot;&gt;Jennifer T. Chayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghi_O/0/1/0/all/0/1&quot;&gt;Omar M. Yaghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05471">
<title>Fine-Grained Analysis of Team Collaborative Dialogue. (arXiv:2312.05471v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.05471</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language analysis of human collaborative chat dialogues is an
understudied domain with many unique challenges: a large number of dialogue act
labels, underspecified and dynamic tasks, interleaved topics, and long-range
contextual dependence. While prior work has studied broad metrics of team
dialogue and associated performance using methods such as LSA, there has been
little effort in generating fine-grained descriptions of team dynamics and
individual performance from dialogue. We describe initial work towards
developing an explainable analytics tool in the software development domain
using Slack chats mined from our organization, including generation of a novel,
hierarchical labeling scheme; design of descriptive metrics based on the
frequency of occurrence of dialogue acts; and initial results using a
transformer + CRF architecture to incorporate long-range context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_I/0/1/0/all/0/1&quot;&gt;Ian Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilber_C/0/1/0/all/0/1&quot;&gt;Carson Wilber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05473">
<title>Self Model for Embodied Intelligence: Modeling Full-Body Human Musculoskeletal System and Locomotion Control with Hierarchical Low-Dimensional Representation. (arXiv:2312.05473v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05473</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and control of the human musculoskeletal system is important for
understanding human motion, developing embodied intelligence, and optimizing
human-robot interaction systems. However, current open-source models are
restricted to a limited range of body parts and often with a reduced number of
muscles. There is also a lack of algorithms capable of controlling over 600
muscles to generate reasonable human movements. To fill this gap, we build a
comprehensive musculoskeletal model with 90 body segments, 206 joints, and 700
muscle-tendon units, allowing simulation of full-body dynamics and interaction
with various devices. We develop a new algorithm using low-dimensional
representation and hierarchical deep reinforcement learning to achieve
state-of-the-art full-body control. We validate the effectiveness of our model
and algorithm in simulations and on real human locomotion data. The
musculoskeletal model, along with its control algorithm, will be made available
to the research community to promote a deeper understanding of human motion
control and better design of interactive robots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kaibo He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_C/0/1/0/all/0/1&quot;&gt;Chenhui Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jing Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yanan Sui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05479">
<title>Exploring Sparsity in Graph Transformers. (arXiv:2312.05479v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05479</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Transformers (GTs) have achieved impressive results on various
graph-related tasks. However, the huge computational cost of GTs hinders their
deployment and application, especially in resource-constrained environments.
Therefore, in this paper, we explore the feasibility of sparsifying GTs, a
significant yet under-explored topic. We first discuss the redundancy of GTs
based on the characteristics of existing GT models, and then propose a
comprehensive \textbf{G}raph \textbf{T}ransformer \textbf{SP}arsification
(GTSP) framework that helps to reduce the computational complexity of GTs from
four dimensions: the input graph data, attention heads, model layers, and model
weights. Specifically, GTSP designs differentiable masks for each individual
compressible component, enabling effective end-to-end pruning. We examine our
GTSP through extensive experiments on prominent GTs, including GraphTrans,
Graphormer, and GraphGPS. The experimental results substantiate that GTSP
effectively cuts computational costs, accompanied by only marginal decreases in
accuracy or, in some cases, even improvements. For instance, GTSP yields a
reduction of 30\% in Floating Point Operations while contributing to a 1.8\%
increase in Area Under the Curve accuracy on OGBG-HIV dataset. Furthermore, we
provide several insights on the characteristics of attention heads and the
behavior of attention mechanisms, all of which have immense potential to
inspire future research endeavors in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xueqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05482">
<title>BARET : Balanced Attention based Real image Editing driven by Target-text Inversion. (arXiv:2312.05482v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05482</link>
<description rdf:parseType="Literal">&lt;p&gt;Image editing approaches with diffusion models have been rapidly developed,
yet their applicability are subject to requirements such as specific editing
types (e.g., foreground or background object editing, style transfer), multiple
conditions (e.g., mask, sketch, caption), and time consuming fine-tuning of
diffusion models. For alleviating these limitations and realizing efficient
real image editing, we propose a novel editing technique that only requires an
input image and target text for various editing types including non-rigid edits
without fine-tuning diffusion model. Our method contains three novelties:(I)
Target-text Inversion Schedule (TTIS) is designed to fine-tune the input target
text embedding to achieve fast image reconstruction without image caption and
acceleration of convergence.(II) Progressive Transition Scheme applies
progressive linear interpolation between target text embedding and its
fine-tuned version to generate transition embedding for maintaining non-rigid
editing capability.(III) Balanced Attention Module (BAM) balances the tradeoff
between textual description and image semantics.By the means of combining
self-attention map from reconstruction process and cross-attention map from
transition process, the guidance of target text embeddings in diffusion process
is optimized.In order to demonstrate editing capability, effectiveness and
efficiency of the proposed BARET, we have conducted extensive qualitative and
quantitative experiments. Moreover, results derived from user study and
ablation study further prove the superiority over other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yuming Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fanyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jingwen Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yunjie Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guo-Jun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05486">
<title>FreeFlow: A Comprehensive Understanding on Diffusion Probabilistic Models via Optimal Transport. (arXiv:2312.05486v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05486</link>
<description rdf:parseType="Literal">&lt;p&gt;The blooming diffusion probabilistic models (DPMs) have garnered significant
interest due to their impressive performance and the elegant inspiration they
draw from physics. While earlier DPMs relied upon the Markovian assumption,
recent methods based on differential equations have been rapidly applied to
enhance the efficiency and capabilities of these models. However, a theoretical
interpretation encapsulating these diverse algorithms is insufficient yet
pressingly required to guide further development of DPMs. In response to this
need, we present FreeFlow, a framework that provides a thorough explanation of
the diffusion formula as time-dependent optimal transport, where the
evolutionary pattern of probability density is given by the gradient flows of a
functional defined in Wasserstein space. Crucially, our framework necessitates
a unified description that not only clarifies the subtle mechanism of DPMs but
also indicates the roots of some defects through creative involvement of
Lagrangian and Eulerian views to understand the evolution of probability flow.
We particularly demonstrate that the core equation of FreeFlow condenses all
stochastic and deterministic DPMs into a single case, showcasing the
expansibility of our method. Furthermore, the Riemannian geometry employed in
our work has the potential to bridge broader subjects in mathematics, which
enable the involvement of more profound tools for the establishment of more
outstanding and generalized models in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bowen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shibao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05488">
<title>Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis. (arXiv:2312.05488v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05488</link>
<description rdf:parseType="Literal">&lt;p&gt;Game theory, as an analytical tool, is frequently utilized to analyze human
behavior in social science research. With the high alignment between the
behavior of Large Language Models (LLMs) and humans, a promising research
direction is to employ LLMs as substitutes for humans in game experiments,
enabling social science research. However, despite numerous empirical
researches on the combination of LLMs and game theory, the capability
boundaries of LLMs in game theory remain unclear. In this research, we endeavor
to systematically analyze LLMs in the context of game theory. Specifically,
rationality, as the fundamental principle of game theory, serves as the metric
for evaluating players&apos; behavior -- building a clear desire, refining belief
about uncertainty, and taking optimal actions. Accordingly, we select three
classical games (dictator game, Rock-Paper-Scissors, and ring-network game) to
analyze to what extent LLMs can achieve rationality in these three aspects. The
experimental results indicate that even the current state-of-the-art LLM
(GPT-4) exhibits substantial disparities compared to humans in game theory. For
instance, LLMs struggle to build desires based on uncommon preferences, fail to
refine belief from many simple patterns, and may overlook or modify refined
belief when taking actions. Therefore, we consider that introducing LLMs into
game experiments in the field of social science should be approached with
greater caution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Caoyun Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jindou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05491">
<title>Using Captum to Explain Generative Language Models. (arXiv:2312.05491v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.05491</link>
<description rdf:parseType="Literal">&lt;p&gt;Captum is a comprehensive library for model explainability in PyTorch,
offering a range of methods from the interpretability literature to enhance
users&apos; understanding of PyTorch models. In this paper, we introduce new
features in Captum that are specifically designed to analyze the behavior of
generative language models. We provide an overview of the available
functionalities and example applications of their potential for understanding
learned associations within generative language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miglani_V/0/1/0/all/0/1&quot;&gt;Vivek Miglani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Aobo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markosyan_A/0/1/0/all/0/1&quot;&gt;Aram H. Markosyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Olano_D/0/1/0/all/0/1&quot;&gt;Diego Garcia-Olano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokhlikyan_N/0/1/0/all/0/1&quot;&gt;Narine Kokhlikyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05503">
<title>Aligner: One Global Token is Worth Millions of Parameters When Aligning Large Language Models. (arXiv:2312.05503v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.05503</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Aligner, a novel Parameter-Efficient Fine-Tuning (PEFT) method
for aligning multi-billion-parameter-sized Large Language Models (LLMs).
Aligner employs a unique design that constructs a globally shared set of
tunable tokens that modify the attention of every layer. Remarkably with this
method, even when using one token accounting for a mere 5,000 parameters,
Aligner can still perform comparably well to state-of-the-art LLM adaptation
methods like LoRA that require millions of parameters. This capacity is
substantiated in both instruction following and value alignment tasks. Besides
the multiple order-of-magnitude improvement in parameter efficiency, the
insight Aligner provides into the internal mechanisms of LLMs is also valuable.
The architectural features and efficacy of our method, in addition to our
experiments demonstrate that an LLM separates its internal handling of &quot;form&quot;
and &quot;knowledge&quot; in a somewhat orthogonal manner. This finding promises to
motivate new research into LLM mechanism understanding and value alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziheng_Z/0/1/0/all/0/1&quot;&gt;Zhou Ziheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingnian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Terzopoulos_D/0/1/0/all/0/1&quot;&gt;Demetri Terzopoulos&lt;/a&gt; (University of California, Los Angeles)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05526">
<title>Reinforcement Neighborhood Selection for Unsupervised Graph Anomaly Detection. (arXiv:2312.05526v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05526</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised graph anomaly detection is crucial for various practical
applications as it aims to identify anomalies in a graph that exhibit rare
patterns deviating significantly from the majority of nodes. Recent
advancements have utilized Graph Neural Networks (GNNs) to learn high-quality
node representations for anomaly detection by aggregating information from
neighborhoods. However, the presence of anomalies may render the observed
neighborhood unreliable and result in misleading information aggregation for
node representation learning. Selecting the proper neighborhood is critical for
graph anomaly detection but also challenging due to the absence of
anomaly-oriented guidance and the interdependence with representation learning.
To address these issues, we utilize the advantages of reinforcement learning in
adaptively learning in complex environments and propose a novel method that
incorporates Reinforcement neighborhood selection for unsupervised graph
ANomaly Detection (RAND). RAND begins by enriching the candidate neighbor pool
of the given central node with multiple types of indirect neighbors. Next, RAND
designs a tailored reinforcement anomaly evaluation module to assess the
reliability and reward of considering the given neighbor. Finally, RAND selects
the most reliable subset of neighbors based on these rewards and introduces an
anomaly-aware aggregator to amplify messages from reliable neighbors while
diminishing messages from unreliable ones. Extensive experiments on both three
synthetic and two real-world datasets demonstrate that RAND outperforms the
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bei_Y/0/1/0/all/0/1&quot;&gt;Yuanchen Bei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qiaoyu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05531">
<title>KEN: Kernel Extensions using Natural Language. (arXiv:2312.05531v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05531</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to modify and extend an operating system is an important feature
for improving a system&apos;s security, reliability, and performance. The extended
Berkeley Packet Filters (eBPF) ecosystem has emerged as the standard mechanism
for extending the Linux kernel and has recently been ported to Windows. eBPF
programs inject new logic into the kernel that the system will execute before
or after existing logic. While the eBPF ecosystem provides a flexible mechanism
for kernel extension, it is difficult for developers to write eBPF programs
today. An eBPF developer must have deep knowledge of the internals of the
operating system to determine where to place logic and cope with programming
limitations on the control flow and data accesses of their eBPF program
enforced by the eBPF verifier. This paper presents KEN, an alternative
framework that alleviates the difficulty of writing an eBPF program by allowing
Kernel Extensions to be written in Natural language. KEN uses recent advances
in large language models (LLMs) to synthesize an eBPF program given a user&apos;s
English language prompt. To ensure that LLM&apos;s output is semantically equivalent
to the user&apos;s prompt, KEN employs a combination of LLM-empowered program
comprehension, symbolic execution, and a series of feedback loops. KEN&apos;s key
novelty is the combination of these techniques. In particular, the system uses
symbolic execution in a novel structure that allows it to combine the results
of program synthesis and program comprehension and build on the recent success
that LLMs have shown for each of these tasks individually. To evaluate KEN, we
developed a new corpus of natural language prompts for eBPF programs. We show
that KEN produces correct eBPF programs on 80% which is an improvement of a
factor of 2.67 compared to an LLM-empowered program synthesis baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yusheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Maolin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_A/0/1/0/all/0/1&quot;&gt;Andrew Quinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05550">
<title>D3A-TS: Denoising-Driven Data Augmentation in Time Series. (arXiv:2312.05550v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05550</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been demonstrated that the amount of data is crucial in data-driven
machine learning methods. Data is always valuable, but in some tasks, it is
almost like gold. This occurs in engineering areas where data is scarce or very
expensive to obtain, such as predictive maintenance, where faults are rare. In
this context, a mechanism to generate synthetic data can be very useful. While
in fields such as Computer Vision or Natural Language Processing synthetic data
generation has been extensively explored with promising results, in other
domains such as time series it has received less attention. This work
specifically focuses on studying and analyzing the use of different techniques
for data augmentation in time series for classification and regression
problems. The proposed approach involves the use of diffusion probabilistic
models, which have recently achieved successful results in the field of Image
Processing, for data augmentation in time series. Additionally, the use of
meta-attributes to condition the data augmentation process is investigated. The
results highlight the high utility of this methodology in creating synthetic
data to train classification and regression models. To assess the results, six
different datasets from diverse domains were employed, showcasing versatility
in terms of input size and output types. Finally, an extensive ablation study
is conducted to further support the obtained outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solis_Martin_D/0/1/0/all/0/1&quot;&gt;David Solis-Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galan_Paez_J/0/1/0/all/0/1&quot;&gt;Juan Galan-Paez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borrego_Diaz_J/0/1/0/all/0/1&quot;&gt;Joaquin Borrego-Diaz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05568">
<title>Sparse Variational Student-t Processes. (arXiv:2312.05568v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05568</link>
<description rdf:parseType="Literal">&lt;p&gt;The theory of Bayesian learning incorporates the use of Student-t Processes
to model heavy-tailed distributions and datasets with outliers. However,
despite Student-t Processes having a similar computational complexity as
Gaussian Processes, there has been limited emphasis on the sparse
representation of this model. This is mainly due to the increased difficulty in
modeling and computation compared to previous sparse Gaussian Processes. Our
motivation is to address the need for a sparse representation framework that
reduces computational complexity, allowing Student-t Processes to be more
flexible for real-world datasets. To achieve this, we leverage the conditional
distribution of Student-t Processes to introduce sparse inducing points.
Bayesian methods and variational inference are then utilized to derive a
well-defined lower bound, facilitating more efficient optimization of our model
through stochastic gradient descent. We propose two methods for computing the
variational lower bound, one utilizing Monte Carlo sampling and the other
employing Jensen&apos;s inequality to compute the KL regularization term in the loss
function. We propose adopting these approaches as viable alternatives to
Gaussian processes when the data might contain outliers or exhibit heavy-tailed
behavior, and we provide specific recommendations for their applicability. We
evaluate the two proposed approaches on various synthetic and real-world
datasets from UCI and Kaggle, demonstrating their effectiveness compared to
baseline methods in terms of computational complexity and accuracy, as well as
their robustness to outliers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Delu Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05571">
<title>Frugal LMs Trained to Invoke Symbolic Solvers Achieve Parameter-Efficient Arithmetic Reasoning. (arXiv:2312.05571v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05571</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLM) exhibit zero-shot mathematical reasoning capacity
as a behavior emergent with scale, commonly manifesting as chain-of-thoughts
(CoT) reasoning. However, multiple empirical findings suggest that this prowess
is exclusive to LLMs with exorbitant sizes (beyond 50 billion parameters).
Meanwhile, educational neuroscientists suggest that symbolic algebraic
manipulation be introduced around the same time as arithmetic word problems to
modularize language-to-formulation, symbolic manipulation of the formulation,
and endgame arithmetic. In this paper, we start with the hypothesis that much
smaller LMs, which are weak at multi-step reasoning, can achieve reasonable
arithmetic reasoning if arithmetic word problems are posed as a
formalize-then-solve task. In our architecture, which we call SYRELM, the LM
serves the role of a translator to map natural language arithmetic questions
into a formal language (FL) description. A symbolic solver then evaluates the
FL expression to obtain the answer. A small frozen LM, equipped with an
efficient low-rank adapter, is capable of generating FL expressions that
incorporate natural language descriptions of the arithmetic problem (e.g.,
variable names and their purposes, formal expressions combining variables,
etc.). We adopt policy-gradient reinforcement learning to train the adapted LM,
informed by the non-differentiable symbolic solver. This marks a sharp
departure from the recent development in tool-augmented LLMs, in which the
external tools (e.g., calculator, Web search, etc.) are essentially detached
from the learning phase of the LM. SYRELM shows massive improvements (e.g.,
+30.65 absolute point improvement in accuracy on the SVAMP dataset using GPT-J
6B model) over base LMs, while keeping our testbed easy to diagnose, interpret
and within reach of most researchers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Subhabrata Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Joykirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_I/0/1/0/all/0/1&quot;&gt;Ishan Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchanda_S/0/1/0/all/0/1&quot;&gt;Sunny Manchanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1&quot;&gt;Soumen Chakrabarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05576">
<title>Dynamic Adjustment of Matching Radii under the Broadcasting Mode: A Novel Multitask Learning Strategy and Temporal Modeling Approach. (arXiv:2312.05576v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05576</link>
<description rdf:parseType="Literal">&lt;p&gt;As ride-hailing services have experienced significant growth, the majority of
research has concentrated on the dispatching mode, where drivers must adhere to
the platform&apos;s assigned routes. However, the broadcasting mode, in which
drivers can freely choose their preferred orders from those broadcast by the
platform, has received less attention. One important but challenging task in
such a system is the determination of the optimal matching radius, which
usually varies across space, time, and real-time supply/demand characteristics.
This study develops a Transformer-Encoder-Based (TEB) model that predicts key
system performance metrics for a range of matching radii, which enables the
ride-hailing platform to select an optimal matching radius that maximizes
overall system performance according to real-time supply and demand
information. To simultaneously maximize multiple system performance metrics for
matching radius determination, we devise a novel multi-task learning algorithm
that enhances convergence speed of each task (corresponding to the optimization
of one metric) and delivers more accurate overall predictions. We evaluate our
methods in a simulation environment specifically designed for
broadcasting-mode-based ride-hailing service. Our findings reveal that
dynamically adjusting matching radii based on our proposed
predict-then-optimize approach significantly improves system performance, e.g.,
increasing platform revenue by 7.55% and enhancing order fulfillment rate by
13% compared to benchmark algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Taijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zijian Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Siyuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Linchuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1&quot;&gt;Jintao Ke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05583">
<title>Better Neural PDE Solvers Through Data-Free Mesh Movers. (arXiv:2312.05583v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05583</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, neural networks have been extensively employed to solve partial
differential equations (PDEs) in physical system modeling. While major studies
focus on learning system evolution on predefined static mesh discretizations,
some methods utilize reinforcement learning or supervised learning techniques
to create adaptive and dynamic meshes, due to the dynamic nature of these
systems. However, these approaches face two primary challenges: (1) the need
for expensive optimal mesh data, and (2) the change of the solution space&apos;s
degree of freedom and topology during mesh refinement. To address these
challenges, this paper proposes a neural PDE solver with a neural mesh adapter.
To begin with, we introduce a novel data-free neural mesh adaptor, called
Data-free Mesh Mover (DMM), with two main innovations. Firstly, it is an
operator that maps the solution to adaptive meshes and is trained using the
Monge-Ampere equation without optimal mesh data. Secondly, it dynamically
changes the mesh by moving existing nodes rather than adding or deleting nodes
and edges. Theoretical analysis shows that meshes generated by DMM have the
lowest interpolation error bound. Based on DMM, to efficiently and accurately
model dynamic systems, we develop a moving mesh based neural PDE solver
(MM-PDE) that embeds the moving mesh with a two-branch architecture and a
learnable interpolation framework to preserve information within the data.
Empirical experiments demonstrate that our method generates suitable meshes and
considerably enhances accuracy when modeling widely considered PDE systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1&quot;&gt;Peiyan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhi-Ming Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05586">
<title>Deeper Understanding of Black-box Predictions via Generalized Influence Functions. (arXiv:2312.05586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05586</link>
<description rdf:parseType="Literal">&lt;p&gt;Influence functions (IFs) elucidate how learning data affects model behavior.
However, growing non-convexity and the number of parameters in modern
large-scale models lead to imprecise influence approximation and instability in
computations. We highly suspect that the first-order approximation in large
models causes such fragility, as IFs change all parameters including possibly
nuisance parameters that are irrelevant to the examined data. Thus, we attempt
to selectively analyze parameters associated with the data. However, simply
computing influence from the chosen parameters can be misleading, as it fails
to nullify the subliminal impact of unselected parameters. Our approach
introduces generalized IFs, precisely estimating target parameters&apos; influence
while considering fixed parameters&apos; effects. Unlike the classic IFs, we newly
adopt a method to identify pertinent target parameters closely associated with
the analyzed data. Furthermore, we tackle computational instability with a
robust inverse-Hessian-vector product approximation. Remarkably, the proposed
approximation algorithm guarantees convergence regardless of the network
configurations. We evaluated our approach on ResNet-18 and VGG-11 for class
removal and backdoor model recovery. Modifying just 10\% of the network yields
results comparable to the network retrained from scratch. Aligned with our
first guess, we also confirm that modifying an excessive number of parameters
results in a decline in network utility. We believe our proposal can become a
versatile tool for model analysis across various AI domains, appealing to both
specialists and general readers. Codes are available at
https://github.com/hslyu/GIF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1&quot;&gt;Hyeonsu Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jonggyu Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_S/0/1/0/all/0/1&quot;&gt;Sehyun Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyun Jong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05588">
<title>Language-assisted Vision Model Debugger: A Sample-Free Approach to Finding Bugs. (arXiv:2312.05588v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05588</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision models with high overall accuracy often exhibit systematic errors in
specific scenarios, posing potential serious safety concerns. Diagnosing bugs
of vision models is gaining increased attention, however traditional diagnostic
approaches require annotation efforts (\eg rich metadata accompanying each
samples of CelebA). To address this issue,We propose a language-assisted
diagnostic method that uses texts instead of images to diagnose bugs in vision
models based on multi-modal models (\eg CLIP). Our approach connects the
embedding space of CLIP with the buggy vision model to be diagnosed; meanwhile,
utilizing a shared classifier and the cross-modal transferability of embedding
space from CLIP, the text-branch of CLIP become a proxy model to find bugs in
the buggy model. The proxy model can classify texts paired with images. During
the diagnosis, a Large Language Model (LLM) is employed to obtain task-relevant
corpora, and this corpora is used to extract keywords. Descriptions constructed
with templates containing these keywords serve as input text to probe errors in
the proxy model. Finally, we validate the ability to diagnose existing visual
models using language on the Waterbirds and CelebA datasets, we can identify
bugs comprehensible to human experts, uncovering not only known bugs but also
previously unknown ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaoquan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1&quot;&gt;Jitao Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05589">
<title>A Review of Hybrid and Ensemble in Deep Learning for Natural Language Processing. (arXiv:2312.05589v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05589</link>
<description rdf:parseType="Literal">&lt;p&gt;This review presents a comprehensive exploration of hybrid and ensemble deep
learning models within Natural Language Processing (NLP), shedding light on
their transformative potential across diverse tasks such as Sentiment Analysis,
Named Entity Recognition, Machine Translation, Question Answering, Text
Classification, Generation, Speech Recognition, Summarization, and Language
Modeling. The paper systematically introduces each task, delineates key
architectures from Recurrent Neural Networks (RNNs) to Transformer-based models
like BERT, and evaluates their performance, challenges, and computational
demands. The adaptability of ensemble techniques is emphasized, highlighting
their capacity to enhance various NLP applications. Challenges in
implementation, including computational overhead, overfitting, and model
interpretation complexities, are addressed alongside the trade-off between
interpretability and performance. Serving as a concise yet invaluable guide,
this review synthesizes insights into tasks, architectures, and challenges,
offering a holistic perspective for researchers and practitioners aiming to
advance language-driven applications through ensemble deep learning in NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jianguo Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1&quot;&gt;Wen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Youzhi Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05594">
<title>Generative AI for Physical Layer Communications: A Survey. (arXiv:2312.05594v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.05594</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent evolution of generative artificial intelligence (GAI) leads to the
emergence of groundbreaking applications such as ChatGPT, which not only
enhances the efficiency of digital content production, such as text, audio,
video, or even network traffic data, but also enriches its diversity. Beyond
digital content creation, GAI&apos;s capability in analyzing complex data
distributions offers great potential for wireless communications, particularly
amidst a rapid expansion of new physical layer communication technologies. For
example, the diffusion model can learn input signal distributions and use them
to improve the channel estimation accuracy, while the variational autoencoder
can model channel distribution and infer latent variables for blind channel
equalization. Therefore, this paper presents a comprehensive investigation of
GAI&apos;s applications for communications at the physical layer, ranging from
traditional issues, including signal classification, channel estimation, and
equalization, to emerging topics, such as intelligent reflecting surfaces and
joint source channel coding. We also compare GAI-enabled physical layer
communications with those supported by traditional AI, highlighting GAI&apos;s
inherent capabilities and unique contributions in these areas. Finally, the
paper discusses open issues and proposes several future research directions,
laying a foundation for further exploration and advancement of GAI in physical
layer communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1&quot;&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1&quot;&gt;Khaled B. Letaief&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05597">
<title>Artificial Intelligence in the automatic coding of interviews on Landscape Quality Objectives. Comparison and case study. (arXiv:2312.05597v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05597</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we conducted a comparative analysis of the automated coding
provided by three Artificial Intelligence functionalities (At-las.ti, ChatGPT
and Google Bard) in relation to the manual coding of 12 research interviews
focused on Landscape Quality Objectives for a small island in the north of Cuba
(Cayo Santa Mar\&apos;ia). For this purpose, the following comparison criteria were
established: Accuracy, Comprehensiveness, Thematic Coherence, Redundancy,
Clarity, Detail and Regularity. The analysis showed the usefulness of AI for
the intended purpose, albeit with numerous flaws and shortcomings. In summary,
today the automatic coding of AIs can be considered useful as a guide towards a
subsequent in-depth and meticulous analysis of the information by the
researcher. However, as this is such a recently developed field, rapid
evolution is expected to bring the necessary improvements to these tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burgui_Burgui_M/0/1/0/all/0/1&quot;&gt;Mario Burgui-Burgui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05599">
<title>Not All Data Matters: An End-to-End Adaptive Dataset Pruning Framework for Enhancing Model Performance and Efficiency. (arXiv:2312.05599v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05599</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep neural networks have demonstrated remarkable performance across
various tasks, they typically require massive training data. Due to the
presence of redundancies and biases in real-world datasets, not all data in the
training dataset contributes to the model performance. To address this issue,
dataset pruning techniques have been introduced to enhance model performance
and efficiency by eliminating redundant training samples and reducing
computational and memory overhead. However, previous works most rely on
manually crafted scalar scores, limiting their practical performance and
scalability across diverse deep networks and datasets. In this paper, we
propose AdaPruner, an end-to-end Adaptive DAtaset PRUNing framEwoRk. AdaPruner
can perform effective dataset pruning without the need for explicitly defined
metrics. Our framework jointly prunes training data and fine-tunes models with
task-specific optimization objectives. AdaPruner leverages (1) An adaptive
dataset pruning (ADP) module, which iteratively prunes redundant samples to an
expected pruning ratio; and (2) A pruning performance controller (PPC) module,
which optimizes the model performance for accurate pruning. Therefore,
AdaPruner exhibits high scalability and compatibility across various datasets
and deep networks, yielding improved dataset distribution and enhanced model
performance. AdaPruner can still significantly enhance model performance even
after pruning up to 10-30\% of the training data. Notably, these improvements
are accompanied by substantial savings in memory and computation costs.
Qualitative and quantitative experiments suggest that AdaPruner outperforms
other state-of-the-art dataset pruning methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Suorong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongchao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Suhan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Furao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.00326">
<title>PyCSP3: Modeling Combinatorial Constrained Problems in Python. (arXiv:2009.00326v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2009.00326</link>
<description rdf:parseType="Literal">&lt;p&gt;In this document, we introduce PyCSP$3$, a Python library that allows us to
write models of combinatorial constrained problems in a declarative manner.
Currently, with PyCSP$3$, you can write models of constraint satisfaction and
optimization problems. More specifically, you can build CSP (Constraint
Satisfaction Problem) and COP (Constraint Optimization Problem) models.
Importantly, there is a complete separation between the modeling and solving
phases: you write a model, you compile it (while providing some data) in order
to generate an XCSP$3$ instance (file), and you solve that problem instance by
means of a constraint solver. You can also directly pilot the solving procedure
in PyCSP$3$, possibly conducting an incremental solving strategy. In this
document, you will find all that you need to know about PyCSP$3$, with more
than 50 illustrative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecoutre_C/0/1/0/all/0/1&quot;&gt;Christophe Lecoutre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szczepanski_N/0/1/0/all/0/1&quot;&gt;Nicolas Szczepanski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.03170">
<title>Food Classification with Convolutional Neural Networks and Multi-Class Linear Discernment Analysis. (arXiv:2012.03170v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.03170</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have been successful in representing the
fully-connected inferencing ability perceived to be seen in the human brain:
they take full advantage of the hierarchy-style patterns commonly seen in
complex data and develop more patterns using simple features. Countless
implementations of CNNs have shown how strong their ability is to learn these
complex patterns, particularly in the realm of image classification. However,
the cost of getting a high performance CNN to a so-called &quot;state of the art&quot;
level is computationally costly. Even when using transfer learning, which
utilize the very deep layers from models such as MobileNetV2, CNNs still take a
great amount of time and resources. Linear discriminant analysis (LDA), a
generalization of Fisher&apos;s linear discriminant, can be implemented in a
multi-class classification method to increase separability of class features
while not needing a high performance system to do so for image classification.
Similarly, we also believe LDA has great promise in performing well. In this
paper, we discuss our process of developing a robust CNN for food
classification as well as our effective implementation of multi-class LDA and
prove that (1) CNN is superior to LDA for image classification and (2) why LDA
should not be left out of the races for image classification, particularly for
binary cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ball_J/0/1/0/all/0/1&quot;&gt;Joshua Ball&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.08642">
<title>POAR: Efficient Policy Optimization via Online Abstract State Representation Learning. (arXiv:2109.08642v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2109.08642</link>
<description rdf:parseType="Literal">&lt;p&gt;While the rapid progress of deep learning fuels end-to-end reinforcement
learning (RL), direct application, especially in high-dimensional space like
robotic scenarios still suffers from low sample efficiency. Therefore State
Representation Learning (SRL) is proposed to specifically learn to encode
task-relevant features from complex sensory data into low-dimensional states.
However, the pervasive implementation of SRL is usually conducted by a
decoupling strategy in which the observation-state mapping is learned
separately, which is prone to over-fit. To handle such problem, we summarize
the state-of-the-art (SOTA) SRL sub-tasks in previous works and present a new
algorithm called Policy Optimization via Abstract Representation which
integrates SRL into the policy optimization phase. Firstly, We engage RL loss
to assist in updating SRL model so that the states can evolve to meet the
demand of RL and maintain a good physical interpretation. Secondly, we
introduce a dynamic loss weighting mechanism so that both models can
efficiently adapt to each other. Thirdly, we introduce a new SRL prior called
domain resemblance to leverage expert demonstration to improve SRL
interpretations. Finally, we provide a real-time access of state graph to
monitor the course of learning. Experiments indicate that POAR significantly
outperforms SOTA RL algorithms and decoupling SRL strategies in terms of sample
efficiency and final rewards. We empirically verify POAR to efficiently handle
tasks in high dimensions and facilitate training real-life robots directly from
scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaorun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Siqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Liang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Binhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Te Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1&quot;&gt;David Filliat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1&quot;&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chengliang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.08644">
<title>Allocating Indivisible Goods to Strategic Agents: Pure Nash Equilibria and Fairness. (arXiv:2109.08644v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2109.08644</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of fairly allocating a set of indivisible goods to a
set of strategic agents with additive valuation functions. We assume no
monetary transfers and, therefore, a mechanism in our setting is an algorithm
that takes as input the reported -- rather than the true -- values of the
agents. Our main goal is to explore whether there exist mechanisms that have
pure Nash equilibria for every instance and, at the same time, provide fairness
guarantees for the allocations that correspond to these equilibria. We focus on
two relaxations of envy-freeness, namely envy-freeness up to one good (EF1),
and envy-freeness up to any good (EFX), and we positively answer the above
question. In particular, we study two algorithms that are known to produce such
allocations in the non-strategic setting: Round-Robin (EF1 allocations for any
number of agents) and a cut-and-choose algorithm of Plaut and Roughgarden [SIAM
Journal of Discrete Mathematics, 2020] (EFX allocations for two agents). For
Round-Robin we show that all of its pure Nash equilibria induce allocations
that are EF1 with respect to the underlying true values, while for the
algorithm of Plaut and Roughgarden we show that the corresponding allocations
not only are EFX but also satisfy maximin share fairness, something that is not
true for this algorithm in the non-strategic setting! Further, we show that a
weaker version of the latter result holds for any mechanism for two agents that
always has pure Nash equilibria which all induce EFX allocations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amanatidis_G/0/1/0/all/0/1&quot;&gt;Georgios Amanatidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birmpas_G/0/1/0/all/0/1&quot;&gt;Georgios Birmpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fusco_F/0/1/0/all/0/1&quot;&gt;Federico Fusco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazos_P/0/1/0/all/0/1&quot;&gt;Philip Lazos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_S/0/1/0/all/0/1&quot;&gt;Stefano Leonardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiffenhauser_R/0/1/0/all/0/1&quot;&gt;Rebecca Reiffenh&amp;#xe4;user&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.08564">
<title>Some Doxastic \L ukasiewicz Logic. (arXiv:2111.08564v4 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2111.08564</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a doxastic \L ukasiewicz logic \textbf{B\L} that is sound and
complete with respect to the class of Kripke-based models in which atomic
propositions and accessibility relations are both infinitely valued in the
standard MV-algebra [0,1]. We also introduce some extensions of \textbf{B\L}
corresponding to axioms \textbf{D}, \textbf{4}, and \textbf{T} of classical
epistemic logic. Furthermore, completeness of these extensions are established
corresponding to the appropriate classes of models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dastgheib_D/0/1/0/all/0/1&quot;&gt;Doratossadat Dastgheib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahani_H/0/1/0/all/0/1&quot;&gt;Hadi Farahani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.03810">
<title>Ancestral Instrument Method for Causal Inference without Complete Knowledge. (arXiv:2201.03810v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2201.03810</link>
<description rdf:parseType="Literal">&lt;p&gt;Unobserved confounding is the main obstacle to causal effect estimation from
observational data. Instrumental variables (IVs) are widely used for causal
effect estimation when there exist latent confounders. With the standard IV
method, when a given IV is valid, unbiased estimation can be obtained, but the
validity requirement on a standard IV is strict and untestable. Conditional IVs
have been proposed to relax the requirement of standard IVs by conditioning on
a set of observed variables (known as a conditioning set for a conditional IV).
However, the criterion for finding a conditioning set for a conditional IV
needs a directed acyclic graph (DAG) representing the causal relationships of
both observed and unobserved variables. This makes it challenging to discover a
conditioning set directly from data. In this paper, by leveraging maximal
ancestral graphs (MAGs) for causal inference with latent variables, we study
the graphical properties of ancestral IVs, a type of conditional IVs using
MAGs, and develop the theory to support data-driven discovery of the
conditioning set for a given ancestral IV in data under the pretreatment
variable assumption. Based on the theory, we develop an algorithm for unbiased
causal effect estimation with a given ancestral IV and observational data.
Extensive experiments on synthetic and real-world datasets demonstrate the
performance of the algorithm in comparison with existing IV methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1&quot;&gt;Debo Cheng&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiuyong Li&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiji Zhang&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thuc duy Le&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jixue Liu&lt;/a&gt; (1) ((1) STEM, University of South Australia, Adelaide, SA, Australia, (2) Department of Religion and Philosophy, Hong Kong Baptist University, Hong Kong, China)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01818">
<title>QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs. (arXiv:2206.01818v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01818</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-relation question answering (QA) is a challenging task, where given
questions usually require long reasoning chains in KGs that consist of multiple
relations. Recently, methods with explicit multi-step reasoning over KGs have
been prominently used in this task and have demonstrated promising performance.
Examples include methods that perform stepwise label propagation through KG
triples and methods that navigate over KG triples based on reinforcement
learning. A main weakness of these methods is that their reasoning mechanisms
are usually complex and difficult to implement or train. In this paper, we
argue that multi-relation QA can be achieved via end-to-end single-step
implicit reasoning, which is simpler, more efficient, and easier to adopt. We
propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based
method that includes a novel GCN architecture with controlled
question-dependent message propagation for the implicit reasoning. Extensive
experiments have been conducted, where QAGCN achieved competitive and even
superior performance compared to state-of-the-art explicit-reasoning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruijie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1&quot;&gt;Luca Rossetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1&quot;&gt;Michael Cochez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1&quot;&gt;Abraham Bernstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10177">
<title>TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks. (arXiv:2206.10177v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10177</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs) are attracting widespread interest due to
their biological plausibility, energy efficiency, and powerful spatio-temporal
information representation ability. Given the critical role of attention
mechanisms in enhancing neural network performance, the integration of SNNs and
attention mechanisms exhibits potential to deliver energy-efficient and
high-performance computing paradigms. We present a novel Temporal-Channel Joint
Attention mechanism for SNNs, referred to as TCJA-SNN. The proposed TCJA-SNN
framework can effectively assess the significance of spike sequence from both
spatial and temporal dimensions. More specifically, our essential technical
contribution lies on: 1) We employ the squeeze operation to compress the spike
stream into an average matrix. Then, we leverage two local attention mechanisms
based on efficient 1D convolutions to facilitate comprehensive feature
extraction at the temporal and channel levels independently. 2) We introduce
the Cross Convolutional Fusion (CCF) layer as a novel approach to model the
inter-dependencies between the temporal and channel scopes. This layer breaks
the independence of these two dimensions and enables the interaction between
features. Experimental results demonstrate that the proposed TCJA-SNN
outperforms SOTA by up to 15.7% accuracy on standard static and neuromorphic
datasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128
Gesture. Furthermore, we apply the TCJA-SNN framework to image generation tasks
by leveraging a variation autoencoder. To the best of our knowledge, this study
is the first instance where the SNN-attention mechanism has been employed for
image classification and generation tasks. Notably, our approach has achieved
SOTA performance in both domains, establishing a significant advancement in the
field. Codes are available at https://github.com/ridgerchu/TCJA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rui-Jie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianjing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Haoyu Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yule Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Malu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Liang-Jian Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11005">
<title>AdaptCL: Adaptive Continual Learning for Tackling Heterogeneity in Sequential Datasets. (arXiv:2207.11005v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11005</link>
<description rdf:parseType="Literal">&lt;p&gt;Managing heterogeneous datasets that vary in complexity, size, and similarity
in continual learning presents a significant challenge. Task-agnostic continual
learning is necessary to address this challenge, as datasets with varying
similarity pose difficulties in distinguishing task boundaries. Conventional
task-agnostic continual learning practices typically rely on rehearsal or
regularization techniques. However, rehearsal methods may struggle with varying
dataset sizes and regulating the importance of old and new data due to rigid
buffer sizes. Meanwhile, regularization methods apply generic constraints to
promote generalization but can hinder performance when dealing with dissimilar
datasets lacking shared features, necessitating a more adaptive approach. In
this paper, we propose AdaptCL, a novel adaptive continual learning method to
tackle heterogeneity in sequential datasets. AdaptCL employs fine-grained
data-driven pruning to adapt to variations in data complexity and dataset size.
It also utilizes task-agnostic parameter isolation to mitigate the impact of
varying degrees of catastrophic forgetting caused by differences in data
similarity. Through a two-pronged case study approach, we evaluate AdaptCL on
both datasets of MNIST Variants and DomainNet, as well as datasets from
different domains. The latter include both large-scale, diverse binary-class
datasets and few-shot, multi-class datasets. Across all these scenarios,
AdaptCL consistently exhibits robust performance, demonstrating its flexibility
and general applicability in handling heterogeneous datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_D/0/1/0/all/0/1&quot;&gt;Divya Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiannong Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00917">
<title>Proceedings of the 2022 XCSP3 Competition. (arXiv:2209.00917v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;This document represents the proceedings of the 2022 XCSP3 Competition. The
results of this competition of constraint solvers were presented at FLOC
(Federated Logic Conference) 2022 Olympic Games, held in Haifa, Israel from
31th July 2022 to 7th August, 2022.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Audemard_G/0/1/0/all/0/1&quot;&gt;Gilles Audemard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecoutre_C/0/1/0/all/0/1&quot;&gt;Christophe Lecoutre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lonca_E/0/1/0/all/0/1&quot;&gt;Emmanuel Lonca&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05580">
<title>Risk-aware Meta-level Decision Making for Exploration Under Uncertainty. (arXiv:2209.05580v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05580</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic exploration of unknown environments is fundamentally a problem of
decision making under uncertainty where the robot must account for uncertainty
in sensor measurements, localization, action execution, as well as many other
factors. For large-scale exploration applications, autonomous systems must
overcome the challenges of sequentially deciding which areas of the environment
are valuable to explore while safely evaluating the risks associated with
obstacles and hazardous terrain. In this work, we propose a risk-aware
meta-level decision making framework to balance the tradeoffs associated with
local and global exploration. Meta-level decision making builds upon classical
hierarchical coverage planners by switching between local and global policies
with the overall objective of selecting the policy that is most likely to
maximize reward in a stochastic environment. We use information about the
environment history, traversability risk, and kinodynamic constraints to reason
about the probability of successful policy execution to switch between local
and global policies. We have validated our solution in both simulation and on a
variety of large-scale real world hardware tests. Our results show that by
balancing local and global exploration we are able to significantly explore
large-scale environments more efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_J/0/1/0/all/0/1&quot;&gt;Joshua Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sung-Kyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouman_A/0/1/0/all/0/1&quot;&gt;Amanda Bouman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peltzer_O/0/1/0/all/0/1&quot;&gt;Oriana Peltzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sobue_M/0/1/0/all/0/1&quot;&gt;Mamoru Sobue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delecki_H/0/1/0/all/0/1&quot;&gt;Harrison Delecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burdick_J/0/1/0/all/0/1&quot;&gt;Joel Burdick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agha_mohammadi_A/0/1/0/all/0/1&quot;&gt;Ali-akbar Agha-mohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.15320">
<title>Bounded Robustness in Reinforcement Learning via Lexicographic Objectives. (arXiv:2209.15320v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.15320</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy robustness in Reinforcement Learning may not be desirable at any cost:
the alterations caused by robustness requirements from otherwise optimal
policies should be explainable, quantifiable and formally verifiable. In this
work we study how policies can be maximally robust to arbitrary observational
noise by analysing how they are altered by this noise through a stochastic
linear operator interpretation of the disturbances, and establish connections
between robustness and properties of the noise kernel and of the underlying
MDPs. Then, we construct sufficient conditions for policy robustness, and
propose a robustness-inducing scheme, applicable to any policy gradient
algorithm, that formally trades off expected policy utility for robustness
through lexicographic optimisation, while preserving convergence and
sub-optimality in the policy synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ornia_D/0/1/0/all/0/1&quot;&gt;Daniel Jarne Ornia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romao_L/0/1/0/all/0/1&quot;&gt;Licio Romao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammond_L/0/1/0/all/0/1&quot;&gt;Lewis Hammond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazo_M/0/1/0/all/0/1&quot;&gt;Manuel Mazo Jr.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01620">
<title>SAM as an Optimal Relaxation of Bayes. (arXiv:2210.01620v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01620</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharpness-aware minimization (SAM) and related adversarial deep-learning
methods can drastically improve generalization, but their underlying mechanisms
are not yet fully understood. Here, we establish SAM as a relaxation of the
Bayes objective where the expected negative-loss is replaced by the optimal
convex lower bound, obtained by using the so-called Fenchel biconjugate. The
connection enables a new Adam-like extension of SAM to automatically obtain
reasonable uncertainty estimates, while sometimes also improving its accuracy.
By connecting adversarial and Bayesian methods, our work opens a new path to
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollenhoff_T/0/1/0/all/0/1&quot;&gt;Thomas M&amp;#xf6;llenhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08964">
<title>PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v5 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08964</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new perspective on time series forecasting. In existing
time series forecasting methods, the models take a sequence of numerical values
as input and yield numerical values as output. The existing SOTA models are
largely based on the Transformer architecture, modified with multiple encoding
mechanisms to incorporate the context and semantics around the historical data.
Inspired by the successes of pre-trained language foundation models, we pose a
question about whether these models can also be adapted to solve time-series
forecasting. Thus, we propose a new forecasting paradigm: prompt-based time
series forecasting (PromptCast). In this novel task, the numerical input and
output are transformed into prompts and the forecasting task is framed in a
sentence-to-sentence manner, making it possible to directly apply language
models for forecasting purposes. To support and facilitate the research of this
task, we also present a large-scale dataset (PISA) that includes three
real-world forecasting scenarios. We evaluate different SOTA numerical-based
forecasting methods and language generation models. The benchmark results with
various forecasting settings demonstrate the proposed PromptCast with language
generation models is a promising research direction. Additionally, in
comparison to conventional numerical-based forecasting, PromptCast shows a much
better generalization ability under the zero-shot setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salim_F/0/1/0/all/0/1&quot;&gt;Flora D. Salim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05523">
<title>Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05523</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial training is widely acknowledged as the most effective defense
against adversarial attacks. However, it is also well established that
achieving both robustness and generalization in adversarially trained models
involves a trade-off. The goal of this work is to provide an in depth
comparison of different approaches for adversarial training in language models.
Specifically, we study the effect of pre-training data augmentation as well as
training time input perturbations vs. embedding space perturbations on the
robustness and generalization of transformer-based language models. Our
findings suggest that better robustness can be achieved by pre-training data
augmentation or by training with input space perturbation. However, training
with embedding space perturbation significantly improves generalization. A
linguistic correlation analysis of neurons of the learned models reveals that
the improved generalization is due to &apos;more specialized&apos; neurons. To the best
of our knowledge, this is the first work to carry out a deep qualitative
analysis of different methods of generating adversarial examples in adversarial
training of language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altinisik_E/0/1/0/all/0/1&quot;&gt;Enes Altinisik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1&quot;&gt;Hassan Sajjad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sencar_H/0/1/0/all/0/1&quot;&gt;Husrev Taha Sencar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messaoud_S/0/1/0/all/0/1&quot;&gt;Safa Messaoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1&quot;&gt;Sanjay Chawla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13436">
<title>Solving Bilevel Knapsack Problem using Graph Neural Networks. (arXiv:2211.13436v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13436</link>
<description rdf:parseType="Literal">&lt;p&gt;The Bilevel Optimization Problem is a hierarchical optimization problem with
two agents, a leader and a follower. The leader make their own decisions first,
and the followers make the best choices accordingly. The leader knows the
information of the followers, and the goal of the problem is to find the
optimal solution by considering the reactions of the followers from the
leader&apos;s point of view. For the Bilevel Optimization Problem, there are no
general and efficient algorithms or commercial solvers to get an optimal
solution, and it is very difficult to get a good solution even for a simple
problem. In this paper, we propose a deep learning approach using Graph Neural
Networks to solve the bilevel knapsack problem. We train the model to predict
the leader&apos;s solution and use it to transform the hierarchical optimization
problem into a single-level optimization problem to get the solution. Our model
found the feasible solution that was about 500 times faster than the exact
algorithm with $1.7\%$ optimal gap. Also, our model performed well on problems
of different size from the size it was trained on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Sunhyeon Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hwayong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungsoo Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13715">
<title>Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal Discovery. (arXiv:2211.13715v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13715</link>
<description rdf:parseType="Literal">&lt;p&gt;Inferring causal structure from data is a challenging task of fundamental
importance in science. Observational data are often insufficient to identify a
system&apos;s causal structure uniquely. While conducting interventions (i.e.,
experiments) can improve the identifiability, such samples are usually
challenging and expensive to obtain. Hence, experimental design approaches for
causal discovery aim to minimize the number of interventions by estimating the
most informative intervention target. In this work, we propose a novel
Gradient-based Intervention Targeting method, abbreviated GIT, that &apos;trusts&apos;
the gradient estimator of a gradient-based causal discovery framework to
provide signals for the intervention acquisition function. We provide extensive
experiments in simulated and real-world datasets and demonstrate that GIT
performs on par with competitive baselines, surpassing them in the low-data
regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Olko_M/0/1/0/all/0/1&quot;&gt;Mateusz Olko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zajac_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Zaj&amp;#x105;c&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nowak_A/0/1/0/all/0/1&quot;&gt;Aleksandra Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scherrer_N/0/1/0/all/0/1&quot;&gt;Nino Scherrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Annadani_Y/0/1/0/all/0/1&quot;&gt;Yashas Annadani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1&quot;&gt;Stefan Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kucinski_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Kuci&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Milos_P/0/1/0/all/0/1&quot;&gt;Piotr Mi&amp;#x142;o&amp;#x15b;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13993">
<title>Combating noisy labels in object detection datasets. (arXiv:2211.13993v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13993</link>
<description rdf:parseType="Literal">&lt;p&gt;The quality of training datasets for deep neural networks is a key factor
contributing to the accuracy of resulting models. This effect is amplified in
difficult tasks such as object detection. Dealing with errors in datasets is
often limited to accepting that some fraction of examples are incorrect,
estimating their confidence, and either assigning appropriate weights or
ignoring uncertain ones during training. In this work, we propose a different
approach. We introduce the Confident Learning for Object Detection (CLOD)
algorithm for assessing the quality of each label in object detection datasets,
identifying missing, spurious, mislabeled, and mislocated bounding boxes and
suggesting corrections. By focusing on finding incorrect examples in the
training datasets, we can eliminate them at the root. Suspicious bounding boxes
can be reviewed to improve the quality of the dataset, leading to better models
without further complicating their already complex architectures. The proposed
method is able to point out nearly 80% of artificially disturbed bounding boxes
with a false positive rate below 0.1. Cleaning the datasets by applying the
most confident automatic suggestions improved mAP scores by 16% to 46%,
depending on the dataset, without any modifications to the network
architectures. This approach shows promising potential in rectifying
state-of-the-art object detection datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chachula_K/0/1/0/all/0/1&quot;&gt;Krystian Chachu&amp;#x142;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyskawa_J/0/1/0/all/0/1&quot;&gt;Jakub &amp;#x141;yskawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olber_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Olber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fratczak_P/0/1/0/all/0/1&quot;&gt;Piotr Fr&amp;#x105;tczak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popowicz_A/0/1/0/all/0/1&quot;&gt;Adam Popowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radlak_K/0/1/0/all/0/1&quot;&gt;Krystian Radlak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04090">
<title>Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems. (arXiv:2301.04090v3 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04090</link>
<description rdf:parseType="Literal">&lt;p&gt;Networked discrete dynamical systems are often used to model the spread of
contagions and decision-making by agents in coordination games. Fixed points of
such dynamical systems represent configurations to which the system converges.
In the dissemination of undesirable contagions (such as rumors and
misinformation), convergence to fixed points with a small number of affected
nodes is a desirable goal. Motivated by such considerations, we formulate a
novel optimization problem of finding a nontrivial fixed point of the system
with the minimum number of affected nodes. We establish that, unless P = NP,
there is no polynomial time algorithm for approximating a solution to this
problem to within the factor n^1-\epsilon for any constant epsilon &amp;gt; 0. To cope
with this computational intractability, we identify several special cases for
which the problem can be solved efficiently. Further, we introduce an integer
linear program to address the problem for networks of reasonable sizes. For
solving the problem on larger networks, we propose a general heuristic
framework along with greedy selection methods. Extensive experimental results
on real-world networks demonstrate the effectiveness of the proposed
heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zirou Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marathe_M/0/1/0/all/0/1&quot;&gt;Madhav V. Marathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1&quot;&gt;S. S. Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenkrantz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Rosenkrantz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stearns_R/0/1/0/all/0/1&quot;&gt;Richard E. Stearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vullikanti_A/0/1/0/all/0/1&quot;&gt;Anil Vullikanti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07629">
<title>Generalisation Through Negation and Predicate Invention. (arXiv:2301.07629v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07629</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to generalise from a small number of examples is a fundamental
challenge in machine learning. To tackle this challenge, we introduce an
inductive logic programming (ILP) approach that combines negation and predicate
invention. Combining these two features allows an ILP system to generalise
better by learning rules with universally quantified body-only variables. We
implement our idea in NOPI, which can learn normal logic programs with
predicate invention, including Datalog programs with stratified negation. Our
experimental results on multiple domains show that our approach can improve
predictive accuracies and learning times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerna_D/0/1/0/all/0/1&quot;&gt;David M. Cerna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cropper_A/0/1/0/all/0/1&quot;&gt;Andrew Cropper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10343">
<title>ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10343</link>
<description rdf:parseType="Literal">&lt;p&gt;Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1&quot;&gt;Johannes Brandstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1&quot;&gt;Ashish Kapoor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1&quot;&gt;Jayesh K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11930">
<title>Deep Quantum Error Correction. (arXiv:2301.11930v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11930</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantum error correction codes (QECC) are a key component for realizing the
potential of quantum computing. QECC, as its classical counterpart (ECC),
enables the reduction of error rates, by distributing quantum logical
information across redundant physical qubits, such that errors can be detected
and corrected. In this work, we efficiently train novel {\emph{end-to-end}}
deep quantum error decoders. We resolve the quantum measurement collapse by
augmenting syndrome decoding to predict an initial estimate of the system
noise, which is then refined iteratively through a deep neural network. The
logical error rates calculated over finite fields are directly optimized via a
differentiable objective, enabling efficient decoding under the constraints
imposed by the code. Finally, our architecture is extended to support faulty
syndrome measurement, by efficient decoding of repeated syndrome sampling. The
proposed method demonstrates the power of neural decoders for QECC by achieving
state-of-the-art accuracy, outperforming {for small distance topological
codes,} the existing {end-to-end }neural and classical decoders, which are
often computationally prohibitive.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Choukroun_Y/0/1/0/all/0/1&quot;&gt;Yoni Choukroun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13589">
<title>Policy Gradient for Rectangular Robust Markov Decision Processes. (arXiv:2301.13589v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13589</link>
<description rdf:parseType="Literal">&lt;p&gt;Policy gradient methods have become a standard for training reinforcement
learning agents in a scalable and efficient manner. However, they do not
account for transition uncertainty, whereas learning robust policies can be
computationally expensive. In this paper, we introduce robust policy gradient
(RPG), a policy-based method that efficiently solves rectangular robust Markov
decision processes (MDPs). We provide a closed-form expression for the worst
occupation measure. Incidentally, we find that the worst kernel is a rank-one
perturbation of the nominal. Combining the worst occupation measure with a
robust Q-value estimation yields an explicit form of the robust gradient. Our
resulting RPG can be estimated from data with the same time complexity as its
non-robust equivalent. Hence, it relieves the computational burden of convex
optimization problems required for training robust policies by current policy
gradient approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Navdeep Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derman_E/0/1/0/all/0/1&quot;&gt;Esther Derman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_K/0/1/0/all/0/1&quot;&gt;Kfir Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00735">
<title>MTP-GO: Graph-Based Probabilistic Multi-Agent Trajectory Prediction with Neural ODEs. (arXiv:2302.00735v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00735</link>
<description rdf:parseType="Literal">&lt;p&gt;Enabling resilient autonomous motion planning requires robust predictions of
surrounding road users&apos; future behavior. In response to this need and the
associated challenges, we introduce our model titled MTP-GO. The model encodes
the scene using temporal graph neural networks to produce the inputs to an
underlying motion model. The motion model is implemented using neural ordinary
differential equations where the state-transition functions are learned with
the rest of the model. Multimodal probabilistic predictions are obtained by
combining the concept of mixture density networks and Kalman filtering. The
results illustrate the predictive capabilities of the proposed model across
various data sets, outperforming several state-of-the-art methods on a number
of metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westny_T/0/1/0/all/0/1&quot;&gt;Theodor Westny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oskarsson_J/0/1/0/all/0/1&quot;&gt;Joel Oskarsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olofsson_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Olofsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frisk_E/0/1/0/all/0/1&quot;&gt;Erik Frisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03629">
<title>Ethical Considerations for Responsible Data Curation. (arXiv:2302.03629v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03629</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-centric computer vision (HCCV) data curation practices often neglect
privacy and bias concerns, leading to dataset retractions and unfair models.
HCCV datasets constructed through nonconsensual web scraping lack crucial
metadata for comprehensive fairness and robustness evaluations. Current
remedies are post hoc, lack persuasive justification for adoption, or fail to
provide proper contextualization for appropriate application. Our research
focuses on proactive, domain-specific recommendations, covering purpose,
privacy and consent, and diversity, for curating HCCV evaluation datasets,
addressing privacy and bias concerns. We adopt an ante hoc reflective
perspective, drawing from current practices, guidelines, dataset withdrawals,
and audits, to inform our considerations and recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrews_J/0/1/0/all/0/1&quot;&gt;Jerone T. A. Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dora Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thong_W/0/1/0/all/0/1&quot;&gt;William Thong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1&quot;&gt;Apostolos Modas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papakyriakopoulos_O/0/1/0/all/0/1&quot;&gt;Orestis Papakyriakopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1&quot;&gt;Alice Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05583">
<title>Procedural generation of meta-reinforcement learning tasks. (arXiv:2302.05583v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05583</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-endedness stands to benefit from the ability to generate an infinite
variety of diverse, challenging environments. One particularly interesting type
of challenge is meta-learning (&quot;learning-to-learn&quot;), a hallmark of intelligent
behavior. However, the number of meta-learning environments in the literature
is limited. Here we describe a parametrized space for simple meta-reinforcement
learning (meta-RL) tasks with arbitrary stimuli. The parametrization allows us
to randomly generate an arbitrary number of novel simple meta-learning tasks.
The parametrization is expressive enough to include many well-known meta-RL
tasks, such as bandit problems, the Harlow task, T-mazes, the Daw two-step task
and others. Simple extensions allow it to capture tasks based on
two-dimensional topological spaces, such as full mazes or find-the-spot
domains. We describe a number of randomly generated meta-RL domains of varying
complexity and discuss potential issues arising from random generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miconi_T/0/1/0/all/0/1&quot;&gt;Thomas Miconi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05906">
<title>On Comparing Fair Classifiers under Data Bias. (arXiv:2302.05906v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05906</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we consider a theoretical model for injecting data bias,
namely, under-representation and label bias (Blum &amp;amp; Stangl, 2019). We
empirically study the effect of varying data biases on the accuracy and
fairness of fair classifiers. Through extensive experiments on both synthetic
and real-world datasets (e.g., Adult, German Credit, Bank Marketing, COMPAS),
we empirically audit pre-, in-, and post-processing fair classifiers from
standard fairness toolkits for their fairness and accuracy by injecting varying
amounts of under-representation and label bias in their training data (but not
the test data). Our main observations are: 1. The fairness and accuracy of many
standard fair classifiers degrade severely as the bias injected in their
training data increases, 2. A simple logistic regression model trained on the
right data can often outperform, in both accuracy and fairness, most fair
classifiers trained on biased training data, and 3. A few, simple fairness
techniques (e.g., reweighing, exponentiated gradients) seem to offer stable
accuracy and fairness guarantees even when their training data is injected with
under-representation and label bias. Our experiments also show how to integrate
a measure of data bias risk in the existing fairness dashboards for real-world
deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mohit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Amit Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1&quot;&gt;Rajiv Ratn Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06527">
<title>An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation. (arXiv:2302.06527v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06527</link>
<description rdf:parseType="Literal">&lt;p&gt;Unit tests play a key role in ensuring the correctness of software. However,
manually creating unit tests is a laborious task, motivating the need for
automation. Large Language Models (LLMs) have recently been applied to this
problem, utilizing additional training or few-shot learning on examples of
existing tests. This paper presents a large-scale empirical evaluation on the
effectiveness of LLMs for automated unit test generation without additional
training or manual effort, providing the LLM with the signature and
implementation of the function under test, along with usage examples extracted
from documentation. We also attempt to repair failed generated tests by
re-prompting the model with the failing test and error message. We implement
our approach in TestPilot, a test generation tool for JavaScript that
automatically generates unit tests for all API functions in an npm package. We
evaluate TestPilot using OpenAI&apos;s gpt3.5-turbo LLM on 25 npm packages with a
total of 1,684 API functions. The generated tests achieve a median statement
coverage of 70.2% and branch coverage of 52.8%, significantly improving on
Nessie, a recent feedback-directed JavaScript test generation technique, which
achieves only 51.3% statement coverage and 25.6% branch coverage. We also find
that 92.8% of TestPilot&apos;s generated tests have no more than 50% similarity with
existing tests (as measured by normalized edit distance), with none of them
being exact copies. Finally, we run TestPilot with two additional LLMs,
OpenAI&apos;s older code-cushman-002 LLM and the open LLM StarCoder. Overall, we
observed similar results with the former (68.2% median statement coverage), and
somewhat worse results with the latter (54.0% median statement coverage),
suggesting that the effectiveness of the approach is influenced by the size and
training set of the LLM, but does not fundamentally depend on the specific
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schafer_M/0/1/0/all/0/1&quot;&gt;Max Sch&amp;#xe4;fer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadi_S/0/1/0/all/0/1&quot;&gt;Sarah Nadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eghbali_A/0/1/0/all/0/1&quot;&gt;Aryaz Eghbali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tip_F/0/1/0/all/0/1&quot;&gt;Frank Tip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08015">
<title>Individual Fairness under Uncertainty. (arXiv:2302.08015v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08015</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithmic fairness, the research field of making machine learning (ML)
algorithms fair, is an established area in ML. As ML technologies expand their
application domains, including ones with high societal impact, it becomes
essential to take fairness into consideration during the building of ML
systems. Yet, despite its wide range of socially sensitive applications, most
work treats the issue of algorithmic bias as an intrinsic property of
supervised learning, i.e., the class label is given as a precondition. Unlike
prior studies in fairness, we propose an individual fairness measure and a
corresponding algorithm that deal with the challenges of uncertainty arising
from censorship in class labels, while enforcing similar individuals to be
treated similarly from a ranking perspective, free of the Lipschitz condition
in the conventional individual fairness definition. We argue that this
perspective represents a more realistic model of fairness research for
real-world application deployment and show how learning with such a relaxed
precondition draws new insights that better explains algorithmic fairness. We
conducted experiments on four real-world datasets to evaluate our proposed
method compared to other fairness models, demonstrating its superiority in
minimizing discrimination while maintaining predictive performance with
uncertainty present.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Juyong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Cheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oommen_T/0/1/0/all/0/1&quot;&gt;Thomas Oommen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1&quot;&gt;Pradeep Ravikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_J/0/1/0/all/0/1&quot;&gt;Jeremy Weiss&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12247">
<title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1&quot;&gt;Chun Kai Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Suzanne Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06121">
<title>Ignorance is Bliss: Robust Control via Information Gating. (arXiv:2303.06121v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06121</link>
<description rdf:parseType="Literal">&lt;p&gt;Informational parsimony provides a useful inductive bias for learning
representations that achieve better generalization by being robust to noise and
spurious correlations. We propose \textit{information gating} as a way to learn
parsimonious representations that identify the minimal information required for
a task. When gating information, we can learn to reveal as little information
as possible so that a task remains solvable, or hide as little information as
possible so that a task becomes unsolvable. We gate information using a
differentiable parameterization of the signal-to-noise ratio, which can be
applied to arbitrary values in a network, e.g., erasing pixels at the input
layer or activations in some intermediate layer. When gating at the input
layer, our models learn which visual cues matter for a given task. When gating
intermediate layers, our models learn which activations are needed for
subsequent stages of computation. We call our approach \textit{InfoGating}. We
apply InfoGating to various objectives such as multi-step forward and inverse
dynamics models, Q-learning, and behavior cloning, highlighting how InfoGating
can naturally help in discarding information not relevant for control. Results
show that learning to identify and use minimal information can improve
generalization in downstream tasks. Policies based on InfoGating are
considerably more robust to irrelevant visual features, leading to improved
pretraining and finetuning of RL models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1&quot;&gt;Manan Tomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Riashat Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1&quot;&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachman_P/0/1/0/all/0/1&quot;&gt;Philip Bachman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17879">
<title>CoSMo: a Framework to Instantiate Conditioned Process Simulation Models. (arXiv:2303.17879v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17879</link>
<description rdf:parseType="Literal">&lt;p&gt;Process simulation is gaining attention for its ability to assess potential
performance improvements and risks associated with business process changes.
The existing literature presents various techniques, generally grounded in
process models discovered from event logs or built upon deep learning
algorithms. These techniques have specific strengths and limitations.
Traditional approaches rooted in process models offer increased
interpretability, while those using deep learning excel at generalizing changes
across large event logs. However, the practical application of deep learning
faces challenges related to managing stochasticity and integrating information
for what-if analysis. This paper introduces a novel recurrent neural
architecture tailored to discover COnditioned process Simulation MOdels (CoSMo)
based on user-based constraints or any other nature of a-priori knowledge. This
architecture facilitates the simulation of event logs that adhere to specific
constraints by incorporating declarative-based rules into the learning phase as
an attempt to fill the gap of incorporating information into deep learning
models to perform what-if analysis. Experimental validation illustrates CoSMo&apos;s
efficacy in simulating event logs while adhering to predefined declarative
conditions, emphasizing both control-flow and data-flow perspectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyamada_R/0/1/0/all/0/1&quot;&gt;Rafael S. Oyamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavares_G/0/1/0/all/0/1&quot;&gt;Gabriel M. Tavares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceravolo_P/0/1/0/all/0/1&quot;&gt;Paolo Ceravolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00192">
<title>Leveraging Neo4j and deep learning for traffic congestion simulation &amp; optimization. (arXiv:2304.00192v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00192</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic congestion has been a major challenge in many urban road networks.
Extensive research studies have been conducted to highlight traffic-related
congestion and address the issue using data-driven approaches. Currently, most
traffic congestion analyses are done using simulation software that offers
limited insight due to the limitations in the tools and utilities being used to
render various traffic congestion scenarios. All that impacts the formulation
of custom business problems which vary from place to place and country to
country. By exploiting the power of the knowledge graph, we model a traffic
congestion problem into the Neo4j graph and then use the load balancing,
optimization algorithm to identify congestion-free road networks. We also show
how traffic propagates backward in case of congestion or accident scenarios and
its overall impact on other segments of the roads. We also train a sequential
RNN-LSTM (Long Short-Term Memory) deep learning model on the real-time traffic
data to assess the accuracy of simulation results based on a road-specific
congestion. Our results show that graph-based traffic simulation, supplemented
by AI ML-based traffic prediction can be more effective in estimating the
congestion level in a road network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shyam Pratap Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Arshad Ali Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souissi_R/0/1/0/all/0/1&quot;&gt;Riad Souissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yusuf_S/0/1/0/all/0/1&quot;&gt;Syed Adnan Yusuf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01844">
<title>Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning. (arXiv:2304.01844v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;Comprehending how the brain interacts with the external world through
generated neural data is crucial for determining its working mechanism,
treating brain diseases, and understanding intelligence. Although many
theoretical models have been proposed, they have thus far been difficult to
integrate and develop. In this study, we were inspired in part by grid cells in
creating a more general and robust grid module and constructing an interactive
and self-reinforcing cognitive system together with Bayesian reasoning, an
approach called space-division and exploration-exploitation with grid-feedback
(Grid-SD2E). Here, a grid module can be used as an interaction medium between
the outside world and a system, as well as a self-reinforcement medium within
the system. The space-division and exploration-exploitation (SD2E) receives the
0/1 signals of a grid through its space-division (SD) module. The system
described in this paper is also a theoretical model derived from experiments
conducted by other researchers and our experience on neural decoding. Herein,
we analyse the rationality of the system based on the existing theories in both
neuroscience and cognitive science, and attempt to propose special and general
rules to explain the different interactions between people and between people
and the external world. What&apos;s more, based on this framework, the smallest
computing unit is extracted, which is analogous to a single neuron in the
brain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jingyi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07460">
<title>Communication and Energy Efficient Wireless Federated Learning with Intrinsic Privacy. (arXiv:2304.07460v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07460</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a collaborative learning framework that enables
edge devices to collaboratively learn a global model while keeping raw data
locally. Although FL avoids leaking direct information from local datasets,
sensitive information can still be inferred from the shared models. To address
the privacy issue in FL, differential privacy (DP) mechanisms are leveraged to
provide formal privacy guarantee. However, when deploying FL at the wireless
edge with over-the-air computation, ensuring client-level DP faces significant
challenges. In this paper, we propose a novel wireless FL scheme called private
federated edge learning with sparsification (PFELS) to provide client-level DP
guarantee with intrinsic channel noise while reducing communication and energy
overhead and improving model accuracy. The key idea of PFELS is for each device
to first compress its model update and then adaptively design the transmit
power of the compressed model update according to the wireless channel status
without any artificial noise addition. We provide a privacy analysis for PFELS
and prove the convergence of PFELS under general non-convex and non-IID
settings. Experimental results show that compared with prior work, PFELS can
improve the accuracy with the same DP guarantee and save communication and
energy costs simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenxiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanxiong Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuguang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yanmin Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10351">
<title>Inducing Stackelberg Equilibrium through Spatio-Temporal Sequential Decision-Making in Multi-Agent Reinforcement Learning. (arXiv:2304.10351v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10351</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-agent reinforcement learning (MARL), self-interested agents attempt
to establish equilibrium and achieve coordination depending on game structure.
However, existing MARL approaches are mostly bound by the simultaneous actions
of all agents in the Markov game (MG) framework, and few works consider the
formation of equilibrium strategies via asynchronous action coordination. In
view of the advantages of Stackelberg equilibrium (SE) over Nash equilibrium,
we construct a spatio-temporal sequential decision-making structure derived
from the MG and propose an N-level policy model based on a conditional
hypernetwork shared by all agents. This approach allows for asymmetric training
with symmetric execution, with each agent responding optimally conditioned on
the decisions made by superior agents. Agents can learn heterogeneous SE
policies while still maintaining parameter sharing, which leads to reduced cost
for learning and storage and enhanced scalability as the number of agents
increases. Experiments demonstrate that our method effectively converges to the
SE policies in repeated matrix game scenarios, and performs admirably in
immensely complex settings including cooperative tasks and mixed tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lijuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dapeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1&quot;&gt;Guoliang Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11171">
<title>Granular-ball computing: an efficient, robust, and interpretable adaptive multi-granularity representation and computation method. (arXiv:2304.11171v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11171</link>
<description rdf:parseType="Literal">&lt;p&gt;Human cognition operates on a &quot;Global-first&quot; cognitive mechanism,
prioritizing information processing based on coarse-grained details. This
mechanism inherently possesses an adaptive multi-granularity description
capacity, resulting in computational traits such as efficiency, robustness, and
interpretability. The analysis pattern reliance on the finest granularity and
single-granularity makes most existing computational methods less efficient,
robust, and interpretable, which is an important reason for the current lack of
interpretability in neural networks. Multi-granularity granular-ball computing
employs granular-balls of varying sizes to daptively represent and envelop the
sample space, facilitating learning based on these granular-balls. Given that
the number of coarse-grained &quot;granular-balls&quot; is fewer than sample points,
granular-ball computing proves more efficient. Moreover, the inherent
coarse-grained nature of granular-balls reduces susceptibility to fine-grained
sample disturbances, enhancing robustness. The multi-granularity construct of
granular-balls generates topological structures and coarse-grained
descriptions, naturally augmenting interpretability. Granular-ball computing
has successfully ventured into diverse AI domains, fostering the development of
innovative theoretical methods, including granular-ball classifiers, clustering
techniques, neural networks, rough sets, and evolutionary computing. This has
notably ameliorated the efficiency, noise robustness, and interpretability of
traditional methods. Overall, granular-ball computing is a rare and innovative
theoretical approach in AI that can adaptively and simultaneously enhance
efficiency, robustness, and interpretability. This article delves into the main
application landscapes for granular-ball computing, aiming to equip future
researchers with references and insights to refine and expand this promising
theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shuyin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Lian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12180">
<title>Variance-Reduced Gradient Estimation via Noise-Reuse in Online Evolution Strategies. (arXiv:2304.12180v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12180</link>
<description rdf:parseType="Literal">&lt;p&gt;Unrolled computation graphs are prevalent throughout machine learning but
present challenges to automatic differentiation (AD) gradient estimation
methods when their loss functions exhibit extreme local sensitivtiy,
discontinuity, or blackbox characteristics. In such scenarios, online evolution
strategies methods are a more capable alternative, while being more
parallelizable than vanilla evolution strategies (ES) by interleaving partial
unrolls and gradient updates. In this work, we propose a general class of
unbiased online evolution strategies methods. We analytically and empirically
characterize the variance of this class of gradient estimators and identify the
one with the least variance, which we term Noise-Reuse Evolution Strategies
(NRES). Experimentally, we show NRES results in faster convergence than
existing AD and ES methods in terms of wall-clock time and number of unroll
steps across a variety of applications, including learning dynamical systems,
meta-training learned optimizers, and reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1&quot;&gt;Oscar Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1&quot;&gt;James Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1&quot;&gt;Jascha Sohl-Dickstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1&quot;&gt;Virginia Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metz_L/0/1/0/all/0/1&quot;&gt;Luke Metz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03520">
<title>Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03520</link>
<description rdf:parseType="Literal">&lt;p&gt;The issue of word sense ambiguity poses a significant challenge in natural
language processing due to the scarcity of annotated data to feed machine
learning models to face the challenge. Therefore, unsupervised word sense
disambiguation methods have been developed to overcome that challenge without
relying on annotated data. This research proposes a new context-aware approach
to unsupervised word sense disambiguation, which provides a flexible mechanism
for incorporating contextual information into the similarity measurement
process. We experiment with a popular benchmark dataset to evaluate the
proposed strategy and compare its performance with state-of-the-art
unsupervised word sense disambiguation techniques. The experimental results
indicate that our approach substantially enhances disambiguation accuracy and
surpasses the performance of several existing techniques. Our findings
underscore the significance of integrating contextual information in semantic
similarity measurements to manage word sense ambiguity in unsupervised
scenarios effectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1&quot;&gt;Jorge Martinez-Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04388">
<title>Language Models Don&apos;t Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting. (arXiv:2305.04388v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04388</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) can achieve strong performance on many tasks by
producing step-by-step reasoning before giving a final output, often referred
to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT
explanations as the LLM&apos;s process for solving a task. This level of
transparency into LLMs&apos; predictions would yield significant safety benefits.
However, we find that CoT explanations can systematically misrepresent the true
reason for a model&apos;s prediction. We demonstrate that CoT explanations can be
heavily influenced by adding biasing features to model inputs--e.g., by
reordering the multiple-choice options in a few-shot prompt to make the answer
always &quot;(A)&quot;--which models systematically fail to mention in their
explanations. When we bias models toward incorrect answers, they frequently
generate CoT explanations rationalizing those answers. This causes accuracy to
drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing
with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task,
model explanations justify giving answers in line with stereotypes without
mentioning the influence of these social biases. Our findings indicate that CoT
explanations can be plausible yet misleading, which risks increasing our trust
in LLMs without guaranteeing their safety. Building more transparent and
explainable systems will require either improving CoT faithfulness through
targeted efforts or abandoning CoT in favor of alternative methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1&quot;&gt;Miles Turpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1&quot;&gt;Julian Michael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06225">
<title>DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06225</link>
<description rdf:parseType="Literal">&lt;p&gt;Predominant techniques on talking head generation largely depend on 2D
information, including facial appearances and motions from input face images.
Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a
critical role in constructing accurate 3D facial structures and suppressing
complex background noises for generation. However, dense 3D annotations for
facial videos is prohibitively costly to obtain. In this work, firstly, we
present a novel self-supervised method for learning dense 3D facial geometry
(ie, depth) from face videos, without requiring camera parameters and 3D
geometry annotations in training. We further propose a strategy to learn
pixel-level uncertainties to perceive more reliable rigid-motion pixels for
geometry learning. Secondly, we design an effective geometry-guided facial
keypoint estimation module, providing accurate keypoints for generating motion
fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth)
attention mechanism, which can be applied to each generation layer, to capture
facial geometries in a coarse-to-fine manner. Extensive experiments are
conducted on three challenging benchmarks (ie, VoxCeleb1, VoxCeleb2, and HDTF).
The results demonstrate that our proposed framework can generate highly
realistic-looking reenacted talking videos, with new state-of-the-art
performances established on these benchmarks. The codes and trained models are
publicly available on the GitHub project page at
https://github.com/harlanhong/CVPR2022-DaGAN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1&quot;&gt;Fa-Ting Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06324">
<title>Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06324</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Integrated Multimodal Perception (IMP), a simple and scalable
multimodal multi-task training and modeling approach. IMP integrates multimodal
inputs including image, video, text, and audio into a single Transformer
encoder with minimal modality-specific components. IMP makes use of a novel
design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts
(MoE) for efficient model and task scaling. We conduct extensive empirical
studies and reveal the following key insights: 1) Performing gradient descent
updates by alternating on diverse modalities, loss functions, and tasks, with
varying input resolutions, efficiently improves the model. 2) Sparsification
with MoE on a single modality-agnostic encoder substantially improves the
performance, outperforming dense models that use modality-specific encoders or
additional fusion layers and greatly mitigates the conflicts between
modalities. IMP achieves competitive performance on a wide range of downstream
tasks including video classification, image classification, image-text, and
video-text retrieval. Most notably, we train a sparse IMP-MoE-L variant
focusing on video tasks that achieves new state-of-the-art in zero-shot video
classification: 77.0% on Kinetics-400, 76.8% on Kinetics-600, and 68.3% on
Kinetics-700, improving the previous state-of-the-art by +5%, +6.7%, and +5.8%,
respectively, while using only 15% of their total training computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_H/0/1/0/all/0/1&quot;&gt;Hassan Akbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1&quot;&gt;Dan Kondratyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hornung_R/0/1/0/all/0/1&quot;&gt;Rachel Hornung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Huisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_H/0/1/0/all/0/1&quot;&gt;Hartwig Adam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13048">
<title>RWKV: Reinventing RNNs for the Transformer Era. (arXiv:2305.13048v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13048</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have revolutionized almost all natural language processing (NLP)
tasks but suffer from memory and computational complexity that scales
quadratically with sequence length. In contrast, recurrent neural networks
(RNNs) exhibit linear scaling in memory and computational requirements but
struggle to match the same performance as Transformers due to limitations in
parallelization and scalability. We propose a novel model architecture,
Receptance Weighted Key Value (RWKV), that combines the efficient
parallelizable training of transformers with the efficient inference of RNNs.
&lt;/p&gt;
&lt;p&gt;Our approach leverages a linear attention mechanism and allows us to
formulate the model as either a Transformer or an RNN, thus parallelizing
computations during training and maintains constant computational and memory
complexity during inference. We scale our models as large as 14 billion
parameters, by far the largest dense RNN ever trained, and find RWKV performs
on par with similarly sized Transformers, suggesting future work can leverage
this architecture to create more efficient models. This work presents a
significant step towards reconciling trade-offs between computational
efficiency and model performance in sequence processing tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bo Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcaide_E/0/1/0/all/0/1&quot;&gt;Eric Alcaide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1&quot;&gt;Quentin Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1&quot;&gt;Alon Albalak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcadinho_S/0/1/0/all/0/1&quot;&gt;Samuel Arcadinho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1&quot;&gt;Stella Biderman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Huanqi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_M/0/1/0/all/0/1&quot;&gt;Michael Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grella_M/0/1/0/all/0/1&quot;&gt;Matteo Grella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+GV_K/0/1/0/all/0/1&quot;&gt;Kranthi Kiran GV&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuzheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Haowen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiaju Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazienko_P/0/1/0/all/0/1&quot;&gt;Przemyslaw Kazienko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1&quot;&gt;Jan Kocon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1&quot;&gt;Jiaming Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koptyra_B/0/1/0/all/0/1&quot;&gt;Bartlomiej Koptyra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_H/0/1/0/all/0/1&quot;&gt;Hayden Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mantri_K/0/1/0/all/0/1&quot;&gt;Krishna Sri Ipsit Mantri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mom_F/0/1/0/all/0/1&quot;&gt;Ferdinand Mom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_A/0/1/0/all/0/1&quot;&gt;Atsushi Saito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guangyu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiangru Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bolun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wind_J/0/1/0/all/0/1&quot;&gt;Johan S. Wind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1&quot;&gt;Stanislaw Wozniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruichong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qihang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Peng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Rui-Jie Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15769">
<title>MERGE: Fast Private Text Generation. (arXiv:2305.15769v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15769</link>
<description rdf:parseType="Literal">&lt;p&gt;The drastic increase in language models&apos; parameters has led to a new trend of
deploying models in cloud servers, raising growing concerns about private
inference for Transformer-based models. Existing two-party privacy-preserving
techniques, however, only take into account natural language understanding
(NLU) scenarios. Private inference in natural language generation (NLG),
crucial for applications like translation and code completion, remains
underexplored.In addition, previous privacy-preserving techniques suffer from
convergence issues during model training and exhibit poor inference speed when
used with NLG models due to the neglect of time-consuming operations in
auto-regressive generations. To address these issues, we propose a fast private
text generation framework for Transformer-based language models, namely
MERGE.MERGE reuses the output hidden state as the word embedding to bypass the
embedding computation and reorganize the linear operations in the Transformer
module to accelerate the forward procedure. Extensive experiments show that
MERGE achieves a 26.5x speedup to the vanilla encrypted model under the
sequence length 512, and reduces 80\% communication cost, with an up to 10x
speedup to state-of-the-art approximated models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pinghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Lifeng Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00393">
<title>Teacher Agent: A Knowledge Distillation-Free Framework for Rehearsal-based Video Incremental Learning. (arXiv:2306.00393v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00393</link>
<description rdf:parseType="Literal">&lt;p&gt;Rehearsal-based video incremental learning often employs knowledge
distillation to mitigate catastrophic forgetting of previously learned data.
However, this method faces two major challenges for video task: substantial
computing resources from loading teacher model and limited replay capability
from performance-limited teacher model. To address these problems, we first
propose a knowledge distillation-free framework for rehearsal-based video
incremental learning called \textit{Teacher Agent}. Instead of loading
parameter-heavy teacher networks, we introduce an agent generator that is
either parameter-free or uses only a few parameters to obtain accurate and
reliable soft labels. This method not only greatly reduces the computing
requirement but also circumvents the problem of knowledge misleading caused by
inaccurate predictions of the teacher model. Moreover, we put forward a
self-correction loss which provides an effective regularization signal for the
review of old knowledge, which in turn alleviates the problem of catastrophic
forgetting. Further, to ensure that the samples in the memory buffer are
memory-efficient and representative, we introduce a unified sampler for
rehearsal-based video incremental learning to mine fixed-length key video
frames. Interestingly, based on the proposed strategies, the network exhibits a
high level of robustness against spatial resolution reduction when compared to
the baseline. Extensive experiments demonstrate the advantages of our method,
yielding significant performance improvements while utilizing only half the
spatial resolution of video clips as network inputs in the incremental phases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shengqin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yaoyu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haokui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yuankai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01804">
<title>Extracting Reward Functions from Diffusion Models. (arXiv:2306.01804v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01804</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved remarkable results in image generation, and
have similarly been used to learn high-performing policies in sequential
decision-making tasks. Decision-making diffusion models can be trained on
lower-quality data, and then be steered with a reward function to generate
near-optimal trajectories. We consider the problem of extracting a reward
function by comparing a decision-making diffusion model that models low-reward
behavior and one that models high-reward behavior; a setting related to inverse
reinforcement learning. We first define the notion of a relative reward
function of two diffusion models and show conditions under which it exists and
is unique. We then devise a practical learning algorithm for extracting it by
aligning the gradients of a reward function -- parametrized by a neural network
-- to the difference in outputs of both diffusion models. Our method finds
correct reward functions in navigation environments, and we demonstrate that
steering the base model with the learned reward functions results in
significantly increased performance in standard locomotion benchmarks. Finally,
we demonstrate that our approach generalizes beyond sequential decision-making
by learning a reward-like function from two large-scale image generation
diffusion models. The extracted reward function successfully assigns lower
rewards to harmful images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuti_F/0/1/0/all/0/1&quot;&gt;Felipe Nuti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franzmeyer_T/0/1/0/all/0/1&quot;&gt;Tim Franzmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02320">
<title>Exploring the Impact of Model Scaling on Parameter-Efficient Tuning. (arXiv:2306.02320v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02320</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter-efficient tuning (PET) methods can effectively drive extremely
large pre-trained language models (PLMs) by training only minimal parameters.
Different PET methods utilize different manually designed tunable modules. In
small PLMs, there are usually noticeable performance differences among PET
methods. Nevertheless, as the model scale increases, the performance
differences become marginal. Hence, we hypothesize that model scaling mitigates
the impact of design differences on PET methods. To investigate this
hypothesis, we introduce a more flexible PET method called Arbitrary PET (APET)
method. The APET method is compatible with a tunable module, which consists of
any number of parameters distributed in arbitrary positions. Then, we utilize
it and conduct experiments on 11 NLP tasks across 3 representative PLMs. Our
investigations reveal that model scaling (1) mitigates the effects of the
positions of tunable parameters on performance, and (2) enables tuning methods
to achieve performance comparable to full-parameter fine-tuning by optimizing
fewer tunable parameters. Intriguingly, we also observe that tuning methods
optimize the similar number of tunable parameters to exceed random guess
performance on different tasks. We collectively discuss this phenomenon and the
two aforementioned findings from an optimization perspective to understand the
underlying mechanisms. These conclusions enhance our understanding of the
impact of model scaling on PET and assist in designing more effective and
efficient PET methods for PLMs of different scales. The source code can be
obtained from this GitHub repository:
\url{https://github.com/yushengsu-thu/PET_Scaling}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yusheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chi-Min Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jiali Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yujia Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengding Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zonghan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingzhi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guotong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Maosong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09364">
<title>TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting. (arXiv:2306.09364v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09364</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have gained popularity in time series forecasting for their
ability to capture long-sequence interactions. However, their high memory and
computing requirements pose a critical bottleneck for long-term forecasting. To
address this, we propose TSMixer, a lightweight neural architecture exclusively
composed of multi-layer perceptron (MLP) modules for multivariate forecasting
and representation learning on patched time series. Inspired by MLP-Mixer&apos;s
success in computer vision, we adapt it for time series, addressing challenges
and introducing validated components for enhanced accuracy. This includes a
novel design paradigm of attaching online reconciliation heads to the MLP-Mixer
backbone, for explicitly modeling the time-series properties such as hierarchy
and channel-correlations. We also propose a novel Hybrid channel modeling and
infusion of a simple gating approach to effectively handle noisy channel
interactions and generalization across diverse datasets. By incorporating these
lightweight components, we significantly enhance the learning capability of
simple MLP structures, outperforming complex Transformer models with minimal
computing usage. Moreover, TSMixer&apos;s modular design enables compatibility with
both supervised and masked self-supervised learning methods, making it a
promising building block for time-series Foundation Models. TSMixer outperforms
state-of-the-art MLP and Transformer models in forecasting by a considerable
margin of 8-60%. It also outperforms the latest strong benchmarks of
Patch-Transformer models (by 1-2%) with a significant reduction in memory and
runtime (2-3X). The source code of our model is officially released as
PatchTSMixer in the HuggingFace. Model:
https://huggingface.co/docs/transformers/main/en/model_doc/patchtsmixer
Examples: https://github.com/ibm/tsfm/#notebooks-links
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ekambaram_V/0/1/0/all/0/1&quot;&gt;Vijay Ekambaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jati_A/0/1/0/all/0/1&quot;&gt;Arindam Jati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Nam Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinthong_P/0/1/0/all/0/1&quot;&gt;Phanwadee Sinthong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalagnanam_J/0/1/0/all/0/1&quot;&gt;Jayant Kalagnanam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11698">
<title>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11698</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/. Additionally, our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust, and a concise version
of our DecodingTrust is accessible at https://openreview.net/pdf?id=kaHpo8OZw2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1&quot;&gt;Hengzhi Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chulin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Mintong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chejian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zidi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_R/0/1/0/all/0/1&quot;&gt;Ritik Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1&quot;&gt;Sang T. Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Simran Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02028">
<title>EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models. (arXiv:2307.02028v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02028</link>
<description rdf:parseType="Literal">&lt;p&gt;While the general machine learning (ML) community has benefited from public
datasets, tasks, and models, the progress of ML in healthcare has been hampered
by a lack of such shared assets. The success of foundation models creates new
challenges for healthcare ML by requiring access to shared pretrained models to
validate performance benefits. We help address these challenges through three
contributions. First, we publish a new dataset, EHRSHOT, which contains
deidentified structured data from the electronic health records (EHRs) of 6,739
patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR
datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients.
Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical
foundation model pretrained on the structured EHR data of 2.57M patients. We
are one of the first to fully release such a model for coded EHR data; in
contrast, most prior models released for clinical data (e.g. GatorTron,
ClinicalBERT) only work with unstructured text and cannot process the rich,
structured data within an EHR. We provide an end-to-end pipeline for the
community to validate and build upon its performance. Third, we define 15
few-shot clinical prediction tasks, enabling evaluation of foundation models on
benefits such as sample efficiency and task adaptation. Our model and dataset
are available via a research data use agreement from our website:
https://ehrshot.stanford.edu. Code to reproduce our results are available at
our Github repo: https://github.com/som-shahlab/ehrshot-benchmark
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wornow_M/0/1/0/all/0/1&quot;&gt;Michael Wornow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_R/0/1/0/all/0/1&quot;&gt;Rahul Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinberg_E/0/1/0/all/0/1&quot;&gt;Ethan Steinberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1&quot;&gt;Jason A. Fries&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Nigam H. Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06483">
<title>Misclassification in Automated Content Analysis Causes Bias in Regression. Can We Fix It? Yes We Can!. (arXiv:2307.06483v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06483</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated classifiers (ACs), often built via supervised machine learning
(SML), can categorize large, statistically powerful samples of data ranging
from text to images and video, and have become widely popular measurement
devices in communication science and related fields. Despite this popularity,
even highly accurate classifiers make errors that cause misclassification bias
and misleading results in downstream analyses-unless such analyses account for
these errors. As we show in a systematic literature review of SML applications,
communication scholars largely ignore misclassification bias. In principle,
existing statistical methods can use &quot;gold standard&quot; validation data, such as
that created by human annotators, to correct misclassification bias and produce
consistent estimates. We introduce and test such methods, including a new
method we design and implement in the R package misclassificationmodels, via
Monte Carlo simulations designed to reveal each method&apos;s limitations, which we
also release. Based on our results, we recommend our new error correction
method as it is versatile and efficient. In sum, automated classifiers, even
those below common accuracy standards or making systematic misclassifications,
can be useful for measurement with careful study design and appropriate error
correction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+TeBlunthuis_N/0/1/0/all/0/1&quot;&gt;Nathan TeBlunthuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_V/0/1/0/all/0/1&quot;&gt;Valerie Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chung-Hong Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07085">
<title>Machine-learned molecular mechanics force field for the simulation of protein-ligand systems and beyond. (arXiv:2307.07085v4 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07085</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of reliable and extensible molecular mechanics (MM) force
fields -- fast, empirical models characterizing the potential energy surface of
molecular systems -- is indispensable for biomolecular simulation and
computer-aided drug design. Here, we introduce a generalized and extensible
machine-learned MM force field, \texttt{espaloma-0.3}, and an end-to-end
differentiable framework using graph neural networks to overcome the
limitations of traditional rule-based methods. Trained in a single GPU-day to
fit a large and diverse quantum chemical dataset of over 1.1M energy and force
calculations, \texttt{espaloma-0.3} reproduces quantum chemical energetic
properties of chemical domains highly relevant to drug discovery, including
small molecules, peptides, and nucleic acids. Moreover, this force field
maintains the quantum chemical energy-minimized geometries of small molecules
and preserves the condensed phase properties of peptides, self-consistently
parametrizing proteins and ligands to produce stable simulations leading to
highly accurate predictions of binding free energies. This methodology
demonstrates significant promise as a path forward for systematically building
more accurate force fields that are easily extensible to new chemical domains
of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Takaba_K/0/1/0/all/0/1&quot;&gt;Kenichiro Takaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pulido_I/0/1/0/all/0/1&quot;&gt;Iv&amp;#xe1;n Pulido&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Behara_P/0/1/0/all/0/1&quot;&gt;Pavan Kumar Behara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cavender_C/0/1/0/all/0/1&quot;&gt;Chapin E. Cavender&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Friedman_A/0/1/0/all/0/1&quot;&gt;Anika J. Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Henry_M/0/1/0/all/0/1&quot;&gt;Michael M. Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Opeskin_H/0/1/0/all/0/1&quot;&gt;Hugo MacDermott Opeskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Iacovella_C/0/1/0/all/0/1&quot;&gt;Christopher R. Iacovella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nagle_A/0/1/0/all/0/1&quot;&gt;Arnav M. Nagle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Payne_A/0/1/0/all/0/1&quot;&gt;Alexander Matthew Payne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shirts_M/0/1/0/all/0/1&quot;&gt;Michael R. Shirts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mobley_D/0/1/0/all/0/1&quot;&gt;David L. Mobley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chodera_J/0/1/0/all/0/1&quot;&gt;John D. Chodera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanqing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09072">
<title>Real-time Inference and Extrapolation via a Diffusion-inspired Temporal Transformer Operator (DiTTO). (arXiv:2307.09072v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09072</link>
<description rdf:parseType="Literal">&lt;p&gt;Extrapolation remains a grand challenge in deep neural networks across all
application domains. We propose an operator learning method to solve
time-dependent partial differential equations (PDEs) continuously and with
extrapolation in time without any temporal discretization. The proposed method,
named Diffusion-inspired Temporal Transformer Operator (DiTTO), is inspired by
latent diffusion models and their conditioning mechanism, which we use to
incorporate the temporal evolution of the PDE, in combination with elements
from the transformer architecture to improve its capabilities. Upon training,
DiTTO can make inferences in real-time. We demonstrate its extrapolation
capability on a climate problem by estimating the temperature around the globe
for several years, and also in modeling hypersonic flows around a double-cone.
We propose different training strategies involving temporal-bundling and
sub-sampling and demonstrate performance improvements for several benchmarks,
performing extrapolation for long time intervals as well as zero-shot
super-resolution in time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ovadia_O/0/1/0/all/0/1&quot;&gt;Oded Ovadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oommen_V/0/1/0/all/0/1&quot;&gt;Vivek Oommen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahana_A/0/1/0/all/0/1&quot;&gt;Adar Kahana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peyvan_A/0/1/0/all/0/1&quot;&gt;Ahmad Peyvan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turkel_E/0/1/0/all/0/1&quot;&gt;Eli Turkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11730">
<title>Mitigating Communications Threats in Decentralized Federated Learning through Moving Target Defense. (arXiv:2307.11730v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11730</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Decentralized Federated Learning (DFL) has enabled the training
of machine learning models across federated participants, fostering
decentralized model aggregation and reducing dependence on a server. However,
this approach introduces unique communication security challenges that have yet
to be thoroughly addressed in the literature. These challenges primarily
originate from the decentralized nature of the aggregation process, the varied
roles and responsibilities of the participants, and the absence of a central
authority to oversee and mitigate threats. Addressing these challenges, this
paper first delineates a comprehensive threat model focused on DFL
communications. In response to these identified risks, this work introduces a
security module to counter communication-based attacks for DFL platforms. The
module combines security techniques such as symmetric and asymmetric encryption
with Moving Target Defense (MTD) techniques, including random neighbor
selection and IP/port switching. The security module is implemented in a DFL
platform, Fedstellar, allowing the deployment and monitoring of the federation.
A DFL scenario with physical and virtual deployments have been executed,
encompassing three security configurations: (i) a baseline without security,
(ii) an encrypted configuration, and (iii) a configuration integrating both
encryption and MTD techniques. The effectiveness of the security module is
validated through experiments with the MNIST dataset and eclipse attacks. The
results showed an average F1 score of 95%, with the most secure configuration
resulting in CPU usage peaking at 68% (+-9%) in virtual deployments and network
traffic reaching 480.8 MB (+-18 MB), effectively mitigating risks associated
with eavesdropping or eclipse attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1&quot;&gt;Enrique Tom&amp;#xe1;s Mart&amp;#xed;nez Beltr&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro Miguel S&amp;#xe1;nchez S&amp;#xe1;nchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1&quot;&gt;Sergio L&amp;#xf3;pez Bernal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1&quot;&gt;G&amp;#xe9;r&amp;#xf4;me Bovet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1&quot;&gt;Manuel Gil P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1&quot;&gt;Gregorio Mart&amp;#xed;nez P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1&quot;&gt;Alberto Huertas Celdr&amp;#xe1;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12267">
<title>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12267</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1&quot;&gt;Lele Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanliang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15967">
<title>Graph Condensation for Inductive Node Representation Learning. (arXiv:2307.15967v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15967</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) encounter significant computational challenges
when handling large-scale graphs, which severely restricts their efficacy
across diverse applications. To address this limitation, graph condensation has
emerged as a promising technique, which constructs a small synthetic graph for
efficiently training GNNs while retaining performance. However, due to the
topology structure among nodes, graph condensation is limited to condensing
only the observed training nodes and their corresponding structure, thus
lacking the ability to effectively handle the unseen data. Consequently, the
original large graph is still required in the inference stage to perform
message passing to inductive nodes, resulting in substantial computational
demands. To overcome this issue, we propose mapping-aware graph condensation
(MCond), explicitly learning the one-to-many node mapping from original nodes
to synthetic nodes to seamlessly integrate new nodes into the synthetic graph
for inductive representation learning. This enables direct information
propagation on the synthetic graph, which is much more efficient than on the
original large graph. Specifically, MCond employs an alternating optimization
scheme with innovative loss terms from transductive and inductive perspectives,
facilitating the mutual promotion between graph condensation and node mapping
learning. Extensive experiments demonstrate the efficacy of our approach in
inductive inference. On the Reddit dataset, MCond achieves up to 121.5x
inference speedup and 55.9x reduction in storage requirements compared with
counterparts based on the original graph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yilong Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Viet Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Kai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07779">
<title>Do We Fully Understand Students&apos; Knowledge States? Identifying and Mitigating Answer Bias in Knowledge Tracing. (arXiv:2308.07779v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07779</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge tracing (KT) aims to monitor students&apos; evolving knowledge states
through their learning interactions with concept-related questions, and can be
indirectly evaluated by predicting how students will perform on future
questions. In this paper, we observe that there is a common phenomenon of
answer bias, i.e., a highly unbalanced distribution of correct and incorrect
answers for each question. Existing models tend to memorize the answer bias as
a shortcut for achieving high prediction performance in KT, thereby failing to
fully understand students&apos; knowledge states. To address this issue, we approach
the KT task from a causality perspective. A causal graph of KT is first
established, from which we identify that the impact of answer bias lies in the
direct causal effect of questions on students&apos; responses. A novel
COunterfactual REasoning (CORE) framework for KT is further proposed, which
separately captures the total causal effect and direct causal effect during
training, and mitigates answer bias by subtracting the latter from the former
in testing. The CORE framework is applicable to various existing KT models, and
we implement it based on the prevailing DKT, DKVMN, and AKT models,
respectively. Extensive experiments on three benchmark datasets demonstrate the
effectiveness of CORE in making the debiased inference for KT. We have released
our code at https://github.com/lucky7-code/CORE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Chaoran Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hebo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yumo Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Meng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuling Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09300">
<title>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09300</link>
<description rdf:parseType="Literal">&lt;p&gt;Building artificial intelligence (AI) systems on top of a set of foundation
models (FMs) is becoming a new paradigm in AI research. Their representative
and generative abilities learnt from vast amounts of data can be easily adapted
and transferred to a wide range of downstream tasks without extra training from
scratch. However, leveraging FMs in cross-modal generation remains
under-researched when audio modality is involved. On the other hand,
automatically generating semantically-relevant sound from visual input is an
important problem in cross-modal generation studies. To solve this
vision-to-audio (V2A) generation problem, existing methods tend to design and
build complex systems from scratch using modestly sized datasets. In this
paper, we propose a lightweight solution to this problem by leveraging
foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate
the domain gap between the latent space of the visual CLIP and the auditory
CLAP models. Then we propose a simple yet effective mapper mechanism
(V2A-Mapper) to bridge the domain gap by translating the visual input between
CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained
audio generative FM AudioLDM is adopted to produce high-fidelity and
visually-aligned sound. Compared to previous approaches, our method only
requires a quick training of the V2A-Mapper. We further analyze and conduct
extensive experiments on the choice of the V2A-Mapper and show that a
generative mapper is better at fidelity and variability (FD) while a regression
mapper is slightly better at relevance (CS). Both objective and subjective
evaluation on two V2A datasets demonstrate the superiority of our proposed
method compared to current state-of-the-art approaches - trained with 86% fewer
parameters but achieving 53% and 19% improvement in FD and CS, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascual_S/0/1/0/all/0/1&quot;&gt;Santiago Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartwright_R/0/1/0/all/0/1&quot;&gt;Richard Cartwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11070">
<title>Temporal-Distributed Backdoor Attack Against Video Based Action Recognition. (arXiv:2308.11070v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11070</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have achieved tremendous success in various
applications including video action recognition, yet remain vulnerable to
backdoor attacks (Trojans). The backdoor-compromised model will mis-classify to
the target class chosen by the attacker when a test instance (from a non-target
class) is embedded with a specific trigger, while maintaining high accuracy on
attack-free instances. Although there are extensive studies on backdoor attacks
against image data, the susceptibility of video-based systems under backdoor
attacks remains largely unexplored. Current studies are direct extensions of
approaches proposed for image data, e.g., the triggers are independently
embedded within the frames, which tend to be detectable by existing defenses.
In this paper, we introduce a simple yet effective backdoor attack against
video data. Our proposed attack, adding perturbations in a transformed domain,
plants an imperceptible, temporally distributed trigger across the video
frames, and is shown to be resilient to existing defensive strategies. The
effectiveness of the proposed attack is demonstrated by extensive experiments
with various well-known models on two video recognition benchmarks, UCF101 and
HMDB51, and a sign language recognition benchmark, Greek Sign Language (GSL)
dataset. We delve into the impact of several influential factors on our
proposed attack and identify an intriguing effect termed &quot;collateral damage&quot;
through extensive studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Songhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruiquan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gowda_M/0/1/0/all/0/1&quot;&gt;Mahanth Gowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kesidis_G/0/1/0/all/0/1&quot;&gt;George Kesidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12050">
<title>Aligning Language Models with Offline Learning from Human Feedback. (arXiv:2308.12050v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12050</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from human preferences is crucial for language models (LMs) to
effectively cater to human needs and societal values. Previous research has
made notable progress by leveraging human feedback to follow instructions.
However, these approaches rely primarily on online learning techniques like
Proximal Policy Optimization (PPO), which have been proven unstable and
challenging to tune for language models. Moreover, PPO requires complex
distributed system implementation, hindering the efficiency of large-scale
distributed training. In this study, we propose an offline learning from human
feedback framework to align LMs without interacting with environments.
Specifically, we explore filtering alignment (FA), reward-weighted regression
(RWR), and conditional alignment (CA) to align language models to human
preferences. By employing a loss function similar to supervised fine-tuning,
our methods ensure more stable model training than PPO with a simple machine
learning system~(MLSys) and much fewer (around 9\%) computing resources.
Experimental results demonstrate that conditional alignment outperforms other
offline alignment methods and is comparable to PPO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Li Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;June Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chandler Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12060">
<title>FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering. (arXiv:2308.12060v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12060</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge base question answering (KBQA) is a critical yet challenging task
due to the vast number of entities within knowledge bases and the diversity of
natural language questions posed by users. Unfortunately, the performance of
most KBQA models tends to decline significantly in real-world scenarios where
high-quality annotated data is insufficient. To mitigate the burden associated
with manual annotation, we introduce FlexKBQA by utilizing Large Language
Models (LLMs) as program translators for addressing the challenges inherent in
the few-shot KBQA task. Specifically, FlexKBQA leverages automated algorithms
to sample diverse programs, such as SPARQL queries, from the knowledge base,
which are subsequently converted into natural language questions via LLMs. This
synthetic dataset facilitates training a specialized lightweight model for the
KB. Additionally, to reduce the barriers of distribution shift between
synthetic data and real user questions, FlexKBQA introduces an executionguided
self-training method to iterative leverage unlabeled user questions.
Furthermore, we explore harnessing the inherent reasoning capability of LLMs to
enhance the entire framework. Consequently, FlexKBQA delivers substantial
flexibility, encompassing data annotation, deployment, and being domain
agnostic. Through extensive experiments on GrailQA, WebQSP, and KQA Pro, we
observe that under the few-shot even the more challenging zero-shot scenarios,
FlexKBQA achieves impressive results with a few annotations, surpassing all
previous baselines and even approaching the performance of supervised models,
achieving a remarkable 93% performance relative to the fully-supervised models.
We posit that FlexKBQA represents a significant advancement towards exploring
better integration of large and lightweight models. The code is open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Sunqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiuxing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1&quot;&gt;Zhichao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bowen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08836">
<title>Bias and Fairness in Chatbots: An Overview. (arXiv:2309.08836v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08836</link>
<description rdf:parseType="Literal">&lt;p&gt;Chatbots have been studied for more than half a century. With the rapid
development of natural language processing (NLP) technologies in recent years,
chatbots using large language models (LLMs) have received much attention
nowadays. Compared with traditional ones, modern chatbots are more powerful and
have been used in real-world applications. There are however, bias and fairness
concerns in modern chatbot design. Due to the huge amounts of training data,
extremely large model sizes, and lack of interpretability, bias mitigation and
fairness preservation of modern chatbots are challenging. Thus, a comprehensive
overview on bias and fairness in chatbot systems is given in this paper. The
history of chatbots and their categories are first reviewed. Then, bias sources
and potential harms in applications are analyzed. Considerations in designing
fair and unbiased chatbot systems are examined. Finally, future research
directions are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jintang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yun-Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1&quot;&gt;Chengwei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jonghye Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1&quot;&gt;C.-C. Jay Kuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09431">
<title>FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised Pre-Training. (arXiv:2309.09431v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09431</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pre-training, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pre-training procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pre-training, we also devise efficient
masking strategies for pre-training each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shaheer Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haghighat_M/0/1/0/all/0/1&quot;&gt;Maryam Haghighat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1&quot;&gt;Tharindu Fernando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12056">
<title>BELT:Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision. (arXiv:2309.12056v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents BELT, a novel model and learning framework for the
pivotal topic of brain-to-language translation research. The translation from
noninvasive brain signals into readable natural language has the potential to
promote the application scenario as well as the development of brain-computer
interfaces (BCI) as a whole. The critical problem in brain signal decoding or
brain-to-language translation is the acquisition of semantically appropriate
and discriminative EEG representation from a dataset of limited scale and
quality. The proposed BELT method is a generic and efficient framework that
bootstraps EEG representation learning using off-the-shelf large-scale
pretrained language models (LMs). With a large LM&apos;s capacity for understanding
semantic information and zero-shot generalization, BELT utilizes large LMs
trained on Internet-scale datasets to bring significant improvements to the
understanding of EEG signals.
&lt;/p&gt;
&lt;p&gt;In particular, the BELT model is composed of a deep conformer encoder and a
vector quantization encoder. Semantical EEG representation is achieved by a
contrastive learning step that provides natural language supervision. We
achieve state-of-the-art results on two featuring brain decoding tasks
including the brain-to-language translation and zero-shot sentiment
classification. Specifically, our model surpasses the baseline model on both
tasks by 5.45% and over 10% and archives a 42.31% BLEU-1 score and 67.32%
precision on the main evaluation metrics for translation and zero-shot
sentiment classification respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jinzhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1&quot;&gt;Yiqun Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13550">
<title>I-AI: A Controllable &amp; Interpretable AI System for Decoding Radiologists&apos; Intense Focus for Accurate CXR Diagnoses. (arXiv:2309.13550v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13550</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of chest X-ray (CXR) diagnosis, existing works often focus
solely on determining where a radiologist looks, typically through tasks such
as detection, segmentation, or classification. However, these approaches are
often designed as black-box models, lacking interpretability. In this paper, we
introduce Interpretable Artificial Intelligence (I-AI) a novel and unified
controllable interpretable pipeline for decoding the intense focus of
radiologists in CXR diagnosis. Our I-AI addresses three key questions: where a
radiologist looks, how long they focus on specific areas, and what findings
they diagnose. By capturing the intensity of the radiologist&apos;s gaze, we provide
a unified solution that offers insights into the cognitive process underlying
radiological interpretation. Unlike current methods that rely on black-box
machine learning models, which can be prone to extracting erroneous information
from the entire input image during the diagnosis process, we tackle this issue
by effectively masking out irrelevant information. Our proposed I-AI leverages
a vision-language model, allowing for precise control over the interpretation
process while ensuring the exclusion of irrelevant features. To train our I-AI
model, we utilize an eye gaze dataset to extract anatomical gaze information
and generate ground truth heatmaps. Through extensive experimentation, we
demonstrate the efficacy of our method. We showcase that the attention
heatmaps, designed to mimic radiologists&apos; focus, encode sufficient and relevant
information, enabling accurate classification tasks using only a portion of
CXR. The code, checkpoints, and data are at https://github.com/UARK-AICV/IAI
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Trong Thang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brecheisen_J/0/1/0/all/0/1&quot;&gt;Jacob Brecheisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14293">
<title>NAS-NeRF: Generative Neural Architecture Search for Neural Radiance Fields. (arXiv:2309.14293v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14293</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRFs) enable high-quality novel view synthesis, but
their high computational complexity limits deployability. While existing
neural-based solutions strive for efficiency, they use one-size-fits-all
architectures regardless of scene complexity. The same architecture may be
unnecessarily large for simple scenes but insufficient for complex ones. Thus,
there is a need to dynamically optimize the neural network component of NeRFs
to achieve a balance between computational complexity and specific targets for
synthesis quality. We introduce NAS-NeRF, a generative neural architecture
search strategy that generates compact, scene-specialized NeRF architectures by
balancing architecture complexity and target synthesis quality metrics. Our
method incorporates constraints on target metrics and budgets to guide the
search towards architectures tailored for each scene. Experiments on the
Blender synthetic dataset show the proposed NAS-NeRF can generate architectures
up to 5.74$\times$ smaller, with 4.19$\times$ fewer FLOPs, and 1.93$\times$
faster on a GPU than baseline NeRFs, without suffering a drop in SSIM.
Furthermore, we illustrate that NAS-NeRF can also achieve architectures up to
23$\times$ smaller, with 22$\times$ fewer FLOPs, and 4.7$\times$ faster than
baseline NeRFs with only a 5.3% average SSIM drop. Our source code is also made
publicly available at https://saeejithnair.github.io/NAS-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14405">
<title>Joint Audio and Speech Understanding. (arXiv:2309.14405v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14405</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are surrounded by audio signals that include both speech and
non-speech sounds. The recognition and understanding of speech and non-speech
audio events, along with a profound comprehension of the relationship between
them, constitute fundamental cognitive capabilities. For the first time, we
build a machine learning model, called LTU-AS, that has a conceptually similar
universal audio perception and advanced reasoning ability. Specifically, by
integrating Whisper as a perception module and LLaMA as a reasoning module,
LTU-AS can simultaneously recognize and jointly understand spoken text, speech
paralinguistics, and non-speech audio events - almost everything perceivable
from audio signals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Alexander H. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hongyin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1&quot;&gt;Leonid Karlinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00752">
<title>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks. (arXiv:2310.00752v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00752</link>
<description rdf:parseType="Literal">&lt;p&gt;We present TIGERScore, a \textbf{T}rained metric that follows
\textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and
\textbf{R}eference-free evaluation over a wide spectrum of text generation
tasks. Different from other automatic evaluation methods that only provide
arcane scores, TIGERScore is guided by natural language instruction to provide
error analysis to pinpoint the mistakes in the generated text. Our metric is
based on LLaMA-2, trained on our meticulously curated instruction-tuning
dataset MetricInstruct which covers 6 text generation tasks and 23 text
generation datasets. The dataset consists of 42K quadruple in the form of
(instruction, input, system output $\rightarrow$ error analysis). We collected
the `system outputs&apos; through from a large variety of models to cover different
types of errors. To quantitatively assess our metric, we evaluate its
correlation with human ratings on 5 held-in datasets, 2 held-out datasets and
show that TIGERScore can achieve the open-source SoTA correlation with human
ratings across these datasets and almost approaches GPT-4 evaluator. As a
reference-free metric, its correlation can even surpass the best existing
reference-based metrics. To further qualitatively assess the rationale
generated by our metric, we conduct human evaluation on the generated
explanations and found that the explanations are 70.8\% accurate. Through these
experimental results, we believe TIGERScore demonstrates the possibility of
building universal explainable metrics to evaluate any text generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yishan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01770">
<title>A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01770</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalization capacity of deep neural networks has been studied in a
variety of ways, including at least two distinct categories of approach: one
based on the shape of the loss landscape in parameter space, and the other
based on the structure of the representation manifold in feature space (that
is, in the space of unit activities). Although these two approaches are
related, they are rarely studied together in an explicit connection. Here, we
present a simple analysis that makes such a connection. We show that, in the
last phase of learning of deep neural networks, compression of the manifold of
neural representations correlates with the flatness of the loss around the
minima explored by SGD. We show that this is predicted by a relatively simple
mathematical relationship: a flatter loss corresponds to a lower upper-bound on
the compression of neural representations. Our results closely build on the
prior work of Ma and Ying, who demonstrated how flatness, characterized by
small eigenvalues of the loss Hessian, develops in late learning phases and
contributes to robustness against perturbations in network inputs. Moreover, we
show a lack of a similarly direct connection between local dimensionality and
sharpness, suggesting that this property may be controlled by different
mechanisms than volume and hence may play a complementary role in neural
representations. Overall, we advance a dual perspective on generalization in
neural networks in both parameter and feature space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shirui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Recanatesi_S/0/1/0/all/0/1&quot;&gt;Stefano Recanatesi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shea_Brown_E/0/1/0/all/0/1&quot;&gt;Eric Shea-Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02003">
<title>L2MAC: Large Language Model Automatic Computer for Unbounded Code Generation. (arXiv:2310.02003v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02003</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based large language models (LLMs) are constrained by the fixed
context window of the underlying transformer architecture, hindering their
ability to produce long and logically consistent code. Memory-augmented LLMs
are a promising solution, but current approaches cannot handle long code
generation tasks since they (1) only focus on reading memory and reduce its
evolution to the concatenation of new memories or (2) use very specialized
memories that cannot adapt to other domains. This paper presents L2MAC, the
first practical LLM-based stored-program automatic computer for long and
consistent code generation. Its memory has two components: the instruction
registry, which is populated with a prompt program to solve the user-given
task, and a file store, which will contain the final and intermediate outputs.
Each instruction is executed by a separate LLM instance, whose context is
managed by a control unit capable of precise memory reading and writing to
ensure effective interaction with the file store. These components enable L2MAC
to generate virtually unbounded code structures, bypassing the constraints of
the finite context window while producing code that fulfills complex
user-specified requirements. We empirically show that L2MAC succeeds in
generating large code bases for system design tasks where other coding methods
fall short in implementing user requirements and provide insight into the
reasons for this performance gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holt_S/0/1/0/all/0/1&quot;&gt;Samuel Holt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luyten_M/0/1/0/all/0/1&quot;&gt;Max Ruiz Luyten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03374">
<title>Design Optimizer for Planar Soft-Growing Robot Manipulators. (arXiv:2310.03374v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03374</link>
<description rdf:parseType="Literal">&lt;p&gt;Soft-growing robots are innovative devices that feature plant-inspired growth
to navigate environments. Thanks to their embodied intelligence of adapting to
their surroundings and the latest innovation in actuation and manufacturing, it
is possible to employ them for specific manipulation tasks. The applications of
these devices include exploration of delicate/dangerous environments,
manipulation of items, or assistance in domestic environments.
&lt;/p&gt;
&lt;p&gt;This work presents a novel approach for design optimization of soft-growing
robots, which will be used prior to manufacturing to suggest engineers -- or
robot designer enthusiasts -- the optimal dimension of the robot to be built
for solving a specific task. I modeled the design process as a multi-objective
optimization problem, in which I optimize the kinematic chain of a soft
manipulator to reach targets and avoid unnecessary overuse of material and
resources. The method exploits the advantages of population-based optimization
algorithms, in particular evolutionary algorithms, to transform the problem
from multi-objective into a single-objective thanks to an efficient
mathematical formulation, the novel rank-partitioning algorithm, and obstacle
avoidance integrated within the optimizer operators.
&lt;/p&gt;
&lt;p&gt;I tested the proposed method on different tasks to access its optimality,
which showed significant performance in solving the problem. Finally,
comparative experiments showed that the proposed method works better than the
one existing in the literature in terms of precision, resource consumption, and
run time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stroppa_F/0/1/0/all/0/1&quot;&gt;Fabio Stroppa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03494">
<title>How the level sampling process impacts zero-shot generalisation in deep reinforcement learning. (arXiv:2310.03494v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03494</link>
<description rdf:parseType="Literal">&lt;p&gt;A key limitation preventing the wider adoption of autonomous agents trained
via deep reinforcement learning (RL) is their limited ability to generalise to
new environments, even when these share similar characteristics with
environments encountered during training. In this work, we investigate how a
non-uniform sampling strategy of individual environment instances, or levels,
affects the zero-shot generalisation (ZSG) ability of RL agents, considering
two failure modes: overfitting and over-generalisation. As a first step, we
measure the mutual information (MI) between the agent&apos;s internal representation
and the set of training levels, which we find to be well-correlated to instance
overfitting. In contrast to uniform sampling, adaptive sampling strategies
prioritising levels based on their value loss are more effective at maintaining
lower MI, which provides a novel theoretical justification for this class of
techniques. We then turn our attention to unsupervised environment design (UED)
methods, which adaptively generate new training levels and minimise MI more
effectively than methods sampling from a fixed set. However, we find UED
methods significantly shift the training distribution, resulting in
over-generalisation and worse ZSG performance over the distribution of
interest. To prevent both instance overfitting and over-generalisation, we
introduce self-supervised environment design (SSED). SSED generates levels
using a variational autoencoder, effectively reducing MI while minimising the
shift with the distribution of interest, and leads to statistically significant
improvements in ZSG over fixed-set level sampling strategies and UED methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcin_S/0/1/0/all/0/1&quot;&gt;Samuel Garcin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doran_J/0/1/0/all/0/1&quot;&gt;James Doran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shangmin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_C/0/1/0/all/0/1&quot;&gt;Christopher G. Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04353">
<title>A Language-Agent Approach to Formal Theorem-Proving. (arXiv:2310.04353v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04353</link>
<description rdf:parseType="Literal">&lt;p&gt;Language agents, which use a large language model (LLM) capable of in-context
learning to interact with an external environment, have recently emerged as a
promising approach to control tasks. We present the first language-agent
approach to formal theorem-proving. Our method, COPRA, uses a high-capacity,
black-box LLM (GPT-4) as part of a policy for a stateful backtracking search.
During the search, the policy can select proof tactics and retrieve lemmas and
definitions from an external database. Each selected tactic is executed in the
underlying proof framework, and the execution feedback is used to build the
prompt for the next policy invocation. The search also tracks selected
information from its history and uses it to reduce hallucinations and
unnecessary LLM queries.
&lt;/p&gt;
&lt;p&gt;We evaluate our implementation of COPRA on the miniF2F benchmark for Lean and
a set of Coq tasks from the Compcert project. On these benchmarks, COPRA
significantly outperforms one-shot invocations of GPT-4, as well as
state-of-the-art models fine-tuned on proof data, at finding correct proofs
quickly. Our code and data are available at
https://github.com/trishullab/copra.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_A/0/1/0/all/0/1&quot;&gt;Amitayush Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yeming Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1&quot;&gt;Swarat Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05365">
<title>Molecular De Novo Design through Transformer-based Reinforcement Learning. (arXiv:2310.05365v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05365</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce a method to fine-tune a Transformer-based
generative model for molecular de novo design. Leveraging the superior sequence
learning capacity of Transformers over Recurrent Neural Networks (RNNs), our
model can generate molecular structures with desired properties effectively. In
contrast to the traditional RNN-based models, our proposed method exhibits
superior performance in generating compounds predicted to be active against
various biological targets, capturing long-term dependencies in the molecular
structure sequence. The model&apos;s efficacy is demonstrated across numerous tasks,
including generating analogues to a query structure and producing compounds
with particular attributes, outperforming the baseline RNN-based methods. Our
approach can be used for scaffold hopping, library expansion starting from a
single molecule, and generating compounds with high predicted activity against
biological targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Tianfan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laghuvarapu_S/0/1/0/all/0/1&quot;&gt;Siddhartha Laghuvarapu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06648">
<title>Diversity from Human Feedback. (arXiv:2310.06648v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06648</link>
<description rdf:parseType="Literal">&lt;p&gt;Diversity plays a significant role in many problems, such as ensemble
learning, reinforcement learning, and combinatorial optimization. How to define
the diversity measure is a longstanding problem. Many methods rely on expert
experience to define a proper behavior space and then obtain the diversity
measure, which is, however, challenging in many scenarios. In this paper, we
propose the problem of learning a behavior space from human feedback and
present a general method called Diversity from Human Feedback (DivHF) to solve
it. DivHF learns a behavior descriptor consistent with human preference by
querying human feedback. The learned behavior descriptor can be combined with
any distance measure to define a diversity measure. We demonstrate the
effectiveness of DivHF by integrating it with the Quality-Diversity
optimization algorithm MAP-Elites and conducting experiments on the QDax suite.
The results show that DivHF learns a behavior space that aligns better with
human requirements compared to direct data-driven approaches and leads to more
diverse solutions under human preference. Our contributions include formulating
the problem, proposing the DivHF method, and demonstrating its effectiveness
through experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ren-Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_K/0/1/0/all/0/1&quot;&gt;Ke Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yutong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Peng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haobo Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06824">
<title>The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets. (arXiv:2310.06824v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06824</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have impressive capabilities, but are also prone
to outputting falsehoods. Recent work has developed techniques for inferring
whether a LLM is telling the truth by training probes on the LLM&apos;s internal
activations. However, this line of work is controversial, with some authors
pointing out failures of these probes to generalize in basic ways, among other
conceptual issues. In this work, we curate high-quality datasets of true/false
statements and use them to study in detail the structure of LLM representations
of truth, drawing on three lines of evidence: 1. Visualizations of LLM
true/false statement representations, which reveal clear linear structure. 2.
Transfer experiments in which probes trained on one dataset generalize to
different datasets. 3. Causal evidence obtained by surgically intervening in a
LLM&apos;s forward pass, causing it to treat false statements as true and vice
versa. Overall, we present evidence that language models linearly represent the
truth or falsehood of factual statements. We also introduce a novel technique,
mass-mean probing, which generalizes better and is more causally implicated in
model outputs than other probing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_S/0/1/0/all/0/1&quot;&gt;Samuel Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07867">
<title>Cheap Talking Algorithms. (arXiv:2310.07867v3 [econ.TH] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We simulate behaviour of independent reinforcement learning algorithms
playing the Crawford and Sobel (1982) game of strategic information
transmission. We show that a sender and a receiver training together converge
to strategies approximating the ex-ante optimal equilibrium of the game.
Communication occurs to the largest extent predicted by Nash equilibrium. The
conclusion is robust to alternative specifications of the learning
hyperparameters and of the game. We discuss implications for theories of
equilibrium selection in information transmission games, for work on emerging
communication among algorithms in computer science, and for the economics of
collusions in markets populated by artificially intelligent agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Condorelli_D/0/1/0/all/0/1&quot;&gt;Daniele Condorelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Furlan_M/0/1/0/all/0/1&quot;&gt;Massimiliano Furlan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08008">
<title>Effects of Human Adversarial and Affable Samples on BERT Generalization. (arXiv:2310.08008v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08008</link>
<description rdf:parseType="Literal">&lt;p&gt;BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training data quality, not quantity, on a model&apos;s generalizability. We consider
two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation. In contrast, h-affables may not contribute to a model&apos;s
generalizability and may even degrade generalization performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1&quot;&gt;Aparna Elangovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiayuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1&quot;&gt;Karin Verspoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12609">
<title>Denoising Heat-inspired Diffusion with Insulators for Collision Free Motion Planning. (arXiv:2310.12609v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12609</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have risen as a powerful tool in robotics due to their
flexibility and multi-modality. While some of these methods effectively address
complex problems, they often depend heavily on inference-time obstacle
detection and require additional equipment. Addressing these challenges, we
present a method that, during inference time, simultaneously generates only
reachable goals and plans motions that avoid obstacles, all from a single
visual input. Central to our approach is the novel use of a collision-avoiding
diffusion kernel for training. Through evaluations against behavior-cloning and
classical diffusion models, our framework has proven its robustness. It is
particularly effective in multi-modal environments, navigating toward goals and
avoiding unreachable ones blocked by obstacles, while ensuring collision
avoidance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Soochul Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_N/0/1/0/all/0/1&quot;&gt;Nikhil Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13191">
<title>Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models. (arXiv:2310.13191v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13191</link>
<description rdf:parseType="Literal">&lt;p&gt;The pruning objective has recently extended beyond accuracy and sparsity to
robustness in language models. Despite this, existing methods struggle to
enhance robustness against adversarial attacks when continually increasing
model sparsity and require a retraining process. As humans step into the era of
large language models, these issues become increasingly prominent. This paper
proposes that the robustness of language models is proportional to the extent
of pre-trained knowledge they encompass. Accordingly, we introduce a
post-training pruning strategy designed to faithfully replicate the embedding
space and feature space of dense language models, aiming to conserve more
pre-trained knowledge during the pruning process. In this setup, each layer&apos;s
reconstruction error not only originates from itself but also includes
cumulative error from preceding layers, followed by an adaptive rectification.
Compared to other state-of-art baselines, our approach demonstrates a superior
balance between accuracy, sparsity, robustness, and pruning cost with BERT on
datasets SST2, IMDB, and AGNews, marking a significant stride towards robust
pruning in language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1&quot;&gt;Qi Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dongkuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15074">
<title>MGAS: Multi-Granularity Architecture Search for Trade-Off Between Model Effectiveness and Efficiency. (arXiv:2310.15074v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15074</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural architecture search (NAS) has gained significant traction in
automating the design of neural networks. To reduce the time cost,
differentiable architecture search (DAS) transforms the traditional paradigm of
discrete candidate sampling and evaluation into that of differentiable
super-net optimization and discretization. However, existing DAS methods fail
to trade off between model performance and model size. They either only conduct
coarse-grained operation-level search, which results in redundant model
parameters, or restrictively explore fine-grained filter-level and weight-level
units with pre-defined remaining ratios, suffering from excessive pruning
problem. Additionally, these methods compromise search quality to save memory
during the search process. To tackle these issues, we introduce
multi-granularity architecture search (MGAS), a unified framework which aims to
discover both effective and efficient neural networks by comprehensively yet
memory-efficiently exploring the multi-granularity search space. Specifically,
we improve the existing DAS methods in two aspects. First, we balance the model
unit numbers at different granularity levels with adaptive pruning. We learn
discretization functions specific to each granularity level to adaptively
determine the unit remaining ratio according to the evolving architecture.
Second, we reduce the memory consumption without degrading the search quality
using multi-stage search. We break down the super-net optimization and
discretization into multiple sub-net stages, and perform progressive
re-evaluation to allow for re-pruning and regrowing of previous units during
subsequent stages, compensating for potential bias. Extensive experiments on
CIFAR-10, CIFAR-100 and ImageNet demonstrate that MGAS outperforms other
state-of-the-art methods in achieving a better trade-off between model
performance and model size.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_D/0/1/0/all/0/1&quot;&gt;Divya Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jiannong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_P/0/1/0/all/0/1&quot;&gt;Penghui Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17793">
<title>&quot;You Are An Expert Linguistic Annotator&quot;: Limits of LLMs as Analyzers of Abstract Meaning Representation. (arXiv:2310.17793v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17793</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) show amazing proficiency and fluency in the use
of language. Does this mean that they have also acquired insightful linguistic
knowledge about the language, to an extent that they can serve as an &quot;expert
linguistic annotator&quot;? In this paper, we examine the successes and limitations
of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning
structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et
al. 2013) parsing formalism, which provides rich graphical representations of
sentence meaning structure while abstracting away from surface forms. We
compare models&apos; analysis of this semantic structure across two settings: 1)
direct production of AMR parses based on zero- and few-shot prompts, and 2)
indirect partial reconstruction of AMR via metalinguistic natural language
queries (e.g., &quot;Identify the primary event of this sentence, and the predicate
corresponding to that event.&quot;). Across these settings, we find that models can
reliably reproduce the basic format of AMR, and can often capture core event,
argument, and modifier structure -- however, model outputs are prone to
frequent and major errors, and holistic analysis of parse acceptability shows
that even with few-shot demonstrations, models have virtually 0% success in
producing fully accurate parses. Eliciting natural language responses produces
similar patterns of errors. Overall, our findings indicate that these models
out-of-the-box can capture aspects of semantic structure, but there remain key
limitations in their ability to support fully accurate semantic analyses or
parses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1&quot;&gt;Allyson Ettinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1&quot;&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1&quot;&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20187">
<title>Self-Supervised Pre-Training for Precipitation Post-Processor. (arXiv:2310.20187v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20187</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining a sufficient forecast lead time for local precipitation is
essential in preventing hazardous weather events. Global warming-induced
climate change increases the challenge of accurately predicting severe
precipitation events, such as heavy rainfall. In this paper, we propose a deep
learning-based precipitation post-processor for numerical weather prediction
(NWP) models. The precipitation post-processor consists of (i) employing
self-supervised pre-training, where the parameters of the encoder are
pre-trained on the reconstruction of the masked variables of the atmospheric
physics domain; and (ii) conducting transfer learning on precipitation
segmentation tasks (the target domain) from the pre-trained encoder. In
addition, we introduced a heuristic labeling approach to effectively train
class-imbalanced datasets. Our experiments on precipitation correction for
regional NWP show that the proposed method outperforms other approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Sojung An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junha Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jiyeon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Na_I/0/1/0/all/0/1&quot;&gt;Inchae Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1&quot;&gt;Wooyeon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Sujeong You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00286">
<title>JADE: A Linguistics-based Safety Evaluation Platform for Large Language Models. (arXiv:2311.00286v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00286</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present JADE, a targeted linguistic fuzzing platform which
strengthens the linguistic complexity of seed questions to simultaneously and
consistently break a wide range of widely-used LLMs categorized in three
groups: eight open-sourced Chinese, six commercial Chinese and four commercial
English LLMs. JADE generates three safety benchmarks for the three groups of
LLMs, which contain unsafe questions that are highly threatening: the questions
simultaneously trigger harmful generation of multiple LLMs, with an average
unsafe generation ratio of $70\%$ (please see the table below), while are still
natural questions, fluent and preserving the core unsafe semantics. We release
the benchmark demos generated for commercial English LLMs and open-sourced
English LLMs in the following link: https://github.com/whitzard-ai/jade-db. For
readers who are interested in evaluating on more questions generated by JADE,
please contact us.
&lt;/p&gt;
&lt;p&gt;JADE is based on Noam Chomsky&apos;s seminal theory of transformational-generative
grammar. Given a seed question with unsafe intention, JADE invokes a sequence
of generative and transformational rules to increment the complexity of the
syntactic structure of the original question, until the safety guardrail is
broken. Our key insight is: Due to the complexity of human language, most of
the current best LLMs can hardly recognize the invariant evil from the infinite
number of different syntactic structures which form an unbound example space
that can never be fully covered. Technically, the generative/transformative
rules are constructed by native speakers of the languages, and, once developed,
can be used to automatically grow and transform the parse tree of a given
question, until the guardrail is broken. For more evaluation results and demo,
please check our website: https://whitzard-ai.github.io/jade.html.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xudong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00693">
<title>On Task-personalized Multimodal Few-shot Learning for Visually-rich Document Entity Retrieval. (arXiv:2311.00693v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00693</link>
<description rdf:parseType="Literal">&lt;p&gt;Visually-rich document entity retrieval (VDER), which extracts key
information (e.g. date, address) from document images like invoices and
receipts, has become an important topic in industrial NLP applications. The
emergence of new document types at a constant pace, each with its unique entity
types, presents a unique challenge: many documents contain unseen entity types
that occur only a couple of times. Addressing this challenge requires models to
have the ability of learning entities in a few-shot manner. However, prior
works for Few-shot VDER mainly address the problem at the document level with a
predefined global entity space, which doesn&apos;t account for the entity-level
few-shot scenario: target entity types are locally personalized by each task
and entity occurrences vary significantly among documents. To address this
unexplored scenario, this paper studies a novel entity-level few-shot VDER
task. The challenges lie in the uniqueness of the label space for each task and
the increased complexity of out-of-distribution (OOD) contents. To tackle this
novel task, we present a task-aware meta-learning based framework, with a
central focus on achieving effective task personalization that distinguishes
between in-task and out-of-task distribution. Specifically, we adopt a
hierarchical decoder (HC) and employ contrastive learning (ContrastProtoNet) to
achieve this goal. Furthermore, we introduce a new dataset, FewVEX, to boost
future research in the field of entity-level few-shot VDER. Experimental
results demonstrate our approaches significantly improve the robustness of
popular meta-learning baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aidong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01223">
<title>Diffusion Models for Reinforcement Learning: A Survey. (arXiv:2311.01223v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01223</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as a prominent class of generative models,
surpassing previous methods regarding sample quality and training stability.
Recent works have shown the advantages of diffusion models in improving
reinforcement learning (RL) solutions, including as trajectory planners,
expressive policy classes, data synthesizers, etc. This survey aims to provide
an overview of the advancements in this emerging field and hopes to inspire new
avenues of research. First, we examine several challenges encountered by
current RL algorithms. Then, we present a taxonomy of existing methods based on
the roles played by diffusion models in RL and explore how the existing
challenges are addressed. We further outline successful applications of
diffusion models in various RL-related tasks while discussing the limitations
of current approaches. Finally, we conclude the survey and offer insights into
future research directions, focusing on enhancing model performance and
applying diffusion models to broader tasks. We are actively maintaining a
GitHub repository for papers and other related resources in applying diffusion
models in RL: https://github.com/apexrl/Diff4RLSurvey
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhengbang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hanye Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yichao Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05112">
<title>A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges. (arXiv:2311.05112v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05112</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as ChatGPT, have received substantial
attention due to their impressive human language understanding and generation
capabilities. Therefore, the application of LLMs in medicine to assist
physicians and patient care emerges as a promising research direction in both
artificial intelligence and clinical medicine. To reflect this trend, this
survey provides a comprehensive overview of the principles, applications, and
challenges faced by LLMs in medicine. Specifically, we aim to address the
following questions: 1) How can medical LLMs be built? 2) What are the
downstream performances of medical LLMs? 3) How can medical LLMs be utilized in
real-world clinical practice? 4) What challenges arise from the use of medical
LLMs? and 5) How can we better construct and utilize medical LLMs? As a result,
this survey aims to provide insights into the opportunities and challenges of
LLMs in medicine and serve as a valuable resource for constructing practical
and effective medical LLMs. A regularly updated list of practical guides on
medical LLMs can be found at
https://github.com/AI-in-Health/MedLLMsPracticalGuide.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongjian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fenglin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Boyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xinyu Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jinfa Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jinge Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiru Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Sam S. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Peilin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junling Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yining Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1&quot;&gt;Chengfeng Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_L/0/1/0/all/0/1&quot;&gt;Lei Clifton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1&quot;&gt;David A. Clifton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08999">
<title>Leveraging AI for Natural Disaster Management : Takeaways From The Moroccan Earthquake. (arXiv:2311.08999v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08999</link>
<description rdf:parseType="Literal">&lt;p&gt;The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023
prompted critical reflections on global disaster management strategies,
resulting in a post-disaster hackathon, using artificial intelligence (AI) to
improve disaster preparedness, response, and recovery. This paper provides (i)
a comprehensive literature review, (ii) an overview of winning projects, (iii)
key insights and challenges, namely real-time open-source data, data scarcity,
and interdisciplinary collaboration barriers, and (iv) a community-call for
further action.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hackathon_M/0/1/0/all/0/1&quot;&gt;Morocco Solidarity Hackathon&lt;/a&gt; (Organizers, Speakers, Mentors and Participant teams)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10792">
<title>Attention Mechanism for Lithium-Ion Battery Lifespan Prediction: Temporal and Cyclic Attention. (arXiv:2311.10792v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10792</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting lithium-ion batteries (LIBs) lifespan is pivotal for
optimizing usage and preventing accidents. Previous approaches often relied on
inputs challenging to measure in real-time, and failed to capture intra- and
inter-cycle data patterns simultaneously. Our study employ attention mechanisms
(AM) to develop data-driven models predicting LIB lifespan using easily
measurable inputs. Developed model integrates recurrent neural network and
convolutional neural network, featuring two types of AMs: temporal attention
(TA) and cyclic attention (CA). TA identifies important time steps within each
cycle, CA strives to capture key features of inter-cycle correlations through
self-attention (SA). We apply the developed model to publicly available data
consisting of three batches of cycling modes. TA scores highlight the rest
phase as a key characteristic to distinguish different batches. By leveraging
CA scores, we decreased the input dimension from 100 cycles to 50 and 30 cycles
with single- and multi-head attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaewook Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_S/0/1/0/all/0/1&quot;&gt;Seongmin Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jay H. Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10931">
<title>FLORIDA: Fake-looking Real Images Dataset. (arXiv:2311.10931v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10931</link>
<description rdf:parseType="Literal">&lt;p&gt;Although extensive research has been carried out to evaluate the
effectiveness of AI tools and models in detecting deep fakes, the question
remains unanswered regarding whether these models can accurately identify
genuine images that appear artificial. In this study, as an initial step
towards addressing this issue, we have curated a dataset of 510 genuine images
that exhibit a fake appearance and conducted an assessment using two AI models.
We show that two models exhibited subpar performance when applied to our
dataset. Additionally, our dataset can serve as a valuable tool for assessing
the ability of deep learning models to comprehend complex visual stimuli. We
anticipate that this research will stimulate further discussions and
investigations in this area. Our dataset is accessible at
https://github.com/aliborji/FLORIDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1&quot;&gt;Ali Borji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13812">
<title>Mechanical Characterization and Inverse Design of Stochastic Architected Metamaterials Using Neural Operators. (arXiv:2311.13812v2 [cond-mat.mtrl-sci] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13812</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) is emerging as a transformative tool for the design of
architected materials, offering properties that far surpass those achievable
through lab-based trial-and-error methods. However, a major challenge in
current inverse design strategies is their reliance on extensive computational
and/or experimental datasets, which becomes particularly problematic for
designing micro-scale stochastic architected materials that exhibit nonlinear
mechanical behaviors. Here, we introduce a new end-to-end scientific ML
framework, leveraging deep neural operators (DeepONet), to directly learn the
relationship between the complete microstructure and mechanical response of
architected metamaterials from sparse but high-quality in situ experimental
data. The approach facilitates the inverse design of structures tailored to
specific nonlinear mechanical behaviors. Results obtained from spinodal
microstructures, printed using two-photon lithography, reveal that the
prediction error for mechanical responses is within a range of 5 - 10%. Our
work underscores that by employing neural operators with advanced
micro-mechanics experimental techniques, the design of complex
micro-architected materials with desired properties becomes feasible, even in
scenarios constrained by data scarcity. Our work marks a significant
advancement in the field of materials-by-design, potentially heralding a new
era in the discovery and development of next-generation metamaterials with
unparalleled mechanical characteristics derived directly from experimental
insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hanxun Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Sridhar Krishnaswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Karniadakis_G/0/1/0/all/0/1&quot;&gt;George Em Karniadakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Espinosa_H/0/1/0/all/0/1&quot;&gt;Horacio D. Espinosa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13884">
<title>Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach. (arXiv:2311.13884v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13884</link>
<description rdf:parseType="Literal">&lt;p&gt;The significant advancements in large language models (LLMs) have presented
novel opportunities for tackling planning and decision-making within
multi-agent systems. However, as the number of agents increases, the issues of
hallucination in LLMs and coordination in multi-agent systems (MAS) have become
increasingly pronounced. Additionally, the efficient utilization of tokens
becomes a critical consideration when employing LLMs to facilitate the
interactions of large numbers of agents. In this paper, we present a novel
framework aimed at enhancing coordination and decision-making capabilities of
LLMs within large-scale multi-agent environments. Our approach draws
inspiration from the actor-critic framework employed in multi-agent
reinforcement learning, and we develop a modular and token-efficient solution
that effectively addresses challenges presented by LLMs and MAS. Through
evaluations conducted in experiments involving system resource allocation and
robot grid transportation, we demonstrate the considerable advantages afforded
by our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hangyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_J/0/1/0/all/0/1&quot;&gt;Jingqing Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Ying Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dapeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lijuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1&quot;&gt;Guoliang Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17431">
<title>Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18054">
<title>I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text. (arXiv:2311.18054v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18054</link>
<description rdf:parseType="Literal">&lt;p&gt;Potential harms of Large Language Models such as mass misinformation and
plagiarism can be partially mitigated if there exists a reliable way to detect
machine generated text. In this paper, we propose a new watermarking method to
detect machine-generated texts. Our method embeds a unique pattern within the
generated text, ensuring that while the content remains coherent and natural to
human readers, it carries distinct markers that can be identified
algorithmically. Specifically, we intervene with the token sampling process in
a way which enables us to trace back our token choices during the detection
phase. We show how watermarking affects textual quality and compare our
proposed method with a state-of-the-art watermarking method in terms of
robustness and detectability. Through extensive experiments, we demonstrate the
effectiveness of our watermarking scheme in distinguishing between watermarked
and non-watermarked text, achieving high detection rates while maintaining
textual quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keles_K/0/1/0/all/0/1&quot;&gt;Kaan Efe Kele&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_O/0/1/0/all/0/1&quot;&gt;&amp;#xd6;mer Kaan G&amp;#xfc;rb&amp;#xfc;z&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1&quot;&gt;Mucahid Kutlu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18760">
<title>TaskBench: Benchmarking Large Language Models for Task Automation. (arXiv:2311.18760v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18760</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the incredible progress of large language models (LLMs) has ignited
the spark of task automation, which decomposes the complex tasks described by
user instructions into sub-tasks, and invokes external tools to execute them,
and plays a central role in autonomous agents. However, there lacks a
systematic and standardized benchmark to foster the development of LLMs in task
automation. To this end, we introduce TaskBench to evaluate the capability of
LLMs in task automation. Specifically, task automation can be formulated into
three critical stages: task decomposition, tool invocation, and parameter
prediction to fulfill user intent. This complexity makes data collection and
evaluation more challenging compared to common NLP tasks. To generate
high-quality evaluation datasets, we introduce the concept of Tool Graph to
represent the decomposed tasks in user intent, and adopt a back-instruct method
to simulate user instruction and annotations. Furthermore, we propose TaskEval
to evaluate the capability of LLMs from different aspects, including task
decomposition, tool invocation, and parameter prediction. Experimental results
demonstrate that TaskBench can effectively reflects the capability of LLMs in
task automation. Benefiting from the mixture of automated data construction and
human verification, TaskBench achieves a high consistency compared to the human
evaluation, which can be utilized as a comprehensive and faithful benchmark for
LLM-based autonomous agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yongliang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kaitao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1&quot;&gt;Kan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Siyu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weiming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00377">
<title>SynFundus: A synthetic fundus images dataset with millions of samples and multi-disease annotations. (arXiv:2312.00377v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00377</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, there are seldom large-scale public datasets
with high-quality annotations due to data privacy and annotation cost. To
address this issue, we release SynFundus-1M, a high-quality synthetic dataset
containing over \textbf{1 million} fundus images w.r.t. 11 disease types.
Moreover, we intentionally diversify the readability of the images and
accordingly provide 4 types of the quality score for each image. To the best of
our knowledge, SynFundus-1M is currently the largest fundus dataset with the
most sophisticated annotations. All the images are generated by a Denoising
Diffusion Probabilistic Model, named SynFundus-Generator. Trained with over 1.3
million private fundus images, our SynFundus-Generator achieves significant
superior performance in generating fundus images compared to some recent
related works. Furthermore, we blend some synthetic images from SynFundus-1M
with real fundus images, and ophthalmologists can hardly distinguish the
synthetic images from real ones. Through extensive experiments, we demonstrate
that both convolutional neural networs (CNN) and Vision Transformer (ViT) can
benefit from SynFundus-1M by pretraining or training directly. Compared to
datasets like ImageNet or EyePACS, models trained on SynFundus-1M not only
achieve better performance but also faster convergence on various downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1&quot;&gt;Fangxin Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yehui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haifeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01339">
<title>ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational Applications. (arXiv:2312.01339v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01339</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first Arabic crossword puzzle generator driven by
advanced AI technology. Leveraging cutting-edge large language models including
GPT4, GPT3-Davinci, GPT3-Curie, GPT3-Babbage, GPT3-Ada, and BERT, the system
generates distinctive and challenging clues. Based on a dataset comprising over
50,000 clue-answer pairs, the generator employs fine-tuning, few/zero-shot
learning strategies, and rigorous quality-checking protocols to enforce the
generation of high-quality clue-answer pairs. Importantly, educational
crosswords contribute to enhancing memory, expanding vocabulary, and promoting
problem-solving skills, thereby augmenting the learning experience through a
fun and engaging approach, reshaping the landscape of traditional learning
methods. The overall system can be exploited as a powerful educational tool
that amalgamates AI and innovative learning techniques, heralding a
transformative era for Arabic crossword puzzles and the intersection of
technology and education.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeinalipour_K/0/1/0/all/0/1&quot;&gt;Kamyar Zeinalipour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_M/0/1/0/all/0/1&quot;&gt;Mohamed Zaky Saad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1&quot;&gt;Marco Maggini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gori_M/0/1/0/all/0/1&quot;&gt;Marco Gori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01544">
<title>KEEC: Embed to Control on An Equivariant Geometry. (arXiv:2312.01544v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01544</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates how representation learning can enable optimal
control in unknown and complex dynamics, such as chaotic and non-linear
systems, without relying on prior domain knowledge of the dynamics. The core
idea is to establish an equivariant geometry that is diffeomorphic to the
manifold defined by a dynamical system and to perform optimal control within
this corresponding geometry, which is a non-trivial task. To address this
challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model
learning and control. Inspired by Lie theory, KEEC begins by learning a
non-linear dynamical system defined on a manifold and embedding trajectories
into a Lie group. Subsequently, KEEC formulates an equivariant value function
equation in reinforcement learning on the equivariant geometry, ensuring an
invariant effect as the value function on the original manifold. By deriving
analytical-form optimal actions on the equivariant value function, KEEC
theoretically achieves quadratic convergence for the optimal equivariant value
function by leveraging the differential information on the equivariant
geometry. The effectiveness of KEEC is demonstrated in challenging dynamical
systems, including chaotic ones like Lorenz-63. Notably, our results show that
isometric functions, which maintain the compactness and completeness of
geometry while preserving metric and differential information, consistently
outperform loss functions lacking these characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xiaoyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yukun Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01648">
<title>Characterizing Large Language Model Geometry Solves Toxicity Detection and Generation. (arXiv:2312.01648v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01648</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models~(LLMs) drive current AI breakthroughs despite very
little being known about their internal representations, e.g., how to extract a
few informative features to solve various downstream tasks. To provide a
practical and principled answer, we propose to characterize LLMs from a
geometric perspective. We obtain in closed form (i) the intrinsic dimension in
which the Multi-Head Attention embeddings are constrained to exist and (ii) the
partition and per-region affine mappings of the per-layer feedforward networks.
Our results are informative, do not rely on approximations, and are actionable.
First, we show that, motivated by our geometric interpretation, we can bypass
Llama$2$&apos;s RLHF by controlling its embedding&apos;s intrinsic dimension through
informed prompt manipulation. Second, we derive $7$ interpretable spline
features that can be extracted from any (pre-trained) LLM layer, providing a
rich abstract representation of their inputs. Those features alone ($224$ for
Mistral-7B/Llama$2$-7B and $560$ for Llama$2$-70B) are sufficient to help solve
toxicity detection, infer the domain of the prompt, and even tackle the Jigsaw
challenge, which aims at characterizing the type of toxicity of various
prompts. Our results demonstrate how, even in large-scale regimes, exact
theoretical results can answer practical questions in language models. Code:
\url{https://github.com/RandallBalestriero/SplineLLM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cosentino_R/0/1/0/all/0/1&quot;&gt;Romain Cosentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekkizhar_S/0/1/0/all/0/1&quot;&gt;Sarath Shekkizhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02021">
<title>VLTSeg: Simple Transfer of CLIP-Based Vision-Language Representations for Domain Generalized Semantic Segmentation. (arXiv:2312.02021v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02021</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization (DG) remains a significant challenge for perception
based on deep neural networks (DNN), where domain shifts occur due to lighting,
weather, or geolocation changes. In this work, we propose VLTSeg to enhance
domain generalization in semantic segmentation, where the network is solely
trained on the source domain and evaluated on unseen target domains. Our method
leverages the inherent semantic robustness of vision-language models. First, by
substituting traditional vision-only backbones with pre-trained encoders from
CLIP and EVA-CLIP as transfer learning setting we find that in the field of DG,
vision-language pre-training significantly outperforms supervised and
self-supervised vision pre-training. We thus propose a new vision-language
approach for domain generalized segmentation, which improves the domain
generalization SOTA by 7.6% mIoU when training on the synthetic GTA5 dataset.
We further show the superior generalization capabilities of vision-language
segmentation models by reaching 76.48% mIoU on the popular Cityscapes-to-ACDC
benchmark, outperforming the previous SOTA approach by 6.9% mIoU on the test
set at the time of writing. Additionally, our approach shows strong in-domain
generalization capabilities indicated by 86.1% mIoU on the Cityscapes test set,
resulting in a shared first place with the previous SOTA on the current
leaderboard at the time of submission.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hummer_C/0/1/0/all/0/1&quot;&gt;Christoph H&amp;#xfc;mmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwonberg_M/0/1/0/all/0/1&quot;&gt;Manuel Schwonberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Liangwei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knoll_A/0/1/0/all/0/1&quot;&gt;Alois Knoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1&quot;&gt;Hanno Gottschalk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02338">
<title>A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics. (arXiv:2312.02338v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02338</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image (T2I) synthesis has recently achieved significant advancements.
However, challenges remain in the model&apos;s compositionality, which is the
ability to create new combinations from known components. We introduce
Winoground-T2I, a benchmark designed to evaluate the compositionality of T2I
models. This benchmark includes 11K complex, high-quality contrastive sentence
pairs spanning 20 categories. These contrastive sentence pairs with subtle
differences enable fine-grained evaluations of T2I synthesis models.
Additionally, to address the inconsistency across different metrics, we propose
a strategy that evaluates the reliability of various metrics by using
comparative sentence pairs. We use Winoground-T2I with a dual objective: to
evaluate the performance of T2I models and the metrics used for their
evaluation. Finally, we provide insights into the strengths and weaknesses of
these metrics and the capabilities of current T2I models in tackling challenges
across a range of complex compositional categories. Our benchmark is publicly
available at https://github.com/zhuxiangru/Winoground-T2I .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiangru Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Penglei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingping Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yanghua Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02957">
<title>Classification for everyone : Building geography agnostic models for fairer recognition. (arXiv:2312.02957v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02957</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we analyze different methods to mitigate inherent geographical
biases present in state of the art image classification models. We first
quantitatively present this bias in two datasets - The Dollar Street Dataset
and ImageNet, using images with location information. We then present different
methods which can be employed to reduce this bias. Finally, we analyze the
effectiveness of the different techniques on making these models more robust to
geographical locations of the images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jindal_A/0/1/0/all/0/1&quot;&gt;Akshat Jindal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shreya Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1&quot;&gt;Soham Gadgil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03585">
<title>Foundation Model Assisted Weakly Supervised Semantic Segmentation. (arXiv:2312.03585v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03585</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to leverage pre-trained foundation models, such as contrastive
language-image pre-training (CLIP) and segment anything model (SAM), to address
weakly supervised semantic segmentation (WSSS) using image-level labels. To
this end, we propose a coarse-to-fine framework based on CLIP and SAM for
generating high-quality segmentation seeds. Specifically, we construct an image
classification task and a seed segmentation task, which are jointly performed
by CLIP with frozen weights and two sets of learnable task-specific prompts. A
SAM-based seeding (SAMS) module is designed and applied to each task to produce
either coarse or fine seed maps. Moreover, we design a multi-label contrastive
loss supervised by image-level labels and a CAM activation loss supervised by
the generated coarse seed map. These losses are used to learn the prompts,
which are the only parts need to be learned in our framework. Once the prompts
are learned, we input each image along with the learned segmentation-specific
prompts into CLIP and the SAMS module to produce high-quality segmentation
seeds. These seeds serve as pseudo labels to train an off-the-shelf
segmentation network like other two-stage WSSS methods. Experiments show that
our method achieves the state-of-the-art performance on PASCAL VOC 2012 and
competitive results on MS COCO 2014. Code is available at
https://github.com/HAL-42/FMA-WSSS.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaobo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiaojin Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03730">
<title>FakeWatch ElectionShield: A Benchmarking Framework to Detect Fake News for Credible US Elections. (arXiv:2312.03730v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03730</link>
<description rdf:parseType="Literal">&lt;p&gt;In today&apos;s technologically driven world, the spread of fake news,
particularly during crucial events such as elections, presents an increasing
challenge to the integrity of information. To address this challenge, we
introduce FakeWatch ElectionShield, an innovative framework carefully designed
to detect fake news. We have created a novel dataset of North American
election-related news articles through a blend of advanced language models
(LMs) and thorough human verification, for precision and relevance. We propose
a model hub of LMs for identifying fake news. Our goal is to provide the
research community with adaptable and accurate classification models in
recognizing the dynamic nature of misinformation. Extensive evaluation of fake
news classifiers on our dataset and a benchmark dataset shows our that while
state-of-the-art LMs slightly outperform the traditional ML models, classical
models are still competitive with their balance of accuracy, explainability,
and computational efficiency. This research sets the foundation for future
studies to address misinformation related to elections.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_T/0/1/0/all/0/1&quot;&gt;Tahniat Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatrath_V/0/1/0/all/0/1&quot;&gt;Veronica Chatrath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bamgbose_O/0/1/0/all/0/1&quot;&gt;Oluwanifemi Bamgbose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shaina Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03815">
<title>LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem. (arXiv:2312.03815v2 [cs.OS] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03815</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper envisions a revolutionary AIOS-Agent ecosystem, where Large
Language Model (LLM) serves as the (Artificial) Intelligent Operating System
(IOS, or AIOS)--an operating system &quot;with soul&quot;. Upon this foundation, a
diverse range of LLM-based AI Agent Applications (Agents, or AAPs) are
developed, enriching the AIOS-Agent ecosystem and signaling a paradigm shift
from the traditional OS-APP ecosystem. We envision that LLM&apos;s impact will not
be limited to the AI application level, instead, it will in turn revolutionize
the design and implementation of computer system, architecture, software, and
programming language, featured by several main concepts: LLM as OS
(system-level), Agents as Applications (application-level), Natural Language as
Programming Interface (user-level), and Tools as Devices/Libraries
(hardware/middleware-level). We begin by introducing the architecture of
traditional OS. Then we formalize a conceptual framework for AIOS through &quot;LLM
as OS (LLMOS)&quot;, drawing analogies between AIOS and traditional OS: LLM is
likened to OS kernel, context window to memory, external storage to file
system, hardware tools to peripheral devices, software tools to programming
libraries, and user prompts to user commands. Subsequently, we introduce the
new AIOS-Agent Ecosystem, where users can easily program Agent Applications
(AAPs) using natural language, democratizing the development of software, which
is different from the traditional OS-APP ecosystem. Following this, we explore
the diverse scope of Agent Applications. We delve into both single-agent and
multi-agent systems, as well as human-agent interaction. Lastly, drawing on the
insights from traditional OS-APP ecosystem, we propose a roadmap for the
evolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the
future research and development, suggesting systematic progresses of AIOS and
its Agent applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yujie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04021">
<title>A Study on the Calibration of In-context Learning. (arXiv:2312.04021v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04021</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern auto-regressive language models are trained to minimize log loss on
broad data by predicting the next token so they are expected to get calibrated
answers in next-token prediction tasks. We study this for in-context learning
(ICL), a widely used way to adapt frozen large language models (LLMs) via
crafting prompts, and investigate the trade-offs between performance and
calibration on a wide range of natural language understanding and reasoning
tasks. We conduct extensive experiments to show that such trade-offs may get
worse as we increase model size, incorporate more ICL examples, and fine-tune
models using instruction, dialog, or reinforcement learning from human feedback
(RLHF) on carefully curated datasets. Furthermore, we find that common
recalibration techniques that are widely effective such as temperature scaling
provide limited gains in calibration errors, suggesting that new methods may be
required for settings where models are expected to be reliable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi-Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madeka_D/0/1/0/all/0/1&quot;&gt;Dhruv Madeka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dean Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Hima Lakkaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04025">
<title>Moirai: Towards Optimal Placement for Distributed Inference on Heterogeneous Devices. (arXiv:2312.04025v2 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04025</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating size of Deep Neural Networks (DNNs) has spurred a growing
research interest in hosting and serving DNN models across multiple devices. A
number of studies have been reported to partition a DNN model across devices,
providing device placement solutions. The methods appeared in the literature,
however, either suffer from poor placement performance due to the exponential
search space or miss an optimal placement as a consequence of the reduced
search space with limited heuristics. Moreover, these methods have ignored the
runtime inter-operator optimization of a computation graph when coarsening the
graph, which degrades the end-to-end inference performance. This paper presents
Moirai that better exploits runtime inter-operator fusion in a model to render
a coarsened computation graph, reducing the search space while maintaining the
inter-operator optimization provided by inference backends. Moirai also
generalizes the device placement algorithm from multiple perspectives by
considering inference constraints and device heterogeneity.Extensive
experimental evaluation with 11 large DNNs demonstrates that Moirai outperforms
the state-of-the-art counterparts, i.e., Placeto, m-SCT, and GETF, up to
4.28$\times$ in reduction of the end-to-end inference latency. Moirai code is
anonymously released at \url{https://github.com/moirai-placement/moirai}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Beibei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongwei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhihui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sean Xiaoyang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04417">
<title>Temporal Fairness in Multiwinner Voting. (arXiv:2312.04417v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04417</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiwinner voting captures a wide variety of settings, from parliamentary
elections in democratic systems to product placement in online shopping
platforms. There is a large body of work dealing with axiomatic
characterizations, computational complexity, and algorithmic analysis of
multiwinner voting rules. Although many challenges remain, significant progress
has been made in showing existence of fair and representative outcomes as well
as efficient algorithmic solutions for many commonly studied settings. However,
much of this work focuses on single-shot elections, even though in numerous
real-world settings elections are held periodically and repeatedly. Hence, it
is imperative to extend the study of multiwinner voting to temporal settings.
Recently, there have been several efforts to address this challenge. However,
these works are difficult to compare, as they model multi-period voting in very
different ways. We propose a unified framework for studying temporal fairness
in this domain, drawing connections with various existing bodies of work, and
consolidating them within a general framework. We also identify gaps in
existing literature, outline multiple opportunities for future work, and put
forward a vision for the future of multiwinner voting in temporal settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elkind_E/0/1/0/all/0/1&quot;&gt;Edith Elkind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obraztsova_S/0/1/0/all/0/1&quot;&gt;Svetlana Obraztsova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teh_N/0/1/0/all/0/1&quot;&gt;Nicholas Teh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04584">
<title>Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04584</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects&apos;
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant&apos; and therefore act as `noises&apos; for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingyan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04688">
<title>BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets. (arXiv:2210.04688v4 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2210.04688</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) makes an agent learn from trial-and-error
experiences gathered during the interaction with the environment. Recently,
offline RL has become a popular RL paradigm because it saves the interactions
with environments. In offline RL, data providers share large pre-collected
datasets, and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. More
specifically, Baffle modifies 10\% of the datasets for four tasks (3 robotic
controls and 1 autonomous driving). Agents trained on the poisoned datasets
perform well in normal settings. However, when triggers are presented, the
agents&apos; performance decreases drastically by 63.2\%, 53.9\%, 64.7\%, and 47.4\%
in the four tasks on average. The backdoor still persists after fine-tuning
poisoned agents on clean datasets. We further show that the inserted backdoor
is also hard to be detected by a popular defensive method. This paper calls
attention to developing more effective protection for the open-source offline
RL dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junda He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jieke Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kecen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Arunesh Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinwen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09304">
<title>Interpretability in Activation Space Analysis of Transformers: A Focused Survey. (arXiv:2302.09304v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2302.09304</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of natural language processing has reached breakthroughs with the
advent of transformers. They have remained state-of-the-art since then, and
there also has been much research in analyzing, interpreting, and evaluating
the attention layers and the underlying embedding space. In addition to the
self-attention layers, the feed-forward layers in the transformer are a
prominent architectural component. From extensive research, we observe that its
role is under-explored. We focus on the latent space, known as the Activation
Space, that consists of the neuron activations from these feed-forward layers.
In this survey paper, we review interpretability methods that examine the
learnings that occurred in this activation space. Since there exists only
limited research in this direction, we conduct a detailed examination of each
work and point out potential future directions of research. We hope our work
provides a step towards strengthening activation space analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1&quot;&gt;Soniya Vijayakumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09936">
<title>BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2308.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), comparing to our
baseline InstructBLIP. BLIVA demonstrates significant capability in decoding
real-world images, irrespective of text presence. To demonstrate the broad
industry applications enabled by BLIVA, we evaluate the model using a new
dataset comprising YouTube thumbnails paired with question-answer sets across
11 diverse categories. For researchers interested in further exploration, our
code and models are freely accessible at https://github.com/mlpc-ucsd/BLIVA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weiyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>