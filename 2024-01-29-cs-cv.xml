<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-25T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14024" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14130" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.14405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.11442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01429" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.13560" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.13697">
<title>Toward Robust Multimodal Learning using Multimodal Foundational Models. (arXiv:2401.13697v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13697</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing multimodal sentiment analysis tasks are highly rely on the
assumption that the training and test sets are complete multimodal data, while
this assumption can be difficult to hold: the multimodal data are often
incomplete in real-world scenarios. Therefore, a robust multimodal model in
scenarios with randomly missing modalities is highly preferred. Recently,
CLIP-based multimodal foundational models have demonstrated impressive
performance on numerous multimodal tasks by learning the aligned cross-modal
semantics of image and text pairs, but the multimodal foundational models are
also unable to directly address scenarios involving modality absence. To
alleviate this issue, we propose a simple and effective framework, namely TRML,
Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML
employs generated virtual modalities to replace missing modalities, and aligns
the semantic spaces between the generated and missing modalities. Concretely,
we design a missing modality inference module to generate virtual modaliites
and replace missing modalities. We also design a semantic matching learning
module to align semantic spaces generated and missing modalities. Under the
prompt of complete modality, our model captures the semantics of missing
modalities by leveraging the aligned cross-modal semantic space. Experiments
demonstrate the superiority of our approach on three multimodal sentiment
analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xianbing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1&quot;&gt;Soujanya Poria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuejiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1&quot;&gt;Buzhou Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13714">
<title>Value-Driven Mixed-Precision Quantization for Patch-Based Inference on Microcontrollers. (arXiv:2401.13714v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13714</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying neural networks on microcontroller units (MCUs) presents
substantial challenges due to their constrained computation and memory
resources. Previous researches have explored patch-based inference as a
strategy to conserve memory without sacrificing model accuracy. However, this
technique suffers from severe redundant computation overhead, leading to a
substantial increase in execution latency. A feasible solution to address this
issue is mixed-precision quantization, but it faces the challenges of accuracy
degradation and a time-consuming search time. In this paper, we propose
QuantMCU, a novel patch-based inference method that utilizes value-driven
mixed-precision quantization to reduce redundant computation. We first utilize
value-driven patch classification (VDPC) to maintain the model accuracy. VDPC
classifies patches into two classes based on whether they contain outlier
values. For patches containing outlier values, we apply 8-bit quantization to
the feature maps on the dataflow branches that follow. In addition, for patches
without outlier values, we utilize value-driven quantization search (VDQS) on
the feature maps of their following dataflow branches to reduce search time.
Specifically, VDQS introduces a novel quantization search metric that takes
into account both computation and accuracy, and it employs entropy as an
accuracy representation to avoid additional training. VDQS also adopts an
iterative approach to determine the bitwidth of each feature map to further
accelerate the search process. Experimental results on real-world MCU devices
show that QuantMCU can reduce computation by 2.2x on average while maintaining
comparable model accuracy compared to the state-of-the-art patch-based
inference methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wei Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Shenglin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kai Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guokuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jiguang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianzong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13719">
<title>Inference Attacks Against Face Recognition Model without Classification Layers. (arXiv:2401.13719v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13719</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition (FR) has been applied to nearly every aspect of daily life,
but it is always accompanied by the underlying risk of leaking private
information. At present, almost all attack models against FR rely heavily on
the presence of a classification layer. However, in practice, the FR model can
obtain complex features of the input via the model backbone, and then compare
it with the target for inference, which does not explicitly involve the outputs
of the classification layer adopting logit or other losses. In this work, we
advocate a novel inference attack composed of two stages for practical FR
models without a classification layer. The first stage is the membership
inference attack. Specifically, We analyze the distances between the
intermediate features and batch normalization (BN) parameters. The results
indicate that this distance is a critical metric for membership inference. We
thus design a simple but effective attack model that can determine whether a
face image is from the training dataset or not. The second stage is the model
inversion attack, where sensitive private data is reconstructed using a
pre-trained generative adversarial network (GAN) guided by the attack model in
the first stage. To the best of our knowledge, the proposed attack model is the
very first in the literature developed for FR models without a classification
layer. We illustrate the application of the proposed attack model in the
establishment of privacy-preserving FR techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanqing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinggui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13721">
<title>Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in Regression. (arXiv:2401.13721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model
from a labeled source domain to an unlabeled target domain for regression
tasks. Recent successful works in UDAR mostly focus on subspace alignment,
involving the alignment of a selected subspace within the entire feature space.
This contrasts with the feature alignment methods used for classification,
which aim at aligning the entire feature space and have proven effective but
are less so in regression settings. Specifically, while classification aims to
identify separate clusters across the entire embedding dimension, regression
induces less structure in the data representation, necessitating additional
guidance for efficient alignment. In this paper, we propose an effective method
for UDAR by incorporating guidance from uncertainty. Our approach serves a dual
purpose: providing a measure of confidence in predictions and acting as a
regularization of the embedding space. Specifically, we leverage the Deep
Evidential Learning framework, which outputs both predictions and uncertainties
for each input sample. We propose aligning the parameters of higher-order
evidential distributions between the source and target domains using
traditional alignment methods at the feature or posterior level. Additionally,
we propose to augment the feature space representation by mixing source samples
with pseudo-labeled target samples based on label similarity. This cross-domain
mixing strategy produces more realistic samples than random mixing and
introduces higher uncertainty, facilitating further alignment. We demonstrate
the effectiveness of our approach on four benchmarks for UDAR, on which we
outperform existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejjar_I/0/1/0/all/0/1&quot;&gt;Ismail Nejjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frusque_G/0/1/0/all/0/1&quot;&gt;Gaetan Frusque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forest_F/0/1/0/all/0/1&quot;&gt;Florent Forest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13751">
<title>A Systematic Approach to Robustness Modelling for Deep Convolutional Neural Networks. (arXiv:2401.13751v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.13751</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks have shown to be widely applicable to a large
number of fields when large amounts of labelled data are available. The recent
trend has been to use models with increasingly larger sets of tunable
parameters to increase model accuracy, reduce model loss, or create more
adversarially robust models -- goals that are often at odds with one another.
In particular, recent theoretical work raises questions about the ability for
even larger models to generalize to data outside of the controlled train and
test sets. As such, we examine the role of the number of hidden layers in the
ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a
variety of parameters including the size of the model, the floating point
precision, and the noise level of both the training data and the model output.
To encapsulate the model&apos;s predictive power and computational cost, we provide
a method that uses induced failures to model the probability of failure as a
function of time and relate that to a novel metric that allows us to quickly
determine whether or not the cost of training a model outweighs the cost of
attacking it. Using this approach, we are able to approximate the expected
failure rate using a small number of specially crafted samples rather than
increasingly larger benchmark datasets. We demonstrate the efficacy of this
technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit
floating-point numbers, various data pre-processing techniques, and several
attacks on five configurations of the ResNet model. Then, using empirical
measurements, we examine the various trade-offs between cost, robustness,
latency, and reliability to find that larger models do not significantly aid in
adversarial robustness despite costing significantly more to train.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyers_C/0/1/0/all/0/1&quot;&gt;Charles Meyers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedghpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Saleh Sedghpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lofstedt_T/0/1/0/all/0/1&quot;&gt;Tommy L&amp;#xf6;fstedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elmroth_E/0/1/0/all/0/1&quot;&gt;Erik Elmroth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13782">
<title>Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility. (arXiv:2401.13782v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2401.13782</link>
<description rdf:parseType="Literal">&lt;p&gt;As the number of accepted papers at AI and ML conferences reaches into the
thousands, it has become unclear how researchers access and read research
publications. In this paper, we investigate the role of social media
influencers in enhancing the visibility of machine learning research,
particularly the citation counts of papers they share. We have compiled a
comprehensive dataset of over 8,000 papers, spanning tweets from December 2018
to October 2023, alongside 1:1 matched controls based on publication year,
venue, and abstract topics. Our analysis reveals a significant increase in
citations for papers endorsed by these influencers, with median citation counts
2-3 times higher than those of the control group. Additionally, the study
delves into the geographic, gender, and institutional diversity of highlighted
authors. These findings highlight the expanding influence of social media in
scholarly communication and underscore the importance of an evolving ecosystem
in today&apos;s digital academic landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weissburg_I/0/1/0/all/0/1&quot;&gt;Iain Xie Weissburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1&quot;&gt;Mehir Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liangming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Yang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13785">
<title>S2TPVFormer: Spatio-Temporal Tri-Perspective View for temporally coherent 3D Semantic Occupancy Prediction. (arXiv:2401.13785v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13785</link>
<description rdf:parseType="Literal">&lt;p&gt;Holistic understanding and reasoning in 3D scenes play a vital role in the
success of autonomous driving systems. The evolution of 3D semantic occupancy
prediction as a pretraining task for autonomous driving and robotic downstream
tasks captures finer 3D details compared to methods like 3D detection. Existing
approaches predominantly focus on spatial cues, often overlooking temporal
cues. Query-based methods tend to converge on computationally intensive Voxel
representation for encoding 3D scene information. This study introduces
S2TPVFormer, an extension of TPVFormer, utilizing a spatiotemporal transformer
architecture for coherent 3D semantic occupancy prediction. Emphasizing the
importance of spatiotemporal cues in 3D scene perception, particularly in 3D
semantic occupancy prediction, our work explores the less-explored realm of
temporal cues. Leveraging Tri-Perspective View (TPV) representation, our
spatiotemporal encoder generates temporally rich embeddings, improving
prediction coherence while maintaining computational efficiency. To achieve
this, we propose a novel Temporal Cross-View Hybrid Attention (TCVHA)
mechanism, facilitating effective spatiotemporal information exchange across
TPV views. Experimental evaluations on the nuScenes dataset demonstrate a
substantial 3.1% improvement in mean Intersection over Union (mIoU) for 3D
Semantic Occupancy compared to TPVFormer, confirming the effectiveness of the
proposed S2TPVFormer in enhancing 3D scene perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_S/0/1/0/all/0/1&quot;&gt;Sathira Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wannigama_S/0/1/0/all/0/1&quot;&gt;Savindu Bhashitha Wannigama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragel_R/0/1/0/all/0/1&quot;&gt;Roshan Ragel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1&quot;&gt;Gihan Jayatilaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13786">
<title>FoVA-Depth: Field-of-View Agnostic Depth Estimation for Cross-Dataset Generalization. (arXiv:2401.13786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13786</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide field-of-view (FoV) cameras efficiently capture large portions of the
scene, which makes them attractive in multiple domains, such as automotive and
robotics. For such applications, estimating depth from multiple images is a
critical task, and therefore, a large amount of ground truth (GT) data is
available. Unfortunately, most of the GT data is for pinhole cameras, making it
impossible to properly train depth estimation models for large-FoV cameras. We
propose the first method to train a stereo depth estimation model on the widely
available pinhole data, and to generalize it to data captured with larger FoVs.
Our intuition is simple: We warp the training data to a canonical, large-FoV
representation and augment it to allow a single network to reason about diverse
types of distortions that otherwise would prevent generalization. We show
strong generalization ability of our approach on both indoor and outdoor
datasets, which was not possible with previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lichy_D/0/1/0/all/0/1&quot;&gt;Daniel Lichy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badki_A/0/1/0/all/0/1&quot;&gt;Abhishek Badki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1&quot;&gt;Jan Kautz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallo_O/0/1/0/all/0/1&quot;&gt;Orazio Gallo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13795">
<title>Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent Diffusion Models for Virtual Try-All. (arXiv:2401.13795v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13795</link>
<description rdf:parseType="Literal">&lt;p&gt;As online shopping is growing, the ability for buyers to virtually visualize
products in their settings-a phenomenon we define as &quot;Virtual Try-All&quot;-has
become crucial. Recent diffusion models inherently contain a world model,
rendering them suitable for this task within an inpainting context. However,
traditional image-conditioned diffusion models often fail to capture the
fine-grained details of products. In contrast, personalization-driven models
such as DreamPaint are good at preserving the item&apos;s details but they are not
optimized for real-time applications. We present &quot;Diffuse to Choose,&quot; a novel
diffusion-based image-conditioned inpainting model that efficiently balances
fast inference with the retention of high-fidelity details in a given reference
item while ensuring accurate semantic manipulations in the given scene content.
Our approach is based on incorporating fine-grained features from the reference
image directly into the latent feature maps of the main diffusion model,
alongside with a perceptual loss to further preserve the reference item&apos;s
details. We conduct extensive testing on both in-house and publicly available
datasets, and show that Diffuse to Choose is superior to existing zero-shot
diffusion inpainting methods as well as few-shot diffusion personalization
algorithms like DreamPaint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1&quot;&gt;Mehmet Saygin Seyfioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouyarmane_K/0/1/0/all/0/1&quot;&gt;Karim Bouyarmane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Suren Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavanaei_A/0/1/0/all/0/1&quot;&gt;Amir Tavanaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tutar_I/0/1/0/all/0/1&quot;&gt;Ismail B. Tutar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13837">
<title>Democratizing Fine-grained Visual Recognition with Large Language Models. (arXiv:2401.13837v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13837</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying subordinate-level categories from images is a longstanding task
in computer vision and is referred to as fine-grained visual recognition
(FGVR). It has tremendous significance in real-world applications since an
average layperson does not excel at differentiating species of birds or
mushrooms due to subtle differences among the species. A major bottleneck in
developing FGVR systems is caused by the need of high-quality paired expert
annotations. To circumvent the need of expert knowledge we propose Fine-grained
Semantic Category Reasoning (FineR) that internally leverages the world
knowledge of large language models (LLMs) as a proxy in order to reason about
fine-grained category names. In detail, to bridge the modality gap between
images and LLM, we extract part-level visual attributes from images as text and
feed that information to a LLM. Based on the visual attributes and its internal
world knowledge the LLM reasons about the subordinate-level category names. Our
training-free FineR outperforms several state-of-the-art FGVR and language and
vision assistant models and shows promise in working in the wild and in new
domains where gathering expert annotation is arduous.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mingxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhankar Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13853">
<title>Dataset and Benchmark: Novel Sensors for Autonomous Vehicle Perception. (arXiv:2401.13853v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.13853</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional cameras employed in autonomous vehicle (AV) systems support many
perception tasks, but are challenged by low-light or high dynamic range scenes,
adverse weather, and fast motion. Novel sensors, such as event and thermal
cameras, offer capabilities with the potential to address these scenarios, but
they remain to be fully exploited. This paper introduces the Novel Sensors for
Autonomous Vehicle Perception (NSAVP) dataset to facilitate future research on
this topic. The dataset was captured with a platform including stereo event,
thermal, monochrome, and RGB cameras as well as a high precision navigation
system providing ground truth poses. The data was collected by repeatedly
driving two ~8 km routes and includes varied lighting conditions and opposing
viewpoint perspectives. We provide benchmarking experiments on the task of
place recognition to demonstrate challenges and opportunities for novel sensors
to enhance critical AV perception tasks. To our knowledge, the NSAVP dataset is
the first to include stereo thermal cameras together with stereo event and
monochrome cameras. The dataset and supporting software suite is available at:
https://umautobots.github.io/nsavp
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmichael_S/0/1/0/all/0/1&quot;&gt;Spencer Carmichael&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buchan_A/0/1/0/all/0/1&quot;&gt;Austin Buchan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanagopal_M/0/1/0/all/0/1&quot;&gt;Mani Ramanagopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_R/0/1/0/all/0/1&quot;&gt;Radhika Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasudevan_R/0/1/0/all/0/1&quot;&gt;Ram Vasudevan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skinner_K/0/1/0/all/0/1&quot;&gt;Katherine A. Skinner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13856">
<title>LAA-Net: Localized Artifact Attention Network for High-Quality Deepfakes Detection. (arXiv:2401.13856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13856</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel approach for high-quality deepfake detection
called Localized Artifact Attention Network (LAA-Net). Existing methods for
high-quality deepfake detection are mainly based on a supervised binary
classifier coupled with an implicit attention mechanism. As a result, they do
not generalize well to unseen manipulations. To handle this issue, two main
contributions are made. First, an explicit attention mechanism within a
multi-task learning framework is proposed. By combining heatmap-based and
self-consistency attention strategies, LAA-Net is forced to focus on a few
small artifact-prone vulnerable regions. Second, an Enhanced Feature Pyramid
Network (E-FPN) is proposed as a simple and effective mechanism for spreading
discriminative low-level features into the final feature output, with the
advantage of limiting redundancy. Experiments performed on several benchmarks
show the superiority of our approach in terms of Area Under the Curve (AUC) and
Average Precision (AP). The code will be released soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dat Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejri_N/0/1/0/all/0/1&quot;&gt;Nesryne Mejri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Inder Pal Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuleshova_P/0/1/0/all/0/1&quot;&gt;Polina Kuleshova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1&quot;&gt;Marcella Astrid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kacem_A/0/1/0/all/0/1&quot;&gt;Anis Kacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_E/0/1/0/all/0/1&quot;&gt;Enjie Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13865">
<title>Appearance Debiased Gaze Estimation via Stochastic Subject-Wise Adversarial Learning. (arXiv:2401.13865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13865</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, appearance-based gaze estimation has been attracting attention in
computer vision, and remarkable improvements have been achieved using various
deep learning techniques. Despite such progress, most methods aim to infer gaze
vectors from images directly, which causes overfitting to person-specific
appearance factors. In this paper, we address these challenges and propose a
novel framework: Stochastic subject-wise Adversarial gaZE learning (SAZE),
which trains a network to generalize the appearance of subjects. We design a
Face generalization Network (Fgen-Net) using a face-to-gaze encoder and face
identity classifier and a proposed adversarial loss. The proposed loss
generalizes face appearance factors so that the identity classifier inferences
a uniform probability distribution. In addition, the Fgen-Net is trained by a
learning mechanism that optimizes the network by reselecting a subset of
subjects at every training step to avoid overfitting. Our experimental results
verify the robustness of the method in that it yields state-of-the-art
performance, achieving 3.89 and 4.42 on the MPIIGaze and EyeDiap datasets,
respectively. Furthermore, we demonstrate the positive generalization effect by
conducting further experiments using face images involving different styles
generated from the generative model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Suneung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_W/0/1/0/all/0/1&quot;&gt;Woo-Jeoung Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13877">
<title>AscDAMs: Advanced SLAM-based channel detection and mapping system. (arXiv:2401.13877v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13877</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining high-resolution, accurate channel topography and deposit conditions
is the prior challenge for the study of channelized debris flow. Currently,
wide-used mapping technologies including satellite imaging and drone
photogrammetry struggle to precisely observe channel interior conditions of
mountainous long-deep gullies, particularly those in the Wenchuan Earthquake
region. SLAM is an emerging tech for 3D mapping; however, extremely rugged
environment in long-deep gullies poses two major challenges even for the
state-of-art SLAM: (1) Atypical features; (2) Violent swaying and oscillation
of sensors. These issues result in large deviation and lots of noise for SLAM
results. To improve SLAM mapping in such environments, we propose an advanced
SLAM-based channel detection and mapping system, namely AscDAMs. It features
three main enhancements to post-process SLAM results: (1) The digital
orthophoto map aided deviation correction algorithm greatly eliminates the
systematic error; (2) The point cloud smoothing algorithm substantially
diminishes noises; (3) The cross section extraction algorithm enables the
quantitative assessment of channel deposits and their changes. Two field
experiments were conducted in Chutou Gully, Wenchuan County in China in
February and November 2023, representing observations before and after the
rainy season. We demonstrate the capability of AscDAMs to greatly improve SLAM
results, promoting SLAM for mapping the specially challenging environment. The
proposed method compensates for the insufficiencies of existing technologies in
detecting debris flow channel interiors including detailed channel morphology,
erosion patterns, deposit distinction, volume estimation and change detection.
It serves to enhance the study of full-scale debris flow mechanisms, long-term
post-seismic evolution, and hazard assessment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Fucheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jintao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Taosheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_H/0/1/0/all/0/1&quot;&gt;Hui Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_P/0/1/0/all/0/1&quot;&gt;Ping Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13888">
<title>Knowledge Graph Supported Benchmark and Video Captioning for Basketball. (arXiv:2401.13888v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13888</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent emergence of video captioning models, how to generate the
text description with specific entity names and fine-grained actions is far
from being solved, which however has great applications such as basketball live
text broadcast. In this paper, a new multimodal knowledge supported basketball
benchmark for video captioning is proposed. Specifically, we construct a
Multimodal Basketball Game Knowledge Graph (MbgKG) to provide knowledge beyond
videos. Then, a Multimodal Basketball Game Video Captioning (MbgVC) dataset
that contains 9 types of fine-grained shooting events and 286 players&apos;
knowledge (i.e., images and names) is constructed based on MbgKG. We develop a
novel framework in the encoder-decoder form named Entity-Aware Captioner (EAC)
for basketball live text broadcast. The temporal information in video is
encoded by introducing the bi-directional GRU (Bi-GRU) module. And the
multi-head self-attention module is utilized to model the relationships among
the players and select the key players. Besides, we propose a new performance
evaluation metric named Game Description Score (GDS), which measures not only
the linguistic performance but also the accuracy of the names prediction.
Extensive experiments on MbgVC dataset demonstrate that EAC effectively
leverages external knowledge and outperforms advanced video captioning models.
The proposed benchmark and corresponding codes will be publicly available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zeyu Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Ge Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lifang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuefen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zilin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13934">
<title>MambaMorph: a Mamba-based Backbone with Contrastive Feature Learning for Deformable MR-CT Registration. (arXiv:2401.13934v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13934</link>
<description rdf:parseType="Literal">&lt;p&gt;Deformable image registration is an essential approach for medical image
analysis.This paper introduces MambaMorph, an innovative multi-modality
deformable registration network, specifically designed for Magnetic Resonance
(MR) and Computed Tomography (CT) image alignment. MambaMorph stands out with
its Mamba-based registration module and a contrastive feature learning
approach, addressing the prevalent challenges in multi-modality registration.
The network leverages Mamba blocks for efficient long-range modeling and
high-dimensional data processing, coupled with a feature extractor that learns
fine-grained features for enhanced registration accuracy. Experimental results
showcase MambaMorph&apos;s superior performance over existing methods in MR-CT
registration, underlining its potential in clinical applications. This work
underscores the significance of feature learning in multi-modality registration
and positions MambaMorph as a trailblazing solution in this field. The code for
MambaMorph is available at: https://github.com/Guo-Stone/MambaMorph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Cai Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13937">
<title>Self-supervised Video Object Segmentation with Distillation Learning of Deformable Attention. (arXiv:2401.13937v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13937</link>
<description rdf:parseType="Literal">&lt;p&gt;Video object segmentation is a fundamental research problem in computer
vision. Recent techniques have often applied attention mechanism to object
representation learning from video sequences. However, due to temporal changes
in the video data, attention maps may not well align with the objects of
interest across video frames, causing accumulated errors in long-term video
processing. In addition, existing techniques have utilised complex
architectures, requiring highly computational complexity and hence limiting the
ability to integrate video object segmentation into low-powered devices. To
address these issues, we propose a new method for self-supervised video object
segmentation based on distillation learning of deformable attention.
Specifically, we devise a lightweight architecture for video object
segmentation that is effectively adapted to temporal changes. This is enabled
by deformable attention mechanism, where the keys and values capturing the
memory of a video sequence in the attention module have flexible locations
updated across frames. The learnt object representations are thus adaptive to
both the spatial and temporal dimensions. We train the proposed architecture in
a self-supervised fashion through a new knowledge distillation paradigm where
deformable attention maps are integrated into the distillation loss. We
qualitatively and quantitatively evaluate our method and compare it with
existing methods on benchmark datasets including DAVIS 2016/2017 and
YouTube-VOS 2018/2019. Experimental results verify the superiority of our
method via its achieved state-of-the-art performance and optimal memory usage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_Q/0/1/0/all/0/1&quot;&gt;Quang-Trung Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Thanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1&quot;&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13942">
<title>StyleInject: Parameter Efficient Tuning of Text-to-Image Diffusion Models. (arXiv:2401.13942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13942</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to fine-tune generative models for text-to-image generation tasks
is crucial, particularly facing the complexity involved in accurately
interpreting and visualizing textual inputs. While LoRA is efficient for
language model adaptation, it often falls short in text-to-image tasks due to
the intricate demands of image generation, such as accommodating a broad
spectrum of styles and nuances. To bridge this gap, we introduce StyleInject, a
specialized fine-tuning approach tailored for text-to-image models. StyleInject
comprises multiple parallel low-rank parameter matrices, maintaining the
diversity of visual features. It dynamically adapts to varying styles by
adjusting the variance of visual features based on the characteristics of the
input signal. This approach significantly minimizes the impact on the original
model&apos;s text-image alignment capabilities while adeptly adapting to various
styles in transfer learning. StyleInject proves particularly effective in
learning from and enhancing a range of advanced, community-fine-tuned
generative models. Our comprehensive experiments, including both small-sample
and large-scale data fine-tuning as well as base model distillation, show that
StyleInject surpasses traditional LoRA in both text-image semantic consistency
and human preference evaluation, all while ensuring greater parameter
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yalong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mohan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13950">
<title>AM-SORT: Adaptable Motion Predictor with Historical Trajectory Embedding for Multi-Object Tracking. (arXiv:2401.13950v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13950</link>
<description rdf:parseType="Literal">&lt;p&gt;Many multi-object tracking (MOT) approaches, which employ the Kalman Filter
as a motion predictor, assume constant velocity and Gaussian-distributed
filtering noises. These assumptions render the Kalman Filter-based trackers
effective in linear motion scenarios. However, these linear assumptions serve
as a key limitation when estimating future object locations within scenarios
involving non-linear motion and occlusions. To address this issue, we propose a
motion-based MOT approach with an adaptable motion predictor, called AM-SORT,
which adapts to estimate non-linear uncertainties. AM-SORT is a novel extension
of the SORT-series trackers that supersedes the Kalman Filter with the
transformer architecture as a motion predictor. We introduce a historical
trajectory embedding that empowers the transformer to extract spatio-temporal
features from a sequence of bounding boxes. AM-SORT achieves competitive
performance compared to state-of-the-art trackers on DanceTrack, with 56.3 IDF1
and 55.6 HOTA. We conduct extensive experiments to demonstrate the
effectiveness of our method in predicting non-linear movement under occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1&quot;&gt;Vitaliy Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_G/0/1/0/all/0/1&quot;&gt;Gunho Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13956">
<title>A New Image Quality Database for Multiple Industrial Processes. (arXiv:2401.13956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13956</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a broader range of applications of image
processing technologies in multiple industrial processes, such as smoke
detection, security monitoring, and workpiece inspection. Different kinds of
distortion types and levels must be introduced into an image during the
processes of acquisition, compression, transmission, storage, and display,
which might heavily degrade the image quality and thus strongly reduce the
final display effect and clarity. To verify the reliability of existing image
quality assessment methods, we establish a new industrial process image
database (IPID), which contains 3000 distorted images generated by applying
different levels of distortion types to each of the 50 source images. We
conduct the subjective test on the aforementioned 3000 images to collect their
subjective quality ratings in a well-suited laboratory environment. Finally, we
perform comparison experiments on IPID database to investigate the performance
of some objective image quality assessment algorithms. The experimental results
show that the state-of-the-art image quality assessment methods have difficulty
in predicting the quality of images that contain multiple distortion types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xuanchao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zehan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengxu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_K/0/1/0/all/0/1&quot;&gt;Ke Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13959">
<title>Conditional Neural Video Coding with Spatial-Temporal Super-Resolution. (arXiv:2401.13959v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.13959</link>
<description rdf:parseType="Literal">&lt;p&gt;This document is an expanded version of a one-page abstract originally
presented at the 2024 Data Compression Conference. It describes our proposed
method for the video track of the Challenge on Learned Image Compression (CLIC)
2024. Our scheme follows the typical hybrid coding framework with some novel
techniques. Firstly, we adopt Spynet network to produce accurate motion vectors
for motion estimation. Secondly, we introduce the context mining scheme with
conditional frame coding to fully exploit the spatial-temporal information. As
for the low target bitrates given by CLIC, we integrate spatial-temporal
super-resolution modules to improve rate-distortion performance. Our team name
is IMCLVC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Henan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaohan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Runsen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zongyu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhibo Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13961">
<title>TriSAM: Tri-Plane SAM for zero-shot cortical blood vessel segmentation in VEM images. (arXiv:2401.13961v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13961</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address a significant gap in the field of neuroimaging by
introducing the largest-to-date public benchmark, BvEM, designed specifically
for cortical blood vessel segmentation in Volume Electron Microscopy (VEM)
images. The intricate relationship between cerebral blood vessels and neural
function underscores the vital role of vascular analysis in understanding brain
health. While imaging techniques at macro and mesoscales have garnered
substantial attention and resources, the microscale VEM imaging, capable of
revealing intricate vascular details, has lacked the necessary benchmarking
infrastructure. As researchers delve deeper into the microscale intricacies of
cerebral vasculature, our BvEM benchmark represents a critical step toward
unraveling the mysteries of neurovascular coupling and its impact on brain
function and pathology. The BvEM dataset is based on VEM image volumes from
three mammal species: adult mouse, macaque, and human. We standardized the
resolution, addressed imaging variations, and meticulously annotated blood
vessels through semi-automatic, manual, and quality control processes, ensuring
high-quality 3D segmentation. Furthermore, we developed a zero-shot cortical
blood vessel segmentation method named TriSAM, which leverages the powerful
segmentation model SAM for 3D segmentation. To lift SAM from 2D segmentation to
3D volume segmentation, TriSAM employs a multi-seed tracking framework,
leveraging the reliability of certain image planes for tracking while using
others to identify potential turning points. This approach, consisting of
Tri-Plane selection, SAM-based tracking, and recursive redirection, effectively
achieves long-term 3D blood vessel segmentation without model training or
fine-tuning. Experimental results show that TriSAM achieved superior
performances on the BvEM benchmark across three species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jia Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wanhua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Atmadeep Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhinarta_J/0/1/0/all/0/1&quot;&gt;Jason Ken Adhinarta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sjostedt_E/0/1/0/all/0/1&quot;&gt;Evelina Sjostedt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingpeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lichtman_J/0/1/0/all/0/1&quot;&gt;Jeff Lichtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Donglai Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13964">
<title>An Extensible Framework for Open Heterogeneous Collaborative Perception. (arXiv:2401.13964v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13964</link>
<description rdf:parseType="Literal">&lt;p&gt;Collaborative perception aims to mitigate the limitations of single-agent
perception, such as occlusions, by facilitating data exchange among multiple
agents. However, most current works consider a homogeneous scenario where all
agents use identity sensors and perception models. In reality, heterogeneous
agent types may continually emerge and inevitably face a domain gap when
collaborating with existing agents. In this paper, we introduce a new open
heterogeneous problem: how to accommodate continually emerging new
heterogeneous agent types into collaborative perception, while ensuring high
perception performance and low integration cost? To address this problem, we
propose HEterogeneous ALliance (HEAL), a novel extensible collaborative
perception framework. HEAL first establishes a unified feature space with
initial agents via a novel multi-scale foreground-aware Pyramid Fusion network.
When heterogeneous new agents emerge with previously unseen modalities or
models, we align them to the established unified space with an innovative
backward alignment. This step only involves individual training on the new
agent type, thus presenting extremely low training costs and high
extensibility. It also protects new agents&apos; model details from disclosure since
the training can be conducted by the agent owner locally. To enrich agents&apos;
data heterogeneity, we bring OPV2V-H, a new large-scale dataset with more
diverse sensor types. Extensive experiments on OPV2V-H and DAIR-V2X datasets
show that HEAL surpasses SOTA methods in performance while reducing the
training parameters by 91.5% when integrating 3 new agent types. Code and data
are available at: https://github.com/yifanlu0227/HEAL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yifan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiqi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dequan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13965">
<title>Improving Pseudo-labelling and Enhancing Robustness for Semi-Supervised Domain Generalization. (arXiv:2401.13965v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13965</link>
<description rdf:parseType="Literal">&lt;p&gt;Beyond attaining domain generalization (DG), visual recognition models should
also be data-efficient during learning by leveraging limited labels. We study
the problem of Semi-Supervised Domain Generalization (SSDG) which is crucial
for real-world applications like automated healthcare. SSDG requires learning a
cross-domain generalizable model when the given training data is only partially
labelled. Empirical investigations reveal that the DG methods tend to
underperform in SSDG settings, likely because they are unable to exploit the
unlabelled data. Semi-supervised learning (SSL) shows improved but still
inferior results compared to fully-supervised learning. A key challenge, faced
by the best-performing SSL-based SSDG methods, is selecting accurate
pseudo-labels under multiple domain shifts and reducing overfitting to source
domains under limited labels. In this work, we propose new SSDG approach, which
utilizes a novel uncertainty-guided pseudo-labelling with model averaging
(UPLM). Our uncertainty-guided pseudo-labelling (UPL) uses model uncertainty to
improve pseudo-labelling selection, addressing poor model calibration under
multi-source unlabelled data. The UPL technique, enhanced by our novel model
averaging (MA) strategy, mitigates overfitting to source domains with limited
labels. Extensive experiments on key representative DG datasets suggest that
our method demonstrates effectiveness against existing methods. Our code and
chosen labelled data seeds are available on GitHub:
https://github.com/Adnan-Khan7/UPLM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Adnan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1&quot;&gt;Mai A. Shaaban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Haris Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13974">
<title>BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models. (arXiv:2401.13974v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13974</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image generation models have demonstrated incredible success
in generating images that faithfully follow input prompts. However, the
requirement of using words to describe a desired concept provides limited
control over the appearance of the generated concepts. In this work, we address
this shortcoming by proposing an approach to enable personalization
capabilities in existing text-to-image diffusion models. We propose a novel
architecture (BootPIG) that allows a user to provide reference images of an
object in order to guide the appearance of a concept in the generated images.
&lt;/p&gt;
&lt;p&gt;The proposed BootPIG architecture makes minimal modifications to a pretrained
text-to-image diffusion model and utilizes a separate UNet model to steer the
generations toward the desired appearance. We introduce a training procedure
that allows us to bootstrap personalization capabilities in the BootPIG
architecture using data generated from pretrained text-to-image models, LLM
chat agents, and image segmentation models. In contrast to existing methods
that require several days of pretraining, the BootPIG architecture can be
trained in approximately 1 hour. Experiments on the DreamBooth dataset
demonstrate that BootPIG outperforms existing zero-shot methods while being
comparable with test-time finetuning approaches. Through a user study, we
validate the preference for BootPIG generations over existing methods both in
maintaining fidelity to the reference object&apos;s appearance and aligning with
textual prompts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1&quot;&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokul_A/0/1/0/all/0/1&quot;&gt;Akash Gokul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_N/0/1/0/all/0/1&quot;&gt;Nikhil Naik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13976">
<title>Learning to Manipulate Artistic Images. (arXiv:2401.13976v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13976</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancement in computer vision has significantly lowered the barriers
to artistic creation. Exemplar-based image translation methods have attracted
much attention due to flexibility and controllability. However, these methods
hold assumptions regarding semantics or require semantic information as the
input, while accurate semantics is not easy to obtain in artistic images.
Besides, these methods suffer from cross-domain artifacts due to training data
prior and generate imprecise structure due to feature compression in the
spatial domain. In this paper, we propose an arbitrary Style Image Manipulation
Network (SIM-Net), which leverages semantic-free information as guidance and a
region transportation strategy in a self-supervised manner for image
generation. Our method balances computational efficiency and high resolution to
a certain extent. Moreover, our method facilitates zero-shot style image
manipulation. Both qualitative and quantitative experiments demonstrate the
superiority of our method over state-of-the-art methods.Code is available at
https://github.com/SnailForce/SIM-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1&quot;&gt;De Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qian Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13990">
<title>Deep Learning Innovations in Diagnosing Diabetic Retinopathy: The Potential of Transfer Learning and the DiaCNN Model. (arXiv:2401.13990v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.13990</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetic retinopathy (DR) is a significant cause of vision impairment,
emphasizing the critical need for early detection and timely intervention to
avert visual deterioration. Diagnosing DR is inherently complex, as it
necessitates the meticulous examination of intricate retinal images by
experienced specialists. This makes the early diagnosis of DR essential for
effective treatment and the prevention of eventual blindness. Traditional
diagnostic methods, relying on human interpretation of these medical images,
face challenges in terms of accuracy and efficiency. In the present research,
we introduce a novel method that offers superior precision in DR diagnosis,
compared to these traditional methods, by employing advanced deep learning
techniques. Central to this approach is the concept of transfer learning. This
entails using pre-existing, well-established models, specifically
InceptionResNetv2 and Inceptionv3, to extract features and fine-tune select
layers to cater to the unique requirements of this specific diagnostic task.
Concurrently, we also present a newly devised model, DiaCNN, which is tailored
for the classification of eye diseases. To validate the efficacy of the
proposed methodology, we leveraged the Ocular Disease Intelligent Recognition
(ODIR) dataset, which comprises eight different eye disease categories. The
results were promising. The InceptionResNetv2 model, incorporating transfer
learning, registered an impressive 97.5% accuracy in both the training and
testing phases. Its counterpart, the Inceptionv3 model, achieved an even more
commendable 99.7% accuracy during training, and 97.5% during testing.
Remarkably, the DiaCNN model showcased unparalleled precision, achieving 100%
accuracy in training and 98.3\% in testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shoaib_M/0/1/0/all/0/1&quot;&gt;Mohamed R. Shoaib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Emara_H/0/1/0/all/0/1&quot;&gt;Heba M. Emara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+El_Shafai_W/0/1/0/all/0/1&quot;&gt;Walid El-Shafai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soliman_N/0/1/0/all/0/1&quot;&gt;Naglaa F. Soliman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mubarak_A/0/1/0/all/0/1&quot;&gt;Ahmed S. Mubarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Omer_O/0/1/0/all/0/1&quot;&gt;Osama A. Omer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+El_Samie_F/0/1/0/all/0/1&quot;&gt;Fathi E. Abd El-Samie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Esmaiel_H/0/1/0/all/0/1&quot;&gt;Hamada Esmaiel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13992">
<title>Diffusion-based Data Augmentation for Object Counting Problems. (arXiv:2401.13992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.13992</link>
<description rdf:parseType="Literal">&lt;p&gt;Crowd counting is an important problem in computer vision due to its wide
range of applications in image understanding. Currently, this problem is
typically addressed using deep learning approaches, such as Convolutional
Neural Networks (CNNs) and Transformers. However, deep networks are data-driven
and are prone to overfitting, especially when the available labeled crowd
dataset is limited. To overcome this limitation, we have designed a pipeline
that utilizes a diffusion model to generate extensive training data. We are the
first to generate images conditioned on a location dot map (a binary dot map
that specifies the location of human heads) with a diffusion model. We are also
the first to use these diverse synthetic data to augment the crowd counting
models. Our proposed smoothed density map input for ControlNet significantly
improves ControlNet&apos;s performance in generating crowds in the correct
locations. Also, Our proposed counting loss for the diffusion model effectively
minimizes the discrepancies between the location dot map and the crowd images
generated. Additionally, our innovative guidance sampling further directs the
diffusion process toward regions where the generated crowd images align most
accurately with the location dot map. Collectively, we have enhanced
ControlNet&apos;s ability to generate specified objects from a location dot map,
which can be used for data augmentation in various counting problems. Moreover,
our framework is versatile and can be easily adapted to all kinds of counting
problems. Extensive experiments demonstrate that our framework improves the
counting performance on the ShanghaiTech, NWPU-Crowd, UCF-QNRF, and TRANCOS
datasets, showcasing its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuelei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jia Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasconcelos_N/0/1/0/all/0/1&quot;&gt;Nuno Vasconcelos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13998">
<title>WAL-Net: Weakly supervised auxiliary task learning network for carotid plaques classification. (arXiv:2401.13998v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.13998</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of carotid artery ultrasound images is a crucial means for
diagnosing carotid plaques, holding significant clinical relevance for
predicting the risk of stroke. Recent research suggests that utilizing plaque
segmentation as an auxiliary task for classification can enhance performance by
leveraging the correlation between segmentation and classification tasks.
However, this approach relies on obtaining a substantial amount of
challenging-to-acquire segmentation annotations. This paper proposes a novel
weakly supervised auxiliary task learning network model (WAL-Net) to explore
the interdependence between carotid plaque classification and segmentation
tasks. The plaque classification task is primary task, while the plaque
segmentation task serves as an auxiliary task, providing valuable information
to enhance the performance of the primary task. Weakly supervised learning is
adopted in the auxiliary task to completely break away from the dependence on
segmentation annotations. Experiments and evaluations are conducted on a
dataset comprising 1270 carotid plaque ultrasound images from Wuhan University
Zhongnan Hospital. Results indicate that the proposed method achieved an
approximately 1.3% improvement in carotid plaque classification accuracy
compared to the baseline network. Specifically, the accuracy of mixed-echoic
plaques classification increased by approximately 3.3%, demonstrating the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gan_H/0/1/0/all/0/1&quot;&gt;Haitao Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Lingchao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Ran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1&quot;&gt;Weiyan Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Furong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongwei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14007">
<title>Semantic Ensemble Loss and Latent Refinement for High-Fidelity Neural Image Compression. (arXiv:2401.14007v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14007</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in neural compression have surpassed traditional codecs
in PSNR and MS-SSIM measurements. However, at low bit-rates, these methods can
introduce visually displeasing artifacts, such as blurring, color shifting, and
texture loss, thereby compromising perceptual quality of images. To address
these issues, this study presents an enhanced neural compression method
designed for optimal visual fidelity. We have trained our model with a
sophisticated semantic ensemble loss, integrating Charbonnier loss, perceptual
loss, style loss, and a non-binary adversarial loss, to enhance the perceptual
quality of image reconstructions. Additionally, we have implemented a latent
refinement process to generate content-aware latent codes. These codes adhere
to bit-rate constraints, balance the trade-off between distortion and fidelity,
and prioritize bit allocation to regions of greater importance. Our empirical
findings demonstrate that this approach significantly improves the statistical
fidelity of neural image compression. On CLIC2024 validation set, our approach
achieves a 62% bitrate saving compared to MS-ILLM under FID metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuanchao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14024">
<title>PLCNet: Patch-wise Lane Correction Network for Automatic Lane Correction in High-definition Maps. (arXiv:2401.14024v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14024</link>
<description rdf:parseType="Literal">&lt;p&gt;In High-definition (HD) maps, lane elements constitute the majority of
components and demand stringent localization requirements to ensure safe
vehicle navigation. Vision lane detection with LiDAR position assignment is a
prevalent method to acquire initial lanes for HD maps. However, due to
incorrect vision detection and coarse camera-LiDAR calibration, initial lanes
may deviate from their true positions within an uncertain range. To mitigate
the need for manual lane correction, we propose a patch-wise lane correction
network (PLCNet) to automatically correct the positions of initial lane points
in local LiDAR images that are transformed from point clouds. PLCNet first
extracts multi-scale image features and crops patch (ROI) features centered at
each initial lane point. By applying ROIAlign, the fix-sized ROI features are
flattened into 1D features. Then, a 1D lane attention module is devised to
compute instance-level lane features with adaptive weights. Finally, lane
correction offsets are inferred by a multi-layer perceptron and used to correct
the initial lane positions. Considering practical applications, our automatic
method supports merging local corrected lanes into global corrected lanes.
Through extensive experiments on a self-built dataset, we demonstrate that
PLCNet achieves fast and effective initial lane correction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Haiyang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yi Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benkang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongtao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14031">
<title>Sparse and Transferable Universal Singular Vectors Attack. (arXiv:2401.14031v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.14031</link>
<description rdf:parseType="Literal">&lt;p&gt;The research in the field of adversarial attacks and models&apos; vulnerability is
one of the fundamental directions in modern machine learning. Recent studies
reveal the vulnerability phenomenon, and understanding the mechanisms behind
this is essential for improving neural network characteristics and
interpretability. In this paper, we propose a novel sparse universal white-box
adversarial attack. Our approach is based on truncated power iteration
providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian
matrices. Using the ImageNet benchmark validation subset, we analyze the
proposed method in various settings, achieving results comparable to dense
baselines with more than a 50% fooling rate while damaging only 5% of pixels
and utilizing 256 samples for perturbation fitting. We also show that our
algorithm admits higher attack magnitude without affecting the human ability to
solve the task. Furthermore, we investigate that the constructed perturbations
are highly transferable among different models without significantly decreasing
the fooling rate. Our findings demonstrate the vulnerability of
state-of-the-art models to sparse attacks and highlight the importance of
developing robust machine learning systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuvshinova_K/0/1/0/all/0/1&quot;&gt;Kseniia Kuvshinova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsymboi_O/0/1/0/all/0/1&quot;&gt;Olga Tsymboi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1&quot;&gt;Ivan Oseledets&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14032">
<title>GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D Reconstruction Dataset Using Gaussian Splatting. (arXiv:2401.14032v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14032</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1&quot;&gt;Butian Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14034">
<title>Unsupervised Spatial-Temporal Feature Enrichment and Fidelity Preservation Network for Skeleton based Action Recognition. (arXiv:2401.14034v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14034</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised skeleton based action recognition has achieved remarkable
progress recently. Existing unsupervised learning methods suffer from severe
overfitting problem, and thus small networks are used, significantly reducing
the representation capability. To address this problem, the overfitting
mechanism behind the unsupervised learning for skeleton based action
recognition is first investigated. It is observed that the skeleton is already
a relatively high-level and low-dimension feature, but not in the same manifold
as the features for action recognition. Simply applying the existing
unsupervised learning method may tend to produce features that discriminate the
different samples instead of action classes, resulting in the overfitting
problem. To solve this problem, this paper presents an Unsupervised
spatial-temporal Feature Enrichment and Fidelity Preservation framework
(U-FEFP) to generate rich distributed features that contain all the information
of the skeleton sequence. A spatial-temporal feature transformation subnetwork
is developed using spatial-temporal graph convolutional network and graph
convolutional gate recurrent unit network as the basic feature extraction
network. The unsupervised Bootstrap Your Own Latent based learning is used to
generate rich distributed features and the unsupervised pretext task based
learning is used to preserve the information of the skeleton sequence. The two
unsupervised learning ways are collaborated as U-FEFP to produce robust and
discriminative representations. Experimental results on three widely used
benchmarks, namely NTU-RGB+D-60, NTU-RGB+D-120 and PKU-MMD dataset, demonstrate
that the proposed U-FEFP achieves the best performance compared with the
state-of-the-art unsupervised learning methods. t-SNE illustrations further
validate that U-FEFP can learn more discriminative features for unsupervised
skeleton based action recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chuankun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yanbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Ping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wanqing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14036">
<title>Diverse and Lifespan Facial Age Transformation Synthesis with Identity Variation Rationality Metric. (arXiv:2401.14036v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14036</link>
<description rdf:parseType="Literal">&lt;p&gt;Face aging has received continuous research attention over the past two
decades. Although previous works on this topic have achieved impressive
success, two longstanding problems remain unsettled: 1) generating diverse and
plausible facial aging patterns at the target age stage; 2) measuring the
rationality of identity variation between the original portrait and its
syntheses with age progression or regression. In this paper, we introduce DLAT
+ , the first algorithm that can realize Diverse and Lifespan Age
Transformation on human faces, where the diversity jointly manifests in the
transformation of facial textures and shapes. Apart from the diversity
mechanism embedded in the model, multiple consistency restrictions are
leveraged to keep it away from counterfactual aging syntheses. Moreover, we
propose a new metric to assess the rationality of Identity Deviation under Age
Gaps (IDAG) between the input face and its series of age-transformed
generations, which is based on statistical laws summarized from plenty of
genuine face-aging data. Extensive experimental results demonstrate the
uniqueness and effectiveness of our method in synthesizing diverse and
perceptually reasonable faces across the whole lifetime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiu-Cheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1&quot;&gt;Feng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14038">
<title>Deep Clustering with Diffused Sampling and Hardness-aware Self-distillation. (arXiv:2401.14038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14038</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep clustering has gained significant attention due to its capability in
learning clustering-friendly representations without labeled data. However,
previous deep clustering methods tend to treat all samples equally, which
neglect the variance in the latent distribution and the varying difficulty in
classifying or clustering different samples. To address this, this paper
proposes a novel end-to-end deep clustering method with diffused sampling and
hardness-aware self-distillation (HaDis). Specifically, we first align one view
of instances with another view via diffused sampling alignment (DSA), which
helps improve the intra-cluster compactness. To alleviate the sampling bias, we
present the hardness-aware self-distillation (HSD) mechanism to mine the
hardest positive and negative samples and adaptively adjust their weights in a
self-distillation fashion, which is able to deal with the potential imbalance
in sample contributions during optimization. Further, the prototypical
contrastive learning is incorporated to simultaneously enhance the
inter-cluster separability and intra-cluster compactness. Experimental results
on five challenging image datasets demonstrate the superior clustering
performance of our HaDis method over the state-of-the-art. Source code is
available at https://github.com/Regan-Zhang/HaDis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hai-Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14051">
<title>A real-time rendering method for high albedo anisotropic materials with multiple scattering. (arXiv:2401.14051v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2401.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a neural network-based real-time volume rendering method for
realistic and efficient rendering of volumetric media. The traditional volume
rendering method uses path tracing to solve the radiation transfer equation,
which requires a huge amount of calculation and cannot achieve real-time
rendering. Therefore, this paper uses neural networks to simulate the iterative
integration process of solving the radiative transfer equation to speed up the
volume rendering of volume media. Specifically, the paper first performs data
processing on the volume medium to generate a variety of sampling features,
including density features, transmittance features and phase features. The
hierarchical transmittance fields are fed into a 3D-CNN network to compute more
important transmittance features. Secondly, the diffuse reflection sampling
template and the highlight sampling template are used to layer the three types
of sampling features into the network. This method can pay more attention to
light scattering, highlights and shadows, and then select important channel
features through the attention module. Finally, the scattering distribution of
the center points of all sampling templates is predicted through the backbone
neural network. This method can achieve realistic volumetric media rendering
effects and greatly increase the rendering speed while maintaining rendering
quality, which is of great significance for real-time rendering applications.
Experimental results indicate that our method outperforms previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xing Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Ming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14066">
<title>CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14066</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale text-to-image generative models have made impressive strides,
showcasing their ability to synthesize a vast array of high-quality images.
However, adapting these models for artistic image editing presents two
significant challenges. Firstly, users struggle to craft textual prompts that
meticulously detail visual elements of the input image. Secondly, prevalent
models, when effecting modifications in specific zones, frequently disrupt the
overall artistic style, complicating the attainment of cohesive and
aesthetically unified artworks. To surmount these obstacles, we build the
innovative unified framework CreativeSynth, which is based on a diffusion model
with the ability to coordinate multimodal inputs and multitask in the field of
artistic image generation. By integrating multimodal features with customized
attention mechanisms, CreativeSynth facilitates the importation of real-world
semantic content into the domain of art through inversion and real-time style
transfer. This allows for the precise manipulation of image style and content
while maintaining the integrity of the original model parameters. Rigorous
qualitative and quantitative evaluations underscore that CreativeSynth excels
in enhancing artistic images&apos; fidelity and preserves their innate aesthetic
essence. By bridging the gap between generative models and artistic finesse,
CreativeSynth becomes a custom digital palette.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nisha Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weiming Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ronghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chongyang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Changsheng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14074">
<title>ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14074</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate
the conflict between annotation cost and model performance by adopting sparse
annotation formats (e.g., point, scribble, block, etc.). Typical approaches
attempt to exploit anatomy and topology priors to directly expand sparse
annotations into pseudo-labels. However, due to a lack of attention to the
ambiguous edges in medical images and insufficient exploration of sparse
supervision, existing approaches tend to generate erroneous and overconfident
pseudo proposals in noisy regions, leading to cumulative model error and
performance degradation. In this work, we propose a novel WSS approach, named
ProCNS, encompassing two synergistic modules devised with the principles of
progressive prototype calibration and noise suppression. Specifically, we
design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the
pair-wise affinities between spatial and semantic elements, providing our model
of interest with more reliable guidance. The affinities are derived from the
input images and the prototype-refined predictions. Meanwhile, we propose an
Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and
representative prototype representations, which adaptively identifies and masks
noisy regions within the pseudo proposals, reducing potential erroneous
interference during prototype computation. Furthermore, we generate specialized
soft pseudo-labels for the noisy regions identified by ANPM, providing
supplementary supervision. Extensive experiments on three medical image
segmentation tasks involving different modalities demonstrate that the proposed
framework significantly outperforms representative state-of-the-art methods
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Y. Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;L. Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;K. K. Y. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;X. Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14088">
<title>Double Trouble? Impact and Detection of Duplicates in Face Image Datasets. (arXiv:2401.14088v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14088</link>
<description rdf:parseType="Literal">&lt;p&gt;Various face image datasets intended for facial biometrics research were
created via web-scraping, i.e. the collection of images publicly available on
the internet. This work presents an approach to detect both exactly and nearly
identical face image duplicates, using file and image hashes. The approach is
extended through the use of face image preprocessing. Additional steps based on
face recognition and face image quality assessment models reduce false
positives, and facilitate the deduplication of the face images both for intra-
and inter-subject duplicate sets. The presented approach is applied to five
datasets, namely LFW, TinyFace, Adience, CASIA-WebFace, and C-MS-Celeb (a
cleaned MS-Celeb-1M variant). Duplicates are detected within every dataset,
with hundreds to hundreds of thousands of duplicates for all except LFW. Face
recognition and quality assessment experiments indicate a minor impact on the
results through the duplicate removal. The final deduplication data is publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1&quot;&gt;Torsten Schlett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1&quot;&gt;Christian Rathgeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1&quot;&gt;Juan Tapia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1&quot;&gt;Christoph Busch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14111">
<title>Scene Graph to Image Synthesis: Integrating CLIP Guidance with Graph Conditioning in Diffusion Models. (arXiv:2401.14111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14111</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in generative models have sparked significant interest in
generating images while adhering to specific structural guidelines. Scene graph
to image generation is one such task of generating images which are consistent
with the given scene graph. However, the complexity of visual scenes poses a
challenge in accurately aligning objects based on specified relations within
the scene graph. Existing methods approach this task by first predicting a
scene layout and generating images from these layouts using adversarial
training. In this work, we introduce a novel approach to generate images from
scene graphs which eliminates the need of predicting intermediate layouts. We
leverage pre-trained text-to-image diffusion models and CLIP guidance to
translate graph knowledge into images. Towards this, we first pre-train our
graph encoder to align graph features with CLIP features of corresponding
images using a GAN based training. Further, we fuse the graph features with
CLIP embedding of object labels present in the given scene graph to create a
graph consistent CLIP guided conditioning signal. In the conditioning input,
object embeddings provide coarse structure of the image and graph features
provide structural alignment based on relationships among objects. Finally, we
fine tune a pre-trained diffusion model with the graph consistent conditioning
signal with reconstruction and CLIP alignment loss. Elaborate experiments
reveal that our method outperforms existing methods on standard benchmarks of
COCO-stuff and Visual Genome dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1&quot;&gt;Rameshwar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanyam_A/0/1/0/all/0/1&quot;&gt;A V Subramanyam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14115">
<title>MIFI: MultI-camera Feature Integration for Roust 3D Distracted Driver Activity Recognition. (arXiv:2401.14115v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14115</link>
<description rdf:parseType="Literal">&lt;p&gt;Distracted driver activity recognition plays a critical role in risk
aversion-particularly beneficial in intelligent transportation systems.
However, most existing methods make use of only the video from a single view
and the difficulty-inconsistent issue is neglected. Different from them, in
this work, we propose a novel MultI-camera Feature Integration (MIFI) approach
for 3D distracted driver activity recognition by jointly modeling the data from
different camera views and explicitly re-weighting examples based on their
degree of difficulty. Our contributions are two-fold: (1) We propose a simple
but effective multi-camera feature integration framework and provide three
types of feature fusion techniques. (2) To address the difficulty-inconsistent
problem in distracted driver activity recognition, a periodic learning method,
named example re-weighting that can jointly learn the easy and hard samples, is
presented. The experimental results on the 3MDAD dataset demonstrate that the
proposed MIFI can consistently boost performance compared to single-view
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_J/0/1/0/all/0/1&quot;&gt;Jian Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhongcheng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14121">
<title>Incorporating Exemplar Optimization into Training with Dual Networks for Human Mesh Recovery. (arXiv:2401.14121v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14121</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel optimization-based human mesh recovery method from a
single image. Given a test exemplar, previous approaches optimize the
pre-trained regression network to minimize the 2D re-projection loss, which
however suffer from over-/under-fitting problems. This is because the
``exemplar optimization&apos;&apos; at testing time has too weak relation to the
pre-training process, and the exemplar optimization loss function is different
from the training loss function. (1) We incorporate exemplar optimization into
the training stage. During training, our method first executes exemplar
optimization and subsequently proceeds with training-time optimization. The
exemplar optimization may run into a wrong direction, while the subsequent
training optimization serves to correct the deviation. Involved in training,
the exemplar optimization learns to adapt its behavior to training data,
thereby acquires generalibility to test exemplars. (2) We devise a dual-network
architecture to convey the novel training paradigm, which is composed of a main
regression network and an auxiliary network, in which we can formulate the
exemplar optimization loss function in the same form as the training loss
function. This further enhances the compatibility between the exemplar and
training optimizations. Experiments demonstrate that our exemplar optimization
after the novel training scheme significantly outperforms state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yongwei Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingxian Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1&quot;&gt;Chengjiang Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuemiao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14130">
<title>Attention-based Efficient Classification for 3D MRI Image of Alzheimer&apos;s Disease. (arXiv:2401.14130v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14130</link>
<description rdf:parseType="Literal">&lt;p&gt;Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to
its subtle and complex clinical symptoms. Deep learning-assisted medical
diagnosis using image recognition techniques has become an important research
topic in this field. The features have to accurately capture main variations of
anatomical brain structures. However, time-consuming is expensive for feature
extraction by deep learning training. This study proposes a novel Alzheimer&apos;s
disease detection model based on Convolutional Neural Networks. The model
utilizes a pre-trained ResNet network as the backbone, incorporating
post-fusion algorithm for 3D medical images and attention mechanisms. The
experimental results indicate that the employed 2D fusion algorithm effectively
improves the model&apos;s training expense. And the introduced attention mechanism
accurately weights important regions in images, further enhancing the model&apos;s
diagnostic accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yihao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Ximeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinshan Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14132">
<title>Enabling Cross-Camera Collaboration for Video Analytics on Distributed Smart Cameras. (arXiv:2401.14132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14132</link>
<description rdf:parseType="Literal">&lt;p&gt;Overlapping cameras offer exciting opportunities to view a scene from
different angles, allowing for more advanced, comprehensive and robust
analysis. However, existing visual analytics systems for multi-camera streams
are mostly limited to (i) per-camera processing and aggregation and (ii)
workload-agnostic centralized processing architectures. In this paper, we
present Argus, a distributed video analytics system with cross-camera
collaboration on smart cameras. We identify multi-camera, multi-target tracking
as the primary task of multi-camera video analytics and develop a novel
technique that avoids redundant, processing-heavy identification tasks by
leveraging object-wise spatio-temporal association in the overlapping fields of
view across multiple cameras. We further develop a set of techniques to perform
these operations across distributed cameras without cloud support at low
latency by (i) dynamically ordering the camera and object inspection sequence
and (ii) flexibly distributing the workload across smart cameras, taking into
account network transmission and heterogeneous computational capacities.
Evaluation of three real-world overlapping camera datasets with two Nvidia
Jetson devices shows that Argus reduces the number of object identifications
and end-to-end latency by up to 7.13x and 2.19x (4.86x and 1.60x compared to
the state-of-the-art), while achieving comparable tracking quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1&quot;&gt;Chulhong Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1&quot;&gt;Juheon Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acer_U/0/1/0/all/0/1&quot;&gt;Utku Gunay Acer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1&quot;&gt;Fahim Kawsar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14136">
<title>Expression-aware video inpainting for HMD removal in XR applications. (arXiv:2401.14136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14136</link>
<description rdf:parseType="Literal">&lt;p&gt;Head-mounted displays (HMDs) serve as indispensable devices for observing
extended reality (XR) environments and virtual content. However, HMDs present
an obstacle to external recording techniques as they block the upper face of
the user. This limitation significantly affects social XR applications,
specifically teleconferencing, where facial features and eye gaze information
play a vital role in creating an immersive user experience. In this study, we
propose a new network for expression-aware video inpainting for HMD removal
(EVI-HRnet) based on generative adversarial networks (GANs). Our model
effectively fills in missing information with regard to facial landmarks and a
single occlusion-free reference image of the user. The framework and its
components ensure the preservation of the user&apos;s identity across frames using
the reference frame. To further improve the level of realism of the inpainted
output, we introduce a novel facial expression recognition (FER) loss function
for emotion preservation. Our results demonstrate the remarkable capability of
the proposed framework to remove HMDs from facial videos while maintaining the
subject&apos;s facial expression and identity. Moreover, the outputs exhibit
temporal consistency along the inpainted frames. This lightweight framework
presents a practical approach for HMD occlusion removal, with the potential to
enhance various collaborative XR applications without the need for additional
hardware.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lohesara_F/0/1/0/all/0/1&quot;&gt;Fatemeh Ghorbani Lohesara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egiazarian_K/0/1/0/all/0/1&quot;&gt;Karen Egiazarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knorr_S/0/1/0/all/0/1&quot;&gt;Sebastian Knorr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14142">
<title>Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14142</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods, such as concept bottleneck models (CBMs), have been
successful in providing concept-based interpretations for black-box deep
learning models. They typically work by predicting concepts given the input and
then predicting the final class label given the predicted concepts. However,
(1) they often fail to capture the high-order, nonlinear interaction between
concepts, e.g., correcting a predicted concept (e.g., &quot;yellow breast&quot;) does not
help correct highly correlated concepts (e.g., &quot;yellow belly&quot;), leading to
suboptimal final accuracy; (2) they cannot naturally quantify the complex
conditional dependencies between different concepts and class labels (e.g., for
an image with the class label &quot;Kentucky Warbler&quot; and a concept &quot;black bill&quot;,
what is the probability that the model correctly predicts another concept
&quot;black crown&quot;), therefore failing to provide deeper insight into how a
black-box model works. In response to these limitations, we propose
Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural
networks to define the joint energy of candidate (input, concept, class)
tuples. With such a unified interface, prediction, concept correction, and
conditional dependency quantification are then represented as conditional
probabilities, which are generated by composing different energy functions. Our
ECBMs address both limitations of existing CBMs, providing higher accuracy and
richer concept interpretations. Empirical results show that our approach
outperforms the state-of-the-art on real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinyue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yi Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1&quot;&gt;Lu Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14148">
<title>LanDA: Language-Guided Multi-Source Domain Adaptation. (arXiv:2401.14148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14148</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Source Domain Adaptation (MSDA) aims to mitigate changes in data
distribution when transferring knowledge from multiple labeled source domains
to an unlabeled target domain. However, existing MSDA techniques assume target
domain images are available, yet overlook image-rich semantic information.
Consequently, an open question is whether MSDA can be guided solely by textual
cues in the absence of target domain images. By employing a multimodal model
with a joint image and language embedding space, we propose a novel
language-guided MSDA approach, termed LanDA, based on optimal transfer theory,
which facilitates the transfer of multiple source domains to a new target
domain, requiring only a textual description of the target domain without
needing even a single target domain image, while retaining task-relevant
information. We present extensive experiments across different transfer
scenarios using a suite of relevant benchmarks, demonstrating that LanDA
outperforms standard fine-tuning and ensemble approaches in both target and
source domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lituan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minjuan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14159">
<title>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks. (arXiv:2401.14159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14159</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Grounded SAM, which uses Grounding DINO as an open-set object
detector to combine with the segment anything model (SAM). This integration
enables the detection and segmentation of any regions based on arbitrary text
inputs and opens a door to connecting various vision models. As shown in Fig.1,
a wide range of vision tasks can be achieved by using the versatile Grounded
SAM pipeline. For example, an automatic annotation pipeline based solely on
input images can be realized by incorporating models such as BLIP and Recognize
Anything. Additionally, incorporating Stable-Diffusion allows for controllable
image editing, while the integration of OSX facilitates promptable 3D human
motion analysis. Grounded SAM also shows superior performance on
open-vocabulary benchmarks, achieving 48.7 mean AP on SegInW (Segmentation in
the wild) zero-shot benchmark with the combination of Grounding DINO-Base and
SAM-Huge models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1&quot;&gt;Tianhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shilong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Ailing Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jing Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunchang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;He Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1&quot;&gt;Feng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14168">
<title>Vivim: a Video Vision Mamba for Medical Video Object Segmentation. (arXiv:2401.14168v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14168</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional convolutional neural networks have a limited receptive field
while transformer-based networks are mediocre in constructing long-term
dependency from the perspective of computational complexity. Such the
bottleneck poses a significant challenge when processing long video sequences
in video analysis tasks. Very recently, the state space models (SSMs) with
efficient hardware-aware designs, famous by Mamba, have exhibited impressive
achievements in long sequence modeling, which facilitates the development of
deep neural networks on many vision tasks. To better capture available cues in
video frames, this paper presents a generic Video Vision Mamba-based framework
for medical video object segmentation tasks, named Vivim. Our Vivim can
effectively compress the long-term spatiotemporal representation into sequences
at varying scales by our designed Temporal Mamba Block. Compared to existing
video-level Transformer-based methods, our model maintains excellent
segmentation results with better speed performance. Extensive experiments on
the breast US dataset demonstrate the effectiveness and efficiency of our
Vivim. The code for Vivim is available at:
https://github.com/scott-yjyang/Vivim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhaohu Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14193">
<title>Clinical Melanoma Diagnosis with Artificial Intelligence: Insights from a Prospective Multicenter Study. (arXiv:2401.14193v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14193</link>
<description rdf:parseType="Literal">&lt;p&gt;Early detection of melanoma, a potentially lethal type of skin cancer with
high prevalence worldwide, improves patient prognosis. In retrospective
studies, artificial intelligence (AI) has proven to be helpful for enhancing
melanoma detection. However, there are few prospective studies confirming these
promising results. Existing studies are limited by low sample sizes, too
homogenous datasets, or lack of inclusion of rare melanoma subtypes, preventing
a fair and thorough evaluation of AI and its generalizability, a crucial aspect
for its application in the clinical setting. Therefore, we assessed &apos;All Data
are Ext&apos; (ADAE), an established open-source ensemble algorithm for detecting
melanomas, by comparing its diagnostic accuracy to that of dermatologists on a
prospectively collected, external, heterogeneous test set comprising eight
distinct hospitals, four different camera setups, rare melanoma subtypes, and
special anatomical sites. We advanced the algorithm with real test-time
augmentation (R-TTA, i.e. providing real photographs of lesions taken from
multiple angles and averaging the predictions), and evaluated its
generalization capabilities. Overall, the AI showed higher balanced accuracy
than dermatologists (0.798, 95% confidence interval (CI) 0.779-0.814 vs. 0.781,
95% CI 0.760-0.802; p&amp;lt;0.001), obtaining a higher sensitivity (0.921, 95% CI
0.900- 0.942 vs. 0.734, 95% CI 0.701-0.770; p&amp;lt;0.001) at the cost of a lower
specificity (0.673, 95% CI 0.641-0.702 vs. 0.828, 95% CI 0.804-0.852; p&amp;lt;0.001).
As the algorithm exhibited a significant performance advantage on our
heterogeneous dataset exclusively comprising melanoma-suspicious lesions, AI
may offer the potential to support dermatologists particularly in diagnosing
challenging cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heinlein_L/0/1/0/all/0/1&quot;&gt;Lukas Heinlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maron_R/0/1/0/all/0/1&quot;&gt;Roman C. Maron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hekler_A/0/1/0/all/0/1&quot;&gt;Achim Hekler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haggenmuller_S/0/1/0/all/0/1&quot;&gt;Sarah Haggenm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wies_C/0/1/0/all/0/1&quot;&gt;Christoph Wies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utikal_J/0/1/0/all/0/1&quot;&gt;Jochen S. Utikal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_F/0/1/0/all/0/1&quot;&gt;Friedegund Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hobelsberger_S/0/1/0/all/0/1&quot;&gt;Sarah Hobelsberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gellrich_F/0/1/0/all/0/1&quot;&gt;Frank F. Gellrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sergon_M/0/1/0/all/0/1&quot;&gt;Mildred Sergon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hauschild_A/0/1/0/all/0/1&quot;&gt;Axel Hauschild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+French_L/0/1/0/all/0/1&quot;&gt;Lars E. French&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heinzerling_L/0/1/0/all/0/1&quot;&gt;Lucie Heinzerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlager_J/0/1/0/all/0/1&quot;&gt;Justin G. Schlager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghoreschi_K/0/1/0/all/0/1&quot;&gt;Kamran Ghoreschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlaak_M/0/1/0/all/0/1&quot;&gt;Max Schlaak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hilke_F/0/1/0/all/0/1&quot;&gt;Franz J. Hilke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poch_G/0/1/0/all/0/1&quot;&gt;Gabriela Poch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Korsing_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Korsing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berking_C/0/1/0/all/0/1&quot;&gt;Carola Berking&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Heppt_M/0/1/0/all/0/1&quot;&gt;Markus V. Heppt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erdmann_M/0/1/0/all/0/1&quot;&gt;Michael Erdmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haferkamp_S/0/1/0/all/0/1&quot;&gt;Sebastian Haferkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Drexler_K/0/1/0/all/0/1&quot;&gt;Konstantin Drexler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schadendorf_D/0/1/0/all/0/1&quot;&gt;Dirk Schadendorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sondermann_W/0/1/0/all/0/1&quot;&gt;Wiebke Sondermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goebeler_M/0/1/0/all/0/1&quot;&gt;Matthias Goebeler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schilling_B/0/1/0/all/0/1&quot;&gt;Bastian Schilling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krieghoff_Henning_E/0/1/0/all/0/1&quot;&gt;Eva Krieghoff-Henning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brinker_T/0/1/0/all/0/1&quot;&gt;Titus J. Brinker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14232">
<title>AR-GAN: Generative Adversarial Network-Based Defense Method Against Adversarial Attacks on the Traffic Sign Classification System of Autonomous Vehicles. (arXiv:2401.14232v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14232</link>
<description rdf:parseType="Literal">&lt;p&gt;This study developed a generative adversarial network (GAN)-based defense
method for traffic sign classification in an autonomous vehicle (AV), referred
to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i)
assuming zero knowledge of adversarial attack models and samples and (ii)
providing consistently high traffic sign classification performance under
various adversarial attack types. The AR-GAN classification system consists of
a generator that denoises an image by reconstruction, and a classifier that
classifies the reconstructed image. The authors have tested the AR-GAN under
no-attack and under various adversarial attacks, such as Fast Gradient Sign
Method (FGSM), DeepFool, Carlini and Wagner (C&amp;amp;W), and Projected Gradient
Descent (PGD). The authors considered two forms of these attacks, i.e., (i)
black-box attacks (assuming the attackers possess no prior knowledge of the
classifier), and (ii) white-box attacks (assuming the attackers possess full
knowledge of the classifier). The classification performance of the AR-GAN was
compared with several benchmark adversarial defense methods. The results showed
that both the AR-GAN and the benchmark defense methods are resilient against
black-box attacks and could achieve similar classification performance to that
of the unperturbed images. However, for all the white-box attacks considered in
this study, the AR-GAN method outperformed the benchmark defense methods. In
addition, the AR-GAN was able to maintain its high classification performance
under varied white-box adversarial perturbation magnitudes, whereas the
performance of the other defense methods dropped abruptly at increased
perturbation magnitudes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salek_M/0/1/0/all/0/1&quot;&gt;M Sabbir Salek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamun_A/0/1/0/all/0/1&quot;&gt;Abdullah Al Mamun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mashrur Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14236">
<title>Exploring the Unexplored: Understanding the Impact of Layer Adjustments on Image Classification. (arXiv:2401.14236v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14236</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates how adjustments to deep learning architectures impact
model performance in image classification. Small-scale experiments generate
initial insights although the trends observed are not consistent with the
entire dataset. Filtering operations in the image processing pipeline are
crucial, with image filtering before pre-processing yielding better results.
The choice and order of layers as well as filter placement significantly impact
model performance. This study provides valuable insights into optimizing deep
learning models, with potential avenues for future research including
collaborative platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haixia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brailsford_T/0/1/0/all/0/1&quot;&gt;Tim Brailsford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goulding_J/0/1/0/all/0/1&quot;&gt;James Goulding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_G/0/1/0/all/0/1&quot;&gt;Gavin Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bull_L/0/1/0/all/0/1&quot;&gt;Larry Bull&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14248">
<title>On generalisability of segment anything model for nuclear instance segmentation in histology images. (arXiv:2401.14248v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.14248</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained on a large and diverse dataset, the segment anything model (SAM)
is the first promptable foundation model in computer vision aiming at object
segmentation tasks. In this work, we evaluate SAM for the task of nuclear
instance segmentation performance with zero-shot learning and finetuning. We
compare SAM with other representative methods in nuclear instance segmentation,
especially in the context of model generalisability. To achieve automatic
nuclear instance segmentation, we propose using a nuclei detection model to
provide bounding boxes or central points of nu-clei as visual prompts for SAM
in generating nuclear instance masks from histology images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kesi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goetz_L/0/1/0/all/0/1&quot;&gt;Lea Goetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1&quot;&gt;Nasir Rajpoot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14250">
<title>JUMP: A joint multimodal registration pipeline for neuroimaging with minimal preprocessing. (arXiv:2401.14250v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14250</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a pipeline for unbiased and robust multimodal registration of
neuroimaging modalities with minimal pre-processing. While typical multimodal
studies need to use multiple independent processing pipelines, with diverse
options and hyperparameters, we propose a single and structured framework to
jointly process different image modalities. The use of state-of-the-art
learning-based techniques enables fast inferences, which makes the presented
method suitable for large-scale and/or multi-cohort datasets with a diverse
number of modalities per session. The pipeline currently works with structural
MRI, resting state fMRI and amyloid PET images. We show the predictive power of
the derived biomarkers using in a case-control study and study the cross-modal
relationship between different image modalities. The code can be found in
https: //github.com/acasamitjana/JUMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casamitjana_A/0/1/0/all/0/1&quot;&gt;Adria Casamitjana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tudela_R/0/1/0/all/0/1&quot;&gt;Raul Tudela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ninerola_Baizan_A/0/1/0/all/0/1&quot;&gt;Aida Ninerola-Baizan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_Llonch_R/0/1/0/all/0/1&quot;&gt;Roser Sala-Llonch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14256">
<title>Producing Plankton Classifiers that are Robust to Dataset Shift. (arXiv:2401.14256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14256</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern plankton high-throughput monitoring relies on deep learning
classifiers for species recognition in water ecosystems. Despite satisfactory
nominal performances, a significant challenge arises from Dataset Shift, which
causes performances to drop during deployment. In our study, we integrate the
ZooLake dataset with manually-annotated images from 10 independent days of
deployment, serving as test cells to benchmark Out-Of-Dataset (OOD)
performances. Our analysis reveals instances where classifiers, initially
performing well in In-Dataset conditions, encounter notable failures in
practical scenarios. For example, a MobileNet with a 92% nominal test accuracy
shows a 77% OOD accuracy. We systematically investigate conditions leading to
OOD performance drops and propose a preemptive assessment method to identify
potential pitfalls when classifying new data, and pinpoint features in OOD
images that adversely impact classification. We present a three-step pipeline:
(i) identifying OOD degradation compared to nominal test performance, (ii)
conducting a diagnostic analysis of degradation causes, and (iii) providing
solutions. We find that ensembles of BEiT vision transformers, with targeted
augmentations addressing OOD robustness, geometric ensembling, and
rotation-based test-time augmentation, constitute the most robust model, which
we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated
on container classes. Moreover, it exhibits lower sensitivity to dataset shift,
and reproduces well the plankton abundances. Our proposed pipeline is
applicable to generic plankton classifiers, contingent on the availability of
suitable test cells. By identifying critical shortcomings and offering
practical procedures to fortify models against dataset shift, our study
contributes to the development of more reliable plankton classification
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyathanahally_S/0/1/0/all/0/1&quot;&gt;Sreenath Kyathanahally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyes_M/0/1/0/all/0/1&quot;&gt;Marta Reyes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merkli_S/0/1/0/all/0/1&quot;&gt;Stefanie Merkli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merz_E/0/1/0/all/0/1&quot;&gt;Ewa Merz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francazi_E/0/1/0/all/0/1&quot;&gt;Emanuele Francazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoege_M/0/1/0/all/0/1&quot;&gt;Marvin Hoege&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pomati_F/0/1/0/all/0/1&quot;&gt;Francesco Pomati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baity_Jesi_M/0/1/0/all/0/1&quot;&gt;Marco Baity-Jesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14257">
<title>Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation. (arXiv:2401.14257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14257</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minglin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weihao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yukun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1&quot;&gt;Zhe Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yisheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zilong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liefeng Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yulan Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14285">
<title>POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for Low-Count PET Attenuation Map Generation. (arXiv:2401.14285v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14285</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-dose PET offers a valuable means of minimizing radiation exposure in PET
imaging. However, the prevalent practice of employing additional CT scans for
generating attenuation maps (u-map) for PET attenuation correction
significantly elevates radiation doses. To address this concern and further
mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an
innovative population-prior-aided over-under-representation network that aims
for high-quality attenuation map generation from low-dose PET. First, POUR-Net
incorporates an over-under-representation network (OUR-Net) to facilitate
efficient feature extraction, encompassing both low-resolution abstracted and
fine-detail features, for assisting deep generation on the full-resolution
level. Second, complementing OUR-Net, a population prior generation machine
(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional
prior information to aid OUR-Net generation. The integration of OUR-Net and
PPGM within a cascade framework enables iterative refinement of $\mu$-map
generation, resulting in the production of high-quality $\mu$-maps.
Experimental results underscore the effectiveness of POUR-Net, showing it as a
promising solution for accurate CT-free low-count PET attenuation correction,
which also surpasses the performance of previous baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yinchi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiongchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Huidong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xueqi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1&quot;&gt;Yu-Jung Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panin_V/0/1/0/all/0/1&quot;&gt;Vladimir Y. Panin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toyonaga_T/0/1/0/all/0/1&quot;&gt;Takuya Toyonaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1&quot;&gt;James S. Duncan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14322">
<title>Generalized People Diversity: Learning a Human Perception-Aligned Diversity Representation for People Images. (arXiv:2401.14322v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14322</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing the diversity of people in images is challenging: recent literature
tends to focus on diversifying one or two attributes, requiring expensive
attribute labels or building classifiers. We introduce a diverse people image
ranking method which more flexibly aligns with human notions of people
diversity in a less prescriptive, label-free manner. The Perception-Aligned
Text-derived Human representation Space (PATHS) aims to capture all or many
relevant features of people-related diversity, and, when used as the
representation space in the standard Maximal Marginal Relevance (MMR) ranking
algorithm, is better able to surface a range of types of people-related
diversity (e.g. disability, cultural attire). PATHS is created in two stages.
First, a text-guided approach is used to extract a person-diversity
representation from a pre-trained image-text model. Then this representation is
fine-tuned on perception judgments from human annotators so that it captures
the aspects of people-related similarity that humans find most salient.
Empirical results show that the PATHS method achieves diversity better than
baseline methods, according to side-by-side ratings from human annotators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_H/0/1/0/all/0/1&quot;&gt;Hansa Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schumann_C/0/1/0/all/0/1&quot;&gt;Candice Schumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Aradhana Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madras_D/0/1/0/all/0/1&quot;&gt;David Madras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olanubi_G/0/1/0/all/0/1&quot;&gt;Gbolahan Oluwafemi Olanubi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1&quot;&gt;Alex Beutel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricco_S/0/1/0/all/0/1&quot;&gt;Susanna Ricco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jilin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14325">
<title>Unlocking Past Information: Temporal Embeddings in Cooperative Bird&apos;s Eye View Prediction. (arXiv:2401.14325v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14325</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and comprehensive semantic segmentation of Bird&apos;s Eye View (BEV) is
essential for ensuring safe and proactive navigation in autonomous driving.
Although cooperative perception has exceeded the detection capabilities of
single-agent systems, prevalent camera-based algorithms in cooperative
perception neglect valuable information derived from historical observations.
This limitation becomes critical during sensor failures or communication issues
as cooperative perception reverts to single-agent perception, leading to
degraded performance and incomplete BEV segmentation maps. This paper
introduces TempCoBEV, a temporal module designed to incorporate historical cues
into current observations, thereby improving the quality and reliability of BEV
map segmentations. We propose an importance-guided attention architecture to
effectively integrate temporal information that prioritizes relevant properties
for BEV map segmentation. TempCoBEV is an independent temporal module that
seamlessly integrates into state-of-the-art camera-based cooperative perception
models. We demonstrate through extensive experiments on the OPV2V dataset that
TempCoBEV performs better than non-temporal models in predicting current and
future BEV map segmentations, particularly in scenarios involving communication
failures. We show the efficacy of TempCoBEV and its capability to integrate
historical cues into the current BEV map, improving predictions under optimal
communication conditions by up to 2% and under communication failures by up to
19%. The code will be published on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossle_D/0/1/0/all/0/1&quot;&gt;Dominik R&amp;#xf6;&amp;#xdf;le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerner_J/0/1/0/all/0/1&quot;&gt;Jeremias Gerner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogenberger_K/0/1/0/all/0/1&quot;&gt;Klaus Bogenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidtner_S/0/1/0/all/0/1&quot;&gt;Stefanie Schmidtner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1&quot;&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14336">
<title>Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for Fine-grained Vehicle Recognition. (arXiv:2401.14336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14336</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained vehicle recognition (FGVR) is an essential fundamental
technology for intelligent transportation systems, but very difficult because
of its inherent intra-class variation. Most previous FGVR studies only focus on
the intra-class variation caused by different shooting angles, positions, etc.,
while the intra-class variation caused by image noise has received little
attention. This paper proposes a progressive multi-task anti-noise learning
(PMAL) framework and a progressive multi-task distilling (PMD) framework to
solve the intra-class variation problem in FGVR due to image noise. The PMAL
framework achieves high recognition accuracy by treating image denoising as an
additional task in image recognition and progressively forcing a model to learn
noise invariance. The PMD framework transfers the knowledge of the PMAL-trained
model into the original backbone network, which produces a model with about the
same recognition accuracy as the PMAL-trained model, but without any additional
overheads over the original backbone network. Combining the two frameworks, we
obtain models that significantly exceed previous state-of-the-art methods in
recognition accuracy on two widely-used, standard FGVR datasets, namely
Stanford Cars, and CompCars, as well as three additional surveillance
image-based vehicle-type classification datasets, namely Beijing Institute of
Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images
Dataset for Make Model Recognition (VIDMMR), without any additional overheads
over the original backbone networks. The source code is available at
https://github.com/Dichao-Liu/Anti-noise_FGVR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dichao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14349">
<title>Learning to navigate efficiently and precisely in real environments. (arXiv:2401.14349v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14349</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of autonomous navigation of terrestrial robots, the creation
of realistic models for agent dynamics and sensing is a widespread habit in the
robotics literature and in commercial applications, where they are used for
model based control and/or for localization and mapping. The more recent
Embodied AI literature, on the other hand, focuses on modular or end-to-end
agents trained in simulators like Habitat or AI-Thor, where the emphasis is put
on photo-realistic rendering and scene diversity, but high-fidelity robot
motion is assigned a less privileged role. The resulting sim2real gap
significantly impacts transfer of the trained models to real robotic platforms.
In this work we explore end-to-end training of agents in simulation in settings
which minimize the sim2real gap both, in sensing and in actuation. Our agent
directly predicts (discretized) velocity commands, which are maintained through
closed-loop control in the real robot. The behavior of the real robot
(including the underlying low-level controller) is identified and simulated in
a modified Habitat simulator. Noise models for odometry and localization
further contribute in lowering the sim2real gap. We evaluate on real navigation
scenarios, explore different localization and point goal calculation methods
and report significant gains in performance and robustness compared to prior
work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bono_G/0/1/0/all/0/1&quot;&gt;Guillaume Bono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poirier_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; Poirier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antsfeld_L/0/1/0/all/0/1&quot;&gt;Leonid Antsfeld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monaci_G/0/1/0/all/0/1&quot;&gt;Gianluca Monaci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chidlovskii_B/0/1/0/all/0/1&quot;&gt;Boris Chidlovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1&quot;&gt;Christian Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14354">
<title>Learning Robust Generalizable Radiance Field with Visibility and Feature Augmented Point Representation. (arXiv:2401.14354v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14354</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel paradigm for the generalizable neural radiance
field (NeRF). Previous generic NeRF methods combine multiview stereo techniques
with image-based neural rendering for generalization, yielding impressive
results, while suffering from three issues. First, occlusions often result in
inconsistent feature matching. Then, they deliver distortions and artifacts in
geometric discontinuities and locally sharp shapes due to their individual
process of sampled points and rough feature aggregation. Third, their
image-based representations experience severe degradations when source views
are not near enough to the target view. To address challenges, we propose the
first paradigm that constructs the generalizable neural field based on
point-based rather than image-based rendering, which we call the Generalizable
neural Point Field (GPF). Our approach explicitly models visibilities by
geometric priors and augments them with neural features. We propose a novel
nonuniform log sampling strategy to improve both rendering speed and
reconstruction quality. Moreover, we present a learnable kernel spatially
augmented with features for feature aggregations, mitigating distortions at
places with drastically varying geometries. Besides, our representation can be
easily manipulated. Experiments show that our model can deliver better
geometries, view consistencies, and rendering quality than all counterparts and
benchmarks on three datasets in both generalization and finetuning settings,
preliminarily proving the potential of the new paradigm for generalizable NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaxu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14379">
<title>UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation and Diffusion Models. (arXiv:2401.14379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14379</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary design practices, the integration of computer vision and
generative artificial intelligence (genAI) represents a transformative shift
towards more interactive and inclusive processes. These technologies offer new
dimensions of image analysis and generation, which are particularly relevant in
the context of urban landscape reconstruction. This paper presents a novel
workflow encapsulated within a prototype application, designed to leverage the
synergies between advanced image segmentation and diffusion models for a
comprehensive approach to urban design. Our methodology encompasses the
OneFormer model for detailed image segmentation and the Stable Diffusion XL
(SDXL) diffusion model, implemented through ControlNet, for generating images
from textual descriptions. Validation results indicated a high degree of
performance by the prototype application, showcasing significant accuracy in
both object detection and text-to-image generation. This was evidenced by
superior Intersection over Union (IoU) and CLIP scores across iterative
evaluations for various categories of urban landscape features. Preliminary
testing included utilising UrbanGenAI as an educational tool enhancing the
learning experience in design pedagogy, and as a participatory instrument
facilitating community-driven urban planning. Early results suggested that
UrbanGenAI not only advances the technical frontiers of urban landscape
reconstruction but also provides significant pedagogical and participatory
planning benefits. The ongoing development of UrbanGenAI aims to further
validate its effectiveness across broader contexts and integrate additional
features such as real-time feedback mechanisms and 3D modelling capabilities.
Keywords: generative AI; panoptic image segmentation; diffusion models; urban
landscape design; design pedagogy; co-design
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapsalis_T/0/1/0/all/0/1&quot;&gt;Timo Kapsalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14387">
<title>Inconsistency Masks: Removing the Uncertainty from Input-Pseudo-Label Pairs. (arXiv:2401.14387v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating sufficient labeled data is a significant hurdle in the efficient
execution of deep learning projects, especially in uncharted territories of
image segmentation where labeling demands extensive time, unlike classification
tasks. Our study confronts this challenge, operating in an environment
constrained by limited hardware resources and the lack of extensive datasets or
pre-trained models. We introduce the novel use of Inconsistency Masks (IM) to
effectively filter uncertainty in image-pseudo-label pairs, substantially
elevating segmentation quality beyond traditional semi-supervised learning
techniques. By integrating IM with other methods, we demonstrate remarkable
binary segmentation performance on the ISIC 2018 dataset, starting with just
10% labeled data. Notably, three of our hybrid models outperform those trained
on the fully labeled dataset. Our approach consistently achieves exceptional
results across three additional datasets and shows further improvement when
combined with other techniques. For comprehensive and robust evaluation, this
paper includes an extensive analysis of prevalent semi-supervised learning
strategies, all trained under identical starting conditions. The full code is
available at: https://github.com/MichaelVorndran/InconsistencyMasks
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vorndran_M/0/1/0/all/0/1&quot;&gt;Michael R. H. Vorndran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roeck_B/0/1/0/all/0/1&quot;&gt;Bernhard F. Roeck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14391">
<title>Rethinking Patch Dependence for Masked Autoencoders. (arXiv:2401.14391v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14391</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we re-examine inter-patch dependencies in the decoding
mechanism of masked autoencoders (MAE). We decompose this decoding mechanism
for masked patch reconstruction in MAE into self-attention and cross-attention.
Our investigations suggest that self-attention between mask patches is not
essential for learning good representations. To this end, we propose a novel
pretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).
CrossMAE&apos;s decoder leverages only cross-attention between masked and visible
tokens, with no degradation in downstream performance. This design also enables
decoding only a small subset of mask tokens, boosting efficiency. Furthermore,
each decoder block can now leverage different encoder features, resulting in
improved representation learning. CrossMAE matches MAE in performance with 2.5
to 3.7$\times$ less decoding compute. It also surpasses MAE on ImageNet
classification and COCO instance segmentation under the same compute. Code and
models: https://crossmae.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Letian Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1&quot;&gt;Long Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Baifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xudong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yala_A/0/1/0/all/0/1&quot;&gt;Adam Yala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14398">
<title>pix2gestalt: Amodal Segmentation by Synthesizing Wholes. (arXiv:2401.14398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14398</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce pix2gestalt, a framework for zero-shot amodal segmentation,
which learns to estimate the shape and appearance of whole objects that are
only partially visible behind occlusions. By capitalizing on large-scale
diffusion models and transferring their representations to this task, we learn
a conditional diffusion model for reconstructing whole objects in challenging
zero-shot cases, including examples that break natural and physical priors,
such as art. As training data, we use a synthetically curated dataset
containing occluded objects paired with their whole counterparts. Experiments
show that our approach outperforms supervised baselines on established
benchmarks. Our model can furthermore be used to significantly improve the
performance of existing object recognition and 3D reconstruction methods in the
presence of occlusions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozguroglu_E/0/1/0/all/0/1&quot;&gt;Ege Ozguroglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruoshi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1&quot;&gt;D&amp;#xed;dac Sur&amp;#xed;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1&quot;&gt;Pavel Tokmakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1&quot;&gt;Carl Vondrick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14401">
<title>Range-Agnostic Multi-View Depth Estimation With Keyframe Selection. (arXiv:2401.14401v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14401</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for 3D reconstruction from posed frames require prior knowledge about
the scene metric range, usually to recover matching cues along the epipolar
lines and narrow the search range. However, such prior might not be directly
available or estimated inaccurately in real scenarios -- e.g., outdoor 3D
reconstruction from video sequences -- therefore heavily hampering performance.
In this paper, we focus on multi-view depth estimation without requiring prior
knowledge about the metric range of the scene by proposing RAMDepth, an
efficient and purely 2D framework that reverses the depth estimation and
matching steps order. Moreover, we demonstrate the capability of our framework
to provide rich insights about the quality of the views used for prediction.
Additional material can be found on our project page
https://andreaconti.github.io/projects/range_agnostic_multi_view_depth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1&quot;&gt;Andrea Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1&quot;&gt;Matteo Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambareri_V/0/1/0/all/0/1&quot;&gt;Valerio Cambareri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1&quot;&gt;Stefano Mattoccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14403">
<title>Adaptive Mobile Manipulation for Articulated Objects In the Open World. (arXiv:2401.14403v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Haoyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendonca_R/0/1/0/all/0/1&quot;&gt;Russell Mendonca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1&quot;&gt;Kenneth Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14404">
<title>Deconstructing Denoising Diffusion Models for Self-Supervised Learning. (arXiv:2401.14404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14404</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Saining Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kaiming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.14405">
<title>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities. (arXiv:2401.14405v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.14405</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11057">
<title>Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images. (arXiv:2111.11057v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of instance segmentation in remote sensing images, aiming at
performing per-pixel labeling of objects at instance level, is of great
importance for various civil applications. Despite previous successes, most
existing instance segmentation methods designed for natural images encounter
sharp performance degradations when they are directly applied to top-view
remote sensing images. Through careful analysis, we observe that the challenges
mainly come from the lack of discriminative object features due to severe scale
variations, low contrasts, and clustered distributions. In order to address
these problems, a novel context aggregation network (CATNet) is proposed to
improve the feature extraction process. The proposed model exploits three
lightweight plug-and-play modules, namely dense feature pyramid network
(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest
extractor (HRoIE), to aggregate global visual context at feature, spatial, and
instance domains, respectively. DenseFPN is a multi-scale feature propagation
module that establishes more flexible information flows by adopting inter-level
residual connections, cross-level dense connections, and feature re-weighting
strategy. Leveraging the attention mechanism, SCP further augments the features
by aggregating global spatial context into local regions. For each instance,
HRoIE adaptively generates RoI features for different downstream tasks.
Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, and
HRSID datasets demonstrate that the proposed approach outperforms
state-of-the-arts under similar computational costs. Source code and
pre-trained models are available at https://github.com/yeliudev/CATNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huifang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shuang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chang Wen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.11442">
<title>Scalable Video Object Segmentation with Identification Mechanism. (arXiv:2203.11442v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.11442</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into the challenges of achieving scalable and effective
multi-object modeling for semi-supervised Video Object Segmentation (VOS).
Previous VOS methods decode features with a single positive object, limiting
the learning of multi-object representation as they must match and segment each
target separately under multi-object scenarios. Additionally, earlier
techniques catered to specific application objectives and lacked the
flexibility to fulfill different speed-accuracy requirements. To address these
problems, we present two innovative approaches, Associating Objects with
Transformers (AOT) and Associating Objects with Scalable Transformers (AOST).
In pursuing effective multi-object modeling, AOT introduces the IDentification
(ID) mechanism to allocate each object a unique identity. This approach enables
the network to model the associations among all objects simultaneously, thus
facilitating the tracking and segmentation of objects in a single network pass.
To address the challenge of inflexible deployment, AOST further integrates
scalable long short-term transformers that incorporate scalable supervision and
layer-wise ID-based attention. This enables online architecture scalability in
VOS for the first time and overcomes ID embeddings&apos; representation limitations.
Given the absence of a benchmark for VOS involving densely multi-object
annotations, we propose a challenging Video Object Segmentation in the Wild
(VOSW) benchmark to validate our approaches. We evaluated various AOT and AOST
variants using extensive experiments across VOSW and five commonly used VOS
benchmarks, including YouTube-VOS 2018 &amp;amp; 2019 Val, DAVIS-2017 Val &amp;amp; Test, and
DAVIS-2016. Our approaches surpass the state-of-the-art competitors and display
exceptional efficiency and scalability consistently across all six benchmarks.
Project page: https://github.com/yoxu515/aot-benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1&quot;&gt;Jiaxu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11723">
<title>Self-Supervised Training with Autoencoders for Visual Anomaly Detection. (arXiv:2206.11723v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11723</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, deep auto-encoders have been used for the task of anomaly detection
in the visual domain. By optimising for the reconstruction error using
anomaly-free examples, the common belief is that a corresponding network should
fail to accurately reconstruct anomalous regions in the application phase. This
goal is typically addressed by controlling the capacity of the network, either
by reducing the size of the bottleneck layer or by enforcing sparsity
constraints on its activations. However, neither of these techniques does
explicitly penalise reconstruction of anomalous signals often resulting in poor
detection. We tackle this problem by adapting a self-supervised learning regime
that allows the use of discriminative information during training but focuses
on the data manifold of normal examples. Precisely, we investigate two
different training objectives inspired by the task of neural image inpainting.
Our main objective regularises the model to produce locally consistent
reconstructions, while replacing irregularities, therefore, acting as a filter
that removes anomalous patterns. Our formal analysis shows that under mild
conditions the corresponding model resembles a non-linear orthogonal projection
of partially corrupted images onto the manifold of uncorrupted (defect-free)
examples. This insight makes the reconstruction error a natural choice for
defining the anomaly score of a sample according to its distance from a
corresponding projection on the data manifold. We emphasise that inference with
our approach is very efficient during training and prediction requiring a
single forward pass for each input image. Our experiments on the MVTec AD
dataset demonstrate high detection and localisation performance. On the
texture-subset, in particular, our approach consistently outperforms recent
anomaly detection methods by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_A/0/1/0/all/0/1&quot;&gt;Alexander Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1&quot;&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12809">
<title>Role and Integration of Image Processing Systems in Maritime Target Tracking. (arXiv:2206.12809v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12809</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, maritime traffic has increased, especially in seaborne
trade. To ensure safety, security, and environmental protection, various
systems have been deployed, often combining data for improved effectiveness.
One key application of this combined data is tracking targets at sea, where the
Automatic Identification System (AIS) and X-band marine radar are crucial.
Recently, there has been growing interest in using visual data from cameras to
enhance tracking. This has led to the development of several tracking
algorithms based on image processing. While much of the existing literature
addresses data fusion, there hasn&apos;t been much focus on why integrating image
processing systems is important given the existence of the other systems. In
our paper, we aim to analyze these surveillance systems and highlight the
reasons for integrating image processing systems. Our main goal is to show how
this integration can improve maritime security, offering practical insights
into enhancing safety and protection at sea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zardoua_Y/0/1/0/all/0/1&quot;&gt;Yassir Zardoua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sebbar_B/0/1/0/all/0/1&quot;&gt;Bilal Sebbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chbeine_M/0/1/0/all/0/1&quot;&gt;Moussab Chbeine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Astito_A/0/1/0/all/0/1&quot;&gt;Abdelali Astito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Boulaala_M/0/1/0/all/0/1&quot;&gt;Mohammed Boulaala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11167">
<title>Vision Transformer with Super Token Sampling. (arXiv:2211.11167v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11167</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformer has achieved impressive performance for many vision tasks.
However, it may suffer from high redundancy in capturing local features for
shallow layers. Local self-attention or early-stage convolutions are thus
utilized, which sacrifice the capacity to capture long-range dependency. A
challenge then arises: can we access efficient and effective global context
modeling at the early stages of a neural network? To address this issue, we
draw inspiration from the design of superpixels, which reduces the number of
image primitives in subsequent processing, and introduce super tokens into
vision transformer. Super tokens attempt to provide a semantically meaningful
tessellation of visual content, thus reducing the token number in
self-attention as well as preserving global modeling. Specifically, we propose
a simple yet strong super token attention (STA) mechanism with three steps: the
first samples super tokens from visual tokens via sparse association learning,
the second performs self-attention on super tokens, and the last maps them back
to the original token space. STA decomposes vanilla global attention into
multiplications of a sparse association map and a low-dimensional attention,
leading to high efficiency in capturing global dependencies. Based on STA, we
develop a hierarchical vision transformer. Extensive experiments demonstrate
its strong performance on various vision tasks. In particular, without any
extra training data or label, it achieves 86.4% top-1 accuracy on ImageNet-1K
with less than 100M parameters. It also achieves 53.9 box AP and 46.8 mask AP
on the COCO detection task, and 51.9 mIOU on the ADE20K semantic segmentation
task. Code is released at https://github.com/hhb072/STViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaoqiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Jie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07778">
<title>Efficient Visual Computing with Camera RAW Snapshots. (arXiv:2212.07778v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07778</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional cameras capture image irradiance on a sensor and convert it to
RGB images using an image signal processor (ISP). The images can then be used
for photography or visual computing tasks in a variety of applications, such as
public safety surveillance and autonomous driving. One can argue that since RAW
images contain all the captured information, the conversion of RAW to RGB using
an ISP is not necessary for visual computing. In this paper, we propose a novel
$\rho$-Vision framework to perform high-level semantic understanding and
low-level compression using RAW images without the ISP subsystem used for
decades. Considering the scarcity of available RAW image datasets, we first
develop an unpaired CycleR2R network based on unsupervised CycleGAN to train
modular unrolled ISP and inverse ISP (invISP) models using unpaired RAW and RGB
images. We can then flexibly generate simulated RAW images (simRAW) using any
existing RGB image dataset and finetune different models originally trained for
the RGB domain to process real-world camera RAW images. We demonstrate object
detection and image compression capabilities in RAW-domain using RAW-domain
YOLOv3 and RAW image compressor (RIC) on snapshots from various cameras.
Quantitative results reveal that RAW-domain task inference provides better
detection accuracy and compression compared to RGB-domain processing.
Furthermore, the proposed \r{ho}-Vision generalizes across various camera
sensors and different task-specific models. Additional advantages of the
proposed $\rho$-Vision that eliminates the ISP are the potential reductions in
computations and processing times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Ming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;M. Salman Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhan Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05994">
<title>Min-Max-Jump distance and its applications. (arXiv:2301.05994v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05994</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore three applications of Min-Max-Jump distance (MMJ distance).
MMJ-based K-means revises K-means with MMJ distance. MMJ-based Silhouette
coefficient revises Silhouette coefficient with MMJ distance. We also tested
the Clustering with Neural Network and Index (CNNI) model with MMJ-based
Silhouette coefficient. In the last application, we tested using Min-Max-Jump
distance for predicting labels of new points, after a clustering analysis of
data. Result shows Min-Max-Jump distance achieves good performances in all the
three proposed applications. In addition, we devise several algorithms for
calculating or estimating the distance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gangli Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01622">
<title>Private, fair and accurate: Training large-scale, privacy-preserving AI models in medical imaging. (arXiv:2302.01622v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01622</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) models are increasingly used in the medical
domain. However, as medical data is highly sensitive, special precautions to
ensure its protection are required. The gold standard for privacy preservation
is the introduction of differential privacy (DP) to model training. Prior work
indicates that DP has negative implications on model accuracy and fairness,
which are unacceptable in medicine and represent a main barrier to the
widespread use of privacy-preserving techniques. In this work, we evaluated the
effect of privacy-preserving training of AI models regarding accuracy and
fairness compared to non-private training. For this, we used two datasets: (1)
A large dataset (N=193,311) of high quality clinical chest radiographs, and (2)
a dataset (N=1,625) of 3D abdominal computed tomography (CT) images, with the
task of classifying the presence of pancreatic ductal adenocarcinoma (PDAC).
Both were retrospectively collected and manually labeled by experienced
radiologists. We then compared non-private deep convolutional neural networks
(CNNs) and privacy-preserving (DP) models with respect to privacy-utility
trade-offs measured as area under the receiver-operator-characteristic curve
(AUROC), and privacy-fairness trade-offs, measured as Pearson&apos;s r or
Statistical Parity Difference. We found that, while the privacy-preserving
trainings yielded lower accuracy, they did largely not amplify discrimination
against age, sex or co-morbidity. Our study shows that -- under the challenging
realistic circumstances of a real-life clinical dataset -- the
privacy-preserving training of diagnostic deep learning models is possible with
excellent diagnostic accuracy and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arasteh_S/0/1/0/all/0/1&quot;&gt;Soroosh Tayebi Arasteh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuhl_C/0/1/0/all/0/1&quot;&gt;Christiane Kuhl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1&quot;&gt;Marcus Makowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nebelung_S/0/1/0/all/0/1&quot;&gt;Sven Nebelung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03868">
<title>A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation. (arXiv:2302.03868v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03868</link>
<description rdf:parseType="Literal">&lt;p&gt;Within medical imaging segmentation, the Dice coefficient and Hausdorff-based
metrics are standard measures of success for deep learning models. However,
modern loss functions for medical image segmentation often only consider the
Dice coefficient or similar region-based metrics during training. As a result,
segmentation architectures trained over such loss functions run the risk of
achieving high accuracy for the Dice coefficient but low accuracy for
Hausdorff-based metrics. Low accuracy on Hausdorff-based metrics can be
problematic for applications such as tumor segmentation, where such benchmarks
are crucial. For example, high Dice scores accompanied by significant Hausdorff
errors could indicate that the predictions fail to detect small tumors. We
propose the Generalized Surface Loss function, a novel loss function to
minimize Hausdorff-based metrics with more desirable numerical properties than
current methods and with weighting terms for class imbalance. Our loss function
outperforms other losses when tested on the LiTS and BraTS datasets using the
state-of-the-art nnUNet architecture. These results suggest we can improve
medical imaging segmentation accuracy with our novel loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1&quot;&gt;Adrian Celaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1&quot;&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1&quot;&gt;David Fuentes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13496">
<title>The effectiveness of MAE pre-pretraining for billion-scale pretraining. (arXiv:2303.13496v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13496</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits the standard pretrain-then-finetune paradigm used in
computer vision for visual recognition tasks. Typically, state-of-the-art
foundation models are pretrained using large scale (weakly) supervised datasets
with billions of images. We introduce an additional pre-pretraining stage that
is simple and uses the self-supervised MAE technique to initialize the model.
While MAE has only been shown to scale with the size of models, we find that it
scales with the size of the training dataset as well. Thus, our MAE-based
pre-pretraining scales with both model and data size making it applicable for
training foundation models. Pre-pretraining consistently improves both the
model convergence and the downstream transfer performance across a range of
model scales (millions to billions of parameters), and dataset sizes (millions
to billions of images). We measure the effectiveness of pre-pretraining on 10
different visual recognition tasks spanning image classification, video
recognition, object detection, low-shot classification and zero-shot
recognition. Our largest model achieves new state-of-the-art results on
iNaturalist-18 (91.7%), ImageNet-ReaL (91.1%), 1-shot ImageNet-1k (63.6%), and
zero-shot transfer on Food-101 (96.2%). Our study reveals that model
initialization plays a significant role, even for web-scale pretraining with
billions of images, and our models are available publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mannat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1&quot;&gt;Quentin Duval&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alwala_K/0/1/0/all/0/1&quot;&gt;Kalyan Vasudev Alwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaibhav Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adcock_A/0/1/0/all/0/1&quot;&gt;Aaron Adcock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1&quot;&gt;Armand Joulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1&quot;&gt;Piotr Doll&amp;#xe1;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1&quot;&gt;Christoph Feichtenhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1&quot;&gt;Ross Girshick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1&quot;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1&quot;&gt;Ishan Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09571">
<title>LLIC: Large Receptive Field Transform Coding with Adaptive Weights for Learned Image Compression. (arXiv:2304.09571v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09571</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective Receptive field (ERF) plays an important role in transform coding,
which determines how much redundancy can be removed at most during transform
and how many spatial priors can be utilized to synthesize textures during
inverse transform. Existing methods rely on stacks of small kernels, whose ERF
remains not large enough instead, or heavy non-local attention mechanisms,
which limit the potential of high resolution image coding. To tackle this
issue, we propose Large Receptive Field Transform Coding with Adaptive Weights
for Learned Image Compression (LLIC). Specifically, for the first time in
learned image compression community, we introduce a few large kernel-based
depth-wise convolutions to reduce more redundancy while maintaining modest
complexity. Due to wide range of image diversity, we propose to enhance the
adaptability of convolutions via generating weights in a self-conditioned
manner. The large kernels cooperate with non-linear embedding and gate
mechanisms for better expressiveness and lighter point-wise interactions. We
also investigate improved training techniques to fully exploit the potential of
large kernels. In addition, to enhance the interactions among channels, we
propose the adaptive channel-wise bit allocation via generating channel
importance factor in a self-conditioned manner. To demonstrate the
effectiveness of proposed transform coding, we align the entropy model to
compare with existing transform methods and obtain models LLIC-STF, LLIC-ELIC,
LLIC-TCM. Extensive experiments demonstrate our proposed LLIC models have
significant improvements over corresponding baselines and achieve
state-of-the-art performances and better trade-off between performance and
complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_P/0/1/0/all/0/1&quot;&gt;Peirong Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08098">
<title>A Theory of General Difference in Continuous and Discrete Domain. (arXiv:2305.08098v2 [cs.DM] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08098</link>
<description rdf:parseType="Literal">&lt;p&gt;Though a core element of the digital age, numerical difference algorithms
struggle with noise susceptibility. This stems from a key disconnect between
the infinitesimal quantities in continuous differentiation and the finite
intervals in its discrete counterpart. This disconnect violates the fundamental
definition of differentiation (Leibniz and Cauchy). To bridge this gap, we
build a novel general difference (Tao General Difference, TGD). Departing from
derivative-by-integration, TGD generalizes differentiation to finite intervals
in continuous domains through three key constraints. This allows us to
calculate the general difference of a sequence in discrete domain via the
continuous step function constructed from the sequence. Two construction
methods, the rotational construction and the orthogonal construction, are
proposed to construct the operators of TGD. The construction TGD operators take
same convolution mode in calculation for continuous functions, discrete
sequences, and arrays across any dimension. Our analysis with example
operations showcases TGD&apos;s capability in both continuous and discrete domains,
paving the way for accurate and noise-resistant differentiation in the digital
era.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Linmi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Donglai Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1&quot;&gt;Wu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Feilong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jingmao Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13819">
<title>WaveDM: Wavelet-Based Diffusion Models for Image Restoration. (arXiv:2305.13819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13819</link>
<description rdf:parseType="Literal">&lt;p&gt;Latest diffusion-based methods for many image restoration tasks outperform
traditional models, but they encounter the long-time inference problem. To
tackle it, this paper proposes a Wavelet-Based Diffusion Model (WaveDM). WaveDM
learns the distribution of clean images in the wavelet domain conditioned on
the wavelet spectrum of degraded images after wavelet transform, which is more
time-saving in each step of sampling than modeling in the spatial domain. To
ensure restoration performance, a unique training strategy is proposed where
the low-frequency and high-frequency spectrums are learned using distinct
modules. In addition, an Efficient Conditional Sampling (ECS) strategy is
developed from experiments, which reduces the number of total sampling steps to
around 5. Evaluations on twelve benchmark datasets including image raindrop
removal, rain steaks removal, dehazing, defocus deblurring, demoir\&apos;eing, and
denoising demonstrate that WaveDM achieves state-of-the-art performance with
the efficiency that is comparable to traditional one-pass methods and over
100$\times$ faster than existing image restoration methods using vanilla
diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Mingfu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yu Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiaxi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaoqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14486">
<title>Point2SSM: Learning Morphological Variations of Anatomies from Point Cloud. (arXiv:2305.14486v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14486</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Point2SSM, a novel unsupervised learning approach for constructing
correspondence-based statistical shape models (SSMs) directly from raw point
clouds. SSM is crucial in clinical research, enabling population-level analysis
of morphological variation in bones and organs. Traditional methods of SSM
construction have limitations, including the requirement of noise-free surface
meshes or binary volumes, reliance on assumptions or templates, and prolonged
inference times due to simultaneous optimization of the entire cohort.
Point2SSM overcomes these barriers by providing a data-driven solution that
infers SSMs directly from raw point clouds, reducing inference burdens and
increasing applicability as point clouds are more easily acquired. While deep
learning on 3D point clouds has seen success in unsupervised representation
learning and shape correspondence, its application to anatomical SSM
construction is largely unexplored. We conduct a benchmark of state-of-the-art
point cloud deep networks on the SSM task, revealing their limited robustness
to clinical challenges such as noisy, sparse, or incomplete input and limited
training data. Point2SSM addresses these issues through an attention-based
module, providing effective correspondence mappings from learned point
features. Our results demonstrate that the proposed method significantly
outperforms existing networks in terms of accurate surface sampling and
correspondence, better capturing population-level statistics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1&quot;&gt;Jadie Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhabian_S/0/1/0/all/0/1&quot;&gt;Shireen Elhabian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19094">
<title>Diffusion Model for Dense Matching. (arXiv:2305.19094v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19094</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective for establishing dense correspondence between paired images
consists of two terms: a data term and a prior term. While conventional
techniques focused on defining hand-designed prior terms, which are difficult
to formulate, recent approaches have focused on learning the data term with
deep neural networks without explicitly modeling the prior, assuming that the
model itself has the capacity to learn an optimal prior from a large-scale
dataset. The performance improvement was obvious, however, they often fail to
address inherent ambiguities of matching, such as textureless regions,
repetitive patterns, and large displacements. To address this, we propose
DiffMatch, a novel conditional diffusion-based framework designed to explicitly
model both the data and prior terms. Unlike previous approaches, this is
accomplished by leveraging a conditional denoising diffusion model. DiffMatch
consists of two main components: conditional denoising diffusion module and
cost injection module. We stabilize the training process and reduce memory
usage with a stage-wise training strategy. Furthermore, to boost performance,
we introduce an inference technique that finds a better path to the accurate
matching field. Our experimental results demonstrate significant performance
improvements of our method over existing approaches, and the ablation studies
validate our design choices along with the effectiveness of each component.
Project page is available at https://ku-cvlab.github.io/DiffMatch/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1&quot;&gt;Jisu Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyuseong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sunwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeonsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyoungwon Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungryong Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12244">
<title>Discovering Intrinsic Spatial-Temporal Logic Rules to Explain Human Actions. (arXiv:2306.12244v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a logic-informed knowledge-driven modeling framework for human
movements by analyzing their trajectories. Our approach is inspired by the fact
that human actions are usually driven by their intentions or desires, and are
influenced by environmental factors such as the spatial relationships with
surrounding objects. In this paper, we introduce a set of spatial-temporal
logic rules as knowledge to explain human actions. These rules will be
automatically discovered from observational data. To learn the model parameters
and the rule content, we design an expectation-maximization (EM) algorithm,
which treats the rule content as latent variables. The EM algorithm alternates
between the E-step and M-step: in the E-step, the posterior distribution over
the latent rule content is evaluated; in the M-step, the rule generator and
model parameters are jointly optimized by maximizing the current expected
log-likelihood. Our model may have a wide range of applications in areas such
as sports analytics, robotics, and autonomous cars, where understanding human
movements are essential. We demonstrate the model&apos;s superior interpretability
and prediction performance on pedestrian and NBA basketball player datasets,
both achieving promising results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chengzhi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15667">
<title>PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment. (arXiv:2306.15667v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15667</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera pose estimation is a long-standing computer vision problem that to
date often relies on classical methods, such as handcrafted keypoint matching,
RANSAC and bundle adjustment. In this paper, we propose to formulate the
Structure from Motion (SfM) problem inside a probabilistic diffusion framework,
modelling the conditional distribution of camera poses given input images. This
novel view of an old problem has several advantages. (i) The nature of the
diffusion framework mirrors the iterative procedure of bundle adjustment. (ii)
The formulation allows a seamless integration of geometric constraints from
epipolar geometry. (iii) It excels in typically difficult scenarios such as
sparse views with wide baselines. (iv) The method can predict intrinsics and
extrinsics for an arbitrary amount of images. We demonstrate that our method
PoseDiffusion significantly improves over the classic SfM pipelines and the
learned approaches on two real-world datasets. Finally, it is observed that our
method can generalize across datasets without further training. Project page:
https://posediffusion.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1&quot;&gt;Christian Rupprecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1&quot;&gt;David Novotny&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09437">
<title>Grounded Object Centric Learning. (arXiv:2307.09437v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09437</link>
<description rdf:parseType="Literal">&lt;p&gt;The extraction of modular object-centric representations for downstream tasks
is an emerging area of research. Learning grounded representations of objects
that are guaranteed to be stable and invariant promises robust performance
across different tasks and environments. Slot Attention (SA) learns
object-centric representations by assigning objects to \textit{slots}, but
presupposes a \textit{single} distribution from which all slots are randomly
initialised. This results in an inability to learn \textit{specialized} slots
which bind to specific object types and remain invariant to identity-preserving
changes in object appearance. To address this, we present
\emph{\textsc{Co}nditional \textsc{S}lot \textsc{A}ttention} (\textsc{CoSA})
using a novel concept of \emph{Grounded Slot Dictionary} (GSD) inspired by
vector quantization. Our proposed GSD comprises (i) canonical object-level
property vectors and (ii) parametric Gaussian distributions, which define a
prior over the slots. We demonstrate the benefits of our method in multiple
downstream tasks such as scene generation, composition, and task adaptation,
whilst remaining competitive with SA in popular object discovery benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kori_A/0/1/0/all/0/1&quot;&gt;Avinash Kori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1&quot;&gt;Francesco Locatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_F/0/1/0/all/0/1&quot;&gt;Fabio De Sousa Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1&quot;&gt;Francesca Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1&quot;&gt;Ben Glocker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10895">
<title>Variational Autoencoding of Dental Point Clouds. (arXiv:2307.10895v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10895</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital dentistry has made significant advancements, yet numerous challenges
remain. This paper introduces the FDI 16 dataset, an extensive collection of
tooth meshes and point clouds. Additionally, we present a novel approach:
Variational FoldingNet (VF-Net), a fully probabilistic variational autoencoder
designed for point clouds. Notably, prior latent variable models for point
clouds lack a one-to-one correspondence between input and output points.
Instead, they rely on optimizing Chamfer distances, a metric that lacks a
normalized distributional counterpart, rendering it unsuitable for
probabilistic modeling. We replace the explicit minimization of Chamfer
distances with a suitable encoder, increasing computational efficiency while
simplifying the probabilistic extension. This allows for straightforward
application in various tasks, including mesh generation, shape completion, and
representation learning. Empirically, we provide evidence of lower
reconstruction error in dental reconstruction and interpolation, showcasing
state-of-the-art performance in dental sample generation while identifying
valuable latent representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Johan Ziruo Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orkild_T/0/1/0/all/0/1&quot;&gt;Thomas &amp;#xd8;rkild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sondergaard_P/0/1/0/all/0/1&quot;&gt;Peter Lempel S&amp;#xf8;ndergaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf8;ren Hauberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02982">
<title>Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation. (arXiv:2308.02982v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02982</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, 3D understanding has turned to 2D vision-language
pre-trained models to overcome data scarcity challenges. However, existing
methods simply transfer 2D alignment strategies, aligning 3D representations
with single-view 2D images and coarse-grained parent category text. These
approaches introduce information degradation and insufficient synergy issues,
leading to performance loss. Information degradation arises from overlooking
the fact that a 3D representation should be equivalent to a series of
multi-view images and more fine-grained subcategory text. Insufficient synergy
neglects the idea that a robust 3D representation should align with the joint
vision-language space, rather than independently aligning with each modality.
In this paper, we propose a multi-view joint modality modeling approach, termed
JM3D, to obtain a unified representation for point cloud, text, and image.
Specifically, a novel Structured Multimodal Organizer (SMO) is proposed to
address the information degradation issue, which introduces contiguous
multi-view images and hierarchical text to enrich the representation of vision
and language modalities. A Joint Multi-modal Alignment (JMA) is designed to
tackle the insufficient synergy problem, which models the joint modality by
incorporating language knowledge into the visual modality. Extensive
experiments on ModelNet40 and ScanObjectNN demonstrate the effectiveness of our
proposed method, JM3D, which achieves state-of-the-art performance in zero-shot
3D classification. JM3D outperforms ULIP by approximately 4.3% on PointMLP and
achieves an improvement of up to 6.5% accuracy on PointNet++ in top-1 accuracy
for zero-shot 3D classification on ModelNet40. The source code and trained
models for all our experiments are publicly available at
https://github.com/Mr-Neko/JM3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiji Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Minda Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lincheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zhao_z/0/1/0/all/0/1&quot;&gt;zeng zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1&quot;&gt;Tangjie Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01429">
<title>Adapting Segment Anything Model for Change Detection in HR Remote Sensing Images. (arXiv:2309.01429v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01429</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Foundation Models (VFMs) such as the Segment Anything Model (SAM)
allow zero-shot or interactive segmentation of visual contents, thus they are
quickly applied in a variety of visual scenes. However, their direct use in
many Remote Sensing (RS) applications is often unsatisfactory due to the
special imaging characteristics of RS images. In this work, we aim to utilize
the strong visual recognition capabilities of VFMs to improve the change
detection of high-resolution Remote Sensing Images (RSIs). We employ the visual
encoder of FastSAM, an efficient variant of the SAM, to extract visual
representations in RS scenes. To adapt FastSAM to focus on some specific ground
objects in the RS scenes, we propose a convolutional adaptor to aggregate the
task-oriented change information. Moreover, to utilize the semantic
representations that are inherent to SAM features, we introduce a task-agnostic
semantic learning branch to model the semantic latent in bi-temporal RSIs. The
resulting method, SAMCD, obtains superior accuracy compared to the SOTA methods
and exhibits a sample-efficient learning ability that is comparable to
semi-supervised CD methods. To the best of our knowledge, this is the first
work that adapts VFMs for the CD of HR RSIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Daifeng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kuiwu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1&quot;&gt;Lorenzo Bruzzone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05527">
<title>ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation. (arXiv:2309.05527v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain shifts such as sensor type changes and geographical situation
variations are prevalent in Autonomous Driving (AD), which poses a challenge
since AD model relying on the previous domain knowledge can be hardly directly
deployed to a new domain without additional costs. In this paper, we provide a
new perspective and approach of alleviating the domain shifts, by proposing a
Reconstruction-Simulation-Perception (ReSimAD) scheme. Specifically, the
implicit reconstruction process is based on the knowledge from the previous old
domain, aiming to convert the domain-related knowledge into domain-invariant
representations, e.g., 3D scene-level meshes. Besides, the point clouds
simulation process of multiple new domains is conditioned on the above
reconstructed 3D meshes, where the target-domain-like simulation samples can be
obtained, thus reducing the cost of collecting and annotating new-domain data
for the subsequent perception process. For experiments, we consider different
cross-domain situations such as Waymo-to-KITTI, Waymo-to-nuScenes,
Waymo-to-ONCE, etc, to verify the zero-shot target-domain perception using
ReSimAD. Results demonstrate that our method is beneficial to boost the domain
generalization ability, even promising for 3D pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiakang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianfei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiangchao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1&quot;&gt;Renqiu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06023">
<title>Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration. (arXiv:2309.06023v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06023</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed &apos;learning from history&apos;, which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive Learning for Image Restoration (MCLIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCLIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08421">
<title>MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08421</link>
<description rdf:parseType="Literal">&lt;p&gt;Label-free cell classification is advantageous for supplying pristine cells
for further use or examination, yet existing techniques frequently fall short
in terms of specificity and speed. In this study, we address these limitations
through the development of a novel machine learning framework, Multiplex Image
Machine Learning (MIML). This architecture uniquely combines label-free cell
images with biomechanical property data, harnessing the vast, often
underutilized morphological information intrinsic to each cell. By integrating
both types of data, our model offers a more holistic understanding of the
cellular properties, utilizing morphological information typically discarded in
traditional machine learning models. This approach has led to a remarkable
98.3\% accuracy in cell classification, a substantial improvement over models
that only consider a single data type. MIML has been proven effective in
classifying white blood cells and tumor cells, with potential for broader
application due to its inherent flexibility and transfer learning capability.
It&apos;s particularly effective for cells with similar morphology but distinct
biomechanical properties. This innovative approach has significant implications
across various fields, from advancing disease diagnostics to understanding
cellular behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1&quot;&gt;Khayrul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paul_R/0/1/0/all/0/1&quot;&gt;Ratul Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yaling Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15519">
<title>Defending Against Physical Adversarial Patch Attacks on Infrared Human Detection. (arXiv:2309.15519v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15519</link>
<description rdf:parseType="Literal">&lt;p&gt;Infrared detection is an emerging technique for safety-critical tasks owing
to its remarkable anti-interference capability. However, recent studies have
revealed that it is vulnerable to physically-realizable adversarial patches,
posing risks in its real-world applications. To address this problem, we are
the first to investigate defense strategies against adversarial patch attacks
on infrared detection, especially human detection. We have devised a
straightforward defense strategy, patch-based occlusion-aware detection (POD),
which efficiently augments training samples with random patches and
subsequently detects them. POD not only robustly detects people but also
identifies adversarial patch locations. Surprisingly, while being extremely
computationally efficient, POD easily generalizes to state-of-the-art
adversarial patch attacks that are unseen during training. Furthermore, POD
improves detection precision even in a clean (i.e., no-attack) situation due to
the data augmentation effect. Evaluation demonstrated that POD is robust to
adversarial patches of various shapes and sizes. The effectiveness of our
baseline approach is shown to be a viable defense mechanism for real-world
infrared human detection systems, paving the way for exploring future research
directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strack_L/0/1/0/all/0/1&quot;&gt;Lukas Strack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waseda_F/0/1/0/all/0/1&quot;&gt;Futa Waseda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huy H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yinqiang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1&quot;&gt;Isao Echizen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01140">
<title>Neural Processing of Tri-Plane Hybrid Neural Fields. (arXiv:2310.01140v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01140</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by the appealing properties of neural fields for storing and
communicating 3D data, the problem of directly processing them to address tasks
such as classification and part segmentation has emerged and has been
investigated in recent works. Early approaches employ neural fields
parameterized by shared networks trained on the whole dataset, achieving good
task performance but sacrificing reconstruction quality. To improve the latter,
later methods focus on individual neural fields parameterized as large
Multi-Layer Perceptrons (MLPs), which are, however, challenging to process due
to the high dimensionality of the weight space, intrinsic weight space
symmetries, and sensitivity to random initialization. Hence, results turn out
significantly inferior to those achieved by processing explicit
representations, e.g., point clouds or meshes. In the meantime, hybrid
representations, in particular based on tri-planes, have emerged as a more
effective and efficient alternative to realize neural fields, but their direct
processing has not been investigated yet. In this paper, we show that the
tri-plane discrete data structure encodes rich information, which can be
effectively processed by standard deep-learning machinery. We define an
extensive benchmark covering a diverse set of fields such as occupancy,
signed/unsigned distance, and, for the first time, radiance fields. While
processing a field with the same reconstruction quality, we achieve task
performance far superior to frameworks that process large MLPs and, for the
first time, almost on par with architectures handling explicit representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardace_A/0/1/0/all/0/1&quot;&gt;Adriano Cardace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1&quot;&gt;Pierluigi Zama Ramirez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballerini_F/0/1/0/all/0/1&quot;&gt;Francesco Ballerini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Allan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1&quot;&gt;Samuele Salti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1&quot;&gt;Luigi Di Stefano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05207">
<title>Facial Action Unit Detection Based on Multi-task Learning Strategy for Unlabeled Facial Images in the Wild. (arXiv:2310.05207v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05207</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial Action Unit (AU) detection often relies on highly-cost accurate
labeling or inaccurate pseudo labeling techniques in recent years. How to
introduce large amounts of unlabeled facial images in the wild into supervised
AU detection frameworks has become a challenging problem. Additionally, nearly
every type of AUs has the problem of unbalanced positive and negative samples.
Inspired by other multi-task learning frameworks, we first propose a multi-task
learning strategy boosting AU detection in the wild through jointing facial
landmark detection and AU domain separation and reconstruction. Our introduced
dual domains facial landmark detection framework can solve the lack of accurate
facial landmark coordinates during the AU domain separation and reconstruction
training process, while the parameters of homostructural facial extraction
modules from these two similar facial tasks are shared. Moreover, we propose a
pixel-level feature alignment scheme to maintain the consistency of features
obtained from two separation and reconstruction processes. Furthermore, a
weighted asymmetric loss is proposed to change the contribution of positive and
negative samples of each type of AUs to model parameters updating. Experimental
results on three widely used benchmarks demonstrate our superiority to most
state-of-the-art methods for AU detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06992">
<title>Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models. (arXiv:2310.06992v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06992</link>
<description rdf:parseType="Literal">&lt;p&gt;Object tracking is central to robot perception and scene understanding.
Tracking-by-detection has long been a dominant paradigm for object tracking of
specific object categories. Recently, large-scale pre-trained models have shown
promising advances in detecting and segmenting objects and parts in 2D static
images in the wild. This begs the question: can we re-purpose these large-scale
pre-trained static image models for open-vocabulary video tracking? In this
paper, we re-purpose an open-vocabulary detector, segmenter, and dense optical
flow estimator, into a model that tracks and segments objects of any category
in 2D videos. Our method predicts object and part tracks with associated
language descriptions in monocular videos, rebuilding the pipeline of Tractor
with modern large pre-trained models for static image detection and
segmentation: we detect open-vocabulary object instances and propagate their
boxes from frame to frame using a flow-based motion model, refine the
propagated boxes with the box regression module of the visual detector, and
prompt an open-world segmenter with the refined box to segment the objects. We
decide the termination of an object track based on the objectness score of the
propagated boxes, as well as forward-backward optical flow consistency. We
re-identify objects across occlusions using deep feature matching. We show that
our model achieves strong performance on multiple established video object
segmentation and tracking benchmarks, and can produce reasonable tracks in
manipulation data. In particular, our model outperforms previous
state-of-the-art in UVO and BURST, benchmarks for open-world object tracking
and segmentation, despite never being explicitly trained for tracking. We hope
that our approach can serve as a simple and extensible framework for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wen-Hsuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1&quot;&gt;Pavel Tokmakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1&quot;&gt;Katerina Fragkiadaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09503">
<title>JM3D &amp; JM3D-LLM: Elevating 3D Understanding with Joint Multi-modal Cues. (arXiv:2310.09503v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09503</link>
<description rdf:parseType="Literal">&lt;p&gt;The rising importance of 3D understanding, pivotal in computer vision,
autonomous driving, and robotics, is evident. However, a prevailing trend,
which straightforwardly resorted to transferring 2D alignment strategies to the
3D domain, encounters three distinct challenges: (1) Information Degradation:
This arises from the alignment of 3D data with mere single-view 2D images and
generic texts, neglecting the need for multi-view images and detailed
subcategory texts. (2) Insufficient Synergy: These strategies align 3D
representations to image and text features individually, hampering the overall
optimization for 3D models. (3) Underutilization: The fine-grained information
inherent in the learned representations is often not fully exploited,
indicating a potential loss in detail. To address these issues, we introduce
JM3D, a comprehensive approach integrating point cloud, text, and image. Key
contributions include the Structured Multimodal Organizer (SMO), enriching
vision-language representation with multiple views and hierarchical text, and
the Joint Multi-modal Alignment (JMA), combining language understanding with
visual representation. Our advanced model, JM3D-LLM, marries 3D representation
with large language models via efficient fine-tuning. Evaluations on ModelNet40
and ScanObjectNN establish JM3D&apos;s superiority. The superior performance of
JM3D-LLM further underscores the effectiveness of our representation transfer
approach. Our code and models are available at https://github.com/Mr-Neko/JM3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jiayi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Changli Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yiwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09543">
<title>Benchmarking the Sim-to-Real Gap in Cloth Manipulation. (arXiv:2310.09543v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09543</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic physics engines play a crucial role for learning to manipulate
deformable objects such as garments in simulation. By doing so, researchers can
circumvent challenges such as sensing the deformation of the object in the
realworld. In spite of the extensive use of simulations for this task, few
works have evaluated the reality gap between deformable object simulators and
real-world data. We present a benchmark dataset to evaluate the sim-to-real gap
in cloth manipulation. The dataset is collected by performing a dynamic as well
as a quasi-static cloth manipulation task involving contact with a rigid table.
We use the dataset to evaluate the reality gap, computational time, and
simulation stability of four popular deformable object simulators: MuJoCo,
Bullet, Flex, and SOFA. Additionally, we discuss the benefits and drawbacks of
each simulator. The benchmark dataset is open-source. Supplementary material,
videos, and code, can be found at
https://sites.google.com/view/cloth-sim2real-benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanco_Mulero_D/0/1/0/all/0/1&quot;&gt;David Blanco-Mulero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbany_O/0/1/0/all/0/1&quot;&gt;Oriol Barbany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alcan_G/0/1/0/all/0/1&quot;&gt;Gokhan Alcan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colome_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe0; Colom&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torras_C/0/1/0/all/0/1&quot;&gt;Carme Torras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1&quot;&gt;Ville Kyrki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09965">
<title>ProteusNeRF: Fast Lightweight NeRF Editing using 3D-Aware Image Context. (arXiv:2310.09965v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09965</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Fields (NeRFs) have recently emerged as a popular option for
photo-realistic object capture due to their ability to faithfully capture
high-fidelity volumetric content even from handheld video input. Although much
research has been devoted to efficient optimization leading to real-time
training and rendering, options for interactive editing NeRFs remain limited.
We present a very simple but effective neural network architecture that is fast
and efficient while maintaining a low memory footprint. This architecture can
be incrementally guided through user-friendly image-based edits. Our
representation allows straightforward object selection via semantic feature
distillation at the training stage. More importantly, we propose a local
3D-aware image context to facilitate view-consistent image editing that can
then be distilled into fine-tuned NeRFs, via geometric and appearance
adjustments. We evaluate our setup on a variety of examples to demonstrate
appearance and geometric edits and report 10-30x speedup over concurrent work
focusing on text-guided NeRF editing. Video results can be seen on our project
webpage at https://proteusnerf.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binglun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutt_N/0/1/0/all/0/1&quot;&gt;Niladri Shekhar Dutt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1&quot;&gt;Niloy J. Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15072">
<title>RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in Dynamic Environments. (arXiv:2310.15072v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15072</link>
<description rdf:parseType="Literal">&lt;p&gt;It is typically challenging for visual or visual-inertial odometry systems to
handle the problems of dynamic scenes and pure rotation. In this work, we
design a novel visual-inertial odometry (VIO) system called RD-VIO to handle
both of these two problems. Firstly, we propose an IMU-PARSAC algorithm which
can robustly detect and match keypoints in a two-stage process. In the first
state, landmarks are matched with new keypoints using visual and IMU
measurements. We collect statistical information from the matching and then
guide the intra-keypoint matching in the second stage. Secondly, to handle the
problem of pure rotation, we detect the motion type and adapt the
deferred-triangulation technique during the data-association process. We make
the pure-rotational frames into the special subframes. When solving the
visual-inertial bundle adjustment, they provide additional constraints to the
pure-rotational motion. We evaluate the proposed VIO system on public datasets.
Experiments show the proposed RD-VIO has obvious advantages over other methods
in dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaokun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Gan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1&quot;&gt;Hujun Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guofeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17170">
<title>MO-YOLO: End-to-End Multiple-Object Tracking Method with YOLO and Decoder. (arXiv:2310.17170v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17170</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of multi-object tracking (MOT), recent Transformer based
end-to-end models like MOTR have demonstrated exceptional performance on
datasets such as DanceTracker. However, the computational demands of these
models present challenges in training and deployment. Drawing inspiration from
successful models like GPT, we present MO-YOLO, an efficient and
computationally frugal end-to-end MOT model. MO-YOLO integrates principles from
You Only Look Once (YOLO) and RT-DETR, adopting a decoder-only approach. By
leveraging the decoder from RT-DETR and architectural components from YOLOv8,
MO-YOLO achieves high speed, shorter training times, and proficient MOT
performance. On the Dancetrack, MO-YOLO not only matches MOTR&apos;s performance but
also surpasses it, achieving over twice the frames per second (MOTR 9.5 FPS,
MO-YOLO 19.6 FPS). Furthermore, MO-YOLO demonstrates significantly reduced
training times and lower hardware requirements compared to MOTR. This research
introduces a promising paradigm for efficient end-to-end MOT, emphasizing
enhanced performance and resource efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_W/0/1/0/all/0/1&quot;&gt;Wu Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1&quot;&gt;Liu Bo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xingle_Z/0/1/0/all/0/1&quot;&gt;Zhang Xingle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04234">
<title>Leveraging sinusoidal representation networks to predict fMRI signals from EEG. (arXiv:2311.04234v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04234</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern neuroscience, functional magnetic resonance imaging (fMRI) has been
a crucial and irreplaceable tool that provides a non-invasive window into the
dynamics of whole-brain activity. Nevertheless, fMRI is limited by hemodynamic
blurring as well as high cost, immobility, and incompatibility with metal
implants. Electroencephalography (EEG) is complementary to fMRI and can
directly record the cortical electrical activity at high temporal resolution,
but has more limited spatial resolution and is unable to recover information
about deep subcortical brain structures. The ability to obtain fMRI information
from EEG would enable cost-effective, imaging across a wider set of brain
regions. Further, beyond augmenting the capabilities of EEG, cross-modality
models would facilitate the interpretation of fMRI signals. However, as both
EEG and fMRI are high-dimensional and prone to artifacts, it is currently
challenging to model fMRI from EEG. To address this challenge, we propose a
novel architecture that can predict fMRI signals directly from multi-channel
EEG without explicit feature engineering. Our model achieves this by
implementing a Sinusoidal Representation Network (SIREN) to learn frequency
information in brain dynamics from EEG, which serves as the input to a
subsequent encoder-decoder to effectively reconstruct the fMRI signal from a
specific brain region. We evaluate our model using a simultaneous EEG-fMRI
dataset with 8 subjects and investigate its potential for predicting
subcortical fMRI signals. The present results reveal that our model outperforms
a recent state-of-the-art model, and indicates the potential of leveraging
periodic activation functions in deep neural networks to model functional
neuroimaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yamin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lou_A/0/1/0/all/0/1&quot;&gt;Ange Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Catie Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15193">
<title>IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction. (arXiv:2311.15193v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15193</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the trajectory of pedestrians in crowd scenarios is indispensable
in self-driving or autonomous mobile robot field because estimating the future
locations of pedestrians around is beneficial for policy decision to avoid
collision. It is a challenging issue because humans have different walking
motions, and the interactions between humans and objects in the current
environment, especially between humans themselves, are complex. Previous
researchers focused on how to model human-human interactions but neglected the
relative importance of interactions. To address this issue, a novel mechanism
based on correntropy is introduced. The proposed mechanism not only can measure
the relative importance of human-human interactions but also can build personal
space for each pedestrian. An interaction module including this data-driven
mechanism is further proposed. In the proposed module, the data-driven
mechanism can effectively extract the feature representations of dynamic
human-human interactions in the scene and calculate the corresponding weights
to represent the importance of different interactions. To share such social
messages among pedestrians, an interaction-aware architecture based on long
short-term memory network for trajectory prediction is designed. Experiments
are conducted on two public datasets. Experimental results demonstrate that our
model can achieve better performance than several latest methods with good
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuehai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18296">
<title>Perceptual Group Tokenizer: Building Perception with Iterative Grouping. (arXiv:2311.18296v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18296</link>
<description rdf:parseType="Literal">&lt;p&gt;Human visual recognition system shows astonishing capability of compressing
visual information into a set of tokens containing rich representations without
label supervision. One critical driving principle behind it is perceptual
grouping. Despite being widely used in computer vision in the early 2010s, it
remains a mystery whether perceptual grouping can be leveraged to derive a
neural visual recognition backbone that generates as powerful representations.
In this paper, we propose the Perceptual Group Tokenizer, a model that entirely
relies on grouping operations to extract visual features and perform
self-supervised representation learning, where a series of grouping operations
are used to iteratively hypothesize the context for pixels or superpixels to
refine feature representations. We show that the proposed model can achieve
competitive performance compared to state-of-the-art vision architectures, and
inherits desirable properties including adaptive computation without
re-training, and interpretability. Specifically, Perceptual Group Tokenizer
achieves 80.3% on ImageNet-1K self-supervised learning benchmark with linear
probe evaluation, marking a new progress under this paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Ting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06968">
<title>Hallucination Augmented Contrastive Learning for Multimodal Large Language Model. (arXiv:2312.06968v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06968</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal large language models (MLLMs) have been shown to efficiently
integrate natural language with visual information to handle multi-modal tasks.
However, MLLMs still face a fundamental limitation of hallucinations, where
they tend to generate erroneous or fabricated information. In this paper, we
address hallucinations in MLLMs from a novel perspective of representation
learning. We first analyzed the representation distribution of textual and
visual tokens in MLLM, revealing two important findings: 1) there is a
significant gap between textual and visual representations, indicating
unsatisfactory cross-modal representation alignment; 2) representations of
texts that contain and do not contain hallucinations are entangled, making it
challenging to distinguish them. These two observations inspire us with a
simple yet effective method to mitigate hallucinations. Specifically, we
introduce contrastive learning into MLLMs and use text with hallucination as
hard negative examples, naturally bringing representations of non-hallucinative
text and visual samples closer while pushing way representations of
non-hallucinating and hallucinative text. We evaluate our method quantitatively
and qualitatively, showing its effectiveness in reducing hallucination
occurrences and improving performance across multiple benchmarks. On the
MMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the
baseline MiniGPT-4/LLaVA. Our code is available on
https://github.com/X-PLUG/mPLUG-HalOwl/tree/main/hacl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chaoya Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haiyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mengfan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaxing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qinghao Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models. (arXiv:2312.11562v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, e.g., Large Language Models (LLMs), there is a growing
interest in exploring their abilities in reasoning tasks. In this paper, we
introduce seminal foundation models proposed or adaptable for reasoning,
highlighting the latest advancements in various reasoning tasks, methods, and
benchmarks. We then delve into the potential future directions behind the
emergence of reasoning abilities within foundation models. We also discuss the
relevance of multimodal learning, autonomous agents, and super alignment in the
context of reasoning. By discussing these future research directions, we hope
to inspire researchers in their exploration of this field, stimulate further
advancements in reasoning with foundation models, and contribute to the
development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13440">
<title>MGAug: Multimodal Geometric Augmentation in Latent Spaces of Image Deformations. (arXiv:2312.13440v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.13440</link>
<description rdf:parseType="Literal">&lt;p&gt;Geometric transformations have been widely used to augment the size of
training images. Existing methods often assume a unimodal distribution of the
underlying transformations between images, which limits their power when data
with multimodal distributions occur. In this paper, we propose a novel model,
Multimodal Geometric Augmentation (MGAug), that for the first time generates
augmenting transformations in a multimodal latent space of geometric
deformations. To achieve this, we first develop a deep network that embeds the
learning of latent geometric spaces of diffeomorphic transformations (a.k.a.
diffeomorphisms) in a variational autoencoder (VAE). A mixture of multivariate
Gaussians is formulated in the tangent space of diffeomorphisms and serves as a
prior to approximate the hidden distribution of image transformations. We then
augment the original training dataset by deforming images using randomly
sampled transformations from the learned multimodal latent space of VAE. To
validate the efficiency of our model, we jointly learn the augmentation
strategy with two distinct domain-specific tasks: multi-class classification on
2D synthetic datasets and segmentation on real 3D brain magnetic resonance
images (MRIs). We also compare MGAug with state-of-the-art transformation-based
image augmentation algorithms. Experimental results show that our proposed
approach outperforms all baselines by significantly improved prediction
accuracy. Our code is publicly available at
https://github.com/tonmoy-hossain/MGAug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_T/0/1/0/all/0/1&quot;&gt;Tonmoy Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05925">
<title>CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with Dual Feature Fusion. (arXiv:2401.05925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05925</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a
method for compact 3D-consistent scene segmentation at fast rendering speed
with only RGB images input. Previous NeRF-based segmentation methods have
relied on time-consuming neural scene optimization. While recent 3D Gaussian
Splatting has notably improved speed, existing Gaussian-based segmentation
methods struggle to produce compact masks, especially in zero-shot
segmentation. This issue probably stems from their straightforward assignment
of learnable parameters to each Gaussian, resulting in a lack of robustness
against cross-view inconsistent 2D machine-generated labels. Our method aims to
address this problem by employing Dual Feature Fusion Network as Gaussians&apos;
segmentation field. Specifically, we first optimize 3D Gaussians under RGB
supervision. After Gaussian Locating, DINO features extracted from images are
applied through explicit unprojection, which are further incorporated with
spatial features from the efficient point cloud processing network. Feature
aggregation is utilized to fuse them in a global-to-local strategy for compact
segmentation features. Experimental results show that our model outperforms
baselines on both semantic and panoptic zero-shot segmentation task, meanwhile
consumes less than 10\% inference time compared to NeRF-based methods. Code and
more results will be available at https://David-Dou.github.io/CoSSegGaussians.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_B/0/1/0/all/0/1&quot;&gt;Bin Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yongjia Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zejian Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08655">
<title>SAiD: Speech-driven Blendshape Facial Animation with Diffusion. (arXiv:2401.08655v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08655</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_I/0/1/0/all/0/1&quot;&gt;Inkyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jaewoong Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09627">
<title>SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of Lumbar Spine MRI. (arXiv:2401.09627v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09627</link>
<description rdf:parseType="Literal">&lt;p&gt;Intervertebral disc disease, a prevalent ailment, frequently leads to
intermittent or persistent low back pain, and diagnosing and assessing of this
disease rely on accurate measurement of vertebral bone and intervertebral disc
geometries from lumbar MR images. Deep neural network (DNN) models may assist
clinicians with more efficient image segmentation of individual instances
(disks and vertebrae) of the lumbar spine in an automated way, which is termed
as instance image segmentation. In this work, we proposed SymTC, an innovative
lumbar spine MR image segmentation model that combines the strengths of
Transformer and Convolutional Neural Network (CNN). Specifically, we designed a
parallel dual-path architecture to merge CNN layers and Transformer layers, and
we integrated a novel position embedding into the self-attention module of
Transformer, enhancing the utilization of positional information for more
accurate segmentation. To further improves model performance, we introduced a
new data augmentation technique to create synthetic yet realistic MR image
dataset, named SSMSpine, which is made publicly available. We evaluated our
SymTC and the other 15 existing image segmentation models on our private
in-house dataset and the public SSMSpine dataset, using two metrics, Dice
Similarity Coefficient and 95% Hausdorff Distance. The results show that our
SymTC has the best performance for segmenting vertebral bones and
intervertebral discs in lumbar spine MR images. The SymTC code and SSMSpine
dataset are available at https://github.com/jiasongchen/SymTC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiasong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Linchen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Linhai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Urakov_T/0/1/0/all/0/1&quot;&gt;Timur Urakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Weiyong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Liang Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10475">
<title>CBVS: A Large-Scale Chinese Image-Text Benchmark for Real-World Short Video Search Scenarios. (arXiv:2401.10475v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10475</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Models pre-trained on large-scale image-text datasets have
shown superior performance in downstream tasks such as image retrieval. Most of
the images for pre-training are presented in the form of open domain
common-sense visual elements. Differently, video covers in short video search
scenarios are presented as user-originated contents that provide important
visual summaries of videos. In addition, a portion of the video covers come
with manually designed cover texts that provide semantic complements. In order
to fill in the gaps in short video cover data, we establish the first
large-scale cover-text benchmark for Chinese short video search scenarios.
Specifically, we release two large-scale datasets CBVS-5M/10M to provide short
video covers, and the manual fine-labeling dataset CBVS-20K to provide real
user queries, which serves as an image-text benchmark test in the Chinese short
video search field. To integrate the semantics of cover text in the case of
modality missing, we propose UniCLIP where cover texts play a guiding role
during training, however are not relied upon by inference. Extensive evaluation
on CBVS-20K demonstrates the excellent performance of our proposal. UniCLIP has
been deployed to Tencent&apos;s online video search systems with hundreds of
millions of visits and achieved significant gains. The dataset and code are
available at https://github.com/QQBrowserVideoSearch/CBVS-UniCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiangshuo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cihang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10529">
<title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs&apos; sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs&apos; sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongjin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Feihong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Taixi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1&quot;&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12689">
<title>Energy-based Automated Model Evaluation. (arXiv:2401.12689v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE&apos;s validity, together with its
superiority compared with prior approaches. We also prove MDE&apos;s versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels. Code and data are
available: https://github.com/pengr/Energy_AutoEval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Ru Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Heming Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yawen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zenan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13388">
<title>UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion. (arXiv:2401.13388v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13388</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing text-to-image diffusion models primarily generate images from text
prompts. However, the inherent conciseness of textual descriptions poses
challenges in faithfully synthesizing images with intricate details, such as
specific entities or scenes. This paper presents UNIMO-G, a simple multimodal
conditional diffusion framework that operates on multimodal prompts with
interleaved textual and visual inputs, which demonstrates a unified ability for
both text-driven and subject-driven image generation. UNIMO-G comprises two
core components: a Multimodal Large Language Model (MLLM) for encoding
multimodal prompts, and a conditional denoising diffusion network for
generating images based on the encoded multimodal input. We leverage a
two-stage training strategy to effectively train the framework: firstly
pre-training on large-scale text-image pairs to develop conditional image
generation capabilities, and then instruction tuning with multimodal prompts to
achieve unified image generation proficiency. A well-designed data processing
pipeline involving language grounding and image segmentation is employed to
construct multi-modal prompts. UNIMO-G excels in both text-to-image generation
and zero-shot subject-driven synthesis, and is notably effective in generating
high-fidelity images from complex multimodal prompts involving multiple image
entities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiachen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xinyan Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.13560">
<title>SegMamba: Long-range Sequential Modeling Mamba For 3D Medical Image Segmentation. (arXiv:2401.13560v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.13560</link>
<description rdf:parseType="Literal">&lt;p&gt;The Transformer architecture has shown a remarkable ability in modeling
global relationships. However, it poses a significant computational challenge
when processing high-dimensional medical images. This hinders its development
and widespread adoption in this task. Mamba, as a State Space Model (SSM),
recently emerged as a notable manner for long-range dependencies in sequential
modeling, excelling in natural language processing filed with its remarkable
memory efficiency and computational speed. Inspired by its success, we
introduce SegMamba, a novel 3D medical image \textbf{Seg}mentation
\textbf{Mamba} model, designed to effectively capture long-range dependencies
within whole volume features at every scale. Our SegMamba, in contrast to
Transformer-based methods, excels in whole volume feature modeling from a state
space model standpoint, maintaining superior processing speed, even with volume
features at a resolution of {$64\times 64\times 64$}. Comprehensive experiments
on the BraTS2023 dataset demonstrate the effectiveness and efficiency of our
SegMamba. The code for SegMamba is available at:
https://github.com/ge-xing/SegMamba
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhaohu Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Tian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yijun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>