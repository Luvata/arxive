<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-02T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00707" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00989" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01170" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01314" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1706.03762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.08657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.10401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.02589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.12351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.15701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.11740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.15925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.01255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14220" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.08360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.00102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.01666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00436" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.00707">
<title>Approximate Model-Based Shielding for Safe Reinforcement Learning. (arXiv:2308.00707v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00707</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has shown great potential for solving complex
tasks in a variety of domains. However, applying RL to safety-critical systems
in the real-world is not easy as many algorithms are sample-inefficient and
maximising the standard RL objective comes with no guarantees on worst-case
performance. In this paper we propose approximate model-based shielding (AMBS),
a principled look-ahead shielding algorithm for verifying the performance of
learned RL policies w.r.t. a set of given safety constraints. Our algorithm
differs from other shielding approaches in that it does not require prior
knowledge of the safety-relevant dynamics of the system. We provide a strong
theoretical justification for AMBS and demonstrate superior performance to
other safety-aware approaches on a set of Atari games with state-dependent
safety-labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodall_A/0/1/0/all/0/1&quot;&gt;Alexander W. Goodall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belardinelli_F/0/1/0/all/0/1&quot;&gt;Francesco Belardinelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00708">
<title>VeriGen: A Large Language Model for Verilog Code Generation. (arXiv:2308.00708v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2308.00708</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we explore the capability of Large Language Models (LLMs) to
automate hardware design by generating high-quality Verilog code, a common
language for designing and modeling digital systems. We fine-tune pre-existing
LLMs on Verilog datasets compiled from GitHub and Verilog textbooks. We
evaluate the functional correctness of the generated Verilog code using a
specially designed test suite, featuring a custom problem set and testing
benches. Here, our fine-tuned open-source CodeGen-16B model outperforms the
commercial state-of-the-art GPT-3.5-turbo model with a 1.1% overall increase.
Upon testing with a more diverse and complex problem set, we find that the
fine-tuned model shows competitive performance against state-of-the-art
gpt-3.5-turbo, excelling in certain scenarios. Notably, it demonstrates a 41%
improvement in generating syntactically correct Verilog code across various
problem categories compared to its pre-trained counterpart, highlighting the
potential of smaller, in-house LLMs in hardware design automation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakur_S/0/1/0/all/0/1&quot;&gt;Shailja Thakur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_B/0/1/0/all/0/1&quot;&gt;Baleegh Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_H/0/1/0/all/0/1&quot;&gt;Hammond Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1&quot;&gt;Benjamin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolan_Gavitt_B/0/1/0/all/0/1&quot;&gt;Brendan Dolan-Gavitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karri_R/0/1/0/all/0/1&quot;&gt;Ramesh Karri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00709">
<title>DeepTSF: Codeless machine learning operations for time series forecasting. (arXiv:2308.00709v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00709</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents DeepTSF, a comprehensive machine learning operations
(MLOps) framework aiming to innovate time series forecasting through workflow
automation and codeless modeling. DeepTSF automates key aspects of the ML
lifecycle, making it an ideal tool for data scientists and MLops engineers
engaged in machine learning (ML) and deep learning (DL)-based forecasting.
DeepTSF empowers users with a robust and user-friendly solution, while it is
designed to seamlessly integrate with existing data analysis workflows,
providing enhanced productivity and compatibility. The framework offers a
front-end user interface (UI) suitable for data scientists, as well as other
higher-level stakeholders, enabling comprehensive understanding through
insightful visualizations and evaluation metrics. DeepTSF also prioritizes
security through identity management and access authorization mechanisms. The
application of DeepTSF in real-life use cases of the I-NERGY project has
already proven DeepTSF&apos;s efficacy in DL-based load forecasting, showcasing its
significant added value in the electrical power and energy systems domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelekis_S/0/1/0/all/0/1&quot;&gt;Sotiris Pelekis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karakolis_E/0/1/0/all/0/1&quot;&gt;Evangelos Karakolis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pountridis_T/0/1/0/all/0/1&quot;&gt;Theodosios Pountridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kormpakis_G/0/1/0/all/0/1&quot;&gt;George Kormpakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampropoulos_G/0/1/0/all/0/1&quot;&gt;George Lampropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mouzakits_S/0/1/0/all/0/1&quot;&gt;Spiros Mouzakits&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1&quot;&gt;Dimitris Askounis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00710">
<title>Towards the Visualization of Aggregated Class Activation Maps to Analyse the Global Contribution of Class Features. (arXiv:2308.00710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00710</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models achieve remarkable performance in classification
tasks. However, models with high complexity can not be used in many
risk-sensitive applications unless a comprehensible explanation is presented.
Explainable artificial intelligence (xAI) focuses on the research to explain
the decision-making of AI systems like DL. We extend a recent method of Class
Activation Maps (CAMs) which visualizes the importance of each feature of a
data sample contributing to the classification. In this paper, we aggregate
CAMs from multiple samples to show a global explanation of the classification
for semantically structured data. The aggregation allows the analyst to make
sophisticated assumptions and analyze them with further drill-down
visualizations. Our visual representation for the global CAM illustrates the
impact of each feature with a square glyph containing two indicators. The color
of the square indicates the classification impact of this feature. The size of
the filled square describes the variability of the impact between single
samples. For interesting features that require further analysis, a detailed
view is necessary that provides the distribution of these values. We propose an
interactive histogram to filter samples and refine the CAM to show relevant
samples only. Our approach allows an analyst to detect important features of
high-dimensional data and derive adjustments to the AI model based on our
global explanation visualization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherepanov_I/0/1/0/all/0/1&quot;&gt;Igor Cherepanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sessler_D/0/1/0/all/0/1&quot;&gt;David Sessler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_A/0/1/0/all/0/1&quot;&gt;Alex Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucke_Tieke_H/0/1/0/all/0/1&quot;&gt;Hendrik L&amp;#xfc;cke-Tieke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohlhammer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn Kohlhammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00715">
<title>Automated COVID-19 CT Image Classification using Multi-head Channel Attention in Deep CNN. (arXiv:2308.00715v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00715</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid spread of COVID-19 has necessitated efficient and accurate
diagnostic methods. Computed Tomography (CT) scan images have emerged as a
valuable tool for detecting the disease. In this article, we present a novel
deep learning approach for automated COVID-19 CT scan classification where a
modified Xception model is proposed which incorporates a newly designed channel
attention mechanism and weighted global average pooling to enhance feature
extraction thereby improving classification accuracy. The channel attention
module selectively focuses on informative regions within each channel, enabling
the model to learn discriminative features for COVID-19 detection. Experiments
on a widely used COVID-19 CT scan dataset demonstrate a very good accuracy of
96.99% and show its superiority to other state-of-the-art techniques. This
research can contribute to the ongoing efforts in using artificial intelligence
to combat current and future pandemics and can offer promising and timely
solutions for efficient medical image analysis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Susmita Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chatterjee_A/0/1/0/all/0/1&quot;&gt;Abhiroop Chatterjee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00718">
<title>Beam Detection Based on Machine Learning Algorithms. (arXiv:2308.00718v1 [physics.data-an])</title>
<link>http://arxiv.org/abs/2308.00718</link>
<description rdf:parseType="Literal">&lt;p&gt;The positions of free electron laser beams on screens are precisely
determined by a sequence of machine learning models. Transfer training is
conducted in a self-constructed convolutional neural network based on VGG16
model. Output of intermediate layers are passed as features to a support vector
regression model. With this sequence, 85.8% correct prediction is achieved on
test data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yin_Q/0/1/0/all/0/1&quot;&gt;Qing Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00720">
<title>Divergence of the ADAM algorithm with fixed-stepsize: a (very) simple example. (arXiv:2308.00720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00720</link>
<description rdf:parseType="Literal">&lt;p&gt;A very simple unidimensional function with Lipschitz continuous gradient is
constructed such that the ADAM algorithm with constant stepsize, started from
the origin, diverges when applied to minimize this function in the absence of
noise on the gradient. Divergence occurs irrespective of the choice of the
method parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toint_P/0/1/0/all/0/1&quot;&gt;Ph. L. Toint&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00721">
<title>A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00721</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of big data, the issue of data quality has become increasingly
prominent. One of the main challenges is the problem of duplicate data, which
can arise from repeated entry or the merging of multiple data sources. These
&quot;dirty data&quot; problems can significantly limit the effective application of big
data. To address the issue of data deduplication, we propose a pre-trained
deduplication model based on active learning, which is the first work that
utilizes active learning to address the problem of deduplication at the
semantic level. The model is built on a pre-trained Transformer and fine-tuned
to solve the deduplication problem as a sequence to classification task, which
firstly integrate the transformer with active learning into an end-to-end
architecture to select the most valuable data for deduplication model training,
and also firstly employ the R-Drop method to perform data augmentation on each
round of labeled data, which can reduce the cost of manual labeling and improve
the model&apos;s performance. Experimental results demonstrate that our proposed
model outperforms previous state-of-the-art (SOTA) for deduplicated data
identification, achieving up to a 28% improvement in Recall score on benchmark
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Shengdong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_F/0/1/0/all/0/1&quot;&gt;Fengmao Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hongtao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianrui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00725">
<title>Latent-Shift: Gradient of Entropy Helps Neural Codecs. (arXiv:2308.00725v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00725</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end image/video codecs are getting competitive compared to traditional
compression techniques that have been developed through decades of manual
engineering efforts. These trainable codecs have many advantages over
traditional techniques such as easy adaptation on perceptual distortion metrics
and high performance on specific domains thanks to their learning ability.
However, state of the art neural codecs does not take advantage of the
existence of gradient of entropy in decoding device. In this paper, we
theoretically show that gradient of entropy (available at decoder side) is
correlated with the gradient of the reconstruction error (which is not
available at decoder side). We then demonstrate experimentally that this
gradient can be used on various compression methods, leading to a $1-2\%$ rate
savings for the same quality. Our method is orthogonal to other improvements
and brings independent rate savings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Balcilar_M/0/1/0/all/0/1&quot;&gt;Muhammet Balcilar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Damodaran_B/0/1/0/all/0/1&quot;&gt;Bharath Bhushan Damodaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Naser_K/0/1/0/all/0/1&quot;&gt;Karam Naser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Galpin_F/0/1/0/all/0/1&quot;&gt;Franck Galpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hellier_P/0/1/0/all/0/1&quot;&gt;Pierre Hellier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00733">
<title>Mapping Computer Science Research: Trends, Influences, and Predictions. (arXiv:2308.00733v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2308.00733</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the current trending research areas in the field of
Computer Science (CS) and investigates the factors contributing to their
emergence. Leveraging a comprehensive dataset comprising papers, citations, and
funding information, we employ advanced machine learning techniques, including
Decision Tree and Logistic Regression models, to predict trending research
areas. Our analysis reveals that the number of references cited in research
papers (Reference Count) plays a pivotal role in determining trending research
areas making reference counts the most relevant factor that drives trend in the
CS field. Additionally, the influence of NSF grants and patents on trending
topics has increased over time. The Logistic Regression model outperforms the
Decision Tree model in predicting trends, exhibiting higher accuracy,
precision, recall, and F1 score. By surpassing a random guess baseline, our
data-driven approach demonstrates higher accuracy and efficacy in identifying
trending research areas. The results offer valuable insights into the trending
research areas, providing researchers and institutions with a data-driven
foundation for decision-making and future research direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almutairi_M/0/1/0/all/0/1&quot;&gt;Mohammed Almutairi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguine_O/0/1/0/all/0/1&quot;&gt;Ozioma Collins Oguine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00755">
<title>The Bias Amplification Paradox in Text-to-Image Generation. (arXiv:2308.00755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00755</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias amplification is a phenomenon in which models increase imbalances
present in the training data. In this paper, we study bias amplification in the
text-to-image domain using Stable Diffusion by comparing gender ratios in
training vs. generated images. We find that the model appears to amplify
gender-occupation biases found in the training data (LAION). However, we
discover that amplification can largely be attributed to discrepancies between
training captions and model prompts. For example, an inherent difference is
that captions from the training data often contain explicit gender information
while the prompts we use do not, which leads to a distribution shift and
consequently impacts bias measures. Once we account for various distributional
differences between texts used for training and generation, we observe that
amplification decreases considerably. Our findings illustrate the challenges of
comparing biases in models and the data they are trained on, and highlight
confounding factors that contribute to bias amplification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1&quot;&gt;Preethi Seshadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sameer Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00762">
<title>Self-Supervised Contrastive BERT Fine-tuning for Fusion-based Reviewed-Item Retrieval. (arXiv:2308.00762v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.00762</link>
<description rdf:parseType="Literal">&lt;p&gt;As natural language interfaces enable users to express increasingly complex
natural language queries, there is a parallel explosion of user review content
that can allow users to better find items such as restaurants, books, or movies
that match these expressive queries. While Neural Information Retrieval (IR)
methods have provided state-of-the-art results for matching queries to
documents, they have not been extended to the task of Reviewed-Item Retrieval
(RIR), where query-review scores must be aggregated (or fused) into item-level
scores for ranking. In the absence of labeled RIR datasets, we extend Neural IR
methodology to RIR by leveraging self-supervised methods for contrastive
learning of BERT embeddings for both queries and reviews. Specifically,
contrastive learning requires a choice of positive and negative samples, where
the unique two-level structure of our item-review data combined with meta-data
affords us a rich structure for the selection of these samples. For contrastive
learning in a Late Fusion scenario, we investigate the use of positive review
samples from the same item and/or with the same rating, selection of hard
positive samples by choosing the least similar reviews from the same anchor
item, and selection of hard negative samples by choosing the most similar
reviews from different items. We also explore anchor sub-sampling and
augmenting with meta-data. For a more end-to-end Early Fusion approach, we
introduce contrastive item embedding learning to fuse reviews into single item
embeddings. Experimental results show that Late Fusion contrastive learning for
Neural RIR outperforms all other contrastive IR configurations, Neural IR, and
sparse retrieval baselines, thus demonstrating the power of exploiting the
two-level structure in Neural RIR approaches as well as the importance of
preserving the nuance of individual review content via Late Fusion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pour_M/0/1/0/all/0/1&quot;&gt;Mohammad Mahdi Abdollah Pour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinneya_P/0/1/0/all/0/1&quot;&gt;Parsa Farinneya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toroghi_A/0/1/0/all/0/1&quot;&gt;Armin Toroghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korikov_A/0/1/0/all/0/1&quot;&gt;Anton Korikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesaranghader_A/0/1/0/all/0/1&quot;&gt;Ali Pesaranghader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajed_T/0/1/0/all/0/1&quot;&gt;Touqir Sajed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_M/0/1/0/all/0/1&quot;&gt;Manasa Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavrin_B/0/1/0/all/0/1&quot;&gt;Borislav Mavrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1&quot;&gt;Scott Sanner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00770">
<title>DYMOND: DYnamic MOtif-NoDes Network Generative Model. (arXiv:2308.00770v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2308.00770</link>
<description rdf:parseType="Literal">&lt;p&gt;Motifs, which have been established as building blocks for network structure,
move beyond pair-wise connections to capture longer-range correlations in
connections and activity. In spite of this, there are few generative graph
models that consider higher-order network structures and even fewer that focus
on using motifs in models of dynamic graphs. Most existing generative models
for temporal graphs strictly grow the networks via edge addition, and the
models are evaluated using static graph structure metrics -- which do not
adequately capture the temporal behavior of the network. To address these
issues, in this work we propose DYnamic MOtif-NoDes (DYMOND) -- a generative
model that considers (i) the dynamic changes in overall graph structure using
temporal motif activity and (ii) the roles nodes play in motifs (e.g., one node
plays the hub role in a wedge, while the remaining two act as spokes). We
compare DYMOND to three dynamic graph generative model baselines on real-world
networks and show that DYMOND performs better at generating graph structure and
node behavior similar to the observed network. We also propose a new
methodology to adapt graph structure metrics to better evaluate the temporal
aspect of the network. These metrics take into account the changes in overall
graph structure and the individual nodes&apos; behavior over time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeno_G/0/1/0/all/0/1&quot;&gt;Giselle Zeno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fond_T/0/1/0/all/0/1&quot;&gt;Timothy La Fond&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neville_J/0/1/0/all/0/1&quot;&gt;Jennifer Neville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00787">
<title>Evaluating Spiking Neural Network On Neuromorphic Platform For Human Activity Recognition. (arXiv:2308.00787v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2308.00787</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy efficiency and low latency are crucial requirements for designing
wearable AI-empowered human activity recognition systems, due to the hard
constraints of battery operations and closed-loop feedback. While neural
network models have been extensively compressed to match the stringent edge
requirements, spiking neural networks and event-based sensing are recently
emerging as promising solutions to further improve performance due to their
inherent energy efficiency and capacity to process spatiotemporal data in very
low latency. This work aims to evaluate the effectiveness of spiking neural
networks on neuromorphic processors in human activity recognition for wearable
applications. The case of workout recognition with wrist-worn wearable motion
sensors is used as a study. A multi-threshold delta modulation approach is
utilized for encoding the input sensor data into spike trains to move the
pipeline into the event-based approach. The spikes trains are then fed to a
spiking neural network with direct-event training, and the trained model is
deployed on the research neuromorphic platform from Intel, Loihi, to evaluate
energy and latency efficiency. Test results show that the spike-based workouts
recognition system can achieve a comparable accuracy (87.5\%) comparable to the
popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional
neural network ( 88.1\%) while achieving two times better energy-delay product
(0.66 \si{\micro\joule\second} vs. 1.32 \si{\micro\joule\second}).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00788">
<title>An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning. (arXiv:2308.00788v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00788</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, bi-level optimization (BLO) has taken center stage in some very
exciting developments in the area of signal processing (SP) and machine
learning (ML). Roughly speaking, BLO is a classical optimization problem that
involves two levels of hierarchy (i.e., upper and lower levels), wherein
obtaining the solution to the upper-level problem requires solving the
lower-level one. BLO has become popular largely because it is powerful in
modeling problems in SP and ML, among others, that involve optimizing nested
objective functions. Prominent applications of BLO range from resource
allocation for wireless systems to adversarial machine learning. In this work,
we focus on a class of tractable BLO problems that often appear in SP and ML
applications. We provide an overview of some basic concepts of this class of
BLO problems, such as their optimality conditions, standard algorithms
(including their optimization principles and practical implementations), as
well as how they can be leveraged to obtain state-of-the-art results for a
number of key SP and ML applications. Further, we discuss some recent advances
in BLO theory, its implications for applications, and point out some
limitations of the state-of-the-art that require significant future research
efforts. Overall, we hope that this article can serve to accelerate the
adoption of BLO as a generic tool to model, analyze, and innovate on a wide
array of emerging SP applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanduri_P/0/1/0/all/0/1&quot;&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsaknakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Tsaknakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuguang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00824">
<title>An Exact Kernel Equivalence for Finite Classification Models. (arXiv:2308.00824v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00824</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the equivalence between neural networks and kernel methods by
deriving the first exact representation of any finite-size parametric
classification model trained with gradient descent as a kernel machine. We
compare our exact representation to the well-known Neural Tangent Kernel (NTK)
and discuss approximation error relative to the NTK and other non-exact path
kernel formulations. We experimentally demonstrate that the kernel can be
computed for realistic networks up to machine precision. We use this exact
kernel to show that our theoretical contribution can provide useful insights
into the predictions made by neural networks, particularly the way in which
they generalize.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bell_B/0/1/0/all/0/1&quot;&gt;Brian Bell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geyer_M/0/1/0/all/0/1&quot;&gt;Michael Geyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_J/0/1/0/all/0/1&quot;&gt;Juston Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glickenstein_D/0/1/0/all/0/1&quot;&gt;David Glickenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_A/0/1/0/all/0/1&quot;&gt;Amanda Fernandez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00852">
<title>CASSINI: Network-Aware Job Scheduling in Machine Learning Clusters. (arXiv:2308.00852v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2308.00852</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CASSINI, a network-aware job scheduler for machine learning (ML)
clusters. CASSINI introduces a novel geometric abstraction to consider the
communication pattern of different jobs while placing them on network links. To
do so, CASSINI uses an affinity graph that finds a series of time-shift values
to adjust the communication phases of a subset of jobs, such that the
communication patterns of jobs sharing the same network link are interleaved
with each other. Experiments with 13 common ML models on a 24-server testbed
demonstrate that compared to the state-of-the-art ML schedulers, CASSINI
improves the average and tail completion time of jobs by up to 1.6x and 2.5x,
respectively. Moreover, we show that CASSINI reduces the number of ECN marked
packets in the cluster by up to 33x.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1&quot;&gt;Sudarsanan Rajasekaran&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghobadi_M/0/1/0/all/0/1&quot;&gt;Manya Ghobadi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akella_A/0/1/0/all/0/1&quot;&gt;Aditya Akella&lt;/a&gt; (2) ((1) Massachusetts Institute of Technology, (2) UT Austin)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00855">
<title>A Comprehensive Study of Groundbreaking Machine Learning Research: Analyzing Highly Cited and Impactful Publications across Six Decades. (arXiv:2308.00855v1 [cs.DL])</title>
<link>http://arxiv.org/abs/2308.00855</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) has emerged as a prominent field of research in
computer science and other related fields, thereby driving advancements in
other domains of interest. As the field continues to evolve, it is crucial to
understand the landscape of highly cited publications to identify key trends,
influential authors, and significant contributions made thus far. In this
paper, we present a comprehensive bibliometric analysis of highly cited ML
publications. We collected a dataset consisting of the top-cited papers from
reputable ML conferences and journals, covering a period of several years from
1959 to 2022. We employed various bibliometric techniques to analyze the data,
including citation analysis, co-authorship analysis, keyword analysis, and
publication trends. Our findings reveal the most influential papers, highly
cited authors, and collaborative networks within the machine learning
community. We identify popular research themes and uncover emerging topics that
have recently gained significant attention. Furthermore, we examine the
geographical distribution of highly cited publications, highlighting the
dominance of certain countries in ML research. By shedding light on the
landscape of highly cited ML publications, our study provides valuable insights
for researchers, policymakers, and practitioners seeking to understand the key
developments and trends in this rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezugwu_A/0/1/0/all/0/1&quot;&gt;Absalom E. Ezugwu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greeff_J/0/1/0/all/0/1&quot;&gt;Japie Greeff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1&quot;&gt;Yuh-Shan Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00856">
<title>Differential Privacy for Adaptive Weight Aggregation in Federated Tumor Segmentation. (arXiv:2308.00856v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00856</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a distributed machine learning approach that
safeguards privacy by creating an impartial global model while respecting the
privacy of individual client data. However, the conventional FL method can
introduce security risks when dealing with diverse client data, potentially
compromising privacy and data integrity. To address these challenges, we
present a differential privacy (DP) federated deep learning framework in
medical image segmentation. In this paper, we extend our similarity weight
aggregation (SimAgg) method to DP-SimAgg algorithm, a differentially private
similarity-weighted aggregation algorithm for brain tumor segmentation in
multi-modal magnetic resonance imaging (MRI). Our DP-SimAgg method not only
enhances model segmentation capabilities but also provides an additional layer
of privacy preservation. Extensive benchmarking and evaluation of our
framework, with computational performance as a key consideration, demonstrate
that DP-SimAgg enables accurate and robust brain tumor segmentation while
minimizing communication costs during model training. This advancement is
crucial for preserving the privacy of medical image data and safeguarding
sensitive information. In conclusion, adding a differential privacy layer in
the global weight aggregation phase of the federated brain tumor segmentation
provides a promising solution to privacy concerns without compromising
segmentation model efficacy. By leveraging DP, we ensure the protection of
client data against adversarial attacks and malicious participants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;Muhammad Irfan Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alhoniemi_E/0/1/0/all/0/1&quot;&gt;Esa Alhoniemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontio_E/0/1/0/all/0/1&quot;&gt;Elina Kontio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Suleiman A. Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafaritadi_M/0/1/0/all/0/1&quot;&gt;Mojtaba Jafaritadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00858">
<title>Understanding Activation Patterns in Artificial Neural Networks by Exploring Stochastic Processes. (arXiv:2308.00858v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00858</link>
<description rdf:parseType="Literal">&lt;p&gt;To gain a deeper understanding of the behavior and learning dynamics of
(deep) artificial neural networks, it is valuable to employ mathematical
abstractions and models. These tools provide a simplified perspective on
network performance and facilitate systematic investigations through
simulations. In this paper, we propose utilizing the framework of stochastic
processes, which has been underutilized thus far.
&lt;/p&gt;
&lt;p&gt;Our approach models activation patterns of thresholded nodes in (deep)
artificial neural networks as stochastic processes. We focus solely on
activation frequency, leveraging neuroscience techniques used for real neuron
spike trains. During a classification task, we extract spiking activity and use
an arrival process following the Poisson distribution.
&lt;/p&gt;
&lt;p&gt;We examine observed data from various artificial neural networks in image
recognition tasks, fitting the proposed model&apos;s assumptions. Through this, we
derive parameters describing activation patterns in each network. Our analysis
covers randomly initialized, generalizing, and memorizing networks, revealing
consistent differences across architectures and training sets.
&lt;/p&gt;
&lt;p&gt;Calculating Mean Firing Rate, Mean Fano Factor, and Variances, we find stable
indicators of memorization during learning, providing valuable insights into
network behavior. The proposed model shows promise in describing activation
patterns and could serve as a general framework for future investigations. It
has potential applications in theoretical simulations, pruning, and transfer
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmler_S/0/1/0/all/0/1&quot;&gt;Stephan Johann Lehmler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saif_ur_Rehman_M/0/1/0/all/0/1&quot;&gt;Muhammad Saif-ur-Rehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glasmachers_T/0/1/0/all/0/1&quot;&gt;Tobias Glasmachers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iossifidis_I/0/1/0/all/0/1&quot;&gt;Ioannis Iossifidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00864">
<title>PeRP: Personalized Residual Policies For Congestion Mitigation Through Co-operative Advisory Systems. (arXiv:2308.00864v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00864</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent driving systems can be used to mitigate congestion through simple
actions, thus improving many socioeconomic factors such as commute time and gas
costs. However, these systems assume precise control over autonomous vehicle
fleets, and are hence limited in practice as they fail to account for
uncertainty in human behavior. Piecewise Constant (PC) Policies address these
issues by structurally modeling the likeness of human driving to reduce traffic
congestion in dense scenarios to provide action advice to be followed by human
drivers. However, PC policies assume that all drivers behave similarly. To this
end, we develop a co-operative advisory system based on PC policies with a
novel driver trait conditioned Personalized Residual Policy, PeRP. PeRP advises
drivers to behave in ways that mitigate traffic congestion. We first infer the
driver&apos;s intrinsic traits on how they follow instructions in an unsupervised
manner with a variational autoencoder. Then, a policy conditioned on the
inferred trait adapts the action of the PC policy to provide the driver with a
personalized recommendation. Our system is trained in simulation with novel
driver modeling of instruction adherence. We show that our approach
successfully mitigates congestion while adapting to different driver behaviors,
with 4 to 22% improvement in average speed over baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_A/0/1/0/all/0/1&quot;&gt;Aamir Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_N/0/1/0/all/0/1&quot;&gt;Neeloy Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haonan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Jung-Hoon Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cathy Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1&quot;&gt;Katherine Driggs-Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00886">
<title>Enhancing Machine Learning Performance with Continuous In-Session Ground Truth Scores: Pilot Study on Objective Skeletal Muscle Pain Intensity Prediction. (arXiv:2308.00886v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00886</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) models trained on subjective self-report scores
struggle to objectively classify pain accurately due to the significant
variance between real-time pain experiences and recorded scores afterwards.
This study developed two devices for acquisition of real-time, continuous
in-session pain scores and gathering of ANS-modulated endodermal activity
(EDA).The experiment recruited N = 24 subjects who underwent a post-exercise
circulatory occlusion (PECO) with stretch, inducing discomfort. Subject data
were stored in a custom pain platform, facilitating extraction of time-domain
EDA features and in-session ground truth scores. Moreover, post-experiment
visual analog scale (VAS) scores were collected from each subject. Machine
learning models, namely Multi-layer Perceptron (MLP) and Random Forest (RF),
were trained using corresponding objective EDA features combined with
in-session scores and post-session scores, respectively. Over a 10-fold
cross-validation, the macro-averaged geometric mean score revealed MLP and RF
models trained with objective EDA features and in-session scores achieved
superior performance (75.9% and 78.3%) compared to models trained with
post-session scores (70.3% and 74.6%) respectively. This pioneering study
demonstrates that using continuous in-session ground truth scores significantly
enhances ML performance in pain intensity characterization, overcoming ground
truth sparsity-related issues, data imbalance, and high variance. This study
informs future objective-based ML pain system training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faremi_B/0/1/0/all/0/1&quot;&gt;Boluwatife E. Faremi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stavres_J/0/1/0/all/0/1&quot;&gt;Jonathon Stavres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_N/0/1/0/all/0/1&quot;&gt;Nuno Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaoxian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_A/0/1/0/all/0/1&quot;&gt;Andrew H. Sung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00887">
<title>Factor Graph Neural Networks. (arXiv:2308.00887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, we have witnessed a surge of Graph Neural Networks (GNNs),
most of which can learn powerful representations in an end-to-end fashion with
great success in many real-world applications. They have resemblance to
Probabilistic Graphical Models (PGMs), but break free from some limitations of
PGMs. By aiming to provide expressive methods for representation learning
instead of computing marginals or most likely configurations, GNNs provide
flexibility in the choice of information flowing rules while maintaining good
performance. Despite their success and inspirations, they lack efficient ways
to represent and learn higher-order relations among variables/nodes. More
expressive higher-order GNNs which operate on k-tuples of nodes need increased
computational resources in order to process higher-order tensors. We propose
Factor Graph Neural Networks (FGNNs) to effectively capture higher-order
relations for inference and learning. To do so, we first derive an efficient
approximate Sum-Product loopy belief propagation inference algorithm for
discrete higher-order PGMs. We then neuralize the novel message passing scheme
into a Factor Graph Neural Network (FGNN) module by allowing richer
representations of the message update rules; this facilitates both efficient
inference and powerful end-to-end learning. We further show that with a
suitable choice of message aggregation operators, our FGNN is also able to
represent Max-Product belief propagation, providing a single family of
architecture that can represent both Max and Sum-Product loopy belief
propagation. Our extensive experimental evaluation on synthetic as well as real
datasets demonstrates the potential of the proposed model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dupty_M/0/1/0/all/0/1&quot;&gt;Mohammed Haroon Dupty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Javen Qinfeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wee Sun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00890">
<title>Tango: rethinking quantization for graph neural network training on GPUs. (arXiv:2308.00890v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00890</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are becoming increasingly popular due to their
superior performance in critical graph-related tasks. While quantization is
widely used to accelerate GNN computation, quantized training faces
unprecedented challenges. Current quantized GNN training systems often have
longer training times than their full-precision counterparts for two reasons:
(i) addressing the accuracy challenge leads to excessive overhead, and (ii) the
optimization potential exposed by quantization is not adequately leveraged.
This paper introduces Tango which re-thinks quantization challenges and
opportunities for graph neural network training on GPUs with three
contributions: Firstly, we introduce efficient rules to maintain accuracy
during quantized GNN training. Secondly, we design and implement
quantization-aware primitives and inter-primitive optimizations that can speed
up GNN training. Finally, we integrate Tango with the popular Deep Graph
Library (DGL) system and demonstrate its superior performance over
state-of-the-art approaches on various GNN models and datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Da Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huan_C/0/1/0/all/0/1&quot;&gt;Chengying Huan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yuede Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00894">
<title>User-Controllable Recommendation via Counterfactual Retrospective and Prospective Explanations. (arXiv:2308.00894v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.00894</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern recommender systems utilize users&apos; historical behaviors to generate
personalized recommendations. However, these systems often lack user
controllability, leading to diminished user satisfaction and trust in the
systems. Acknowledging the recent advancements in explainable recommender
systems that enhance users&apos; understanding of recommendation mechanisms, we
propose leveraging these advancements to improve user controllability. In this
paper, we present a user-controllable recommender system that seamlessly
integrates explainability and controllability within a unified framework. By
providing both retrospective and prospective explanations through
counterfactual reasoning, users can customize their control over the system by
interacting with these explanations.
&lt;/p&gt;
&lt;p&gt;Furthermore, we introduce and assess two attributes of controllability in
recommendation systems: the complexity of controllability and the accuracy of
controllability. Experimental evaluations on MovieLens and Yelp datasets
substantiate the effectiveness of our proposed framework. Additionally, our
experiments demonstrate that offering users control options can potentially
enhance recommendation accuracy in the future. Source code and data are
available at \url{https://github.com/chrisjtan/ucr}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yinglong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00904">
<title>VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00904</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference plays a vital role in diverse domains like epidemiology,
healthcare, and economics. De-confounding and counterfactual prediction in
observational data has emerged as a prominent concern in causal inference
research. While existing models tackle observed confounders, the presence of
unobserved confounders remains a significant challenge, distorting causal
inference and impacting counterfactual outcome accuracy. To address this, we
propose a novel variational learning model of unobserved confounders for
counterfactual inference (VLUCI), which generates the posterior distribution of
unobserved confounders. VLUCI relaxes the unconfoundedness assumption often
overlooked by most causal inference methods. By disentangling observed and
unobserved confounders, VLUCI constructs a doubly variational inference model
to approximate the distribution of unobserved confounders, which are used for
inferring more accurate counterfactual outcomes. Extensive experiments on
synthetic and semi-synthetic datasets demonstrate VLUCI&apos;s superior performance
in inferring unobserved confounders. It is compatible with state-of-the-art
counterfactual inference models, significantly improving inference accuracy at
both group and individual levels. Additionally, VLUCI provides confidence
intervals for counterfactual outcomes, aiding decision-making in risk-sensitive
domains. We further clarify the considerations when applying VLUCI to cases
where unobserved confounders don&apos;t strictly conform to our model assumptions
using the public IHDP dataset as an example, highlighting the practical
advantages of VLUCI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yonghe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huiyan Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00920">
<title>Virtual histological staining of unlabeled autopsy tissue. (arXiv:2308.00920v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2308.00920</link>
<description rdf:parseType="Literal">&lt;p&gt;Histological examination is a crucial step in an autopsy; however, the
traditional histochemical staining of post-mortem samples faces multiple
challenges, including the inferior staining quality due to autolysis caused by
delayed fixation of cadaver tissue, as well as the resource-intensive nature of
chemical staining procedures covering large tissue areas, which demand
substantial labor, cost, and time. These challenges can become more pronounced
during global health crises when the availability of histopathology services is
limited, resulting in further delays in tissue fixation and more severe
staining artifacts. Here, we report the first demonstration of virtual staining
of autopsy tissue and show that a trained neural network can rapidly transform
autofluorescence images of label-free autopsy tissue sections into brightfield
equivalent images that match hematoxylin and eosin (H&amp;amp;E) stained versions of
the same samples, eliminating autolysis-induced severe staining artifacts
inherent in traditional histochemical staining of autopsied tissue. Our virtual
H&amp;amp;E model was trained using &amp;gt;0.7 TB of image data and a data-efficient
collaboration scheme that integrates the virtual staining network with an image
registration network. The trained model effectively accentuated nuclear,
cytoplasmic and extracellular features in new autopsy tissue samples that
experienced severe autolysis, such as COVID-19 samples never seen before, where
the traditional histochemical staining failed to provide consistent staining
quality. This virtual autopsy staining technique can also be extended to
necrotic tissue, and can rapidly and cost-effectively generate artifact-free
H&amp;amp;E stains despite severe autolysis and cell death, also reducing labor, cost
and infrastructure requirements associated with the standard histochemical
staining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuzhu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pillar_N/0/1/0/all/0/1&quot;&gt;Nir Pillar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tairan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Songyu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guangdong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Haan_K/0/1/0/all/0/1&quot;&gt;Kevin de Haan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Luzhe Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hamidi_S/0/1/0/all/0/1&quot;&gt;Sepehr Hamidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Urisman_A/0/1/0/all/0/1&quot;&gt;Anatoly Urisman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Haran_T/0/1/0/all/0/1&quot;&gt;Tal Keidar Haran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wallace_W/0/1/0/all/0/1&quot;&gt;William Dean Wallace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zuckerman_J/0/1/0/all/0/1&quot;&gt;Jonathan E. Zuckerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1&quot;&gt;Aydogan Ozcan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00924">
<title>Continual Domain Adaptation on Aerial Images under Gradually Degrading Weather. (arXiv:2308.00924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00924</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation (DA) strives to mitigate the domain gap between the source
domain where a model is trained, and the target domain where the model is
deployed. When a deep learning model is deployed on an aerial platform, it may
face gradually degrading weather conditions during operation, leading to
widening domain gaps between the training data and the encountered evaluation
data. We synthesize two such gradually worsening weather conditions on real
images from two existing aerial imagery datasets, generating a total of four
benchmark datasets. Under the continual, or test-time adaptation setting, we
evaluate three DA models on our datasets: a baseline standard DA model and two
continual DA models. In such setting, the models can access only one small
portion, or one batch of the target data at a time, and adaptation takes place
continually, and over only one epoch of the data. The combination of the
constraints of continual adaptation, and gradually deteriorating weather
conditions provide the practical DA scenario for aerial deployment. Among the
evaluated models, we consider both convolutional and transformer architectures
for comparison. We discover stability issues during adaptation for existing
buffer-fed continual DA methods, and offer gradient normalization as a simple
solution to curb training instability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_C/0/1/0/all/0/1&quot;&gt;Chowdhury Sadman Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1&quot;&gt;Andreas Savakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00928">
<title>QUANT: A Minimalist Interval Method for Time Series Classification. (arXiv:2308.00928v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00928</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that it is possible to achieve the same accuracy, on average, as the
most accurate existing interval methods for time series classification on a
standard set of benchmark datasets using a single type of feature (quantiles),
fixed intervals, and an &apos;off the shelf&apos; classifier. This distillation of
interval-based approaches represents a fast and accurate method for time series
classification, achieving state-of-the-art accuracy on the expanded set of 142
datasets in the UCR archive with a total compute time (training and inference)
of less than 15 minutes using a single CPU core.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dempster_A/0/1/0/all/0/1&quot;&gt;Angus Dempster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1&quot;&gt;Daniel F. Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00942">
<title>On the use of deep learning for phase recovery. (arXiv:2308.00942v1 [physics.optics])</title>
<link>http://arxiv.org/abs/2308.00942</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase recovery (PR) refers to calculating the phase of the light field from
its intensity measurements. As exemplified from quantitative phase imaging and
coherent diffraction imaging to adaptive optics, PR is essential for
reconstructing the refractive index distribution or topography of an object and
correcting the aberration of an imaging system. In recent years, deep learning
(DL), often implemented through deep neural networks, has provided
unprecedented support for computational imaging, leading to more efficient
solutions for various PR problems. In this review, we first briefly introduce
conventional methods for PR. Then, we review how DL provides support for PR
from the following three stages, namely, pre-processing, in-processing, and
post-processing. We also review how DL is used in phase image processing.
Finally, we summarize the work in DL for PR and outlook on how to better use DL
to improve the reliability and efficiency in PR. Furthermore, we present a
live-updating resource (https://github.com/kqwang/phase-recovery) for readers
to learn more about PR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Li Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chutian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhenbo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guangyuan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dou_J/0/1/0/all/0/1&quot;&gt;Jiazhen Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Di_J/0/1/0/all/0/1&quot;&gt;Jianglei Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Barbastathis_G/0/1/0/all/0/1&quot;&gt;George Barbastathis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Renjie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianlin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lam_E/0/1/0/all/0/1&quot;&gt;Edmund Y. Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00947">
<title>Decomposing and Coupling Saliency Map for Lesion Segmentation in Ultrasound Images. (arXiv:2308.00947v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.00947</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex scenario of ultrasound image, in which adjacent tissues (i.e.,
background) share similar intensity with and even contain richer texture
patterns than lesion region (i.e., foreground), brings a unique challenge for
accurate lesion segmentation. This work presents a decomposition-coupling
network, called DC-Net, to deal with this challenge in a
(foreground-background) saliency map disentanglement-fusion manner. The DC-Net
consists of decomposition and coupling subnets, and the former preliminarily
disentangles original image into foreground and background saliency maps,
followed by the latter for accurate segmentation under the assistance of
saliency prior fusion. The coupling subnet involves three aspects of fusion
strategies, including: 1) regional feature aggregation (via differentiable
context pooling operator in the encoder) to adaptively preserve local
contextual details with the larger receptive field during dimension reduction;
2) relation-aware representation fusion (via cross-correlation fusion module in
the decoder) to efficiently fuse low-level visual characteristics and
high-level semantic features during resolution restoration; 3) dependency-aware
prior incorporation (via coupler) to reinforce foreground-salient
representation with the complementary information derived from background
representation. Furthermore, a harmonic loss function is introduced to
encourage the network to focus more attention on low-confidence and hard
samples. The proposed method is evaluated on two ultrasound lesion segmentation
tasks, which demonstrates the remarkable performance improvement over existing
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yixiao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Q/0/1/0/all/0/1&quot;&gt;Qianjin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shengzhou Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00951">
<title>From Sparse to Soft Mixtures of Experts. (arXiv:2308.00951v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00951</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse mixture of expert architectures (MoEs) scale model capacity without
large increases in training or inference costs. Despite their success, MoEs
suffer from a number of issues: training instability, token dropping, inability
to scale the number of experts, or ineffective finetuning. In this work, we
proposeSoft MoE, a fully-differentiable sparse Transformer that addresses these
challenges, while maintaining the benefits of MoEs. Soft MoE performs an
implicit soft assignment by passing different weighted combinations of all
input tokens to each expert. As in other MoE works, experts in Soft MoE only
process a subset of the (combined) tokens, enabling larger model capacity at
lower inference cost. In the context of visual recognition, Soft MoE greatly
outperforms standard Transformers (ViTs) and popular MoE variants (Tokens
Choice and Experts Choice). For example, Soft MoE-Base/16 requires 10.5x lower
inference cost (5.7x lower wall-clock time) than ViT-Huge/14 while matching its
performance after similar training. Soft MoE also scales well: Soft MoE Huge/14
with 128 experts in 16 MoE layers has over 40x more parameters than ViT
Huge/14, while inference time cost grows by only 2%, and it performs
substantially better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riquelme_C/0/1/0/all/0/1&quot;&gt;Carlos Riquelme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00956">
<title>Curriculum Guided Domain Adaptation in the Dark. (arXiv:2308.00956v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00956</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the rising concerns of privacy and security, domain adaptation in
the dark aims to adapt a black-box source trained model to an unlabeled target
domain without access to any source data or source model parameters. The need
for domain adaptation of black-box predictors becomes even more pronounced to
protect intellectual property as deep learning based solutions are becoming
increasingly commercialized. Current methods distill noisy predictions on the
target data obtained from the source model to the target model, and/or separate
clean/noisy target samples before adapting using traditional noisy label
learning algorithms. However, these methods do not utilize the easy-to-hard
learning nature of the clean/noisy data splits. Also, none of the existing
methods are end-to-end, and require a separate fine-tuning stage and an initial
warmup stage. In this work, we present Curriculum Adaptation for Black-Box
(CABB) which provides a curriculum guided adaptation approach to gradually
train the target model, first on target data with high confidence (clean)
labels, and later on target data with noisy labels. CABB utilizes
Jensen-Shannon divergence as a better criterion for clean-noisy sample
separation, compared to the traditional criterion of cross entropy loss. Our
method utilizes co-training of a dual-branch network to suppress error
accumulation resulting from confirmation bias. The proposed approach is
end-to-end trainable and does not require any extra finetuning stage, unlike
existing methods. Empirical results on standard domain adaptation datasets show
that CABB outperforms existing state-of-the-art black-box DA models and is
comparable to white-box domain adaptation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jahan_C/0/1/0/all/0/1&quot;&gt;Chowdhury Sadman Jahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1&quot;&gt;Andreas Savakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00957">
<title>Causal Inference with Differentially Private (Clustered) Outcomes. (arXiv:2308.00957v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.00957</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating causal effects from randomized experiments is only feasible if
participants agree to reveal their potentially sensitive responses. Of the many
ways of ensuring privacy, label differential privacy is a widely used measure
of an algorithm&apos;s privacy guarantee, which might encourage participants to
share responses without running the risk of de-anonymization. Many
differentially private mechanisms inject noise into the original data-set to
achieve this privacy guarantee, which increases the variance of most
statistical estimators and makes the precise measurement of causal effects
difficult: there exists a fundamental privacy-variance trade-off to performing
causal analyses from differentially private data. With the aim of achieving
lower variance for stronger privacy guarantees, we suggest a new differential
privacy mechanism, &quot;Cluster-DP&quot;, which leverages any given cluster structure of
the data while still allowing for the estimation of causal effects. We show
that, depending on an intuitive measure of cluster quality, we can improve the
variance loss while maintaining our privacy guarantees. We compare its
performance, theoretically and empirically, to that of its unclustered version
and a more extreme uniform-prior version which does not use any of the original
response distribution, both of which are special cases of the &quot;Cluster-DP&quot;
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Javanmard_A/0/1/0/all/0/1&quot;&gt;Adel Javanmard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mirrokni_V/0/1/0/all/0/1&quot;&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pouget_Abadie_J/0/1/0/all/0/1&quot;&gt;Jean Pouget-Abadie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00963">
<title>Integrating Homomorphic Encryption and Trusted Execution Technology for Autonomous and Confidential Model Refining in Cloud. (arXiv:2308.00963v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.00963</link>
<description rdf:parseType="Literal">&lt;p&gt;With the popularity of cloud computing and machine learning, it has been a
trend to outsource machine learning processes (including model training and
model-based inference) to cloud. By the outsourcing, other than utilizing the
extensive and scalable resource offered by the cloud service provider, it will
also be attractive to users if the cloud servers can manage the machine
learning processes autonomously on behalf of the users. Such a feature will be
especially salient when the machine learning is expected to be a long-term
continuous process and the users are not always available to participate. Due
to security and privacy concerns, it is also desired that the autonomous
learning preserves the confidentiality of users&apos; data and models involved.
Hence, in this paper, we aim to design a scheme that enables autonomous and
confidential model refining in cloud. Homomorphic encryption and trusted
execution environment technology can protect confidentiality for autonomous
computation, but each of them has their limitations respectively and they are
complementary to each other. Therefore, we further propose to integrate these
two techniques in the design of the model refining scheme. Through
implementation and experiments, we evaluate the feasibility of our proposed
scheme. The results indicate that, with our proposed scheme the cloud server
can autonomously refine an encrypted model with newly provided encrypted
training data to continuously improve its accuracy. Though the efficiency is
still significantly lower than the baseline scheme that refines plaintext-model
with plaintext-data, we expect that it can be improved by fully utilizing the
higher level of parallelism and the computational power of GPU at the cloud
server.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Pinglan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wensheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00978">
<title>Certified Multi-Fidelity Zeroth-Order Optimization. (arXiv:2308.00978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00978</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of multi-fidelity zeroth-order optimization, where
one can evaluate a function $f$ at various approximation levels (of varying
costs), and the goal is to optimize $f$ with the cheapest evaluations possible.
In this paper, we study \emph{certified} algorithms, which are additionally
required to output a data-driven upper bound on the optimization error. We
first formalize the problem in terms of a min-max game between an algorithm and
an evaluation environment. We then propose a certified variant of the MFDOO
algorithm and derive a bound on its cost complexity for any Lipschitz function
$f$. We also prove an $f$-dependent lower bound showing that this algorithm has
a near-optimal cost complexity. We close the paper by addressing the special
case of noisy (stochastic) evaluations as a direct example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montbrun_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;tienne de Montbrun&lt;/a&gt; (TSE-R), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerchinovitz_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Gerchinovitz&lt;/a&gt; (IMT)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00989">
<title>Wasserstein Diversity-Enriched Regularizer for Hierarchical Reinforcement Learning. (arXiv:2308.00989v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.00989</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical reinforcement learning composites subpolicies in different
hierarchies to accomplish complex tasks.Automated subpolicies discovery, which
does not depend on domain knowledge, is a promising approach to generating
subpolicies.However, the degradation problem is a challenge that existing
methods can hardly deal with due to the lack of consideration of diversity or
the employment of weak regularizers. In this paper, we propose a novel
task-agnostic regularizer called the Wasserstein Diversity-Enriched Regularizer
(WDER), which enlarges the diversity of subpolicies by maximizing the
Wasserstein distances among action distributions. The proposed WDER can be
easily incorporated into the loss function of existing methods to boost their
performance further.Experimental results demonstrate that our WDER improves
performance and sample efficiency in comparison with prior work without
modifying hyperparameters, which indicates the applicability and robustness of
the WDER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haorui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Daniel Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00994">
<title>Exploiting Synthetic Data for Data Imbalance Problems: Baselines from a Data Perspective. (arXiv:2308.00994v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.00994</link>
<description rdf:parseType="Literal">&lt;p&gt;We live in a vast ocean of data, and deep neural networks are no exception to
this. However, this data exhibits an inherent phenomenon of imbalance. This
imbalance poses a risk of deep neural networks producing biased predictions,
leading to potentially severe ethical and social consequences. To address these
challenges, we believe that the use of generative models is a promising
approach for comprehending tasks, given the remarkable advancements
demonstrated by recent diffusion models in generating high-quality images. In
this work, we propose a simple yet effective baseline, SYNAuG, that utilizes
synthetic data as a preliminary step before employing task-specific algorithms
to address data imbalance problems. This straightforward approach yields
impressive performance on datasets such as CIFAR100-LT, ImageNet100-LT,
UTKFace, and Waterbird, surpassing the performance of existing task-specific
methods. While we do not claim that our approach serves as a complete solution
to the problem of data imbalance, we argue that supplementing the existing data
with synthetic data proves to be an effective and crucial preliminary step in
addressing data imbalance concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1&quot;&gt;Moon Ye-Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1&quot;&gt;Nam Hyeon-Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1&quot;&gt;Wonseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Nayeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_S/0/1/0/all/0/1&quot;&gt;Suha Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1&quot;&gt;Tae-Hyun Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01000">
<title>MDT3D: Multi-Dataset Training for LiDAR 3D Object Detection Generalization. (arXiv:2308.01000v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01000</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised 3D Object Detection models have been displaying increasingly
better performance in single-domain cases where the training data comes from
the same environment and sensor as the testing data. However, in real-world
scenarios data from the target domain may not be available for finetuning or
for domain adaptation methods. Indeed, 3D object detection models trained on a
source dataset with a specific point distribution have shown difficulties in
generalizing to unseen datasets. Therefore, we decided to leverage the
information available from several annotated source datasets with our
Multi-Dataset Training for 3D Object Detection (MDT3D) method to increase the
robustness of 3D object detection models when tested in a new environment with
a different sensor configuration. To tackle the labelling gap between datasets,
we used a new label mapping based on coarse labels. Furthermore, we show how we
managed the mix of datasets during training and finally introduce a new
cross-dataset augmentation method: cross-dataset object injection. We
demonstrate that this training paradigm shows improvements for different types
of 3D object detection models. The source code and additional results for this
research project will be publicly available on GitHub for interested parties to
access and utilize: https://github.com/LouisSF/MDT3D
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soum_Fontez_L/0/1/0/all/0/1&quot;&gt;Louis Soum-Fontez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1&quot;&gt;Jean-Emmanuel Deschaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Goulette&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01011">
<title>Enhancing Representation Learning for Periodic Time Series with Floss: A Frequency Domain Regularization Approach. (arXiv:2308.01011v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01011</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series analysis is a fundamental task in various application domains,
and deep learning approaches have demonstrated remarkable performance in this
area. However, many real-world time series data exhibit significant periodic or
quasi-periodic dynamics that are often not adequately captured by existing deep
learning-based solutions. This results in an incomplete representation of the
underlying dynamic behaviors of interest. To address this gap, we propose an
unsupervised method called Floss that automatically regularizes learned
representations in the frequency domain. The Floss method first automatically
detects major periodicities from the time series. It then employs periodic
shift and spectral density similarity measures to learn meaningful
representations with periodic consistency. In addition, Floss can be easily
incorporated into both supervised, semi-supervised, and unsupervised learning
frameworks. We conduct extensive experiments on common time series
classification, forecasting, and anomaly detection tasks to demonstrate the
effectiveness of Floss. We incorporate Floss into several representative deep
learning solutions to justify our design choices and demonstrate that it is
capable of automatically discovering periodic dynamics and improving
state-of-the-art deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunwei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lijun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuankai Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01028">
<title>Maximizing Success Rate of Payment Routing using Non-stationary Bandits. (arXiv:2308.01028v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01028</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper discusses the system architecture design and deployment of
non-stationary multi-armed bandit approaches to determine a near-optimal
payment routing policy based on the recent history of transactions. We propose
a Routing Service architecture using a novel Ray-based implementation for
optimally scaling bandit-based payment routing to over 10000 transactions per
second, adhering to the system design requirements and ecosystem constraints
with Payment Card Industry Data Security Standard (PCI DSS). We first evaluate
the effectiveness of multiple bandit-based payment routing algorithms on a
custom simulator to benchmark multiple non-stationary bandit approaches and
identify the best hyperparameters. We then conducted live experiments on the
payment transaction system on a fantasy sports platform Dream11. In the live
experiments, we demonstrated that our non-stationary bandit-based algorithm
consistently improves the success rate of transactions by 0.92\% compared to
the traditional rule-based methods over one month.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1&quot;&gt;Aayush Chaudhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1&quot;&gt;Abhinav Rai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01030">
<title>Three Factors to Improve Out-of-Distribution Detection. (arXiv:2308.01030v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01030</link>
<description rdf:parseType="Literal">&lt;p&gt;In the problem of out-of-distribution (OOD) detection, the usage of auxiliary
data as outlier data for fine-tuning has demonstrated encouraging performance.
However, previous methods have suffered from a trade-off between classification
accuracy (ACC) and OOD detection performance (AUROC, FPR, AUPR). To improve
this trade-off, we make three contributions: (i) Incorporating a self-knowledge
distillation loss can enhance the accuracy of the network; (ii) Sampling
semi-hard outlier data for training can improve OOD detection performance with
minimal impact on accuracy; (iii) The introduction of our novel supervised
contrastive learning can simultaneously improve OOD detection performance and
the accuracy of the network. By incorporating all three factors, our approach
enhances both accuracy and OOD detection performance by addressing the
trade-off between classification and OOD detection. Our method achieves
improvements over previous approaches in both performance metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hyunjun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;JaeHo Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1&quot;&gt;Hawook Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jin Young Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01039">
<title>Computing the Distance between unbalanced Distributions -- The flat Metric. (arXiv:2308.01039v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01039</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide an implementation to compute the flat metric in any dimension. The
flat metric, also called dual bounded Lipschitz distance, generalizes the
well-known Wasserstein distance W1 to the case that the distributions are of
unequal total mass. This is of particular interest for unbalanced optimal
transport tasks and for the analysis of data distributions where the sample
size is important or normalization is not possible. The core of the method is
based on a neural network to determine on optimal test function realizing the
distance between two given measures. Special focus was put on achieving
comparability of pairwise computed distances from independently trained
networks. We tested the quality of the output in several experiments where
ground truth was available as well as with simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_H/0/1/0/all/0/1&quot;&gt;Henri Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dull_C/0/1/0/all/0/1&quot;&gt;Christian D&amp;#xfc;ll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01050">
<title>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles&apos; Riskiness. (arXiv:2308.01050v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.01050</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles (AVs) have the potential to provide numerous societal
benefits, such as decreased road accidents and increased overall transportation
efficiency. However, quantifying the risk associated with AVs is challenging
due to the lack of historical data and the rapidly evolving technology. This
paper presents a data-driven framework for comparing the risk of different AVs&apos;
behaviors in various operational design domains (ODDs), based on counterfactual
simulations of &quot;misbehaving&quot; road users. We introduce the concept of
counterfactual safety margin, which represents the minimum deviation from
normal behavior that could lead to a collision. This concept helps to find the
most critical scenarios but also to assess the frequency and severity of risk
of AVs. We show that the proposed methodology is applicable even when the AV&apos;s
behavioral policy is unknown -- through worst- and best-case analyses -- making
the method useful also to external third-party risk assessors. Our experimental
results demonstrate the correlation between the safety margin, the driving
policy quality, and the ODD shedding light on the relative risk associated with
different AV providers. This work contributes to AV safety assessment and aids
in addressing legislative and insurance concerns surrounding this emerging
technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanardi_A/0/1/0/all/0/1&quot;&gt;Alessandro Zanardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Censi_A/0/1/0/all/0/1&quot;&gt;Andrea Censi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzei_M/0/1/0/all/0/1&quot;&gt;Margherita Atzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillo_L/0/1/0/all/0/1&quot;&gt;Luigi Di Lillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frazzoli_E/0/1/0/all/0/1&quot;&gt;Emilio Frazzoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01054">
<title>Simulation-based inference using surjective sequential neural likelihood estimation. (arXiv:2308.01054v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.01054</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Surjective Sequential Neural Likelihood (SSNL) estimation, a novel
method for simulation-based inference in models where the evaluation of the
likelihood function is not tractable and only a simulator that can generate
synthetic data is available. SSNL fits a dimensionality-reducing surjective
normalizing flow model and uses it as a surrogate likelihood function which
allows for conventional Bayesian inference using either Markov chain Monte
Carlo methods or variational inference. By embedding the data in a
low-dimensional space, SSNL solves several issues previous likelihood-based
methods had when applied to high-dimensional data sets that, for instance,
contain non-informative data dimensions or lie along a lower-dimensional
manifold. We evaluate SSNL on a wide variety of experiments and show that it
generally outperforms contemporary methods used in simulation-based inference,
for instance, on a challenging real-world example from astrophysics which
models the magnetic field strength of the sun using a solar dynamo model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dirmeier_S/0/1/0/all/0/1&quot;&gt;Simon Dirmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Albert_C/0/1/0/all/0/1&quot;&gt;Carlo Albert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perez_Cruz_F/0/1/0/all/0/1&quot;&gt;Fernando Perez-Cruz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01063">
<title>Graph Anomaly Detection at Group Level: A Topology Pattern Enhanced Unsupervised Approach. (arXiv:2308.01063v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01063</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph anomaly detection (GAD) has achieved success and has been widely
applied in various domains, such as fraud detection, cybersecurity, finance
security, and biochemistry. However, existing graph anomaly detection
algorithms focus on distinguishing individual entities (nodes or graphs) and
overlook the possibility of anomalous groups within the graph. To address this
limitation, this paper introduces a novel unsupervised framework for a new task
called Group-level Graph Anomaly Detection (Gr-GAD). The proposed framework
first employs a variant of Graph AutoEncoder (GAE) to locate anchor nodes that
belong to potential anomaly groups by capturing long-range inconsistencies.
Subsequently, group sampling is employed to sample candidate groups, which are
then fed into the proposed Topology Pattern-based Graph Contrastive Learning
(TPGCL) method. TPGCL utilizes the topology patterns of groups as clues to
generate embeddings for each candidate group and thus distinct anomaly groups.
The experimental results on both real-world and synthetic datasets demonstrate
that the proposed framework shows superior performance in identifying and
localizing anomaly groups, highlighting it as a promising solution for Gr-GAD.
Datasets and codes of the proposed framework are at the github repository
https://anonymous.4open.science/r/Topology-Pattern-Enhanced-Unsupervised-Group-level-Graph-Anomaly-Detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_X/0/1/0/all/0/1&quot;&gt;Xing Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jialong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yulin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gaolei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalak_T/0/1/0/all/0/1&quot;&gt;Tomasz P. Michalak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiapu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kai Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01070">
<title>When Analytic Calculus Cracks AdaBoost Code. (arXiv:2308.01070v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01070</link>
<description rdf:parseType="Literal">&lt;p&gt;The principle of boosting in supervised learning involves combining multiple
weak classifiers to obtain a stronger classifier. AdaBoost has the reputation
to be a perfect example of this approach. We have previously shown that
AdaBoost is not truly an optimization algorithm. This paper shows that AdaBoost
is an algorithm in name only, as the resulting combination of weak classifiers
can be explicitly calculated using a truth table. This study is carried out by
considering a problem with two classes and is illustrated by the particular
case of three binary classifiers and presents results in comparison with those
from the implementation of AdaBoost algorithm of the Python library
scikit-learn.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brossier_J/0/1/0/all/0/1&quot;&gt;Jean-Marc Brossier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafitte_O/0/1/0/all/0/1&quot;&gt;Olivier Lafitte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rethore_L/0/1/0/all/0/1&quot;&gt;Lenny R&amp;#xe9;thor&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01071">
<title>Automatic Feature Engineering for Time Series Classification: Evaluation and Discussion. (arXiv:2308.01071v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01071</link>
<description rdf:parseType="Literal">&lt;p&gt;Time Series Classification (TSC) has received much attention in the past two
decades and is still a crucial and challenging problem in data science and
knowledge engineering. Indeed, along with the increasing availability of time
series data, many TSC algorithms have been suggested by the research community
in the literature. Besides state-of-the-art methods based on similarity
measures, intervals, shapelets, dictionaries, deep learning methods or hybrid
ensemble methods, several tools for extracting unsupervised informative summary
statistics, aka features, from time series have been designed in the recent
years. Originally designed for descriptive analysis and visualization of time
series with informative and interpretable features, very few of these feature
engineering tools have been benchmarked for TSC problems and compared with
state-of-the-art TSC algorithms in terms of predictive performance. In this
article, we aim at filling this gap and propose a simple TSC process to
evaluate the potential predictive performance of the feature sets obtained with
existing feature engineering tools. Thus, we present an empirical study of 11
feature engineering tools branched with 9 supervised classifiers over 112 time
series data sets. The analysis of the results of more than 10000 learning
experiments indicate that feature-based methods perform as accurately as
current state-of-the-art TSC algorithms, and thus should rightfully be
considered further in the TSC literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renault_A/0/1/0/all/0/1&quot;&gt;Aur&amp;#xe9;lien Renault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondu_A/0/1/0/all/0/1&quot;&gt;Alexis Bondu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemaire_V/0/1/0/all/0/1&quot;&gt;Vincent Lemaire&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gay_D/0/1/0/all/0/1&quot;&gt;Dominique Gay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01074">
<title>A Practical Deep Learning-Based Acoustic Side Channel Attack on Keyboards. (arXiv:2308.01074v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.01074</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent developments in deep learning, the ubiquity of micro-phones and
the rise in online services via personal devices, acoustic side channel attacks
present a greater threat to keyboards than ever. This paper presents a
practical implementation of a state-of-the-art deep learning model in order to
classify laptop keystrokes, using a smartphone integrated microphone. When
trained on keystrokes recorded by a nearby phone, the classifier achieved an
accuracy of 95%, the highest accuracy seen without the use of a language model.
When trained on keystrokes recorded using the video-conferencing software Zoom,
an accuracy of 93% was achieved, a new best for the medium. Our results prove
the practicality of these side channel attacks via off-the-shelf equipment and
algorithms. We discuss a series of mitigation methods to protect users against
these series of attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1&quot;&gt;Joshua Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toreini_E/0/1/0/all/0/1&quot;&gt;Ehsan Toreini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrnezhad_M/0/1/0/all/0/1&quot;&gt;Maryam Mehrnezhad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01084">
<title>Data-Driven Identification of Quadratic Symplectic Representations of Nonlinear Hamiltonian Systems. (arXiv:2308.01084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01084</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a framework for learning Hamiltonian systems using data. This work
is based on the lifting hypothesis, which posits that nonlinear Hamiltonian
systems can be written as nonlinear systems with cubic Hamiltonians. By
leveraging this, we obtain quadratic dynamics that are Hamiltonian in a
transformed coordinate system. To that end, for given generalized position and
momentum data, we propose a methodology to learn quadratic dynamical systems,
enforcing the Hamiltonian structure in combination with a symplectic
auto-encoder. The enforced Hamiltonian structure exhibits long-term stability
of the system, while the cubic Hamiltonian function provides relatively low
model complexity. For low-dimensional data, we determine a higher-order
transformed coordinate system, whereas, for high-dimensional data, we find a
lower-order coordinate system with the desired properties. We demonstrate the
proposed methodology by means of both low-dimensional and high-dimensional
nonlinear Hamiltonian systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildiz_S/0/1/0/all/0/1&quot;&gt;S&amp;#xfc;leyman Yildiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Pawan Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendokat_T/0/1/0/all/0/1&quot;&gt;Thomas Bendokat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benner_P/0/1/0/all/0/1&quot;&gt;Peter Benner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01086">
<title>Homography Estimation in Complex Topological Scenes. (arXiv:2308.01086v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01086</link>
<description rdf:parseType="Literal">&lt;p&gt;Surveillance videos and images are used for a broad set of applications,
ranging from traffic analysis to crime detection. Extrinsic camera calibration
data is important for most analysis applications. However, security cameras are
susceptible to environmental conditions and small camera movements, resulting
in a need for an automated re-calibration method that can account for these
varying conditions. In this paper, we present an automated camera-calibration
process leveraging a dictionary-based approach that does not require prior
knowledge on any camera settings. The method consists of a custom
implementation of a Spatial Transformer Network (STN) and a novel topological
loss function. Experiments reveal that the proposed method improves the IoU
metric by up to 12% w.r.t. a state-of-the-art model across five synthetic
datasets and the World Cup 2014 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAmicantonio_G/0/1/0/all/0/1&quot;&gt;Giacomo D&amp;#x27;Amicantonio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondarau_E/0/1/0/all/0/1&quot;&gt;Egor Bondarau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1&quot;&gt;Peter H.N. De With&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01097">
<title>Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction (HMP) has emerged as a popular research topic due to
its diverse applications, but it remains a challenging task due to the
stochastic and aperiodic nature of future poses. Traditional methods rely on
hand-crafted features and machine learning techniques, which often struggle to
model the complex dynamics of human motion. Recent deep learning-based methods
have achieved success by learning spatio-temporal representations of motion,
but these models often overlook the reliability of motion data. Additionally,
the temporal and spatial dependencies of skeleton nodes are distinct. The
temporal relationship captures motion information over time, while the spatial
relationship describes body structure and the relationships between different
nodes. In this paper, we propose a novel spatio-temporal branching network
using incremental information for HMP, which decouples the learning of
temporal-domain and spatial-domain features, extracts more motion information,
and achieves complementary cross-domain knowledge learning through knowledge
distillation. Our approach effectively reduces noise interference and provides
more expressive information for characterizing motion by separately extracting
temporal and spatial features. We evaluate our approach on standard HMP
benchmarks and outperform state-of-the-art methods in terms of prediction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yujie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1&quot;&gt;Ying Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01118">
<title>A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01118</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems help people find relevant content in a personalized way.
One main promise of such systems is that they are able to increase the
visibility of items in the long tail, i.e., the lesser-known items in a
catalogue. Existing research, however, suggests that in many situations today&apos;s
recommendation algorithms instead exhibit a popularity bias, meaning that they
often focus on rather popular items in their recommendations. Such a bias may
not only lead to limited value of the recommendations for consumers and
providers in the short run, but it may also cause undesired reinforcement
effects over time. In this paper, we discuss the potential reasons for
popularity bias and we review existing approaches to detect, quantify and
mitigate popularity bias in recommender systems. Our survey therefore includes
both an overview of the computational metrics used in the literature as well as
a review of the main technical approaches to reduce the bias. We furthermore
critically discuss today&apos;s literature, where we observe that the research is
almost entirely based on computational experiments and on certain assumptions
regarding the practical effects of including long-tail items in the
recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimashevskaia_A/0/1/0/all/0/1&quot;&gt;Anastasiia Klimashevskaia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1&quot;&gt;Dietmar Jannach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1&quot;&gt;Mehdi Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trattner_C/0/1/0/all/0/1&quot;&gt;Christoph Trattner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01119">
<title>Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image classification models are frequently trained using training
datasets derived from multiple data sources. While leveraging multiple data
sources is crucial for achieving model generalization, it is important to
acknowledge that the diverse nature of these sources inherently introduces
unintended confounders and other challenges that can impact both model accuracy
and transparency. A notable confounding factor in medical image classification,
particularly in musculoskeletal image classification, is skeletal
maturation-induced bone growth observed during adolescence. We train a deep
learning model using a Covid-19 chest X-ray dataset and we showcase how this
dataset can lead to spurious correlations due to unintended confounding
regions. eXplanation Based Learning (XBL) is a deep learning approach that goes
beyond interpretability by utilizing model explanations to interactively
unlearn spurious correlations. This is achieved by integrating interactive user
feedback, specifically feature annotations. In our study, we employed two
non-demanding manual feedback mechanisms to implement an XBL-based approach for
effectively eliminating these spurious correlations. Our results underscore the
promising potential of XBL in constructing robust models even in the presence
of confounding factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01137">
<title>Multi-task learning for classification, segmentation, reconstruction, and detection on chest CT scans. (arXiv:2308.01137v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01137</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer and covid-19 have one of the highest morbidity and mortality
rates in the world. For physicians, the identification of lesions is difficult
in the early stages of the disease and time-consuming. Therefore, multi-task
learning is an approach to extracting important features, such as lesions, from
small amounts of medical data because it learns to generalize better. We
propose a novel multi-task framework for classification, segmentation,
reconstruction, and detection. To the best of our knowledge, we are the first
ones who added detection to the multi-task solution. Additionally, we checked
the possibility of using two different backbones and different loss functions
in the segmentation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hryniewska_Guzik_W/0/1/0/all/0/1&quot;&gt;Weronika Hryniewska-Guzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kedzierska_M/0/1/0/all/0/1&quot;&gt;Maria K&amp;#x119;dzierska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biecek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Biecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01138">
<title>Can We Transfer Noise Patterns? An Multi-environment Spectrum Analysis Model Using Generated Cases. (arXiv:2308.01138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01138</link>
<description rdf:parseType="Literal">&lt;p&gt;Spectrum analysis systems in online water quality testing are designed to
detect types and concentrations of pollutants and enable regulatory agencies to
respond promptly to pollution incidents. However, spectral data-based testing
devices suffer from complex noise patterns when deployed in non-laboratory
environments. To make the analysis model applicable to more environments, we
propose a noise patterns transferring model, which takes the spectrum of
standard water samples in different environments as cases and learns the
differences in their noise patterns, thus enabling noise patterns to transfer
to unknown samples. Unfortunately, the inevitable sample-level baseline noise
makes the model unable to obtain the paired data that only differ in
dataset-level environmental noise. To address the problem, we generate a
sample-to-sample case-base to exclude the interference of sample-level noise on
dataset-level noise learning, enhancing the system&apos;s learning performance.
Experiments on spectral data with different background noises demonstrate the
good noise-transferring ability of the proposed method against baseline systems
ranging from wavelet denoising, deep neural networks, and generative models.
From this research, we posit that our method can enhance the performance of DL
models by generating high-quality cases. The source code is made publicly
available online at https://github.com/Magnomic/CNST.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Haiwen Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_Z/0/1/0/all/0/1&quot;&gt;Zheng Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1&quot;&gt;Yu An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Honghui Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongjie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhaoshuo Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawlor_A/0/1/0/all/0/1&quot;&gt;Aonghus Lawlor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1&quot;&gt;Ruihai Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01139">
<title>Dynamic Privacy Allocation for Locally Differentially Private Federated Learning with Composite Objectives. (arXiv:2308.01139v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01139</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a locally differentially private federated learning
algorithm for strongly convex but possibly nonsmooth problems that protects the
gradients of each worker against an honest but curious server. The proposed
algorithm adds artificial noise to the shared information to ensure privacy and
dynamically allocates the time-varying noise variance to minimize an upper
bound of the optimization error subject to a predefined privacy budget
constraint. This allows for an arbitrarily large but finite number of
iterations to achieve both privacy protection and utility up to a neighborhood
of the optimal solution, removing the need for tuning the number of iterations.
Numerical results show the superiority of the proposed algorithm over
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaojiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fay_D/0/1/0/all/0/1&quot;&gt;Dominik Fay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1&quot;&gt;Mikael Johansson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01140">
<title>DySTreSS: Dynamically Scaled Temperature in Self-Supervised Contrastive Learning. (arXiv:2308.01140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01140</link>
<description rdf:parseType="Literal">&lt;p&gt;In contemporary self-supervised contrastive algorithms like SimCLR, MoCo,
etc., the task of balancing attraction between two semantically similar samples
and repulsion between two samples from different classes is primarily affected
by the presence of hard negative samples. While the InfoNCE loss has been shown
to impose penalties based on hardness, the temperature hyper-parameter is the
key to regulating the penalties and the trade-off between uniformity and
tolerance. In this work, we focus our attention to improve the performance of
InfoNCE loss in SSL by studying the effect of temperature hyper-parameter
values. We propose a cosine similarity-dependent temperature scaling function
to effectively optimize the distribution of the samples in the feature space.
We further analyze the uniformity and tolerance metrics to investigate the
optimal regions in the cosine similarity space for better optimization.
Additionally, we offer a comprehensive examination of the behavior of local and
global structures in the feature space throughout the pre-training phase, as
the temperature varies. Experimental evidence shows that the proposed framework
outperforms or is at par with the contrastive loss-based SSL algorithms. We
believe our work (DySTreSS) on temperature scaling in SSL provides a foundation
for future research in contrastive learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manna_S/0/1/0/all/0/1&quot;&gt;Siladittya Manna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1&quot;&gt;Soumitri Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_R/0/1/0/all/0/1&quot;&gt;Rakesh Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1&quot;&gt;Umapada Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01157">
<title>LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs. (arXiv:2308.01157v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.01157</link>
<description rdf:parseType="Literal">&lt;p&gt;We show that large language models (LLMs) are remarkably good at working with
interpretable models that decompose complex outcomes into univariate
graph-represented components. By adopting a hierarchical approach to reasoning,
LLMs can provide comprehensive model-level summaries without ever requiring the
entire model to fit in context. This approach enables LLMs to apply their
extensive background knowledge to automate common tasks in data science such as
detecting anomalies that contradict prior knowledge, describing potential
reasons for the anomalies, and suggesting repairs that would remove the
anomalies. We use multiple examples in healthcare to demonstrate the utility of
these new capabilities of LLMs, with particular emphasis on Generalized
Additive Models (GAMs). Finally, we present the package $\texttt{TalkToEBM}$ as
an open-source LLM-GAM interface.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lengerich_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Lengerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bordt_S/0/1/0/all/0/1&quot;&gt;Sebastian Bordt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nori_H/0/1/0/all/0/1&quot;&gt;Harsha Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nunnally_M/0/1/0/all/0/1&quot;&gt;Mark E. Nunnally&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aphinyanaphongs_Y/0/1/0/all/0/1&quot;&gt;Yin Aphinyanaphongs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kellis_M/0/1/0/all/0/1&quot;&gt;Manolis Kellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caruana_R/0/1/0/all/0/1&quot;&gt;Rich Caruana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01170">
<title>Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01170</link>
<description rdf:parseType="Literal">&lt;p&gt;Off-policy learning enables a reinforcement learning (RL) agent to reason
counterfactually about policies that are not executed and is one of the most
important ideas in RL. It, however, can lead to instability when combined with
function approximation and bootstrapping, two arguably indispensable
ingredients for large-scale reinforcement learning. This is the notorious
deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve
the deadly triad. Its success results from solving a doubling sampling issue
indirectly with weight duplication or Fenchel duality. In this paper, we
instead propose a direct method to solve the double sampling issue by simply
using two samples in a Markovian data stream with an increasing gap. The
resulting algorithm is as computationally efficient as GTD but gets rid of
GTD&apos;s extra weights. The only price we pay is a logarithmically increasing
memory as time progresses. We provide both asymptotic and finite sample
analysis, where the convergence rate is on-par with the canonical on-policy
temporal difference learning. Key to our analysis is a novel refined
discretization of limiting ODEs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaochi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shangtong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01184">
<title>Generative Noisy-Label Learning by Implicit Dicriminative Approximation with Partial Label Prior. (arXiv:2308.01184v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01184</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning with noisy labels has been addressed with both discriminative
and generative models. Although discriminative models have dominated the field
due to their simpler modeling and more efficient computational training
processes, generative models offer a more effective means of disentangling
clean and noisy labels and improving the estimation of the label transition
matrix. However, generative approaches maximize the joint likelihood of noisy
labels and data using a complex formulation that only indirectly optimizes the
model of interest associating data and clean labels. Additionally, these
approaches rely on generative models that are challenging to train and tend to
use uninformative clean label priors. In this paper, we propose a new
generative noisy-label learning approach that addresses these three issues.
First, we propose a new model optimisation that directly associates data and
clean labels. Second, the generative model is implicitly estimated using a
discriminative model, eliminating the inefficient training of a generative
model. Third, we propose a new informative label prior inspired by partial
label learning as supervision signal for noisy label learning. Extensive
experiments on several noisy-label benchmarks demonstrate that our generative
model provides state-of-the-art results while maintaining a similar
computational complexity as discriminative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengbei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01195">
<title>Personalized Category Frequency prediction for Buy It Again recommendations. (arXiv:2308.01195v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01195</link>
<description rdf:parseType="Literal">&lt;p&gt;Buy It Again (BIA) recommendations are crucial to retailers to help improve
user experience and site engagement by suggesting items that customers are
likely to buy again based on their own repeat purchasing patterns. Most
existing BIA studies analyze guests personalized behavior at item granularity.
A category-based model may be more appropriate in such scenarios. We propose a
recommendation system called a hierarchical PCIC model that consists of a
personalized category model (PC model) and a personalized item model within
categories (IC model). PC model generates a personalized list of categories
that customers are likely to purchase again. IC model ranks items within
categories that guests are likely to consume within a category. The
hierarchical PCIC model captures the general consumption rate of products using
survival models. Trends in consumption are captured using time series models.
Features derived from these models are used in training a category-grained
neural network. We compare PCIC to twelve existing baselines on four standard
open datasets. PCIC improves NDCG up to 16 percent while improving recall by
around 2 percent. We were able to scale and train (over 8 hours) PCIC on a
large dataset of 100M guests and 3M items where repeat categories of a guest
out number repeat items. PCIC was deployed and AB tested on the site of a major
retailer, leading to significant gains in guest engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pande_A/0/1/0/all/0/1&quot;&gt;Amit Pande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1&quot;&gt;Kunal Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_R/0/1/0/all/0/1&quot;&gt;Rankyung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01196">
<title>Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01196</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender Systems have become crucial in the modern world, commonly guiding
users towards relevant content or products, and having a large influence over
the decisions of users and citizens. However, ensuring transparency and user
trust in these systems remains a challenge; personalized explanations have
emerged as a solution, offering justifications for recommendations. Among the
existing approaches for generating personalized explanations, using visual
content created by the users is one particularly promising option, showing a
potential to maximize transparency and user trust. Existing models for
explaining recommendations in this context face limitations: sustainability has
been a critical concern, as they often require substantial computational
resources, leading to significant carbon emissions comparable to the
Recommender Systems where they would be integrated. Moreover, most models
employ surrogate learning goals that do not align with the objective of ranking
the most effective personalized explanations for a given recommendation,
leading to a suboptimal learning process and larger model sizes. To address
these limitations, we present BRIE, a novel model designed to tackle the
existing challenges by adopting a more adequate learning goal based on Bayesian
Pairwise Ranking, enabling it to achieve consistently superior performance than
state-of-the-art models in six real-world datasets, while exhibiting remarkable
efficiency, emitting up to 75% less CO${_2}$ during training and inference with
a model up to 64 times smaller than previous approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paz_Ruza_J/0/1/0/all/0/1&quot;&gt;Jorge Paz-Ruza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Betanzos_A/0/1/0/all/0/1&quot;&gt;Amparo Alonso-Betanzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guijarro_Berdinas_B/0/1/0/all/0/1&quot;&gt;Berta Guijarro-Berdi&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cancela_B/0/1/0/all/0/1&quot;&gt;Brais Cancela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiras_Franco_C/0/1/0/all/0/1&quot;&gt;Carlos Eiras-Franco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01197">
<title>GNN4FR: A Lossless GNN-based Federated Recommendation Framework. (arXiv:2308.01197v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01197</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have gained wide popularity in recommender
systems due to their capability to capture higher-order structure information
among the nodes of users and items. However, these methods need to collect
personal interaction data between a user and the corresponding items and then
model them in a central server, which would break the privacy laws such as
GDPR. So far, no existing work can construct a global graph without leaking
each user&apos;s private interaction data (i.e., his or her subgraph). In this
paper, we are the first to design a novel lossless federated recommendation
framework based on GNN, which achieves full-graph training with complete
high-order structure information, enabling the training process to be
equivalent to the corresponding un-federated counterpart. In addition, we use
LightGCN to instantiate an example of our framework and show its equivalence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guowei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1&quot;&gt;Weike Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1&quot;&gt;Zhong Ming&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01204">
<title>Methodologies for Improving Modern Industrial Recommender Systems. (arXiv:2308.01204v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01204</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender system (RS) is an established technology with successful
applications in social media, e-commerce, entertainment, and more. RSs are
indeed key to the success of many popular APPs, such as YouTube, Tik Tok,
Xiaohongshu, Bilibili, and others. This paper explores the methodology for
improving modern industrial RSs. It is written for experienced RS engineers who
are diligently working to improve their key performance indicators, such as
retention and duration. The experiences shared in this paper have been tested
in some real industrial RSs and are likely to be generalized to other RSs as
well. Most contents in this paper are industry experience without publicly
available references.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shusen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01207">
<title>BiERL: A Meta Evolutionary Reinforcement Learning Framework via Bilevel Optimization. (arXiv:2308.01207v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2308.01207</link>
<description rdf:parseType="Literal">&lt;p&gt;Evolutionary reinforcement learning (ERL) algorithms recently raise attention
in tackling complex reinforcement learning (RL) problems due to high
parallelism, while they are prone to insufficient exploration or model collapse
without carefully tuning hyperparameters (aka meta-parameters). In the paper,
we propose a general meta ERL framework via bilevel optimization (BiERL) to
jointly update hyperparameters in parallel to training the ERL model within a
single agent, which relieves the need for prior domain knowledge or costly
optimization procedure before model deployment. We design an elegant meta-level
architecture that embeds the inner-level&apos;s evolving experience into an
informative population representation and introduce a simple and feasible
evaluation of the meta-level fitness function to facilitate learning
efficiency. We perform extensive experiments in MuJoCo and Box2D tasks to
verify that as a general framework, BiERL outperforms various baselines and
consistently improves the learning performance for a diversity of ERL
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuanyang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunlin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01208">
<title>Adaptive Collaborative Filtering with Personalized Time Decay Functions for Financial Product Recommendation. (arXiv:2308.01208v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01208</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical recommender systems often assume that historical data are
stationary and fail to account for the dynamic nature of user preferences,
limiting their ability to provide reliable recommendations in time-sensitive
settings. This assumption is particularly problematic in finance, where
financial products exhibit continuous changes in valuations, leading to
frequent shifts in client interests. These evolving interests, summarized in
the past client-product interactions, see their utility fade over time with a
degree that might differ from one client to another. To address this challenge,
we propose a time-dependent collaborative filtering algorithm that can
adaptively discount distant client-product interactions using personalized
decay functions. Our approach is designed to handle the non-stationarity of
financial data and produce reliable recommendations by modeling the dynamic
collaborative signals between clients and products. We evaluate our method
using a proprietary dataset from BNP Paribas and demonstrate significant
improvements over state-of-the-art benchmarks from relevant literature. Our
findings emphasize the importance of incorporating time explicitly in the model
to enhance the accuracy of financial product recommendation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghiye_A/0/1/0/all/0/1&quot;&gt;Ashraf Ghiye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barreau_B/0/1/0/all/0/1&quot;&gt;Baptiste Barreau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlier_L/0/1/0/all/0/1&quot;&gt;Laurent Carlier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01210">
<title>Global Hierarchical Neural Networks using Hierarchical Softmax. (arXiv:2308.01210v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.01210</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a framework in which hierarchical softmax is used to
create a global hierarchical classifier. The approach is applicable for any
classification task where there is a natural hierarchy among classes. We show
empirical results on four text classification datasets. In all datasets the
hierarchical softmax improved on the regular softmax used in a flat classifier
in terms of macro-F1 and macro-recall. In three out of four datasets
hierarchical softmax achieved a higher micro-accuracy and macro-precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schuurmans_J/0/1/0/all/0/1&quot;&gt;Jetze Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frasincar_F/0/1/0/all/0/1&quot;&gt;Flavius Frasincar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01220">
<title>Using ScrutinAI for Visual Inspection of DNN Performance in a Medical Use Case. (arXiv:2308.01220v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01220</link>
<description rdf:parseType="Literal">&lt;p&gt;Our Visual Analytics (VA) tool ScrutinAI supports human analysts to
investigate interactively model performanceand data sets. Model performance
depends on labeling quality to a large extent. In particular in medical
settings, generation of high quality labels requires in depth expert knowledge
and is very costly. Often, data sets are labeled by collecting opinions of
groups of experts. We use our VA tool to analyse the influence of label
variations between different experts on the model performance. ScrutinAI
facilitates to perform a root cause analysis that distinguishes weaknesses of
deep neural network (DNN) models caused by varying or missing labeling quality
from true weaknesses. We scrutinize the overall detection of intracranial
hemorrhages and the more subtle differentiation between subtypes in a publicly
available data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorge_R/0/1/0/all/0/1&quot;&gt;Rebekka G&amp;#xf6;rge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haedecke_E/0/1/0/all/0/1&quot;&gt;Elena Haedecke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mock_M/0/1/0/all/0/1&quot;&gt;Michael Mock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01222">
<title>Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01222</link>
<description rdf:parseType="Literal">&lt;p&gt;Calibrating deep neural models plays an important role in building reliable,
robust AI systems in safety-critical applications. Recent work has shown that
modern neural networks that possess high predictive capability are poorly
calibrated and produce unreliable model predictions. Though deep learning
models achieve remarkable performance on various benchmarks, the study of model
calibration and reliability is relatively underexplored. Ideal deep models
should have not only high predictive performance but also be well calibrated.
There have been some recent methods proposed to calibrate deep models by using
different mechanisms. In this survey, we review the state-of-the-art
calibration methods and provide an understanding of their principles for
performing model calibration. First, we start with the definition of model
calibration and explain the root causes of model miscalibration. Then we
introduce the key metrics that can measure this aspect. It is followed by a
summary of calibration methods that we roughly classified into four categories:
post-hoc calibration, regularization methods, uncertainty estimation, and
composition methods. We also covered some recent advancements in calibrating
large models, particularly large language models (LLMs). Finally, we discuss
some open issues, challenges, and potential directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01223">
<title>Do Multilingual Language Models Think Better in English?. (arXiv:2308.01223v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.01223</link>
<description rdf:parseType="Literal">&lt;p&gt;Translate-test is a popular technique to improve the performance of
multilingual language models. This approach works by translating the input into
English using an external machine translation system, and running inference
over the translated input. However, these improvements can be attributed to the
use of a separate translation system, which is typically trained on large
amounts of parallel data not seen by the language model. In this work, we
introduce a new approach called self-translate, which overcomes the need of an
external translation system by leveraging the few-shot translation capabilities
of multilingual language models. Experiments over 5 tasks show that
self-translate consistently outperforms direct inference, demonstrating that
language models are unable to leverage their full multilingual potential when
prompted in non-English languages. Our code is available at
https://github.com/juletx/self-translate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etxaniz_J/0/1/0/all/0/1&quot;&gt;Julen Etxaniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azkune_G/0/1/0/all/0/1&quot;&gt;Gorka Azkune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1&quot;&gt;Aitor Soroa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1&quot;&gt;Oier Lopez de Lacalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01231">
<title>Unleash the Power of Context: Enhancing Large-Scale Recommender Systems with Context-Based Prediction Models. (arXiv:2308.01231v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01231</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce the notion of Context-Based Prediction Models. A
Context-Based Prediction Model determines the probability of a user&apos;s action
(such as a click or a conversion) solely by relying on user and contextual
features, without considering any specific features of the item itself. We have
identified numerous valuable applications for this modeling approach, including
training an auxiliary context-based model to estimate click probability and
incorporating its prediction as a feature in CTR prediction models. Our
experiments indicate that this enhancement brings significant improvements in
offline and online business metrics while having minimal impact on the cost of
serving. Overall, our work offers a simple and scalable, yet powerful approach
for enhancing the performance of large-scale commercial recommender systems,
with broad implications for the field of personalized recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartman_J/0/1/0/all/0/1&quot;&gt;Jan Hartman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1&quot;&gt;Assaf Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopic_D/0/1/0/all/0/1&quot;&gt;Davorin Kopi&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silberstein_N/0/1/0/all/0/1&quot;&gt;Natalia Silberstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01246">
<title>Tirtha -- An Automated Platform to Crowdsource Images and Create 3D Models of Heritage Sites. (arXiv:2308.01246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01246</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital preservation of Cultural Heritage (CH) sites is crucial to protect
them against damage from natural disasters or human activities. Creating 3D
models of CH sites has become a popular method of digital preservation thanks
to advancements in computer vision and photogrammetry. However, the process is
time-consuming, expensive, and typically requires specialized equipment and
expertise, posing challenges in resource-limited developing countries.
Additionally, the lack of an open repository for 3D models hinders research and
public engagement with their heritage. To address these issues, we propose
Tirtha, a web platform for crowdsourcing images of CH sites and creating their
3D models. Tirtha utilizes state-of-the-art Structure from Motion (SfM) and
Multi-View Stereo (MVS) techniques. It is modular, extensible and
cost-effective, allowing for the incorporation of new techniques as
photogrammetry advances. Tirtha is accessible through a web interface at
https://tirtha.niser.ac.in and can be deployed on-premise or in a cloud
environment. In our case studies, we demonstrate the pipeline&apos;s effectiveness
by creating 3D models of temples in Odisha, India, using crowdsourced images.
These models are available for viewing, interaction, and download on the Tirtha
website. Our work aims to provide a dataset of crowdsourced images and 3D
reconstructions for research in computer vision, heritage conservation, and
related domains. Overall, Tirtha is a step towards democratizing digital
preservation, primarily in resource-limited developing countries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivottam_J/0/1/0/all/0/1&quot;&gt;Jyotirmaya Shivottam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Subhankar Mishra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01265">
<title>Deep learning for unsupervised domain adaptation in medical imaging: Recent advancements and future perspectives. (arXiv:2308.01265v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01265</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has demonstrated remarkable performance across various tasks in
medical imaging. However, these approaches primarily focus on supervised
learning, assuming that the training and testing data are drawn from the same
distribution. Unfortunately, this assumption may not always hold true in
practice. To address these issues, unsupervised domain adaptation (UDA)
techniques have been developed to transfer knowledge from a labeled domain to a
related but unlabeled domain. In recent years, significant advancements have
been made in UDA, resulting in a wide range of methodologies, including feature
alignment, image translation, self-supervision, and disentangled representation
methods, among others. In this paper, we provide a comprehensive literature
review of recent deep UDA approaches in medical imaging from a technical
perspective. Specifically, we categorize current UDA research in medical
imaging into six groups and further divide them into finer subcategories based
on the different tasks they perform. We also discuss the respective datasets
used in the studies to assess the divergence between the different domains.
Finally, we discuss emerging areas and provide insights and discussions on
future research directions to conclude this survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumari_S/0/1/0/all/0/1&quot;&gt;Suruchi Kumari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pravendra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01271">
<title>A Probabilistic Approach to Self-Supervised Learning using Cyclical Stochastic Gradient MCMC. (arXiv:2308.01271v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01271</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a practical Bayesian self-supervised learning method
with Cyclical Stochastic Gradient Hamiltonian Monte Carlo (cSGHMC). Within this
framework, we place a prior over the parameters of a self-supervised learning
model and use cSGHMC to approximate the high dimensional and multimodal
posterior distribution over the embeddings. By exploring an expressive
posterior over the embeddings, Bayesian self-supervised learning produces
interpretable and diverse representations. Marginalizing over these
representations yields a significant gain in performance, calibration and
out-of-distribution detection on a variety of downstream classification tasks.
We provide experimental results on multiple classification tasks on four
challenging datasets. Moreover, we demonstrate the effectiveness of the
proposed method in out-of-distribution detection using the SVHN and CIFAR-10
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javanbakhat_M/0/1/0/all/0/1&quot;&gt;Masoumeh Javanbakhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lippert_C/0/1/0/all/0/1&quot;&gt;Christoph Lippert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01274">
<title>BRNES: Enabling Security and Privacy-aware Experience Sharing in Multiagent Robotic and Autonomous Systems. (arXiv:2308.01274v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.01274</link>
<description rdf:parseType="Literal">&lt;p&gt;Although experience sharing (ES) accelerates multiagent reinforcement
learning (MARL) in an advisor-advisee framework, attempts to apply ES to
decentralized multiagent systems have so far relied on trusted environments and
overlooked the possibility of adversarial manipulation and inference.
Nevertheless, in a real-world setting, some Byzantine attackers, disguised as
advisors, may provide false advice to the advisee and catastrophically degrade
the overall learning performance. Also, an inference attacker, disguised as an
advisee, may conduct several queries to infer the advisors&apos; private information
and make the entire ES process questionable in terms of privacy leakage. To
address and tackle these issues, we propose a novel MARL framework (BRNES) that
heuristically selects a dynamic neighbor zone for each advisee at each learning
step and adopts a weighted experience aggregation technique to reduce Byzantine
attack impact. Furthermore, to keep the agent&apos;s private information safe from
adversarial inference attacks, we leverage the local differential privacy
(LDP)-induced noise during the ES process. Our experiments show that our
framework outperforms the state-of-the-art in terms of the steps to goal,
obtained reward, and time to goal metrics. Particularly, our evaluation shows
that the proposed framework is 8.32x faster than the current non-private
frameworks and 1.41x faster than the private frameworks in an adversarial
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Md Tamjid Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+La_H/0/1/0/all/0/1&quot;&gt;Hung Manh La&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badsha_S/0/1/0/all/0/1&quot;&gt;Shahriar Badsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netchaev_A/0/1/0/all/0/1&quot;&gt;Anton Netchaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01308">
<title>Masked and Swapped Sequence Modeling for Next Novel Basket Recommendation in Grocery Shopping. (arXiv:2308.01308v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.01308</link>
<description rdf:parseType="Literal">&lt;p&gt;Next basket recommendation (NBR) is the task of predicting the next set of
items based on a sequence of already purchased baskets. It is a recommendation
task that has been widely studied, especially in the context of grocery
shopping. In next basket recommendation (NBR), it is useful to distinguish
between repeat items, i.e., items that a user has consumed before, and explore
items, i.e., items that a user has not consumed before. Most NBR work either
ignores this distinction or focuses on repeat items. We formulate the next
novel basket recommendation (NNBR) task, i.e., the task of recommending a
basket that only consists of novel items, which is valuable for both real-world
application and NBR evaluation. We evaluate how existing NBR methods perform on
the NNBR task and find that, so far, limited progress has been made w.r.t. the
NNBR task. To address the NNBR task, we propose a simple bi-directional
transformer basket recommendation model (BTBR), which is focused on directly
modeling item-to-item correlations within and across baskets instead of
learning complex basket representations. To properly train BTBR, we propose and
investigate several masking strategies and training objectives: (i) item-level
random masking, (ii) item-level select masking, (iii) basket-level all masking,
(iv) basket-level explore masking, and (v) joint masking. In addition, an
item-basket swapping strategy is proposed to enrich the item interactions
within the same baskets. We conduct extensive experiments on three open
datasets with various characteristics. The results demonstrate the
effectiveness of BTBR and our masking and swapping strategies for the NNBR
task. BTBR with a properly selected masking and swapping strategy can
substantially improve NNBR performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ariannezhad_M/0/1/0/all/0/1&quot;&gt;Mozhdeh Ariannezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1&quot;&gt;Andrew Yates&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1&quot;&gt;Maarten de Rijke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01312">
<title>Lode Encoder: AI-constrained co-creativity. (arXiv:2308.01312v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01312</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Lode Encoder, a gamified mixed-initiative level creation system
for the classic platform-puzzle game Lode Runner. The system is built around
several autoencoders which are trained on sets of Lode Runner levels. When fed
with the user&apos;s design, each autoencoder produces a version of that design
which is closer in style to the levels that it was trained on. The Lode Encoder
interface allows the user to build and edit levels through &apos;painting&apos; from the
suggestions provided by the autoencoders. Crucially, in order to encourage
designers to explore new possibilities, the system does not include more
traditional editing tools. We report on the system design and training
procedure, as well as on the evolution of the system itself and user tests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhaumik_D/0/1/0/all/0/1&quot;&gt;Debosmita Bhaumik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifa_A/0/1/0/all/0/1&quot;&gt;Ahmed Khalifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01313">
<title>More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes. (arXiv:2308.01313v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01313</link>
<description rdf:parseType="Literal">&lt;p&gt;CLIP, as a foundational vision language model, is widely used in zero-shot
image classification due to its ability to understand various visual concepts
and natural language descriptions. However, how to fully leverage CLIP&apos;s
unprecedented human-like understanding capabilities to achieve better zero-shot
classification is still an open question. This paper draws inspiration from the
human visual perception process: a modern neuroscience view suggests that in
classifying an object, humans first infer its class-independent attributes
(e.g., background and orientation) which help separate the foreground object
from the background, and then make decisions based on this information.
Inspired by this, we observe that providing CLIP with contextual attributes
improves zero-shot classification and mitigates reliance on spurious features.
We also observe that CLIP itself can reasonably infer the attributes from an
image. With these observations, we propose a training-free, two-step zero-shot
classification method named PerceptionCLIP. Given an image, it first infers
contextual attributes (e.g., background) and then performs object
classification conditioning on them. Our experiments show that PerceptionCLIP
achieves better generalization, group robustness, and better interpretability.
For example, PerceptionCLIP with ViT-L/14 improves the worst group accuracy by
16.5% on the Waterbirds dataset and by 3.5% on CelebA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panaitescu_Liess_M/0/1/0/all/0/1&quot;&gt;Michael-Andrei Panaitescu-Liess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mummadi_C/0/1/0/all/0/1&quot;&gt;Chaithanya Kumar Mummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01314">
<title>Evaluating the Robustness of Test Selection Methods for Deep Neural Networks. (arXiv:2308.01314v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01314</link>
<description rdf:parseType="Literal">&lt;p&gt;Testing deep learning-based systems is crucial but challenging due to the
required time and labor for labeling collected raw data. To alleviate the
labeling effort, multiple test selection methods have been proposed where only
a subset of test data needs to be labeled while satisfying testing
requirements. However, we observe that such methods with reported promising
results are only evaluated under simple scenarios, e.g., testing on original
test data. This brings a question to us: are they always reliable? In this
paper, we explore when and to what extent test selection methods fail for
testing. Specifically, first, we identify potential pitfalls of 11 selection
methods from top-tier venues based on their construction. Second, we conduct a
study on five datasets with two model architectures per dataset to empirically
confirm the existence of these pitfalls. Furthermore, we demonstrate how
pitfalls can break the reliability of these methods. Concretely, methods for
fault detection suffer from test data that are: 1) correctly classified but
uncertain, or 2) misclassified but confident. Remarkably, the test relative
coverage achieved by such methods drops by up to 86.85%. On the other hand,
methods for performance estimation are sensitive to the choice of
intermediate-layer output. The effectiveness of such methods can be even worse
than random selection when using an inappropriate layer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuejun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaofei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordy_M/0/1/0/all/0/1&quot;&gt;Maxime Cordy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadakis_M/0/1/0/all/0/1&quot;&gt;Mike Papadakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1&quot;&gt;Yves Le Traon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1706.03762">
<title>Attention Is All You Need. (arXiv:1706.03762v7 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/1706.03762</link>
<description rdf:parseType="Literal">&lt;p&gt;The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks in an encoder-decoder configuration. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer, based
solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to be
superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014
English-to-German translation task, improving over the existing best results,
including ensembles by over 2 BLEU. On the WMT 2014 English-to-French
translation task, our model establishes a new single-model state-of-the-art
BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction
of the training costs of the best models from the literature. We show that the
Transformer generalizes well to other tasks by applying it successfully to
English constituency parsing both with large and limited training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1&quot;&gt;Ashish Vaswani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1&quot;&gt;Noam Shazeer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1&quot;&gt;Niki Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1&quot;&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_L/0/1/0/all/0/1&quot;&gt;Llion Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Aidan N. Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaiser_L/0/1/0/all/0/1&quot;&gt;Lukasz Kaiser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polosukhin_I/0/1/0/all/0/1&quot;&gt;Illia Polosukhin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.08657">
<title>Class-incremental Learning with Pre-allocated Fixed Classifiers. (arXiv:2010.08657v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.08657</link>
<description rdf:parseType="Literal">&lt;p&gt;In class-incremental learning, a learning agent faces a stream of data with
the goal of learning new classes while not forgetting previous ones. Neural
networks are known to suffer under this setting, as they forget previously
acquired knowledge. To address this problem, effective methods exploit past
data stored in an episodic memory while expanding the final classifier nodes to
accommodate the new classes.
&lt;/p&gt;
&lt;p&gt;In this work, we substitute the expanding classifier with a novel fixed
classifier in which a number of pre-allocated output nodes are subject to the
classification loss right from the beginning of the learning phase. Contrarily
to the standard expanding classifier, this allows: (a) the output nodes of
future unseen classes to firstly see negative samples since the beginning of
learning together with the positive samples that incrementally arrive; (b) to
learn features that do not change their geometric configuration as novel
classes are incorporated in the learning model.
&lt;/p&gt;
&lt;p&gt;Experiments with public datasets show that the proposed approach is as
effective as the expanding classifier while exhibiting novel intriguing
properties of the internal feature representation that are otherwise
not-existent. Our ablation study on pre-allocating a large number of classes
further validates the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pernici_F/0/1/0/all/0/1&quot;&gt;Federico Pernici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruni_M/0/1/0/all/0/1&quot;&gt;Matteo Bruni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baecchi_C/0/1/0/all/0/1&quot;&gt;Claudio Baecchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turchini_F/0/1/0/all/0/1&quot;&gt;Francesco Turchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.10401">
<title>Multi-Attention-Based Soft Partition Network for Vehicle Re-Identification. (arXiv:2104.10401v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2104.10401</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle re-identification helps in distinguishing between images of the same
and other vehicles. It is a challenging process because of significant
intra-instance differences between identical vehicles from different views and
subtle inter-instance differences between similar vehicles. To solve this
issue, researchers have extracted view-aware or part-specific features via
spatial attention mechanisms, which usually result in noisy attention maps or
otherwise require expensive additional annotation for metadata, such as key
points, to improve the quality. Meanwhile, based on the researchers&apos; insights,
various handcrafted multi-attention architectures for specific viewpoints or
vehicle parts have been proposed. However, this approach does not guarantee
that the number and nature of attention branches will be optimal for real-world
re-identification tasks. To address these problems, we proposed a new vehicle
re-identification network based on a multiple soft attention mechanism for
capturing various discriminative regions from different viewpoints more
efficiently. Furthermore, this model can significantly reduce the noise in
spatial attention maps by devising a new method for creating an attention map
for insignificant regions and then excluding it from generating the final
result. We also combined a channel-wise attention mechanism with a spatial
attention mechanism for the efficient selection of important semantic
attributes for vehicle re-identification. Our experiments showed that our
proposed model achieved a state-of-the-art performance among the
attention-based methods without metadata and was comparable to the approaches
using metadata for the VehicleID and VERI-Wild datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangrok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_T/0/1/0/all/0/1&quot;&gt;Taekang Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang Hun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.02589">
<title>Bandit based centralized matching in two-sided markets for peer to peer lending. (arXiv:2105.02589v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2105.02589</link>
<description rdf:parseType="Literal">&lt;p&gt;Sequential fundraising in two sided online platforms enable peer to peer
lending by sequentially bringing potential contributors, each of whose
decisions impact other contributors in the market. However, understanding the
dynamics of sequential contributions in online platforms for peer lending has
been an open ended research question. The centralized investment mechanism in
these platforms makes it difficult to understand the implicit competition that
borrowers face from a single lender at any point in time. Matching markets are
a model of pairing agents where the preferences of agents from both sides in
terms of their preferred pairing for transactions can allow to decentralize the
market. We study investment designs in two sided platforms using matching
markets when the investors or lenders also face restrictions on the investments
based on borrower preferences. This situation creates an implicit competition
among the lenders in addition to the existing borrower competition, especially
when the lenders are uncertain about their standing in the market and thereby
the probability of their investments being accepted or the borrower loan
requests for projects reaching the reserve price. We devise a technique based
on sequential decision making that allows the lenders to adjust their choices
based on the dynamics of uncertainty from competition over time. We simulate
two sided market matchings in a sequential decision framework and show the
dynamics of the lender regret amassed compared to the optimal borrower-lender
matching and find that the lender regret depends on the initial preferences set
by the lenders which could affect their learning over decision making steps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1&quot;&gt;Soumajyoti Sarkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.12351">
<title>Integrated Conditional Estimation-Optimization. (arXiv:2110.12351v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2110.12351</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world optimization problems involve uncertain parameters with
probability distributions that can be estimated using contextual feature
information. In contrast to the standard approach of first estimating the
distribution of uncertain parameters and then optimizing the objective based on
the estimation, we propose an integrated conditional estimation-optimization
(ICEO) framework that estimates the underlying conditional distribution of the
random parameter while considering the structure of the optimization problem.
We directly model the relationship between the conditional distribution of the
random parameter and the contextual features, and then estimate the
probabilistic model with an objective that aligns with the downstream
optimization problem. We show that our ICEO approach is asymptotically
consistent under moderate regularity conditions and further provide finite
performance guarantees in the form of generalization bounds. Computationally,
performing estimation with the ICEO approach is a non-convex and often
non-differentiable optimization problem. We propose a general methodology for
approximating the potentially non-differentiable mapping from estimated
conditional distribution to the optimal decision by a differentiable function,
which greatly improves the performance of gradient-based algorithms applied to
the non-convex problem. We also provide a polynomial optimization solution
approach in the semi-algebraic case. Numerical experiments are also conducted
to show the empirical success of our approach in different situations including
with limited data samples and model mismatches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qi_M/0/1/0/all/0/1&quot;&gt;Meng Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Grigas_P/0/1/0/all/0/1&quot;&gt;Paul Grigas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zuo-Jun Max Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.15701">
<title>Successor Feature Representations. (arXiv:2110.15701v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.15701</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer in Reinforcement Learning aims to improve learning performance on
target tasks using knowledge from experienced source tasks. Successor
Representations (SR) and their extension Successor Features (SF) are prominent
transfer mechanisms in domains where reward functions change between tasks.
They reevaluate the expected return of previously learned policies in a new
target task to transfer their knowledge. The SF framework extended SR by
linearly decomposing rewards into successor features and a reward weight vector
allowing their application in high-dimensional tasks. But this came with the
cost of having a linear relationship between reward functions and successor
features, limiting its application to tasks where such a linear relationship
exists. We propose a novel formulation of SR based on learning the cumulative
discounted probability of successor features, called Successor Feature
Representations (SFR). Crucially, SFR allows to reevaluate the expected return
of policies for general reward functions. We introduce different SFR
variations, prove its convergence, and provide a guarantee on its transfer
performance. Experimental evaluations based on SFR with function approximation
demonstrate its advantage over SF not only for general reward functions, but
also in the case of linearly decomposable reward functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinke_C/0/1/0/all/0/1&quot;&gt;Chris Reinke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1&quot;&gt;Xavier Alameda-Pineda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05877">
<title>Fabricated Flips: Poisoning Federated Learning without Data. (arXiv:2202.05877v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05877</link>
<description rdf:parseType="Literal">&lt;p&gt;Attacks on Federated Learning (FL) can severely reduce the quality of the
generated models and limit the usefulness of this emerging learning paradigm
that enables on-premise decentralized learning. However, existing untargeted
attacks are not practical for many scenarios as they assume that i) the
attacker knows every update of benign clients, or ii) the attacker has a large
dataset to locally train updates imitating benign parties. In this paper, we
propose a data-free untargeted attack (DFA) that synthesizes malicious data to
craft adversarial models without eavesdropping on the transmission of benign
clients at all or requiring a large quantity of task-specific training data. We
design two variants of DFA, namely DFA-R and DFA-G, which differ in how they
trade off stealthiness and effectiveness. Specifically, DFA-R iteratively
optimizes a malicious data layer to minimize the prediction confidence of all
outputs of the global model, whereas DFA-G interactively trains a malicious
data generator network by steering the output of the global model toward a
particular class. Experimental results on Fashion-MNIST, Cifar-10, and SVHN
show that DFA, despite requiring fewer assumptions than existing attacks,
achieves similar or even higher attack success rate than state-of-the-art
untargeted attacks against various state-of-the-art defense mechanisms.
Concretely, they can evade all considered defense mechanisms in at least 50% of
the cases for CIFAR-10 and often reduce the accuracy by more than a factor of
2. Consequently, we design REFD, a defense specifically crafted to protect
against data-free attacks. REFD leverages a reference dataset to detect updates
that are biased or have a low confidence. It greatly improves upon existing
defenses by filtering out the malicious updates and achieves high global model
accuracy
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiyue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zilong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lydia Y. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roos_S/0/1/0/all/0/1&quot;&gt;Stefanie Roos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.11740">
<title>Different brain regions and their functions to simulate Multiple cortexes Heart-Brain. (arXiv:2203.11740v16 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2203.11740</link>
<description rdf:parseType="Literal">&lt;p&gt;In addition to the shared weights of the synaptic connections, we proposed a
new neural network that includes the synaptic effective range weights for both
the forward and back propagation. We try to simulate the prefrontal lobe,
amygdala, and hippocampus. Along the forward direction, the fear memory
gradually increases. Along the direction of back propagation, the optimization
order increases. The brain plasticity in positive or negative memory may be
quantum and produce short-term memory and exhibits an exponential decay in the
wave function over a period of time, produced in the hippocampus. The quantum
entanglement from the heart frequency and the brain architecture. Memory flow
may be considered to be the transmission of the rate of change of the
architecture, then the nth cortex is the nth derivative of brain plasticity. It
is PNN model of memory Generation-Consolidation-Loss. The positive gradient of
Back propagation can explain synaptic inhibition and excitatory are reversed,
synaptic loss, hippocampal sclerosis, and shrinking hippocampus. Reverse of
Forward propagation can explain the hallucinations of Alzheimer&apos;s disease.
Astrocytic cortex memory persistence factor also inhibits local synaptic
accumulation, and the model inspires experiments. This could be the process of
astrocytes phagocytose synapses is driven by both positive and negative
memories of plasticity which reflect heart frequency. In simulation, it is
possible that thicker cortices and more diverse individuals within the brain
could have high IQ, but thickest cortices and most diverse individuals may have
low IQ in simulation and tries to give the mechanism of Cognitive impairment.
PSO considers global solution or best previous solution, but also considers
relatively good and relatively inferior solution. And PNN modified ResNet to
consider memory Jacobian and Heisen matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jun-Bo Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bai-Qing Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wei-Dong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1&quot;&gt;Shi-You Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling-Kun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia-Qiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guo-Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.15925">
<title>Asynchronous, Option-Based Multi-Agent Policy Gradient: A Conditional Reasoning Approach. (arXiv:2203.15925v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2203.15925</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative multi-agent problems often require coordination between agents,
which can be achieved through a centralized policy that considers the global
state. Multi-agent policy gradient (MAPG) methods are commonly used to learn
such policies, but they are often limited to problems with low-level action
spaces. In complex problems with large state and action spaces, it is
advantageous to extend MAPG methods to use higher-level actions, also known as
options, to improve the policy search efficiency. However, multi-robot option
executions are often asynchronous, that is, agents may select and complete
their options at different time steps. This makes it difficult for MAPG methods
to derive a centralized policy and evaluate its gradient, as centralized policy
always select new options at the same time. In this work, we propose a novel,
conditional reasoning approach to address this problem and demonstrate its
effectiveness on representative option-based multi-agent cooperative tasks
through empirical validation. Find code and videos at:
\href{https://sites.google.com/view/mahrlsupp/}{https://sites.google.com/view/mahrlsupp/}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xubo Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banitalebi_Dehkordi_A/0/1/0/all/0/1&quot;&gt;Amin Banitalebi-Dehkordi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13492">
<title>Sparse Graph Learning from Spatiotemporal Time Series. (arXiv:2205.13492v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13492</link>
<description rdf:parseType="Literal">&lt;p&gt;Outstanding achievements of graph neural networks for spatiotemporal time
series analysis show that relational constraints introduce an effective
inductive bias into neural forecasting architectures. Often, however, the
relational information characterizing the underlying data-generating process is
unavailable and the practitioner is left with the problem of inferring from
data which relational graph to use in the subsequent processing stages. We
propose novel, principled - yet practical - probabilistic score-based methods
that learn the relational dependencies as distributions over graphs while
maximizing end-to-end the performance at task. The proposed graph learning
framework is based on consolidated variance reduction techniques for Monte
Carlo score-based gradient estimation, is theoretically grounded, and, as we
show, effective in practice. In this paper, we focus on the time series
forecasting problem and show that, by tailoring the gradient estimators to the
graph learning problem, we are able to achieve state-of-the-art performance
while controlling the sparsity of the learned graph and the computational
scalability. We empirically assess the effectiveness of the proposed method on
synthetic and real-world benchmarks, showing that the proposed solution can be
used as a stand-alone graph identification procedure as well as a graph
learning component of an end-to-end forecasting architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1&quot;&gt;Andrea Cini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambon_D/0/1/0/all/0/1&quot;&gt;Daniele Zambon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1&quot;&gt;Cesare Alippi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.01255">
<title>Compressive Fourier collocation methods for high-dimensional diffusion equations with periodic boundary conditions. (arXiv:2206.01255v3 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2206.01255</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional Partial Differential Equations (PDEs) are a popular
mathematical modelling tool, with applications ranging from finance to
computational chemistry. However, standard numerical techniques for solving
these PDEs are typically affected by the curse of dimensionality. In this work,
we tackle this challenge while focusing on stationary diffusion equations
defined over a high-dimensional domain with periodic boundary conditions.
Inspired by recent progress in sparse function approximation in high
dimensions, we propose a new method called compressive Fourier collocation.
Combining ideas from compressive sensing and spectral collocation, our method
replaces the use of structured collocation grids with Monte Carlo sampling and
employs sparse recovery techniques, such as orthogonal matching pursuit and
$\ell^1$ minimization, to approximate the Fourier coefficients of the PDE
solution. We conduct a rigorous theoretical analysis showing that the
approximation error of the proposed method is comparable with the best $s$-term
approximation (with respect to the Fourier basis) to the solution. Using the
recently introduced framework of random sampling in bounded Riesz systems, our
analysis shows that the compressive Fourier collocation method mitigates the
curse of dimensionality with respect to the number of collocation points under
sufficient conditions on the regularity of the diffusion coefficient. We also
present numerical experiments that illustrate the accuracy and stability of the
method for the approximation of sparse and compressible solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Brugiapaglia_S/0/1/0/all/0/1&quot;&gt;Simone Brugiapaglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02231">
<title>Models of human preference for learning reward functions. (arXiv:2206.02231v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02231</link>
<description rdf:parseType="Literal">&lt;p&gt;The utility of reinforcement learning is limited by the alignment of reward
functions with the interests of human stakeholders. One promising method for
alignment is to learn the reward function from human-generated preferences
between pairs of trajectory segments, a type of reinforcement learning from
human feedback (RLHF). These human preferences are typically assumed to be
informed solely by partial return, the sum of rewards along each segment. We
find this assumption to be flawed and propose modeling human preferences
instead as informed by each segment&apos;s regret, a measure of a segment&apos;s
deviation from optimal decision-making. Given infinitely many preferences
generated according to regret, we prove that we can identify a reward function
equivalent to the reward function that generated those preferences, and we
prove that the previous partial return model lacks this identifiability
property in multiple contexts. We empirically show that our proposed regret
preference model outperforms the partial return preference model with finite
training data in otherwise the same setting. Additionally, we find that our
proposed regret preference model better predicts real human preferences and
also learns reward functions from these preferences that lead to policies that
are better human-aligned. Overall, this work establishes that the choice of
preference model is impactful, and our proposed regret preference model
provides an improvement upon a core assumption of recent research. We have open
sourced our experimental code, the human preferences dataset we gathered, and
our training and preference elicitation interfaces for gathering a such a
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knox_W/0/1/0/all/0/1&quot;&gt;W. Bradley Knox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatgis_Kessell_S/0/1/0/all/0/1&quot;&gt;Stephane Hatgis-Kessell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1&quot;&gt;Serena Booth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1&quot;&gt;Alessandro Allievi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10276">
<title>ProMix: Combating Label Noise via Maximizing Clean Sample Utility. (arXiv:2207.10276v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10276</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with Noisy Labels (LNL) has become an appealing topic, as
imperfectly annotated data are relatively cheaper to obtain. Recent
state-of-the-art approaches employ specific selection mechanisms to separate
clean and noisy samples and then apply Semi-Supervised Learning (SSL)
techniques for improved performance. However, the selection step mostly
provides a medium-sized and decent-enough clean subset, which overlooks a rich
set of clean samples. To fulfill this, we propose a novel LNL framework ProMix
that attempts to maximize the utility of clean samples for boosted performance.
Key to our method, we propose a matched high confidence selection technique
that selects those examples with high confidence scores and matched predictions
with given labels to dynamically expand a base clean sample set. To overcome
the potential side effect of excessive clean set selection procedure, we
further devise a novel SSL framework that is able to train balanced and
unbiased classifiers on the separated clean and noisy samples. Extensive
experiments demonstrate that ProMix significantly advances the current
state-of-the-art results on multiple benchmarks with different types and levels
of noise. It achieves an average improvement of 2.48\% on the CIFAR-N dataset.
The code is available at https://github.com/Justherozen/ProMix
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1&quot;&gt;Ruixuan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yiwen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Lei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Runze Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13964">
<title>Graph Soft-Contrastive Learning via Neighborhood Ranking. (arXiv:2209.13964v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13964</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Contrastive Learning (GCL) has emerged as a promising approach in the
realm of graph self-supervised learning. Prevailing GCL methods mainly derive
from the principles of contrastive learning in the field of computer vision:
modeling invariance by specifying absolutely similar pairs. However, when
applied to graph data, this paradigm encounters two significant limitations:
(1) the validity of the generated views cannot be guaranteed: graph
perturbation may produce invalid views against semantics and intrinsic topology
of graph data; (2) specifying absolutely similar pairs in the graph views is
unreliable: for abstract and non-Euclidean graph data, it is difficult for
humans to decide the absolute similarity and dissimilarity intuitively. Despite
the notable performance of current GCL methods, these challenges necessitate a
reevaluation: Could GCL be more effectively tailored to the intrinsic
properties of graphs, rather than merely adopting principles from computer
vision? In response to this query, we propose a novel paradigm, Graph
Soft-Contrastive Learning (GSCL). This approach facilitates GCL via
neighborhood ranking, avoiding the need to specify absolutely similar pairs.
GSCL leverages the underlying graph characteristic of diminishing label
consistency, asserting that nodes that are closer in the graph are overall more
similar than far-distant nodes. Within the GSCL framework, we introduce
pairwise and listwise gated ranking InfoNCE loss functions to effectively
preserve the relative similarity ranking within neighborhoods. Moreover, as the
neighborhood size exponentially expands with more hops considered, we propose
neighborhood sampling strategies to improve learning efficiency. Our extensive
empirical results across 11 commonly used graph datasets-including 8 homophily
graphs and 3 heterophily graphs-demonstrate GSCL&apos;s superior performance
compared to 20 SOTA GCL methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1&quot;&gt;Ziyue Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Denghui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04052">
<title>FedDef: Defense Against Gradient Leakage in Federated Learning-based Network Intrusion Detection Systems. (arXiv:2210.04052v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04052</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) methods have been widely applied to anomaly-based network
intrusion detection system (NIDS) to detect malicious traffic. To expand the
usage scenarios of DL-based methods, federated learning (FL) allows multiple
users to train a global model on the basis of respecting individual data
privacy. However, it has not yet been systematically evaluated how robust
FL-based NIDSs are against existing privacy attacks under existing defenses. To
address this issue, we propose two privacy evaluation metrics designed for
FL-based NIDSs, including (1) privacy score that evaluates the similarity
between the original and recovered traffic features using reconstruction
attacks, and (2) evasion rate against NIDSs using adversarial attack with the
recovered traffic. We conduct experiments to illustrate that existing defenses
provide little protection and the corresponding adversarial traffic can even
evade the SOTA NIDS Kitsune. To defend against such attacks and build a more
robust FL-based NIDS, we further propose FedDef, a novel optimization-based
input perturbation defense strategy with theoretical guarantee. It achieves
both high utility by minimizing the gradient distance and strong privacy
protection by maximizing the input distance. We experimentally evaluate four
existing defenses on four datasets and show that our defense outperforms all
the baselines in terms of privacy protection with up to 7 times higher privacy
score, while maintaining model accuracy loss within 3% under optimal parameter
combination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiahui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xuewei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Ke Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04635">
<title>FaDIn: Fast Discretized Inference for Hawkes Processes with General Parametric Kernels. (arXiv:2210.04635v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04635</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal point processes (TPP) are a natural tool for modeling event-based
data. Among all TPP models, Hawkes processes have proven to be the most widely
used, mainly due to their adequate modeling for various applications,
particularly when considering exponential or non-parametric kernels. Although
non-parametric kernels are an option, such models require large datasets. While
exponential kernels are more data efficient and relevant for specific
applications where events immediately trigger more events, they are ill-suited
for applications where latencies need to be estimated, such as in neuroscience.
This work aims to offer an efficient solution to TPP inference using general
parametric kernels with finite support. The developed solution consists of a
fast $\ell_2$ gradient-based solver leveraging a discretized version of the
events. After theoretically supporting the use of discretization, the
statistical and computational efficiency of the novel approach is demonstrated
through various numerical experiments. Finally, the method&apos;s effectiveness is
evaluated by modeling the occurrence of stimuli-induced patterns from brain
signals recorded with magnetoencephalography (MEG). Given the use of general
parametric kernels, results show that the proposed approach leads to an
improved estimation of pattern latency than the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Staerman_G/0/1/0/all/0/1&quot;&gt;Guillaume Staerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Allain_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Allain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Moreau_T/0/1/0/all/0/1&quot;&gt;Thomas Moreau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07229">
<title>Mass-Editing Memory in a Transformer. (arXiv:2210.07229v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07229</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work has shown exciting promise in updating large language models with
new memories, so as to replace obsolete information or add specialized
knowledge. However, this line of work is predominantly limited to updating
single associations. We develop MEMIT, a method for directly updating a
language model with many memories, demonstrating experimentally that it can
scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B),
exceeding prior work by orders of magnitude. Our code and data are at
https://memit.baulab.info.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1&quot;&gt;Kevin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Arnab Sen Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1&quot;&gt;Alex Andonian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1&quot;&gt;Yonatan Belinkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07420">
<title>Learning to Efficiently Plan Robust Frictional Multi-Object Grasps. (arXiv:2210.07420v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a decluttering problem where multiple rigid convex polygonal
objects rest in randomly placed positions and orientations on a planar surface
and must be efficiently transported to a packing box using both single and
multi-object grasps. Prior work considered frictionless multi-object grasping.
In this paper, we introduce friction to increase the number of potential grasps
for a given group of objects, and thus increase picks per hour. We train a
neural network using real examples to plan robust multi-object grasps. In
physical experiments, we find a 13.7% increase in success rate, a 1.6x increase
in picks per hour, and a 6.3x decrease in grasp planning time compared to prior
work on multi-object grasping. Compared to single-object grasping, we find a
3.1x increase in picks per hour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agboh_W/0/1/0/all/0/1&quot;&gt;Wisdom C. Agboh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Satvik Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivas_K/0/1/0/all/0/1&quot;&gt;Kishore Srinivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parulekar_M/0/1/0/all/0/1&quot;&gt;Mallika Parulekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Datta_G/0/1/0/all/0/1&quot;&gt;Gaurav Datta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_T/0/1/0/all/0/1&quot;&gt;Tianshuang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1&quot;&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solowjow_E/0/1/0/all/0/1&quot;&gt;Eugen Solowjow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dogar_M/0/1/0/all/0/1&quot;&gt;Mehmet Dogar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1&quot;&gt;Ken Goldberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08549">
<title>Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08549</link>
<description rdf:parseType="Literal">&lt;p&gt;With a rising attention for the issue of PM2.5 or PM0.3, particulate matters
have become not only a potential threat to both the environment and human, but
also a harming existence to instruments onboard International Space Station
(ISS). Our team is aiming to relate various concentration of particulate
matters to magnetic fields, humidity, acceleration, temperature, pressure and
CO2 concentration. Our goal is to establish an early warning system (EWS),
which is able to forecast the levels of particulate matters and provides ample
reaction time for astronauts to protect their instruments in some experiments
or increase the accuracy of the measurements; In addition, the constructed
model can be further developed into a prototype of a remote-sensing smoke alarm
for applications related to fires. In this article, we will implement the
Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for
past 90 minutes and predict the levels of particulates which over 2.5
micrometer per 0.1 liter for the next 1 minute, which is classified as an early
warning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_P/0/1/0/all/0/1&quot;&gt;Po-Han Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei-Chih Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Hong-Chun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu-Hao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shue_J/0/1/0/all/0/1&quot;&gt;Jih-Hong Shue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01258">
<title>Instance-Dependent Generalization Bounds via Optimal Transport. (arXiv:2211.01258v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01258</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing generalization bounds fail to explain crucial factors that drive
generalization of modern neural networks. Since such bounds often hold
uniformly over all parameters, they suffer from over-parametrization, and fail
to account for the strong inductive bias of initialization and stochastic
gradient descent. As an alternative, we propose a novel optimal transport
interpretation of the generalization problem. This allows us to derive
instance-dependent generalization bounds that depend on the local Lipschitz
regularity of the earned prediction function in the data space. Therefore, our
bounds are agnostic to the parametrization of the model and work well when the
number of training samples is much smaller than the number of parameters. With
small modifications, our approach yields accelerated rates for data on
low-dimensional manifolds, and guarantees under distribution shifts. We
empirically analyze our generalization bounds for neural networks, showing that
the bound values are meaningful and capture the effect of popular
regularization methods during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_S/0/1/0/all/0/1&quot;&gt;Songyan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kassraie_P/0/1/0/all/0/1&quot;&gt;Parnian Kassraie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kratsios_A/0/1/0/all/0/1&quot;&gt;Anastasis Kratsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1&quot;&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08043">
<title>The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. (arXiv:2211.08043v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08043</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine the last-iterate convergence rate of Bregman proximal methods -
from mirror descent to mirror-prox and its optimistic variants - as a function
of the local geometry induced by the prox-mapping defining the method. For
generality, we focus on local solutions of constrained, non-monotone
variational inequalities, and we show that the convergence rate of a given
method depends sharply on its associated Legendre exponent, a notion that
measures the growth rate of the underlying Bregman function (Euclidean,
entropic, or other) near a solution. In particular, we show that boundary
solutions exhibit a stark separation of regimes between methods with a zero and
non-zero Legendre exponent: the former converge at a linear rate, while the
latter converge, in general, sublinearly. This dichotomy becomes even more
pronounced in linearly constrained problems where methods with entropic
regularization achieve a linear convergence rate along sharp directions,
compared to convergence in a finite number of steps under Euclidean
regularization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Azizian_W/0/1/0/all/0/1&quot;&gt;Wa&amp;#xef;ss Azizian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Iutzeler_F/0/1/0/all/0/1&quot;&gt;Franck Iutzeler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malick_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me Malick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mertikopoulos_P/0/1/0/all/0/1&quot;&gt;Panayotis Mertikopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14220">
<title>Data-driven identification and analysis of the glass transition in polymer melts. (arXiv:2211.14220v2 [cond-mat.soft] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14220</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the nature of glass transition, as well as precise estimation
of the glass transition temperature for polymeric materials, remain open
questions in both experimental and theoretical polymer sciences. We propose a
data-driven approach, which utilizes the high-resolution details accessible
through the molecular dynamics simulation and considers the structural
information of individual chains. It clearly identifies the glass transition
temperature of polymer melts of weakly semiflexible chains. By combining
principal component analysis and clustering, we identify the glass transition
temperature in the asymptotic limit even from relatively short-time
trajectories, which just reach into the Rouse-like monomer displacement regime.
We demonstrate that fluctuations captured by the principal component analysis
reflect the change in a chain&apos;s behaviour: from conformational rearrangement
above to small rearrangements below the glass transition temperature. Our
approach is straightforward to apply, and should be applicable to other
polymeric glass-forming liquids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Banerjee_A/0/1/0/all/0/1&quot;&gt;Atreyee Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hsu_H/0/1/0/all/0/1&quot;&gt;Hsiao-Ping Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kremer_K/0/1/0/all/0/1&quot;&gt;Kurt Kremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Kukharenko_O/0/1/0/all/0/1&quot;&gt;Oleksandra Kukharenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05206">
<title>Thinking Fast and Slow in Large Language Models. (arXiv:2212.05206v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05206</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Therefore, it is of
great importance to evaluate their emerging abilities. In this study, we show
that LLMs like GPT-3 exhibit behavior that strikingly resembles human-like
intuition - and the cognitive errors that come with it. However, LLMs with
higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to
avoid succumbing to these errors and perform in a hyperrational manner. For our
experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as
semantic illusions that were originally designed to investigate intuitive
decision-making in humans. Our study demonstrates that investigating LLMs with
methods from psychology has the potential to reveal otherwise unknown emergent
traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1&quot;&gt;Thilo Hagendorff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabi_S/0/1/0/all/0/1&quot;&gt;Sarah Fabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosinski_M/0/1/0/all/0/1&quot;&gt;Michal Kosinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.08360">
<title>Domain-adapted Learning and Imitation: DRL for Power Arbitrage. (arXiv:2301.08360v2 [q-fin.TR] UPDATED)</title>
<link>http://arxiv.org/abs/2301.08360</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we discuss the Dutch power market, which is comprised of a
day-ahead market and an intraday balancing market that operates like an
auction. Due to fluctuations in power supply and demand, there is often an
imbalance that leads to different prices in the two markets, providing an
opportunity for arbitrage. To address this issue, we restructure the problem
and propose a collaborative dual-agent reinforcement learning approach for this
bi-level simulation and optimization of European power arbitrage trading. We
also introduce two new implementations designed to incorporate domain-specific
knowledge by imitating the trading behaviours of power traders. By utilizing
reward engineering to imitate domain expertise, we are able to reform the
reward system for the RL agent, which improves convergence during training and
enhances overall performance. Additionally, the tranching of orders increases
bidding success rates and significantly boosts profit and loss (P&amp;amp;L). Our study
demonstrates that by leveraging domain expertise in a general learning problem,
the performance can be improved substantially, and the final integrated
approach leads to a three-fold improvement in cumulative P&amp;amp;L compared to the
original agent. Furthermore, our methodology outperforms the highest benchmark
policy by around 50% while maintaining efficient computational performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuanrong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Swaminathan_V/0/1/0/all/0/1&quot;&gt;Vignesh Raja Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Granger_N/0/1/0/all/0/1&quot;&gt;Nikita P. Granger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Perez_C/0/1/0/all/0/1&quot;&gt;Carlos Ros Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Michler_C/0/1/0/all/0/1&quot;&gt;Christian Michler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.00102">
<title>Towards Detecting Harmful Agendas in News Articles. (arXiv:2302.00102v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.00102</link>
<description rdf:parseType="Literal">&lt;p&gt;Manipulated news online is a growing problem which necessitates the use of
automated systems to curtail its spread. We argue that while misinformation and
disinformation detection have been studied, there has been a lack of investment
in the important open challenge of detecting harmful agendas in news articles;
identifying harmful agendas is critical to flag news campaigns with the
greatest potential for real world harm. Moreover, due to real concerns around
censorship, harmful agenda detectors must be interpretable to be effective. In
this work, we propose this new task and release a dataset, NewsAgendas, of
annotated news articles for agenda identification. We show how interpretable
systems can be effective on this task and demonstrate that they can perform
comparably to black-box models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subbiah_M/0/1/0/all/0/1&quot;&gt;Melanie Subbiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1&quot;&gt;Amrita Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yilun Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1&quot;&gt;Tharindu Kumarage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1&quot;&gt;Kathleen McKeown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01486">
<title>Understanding plasticity in neural networks. (arXiv:2303.01486v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01486</link>
<description rdf:parseType="Literal">&lt;p&gt;Plasticity, the ability of a neural network to quickly change its predictions
in response to new information, is essential for the adaptability and
robustness of deep reinforcement learning systems. Deep neural networks are
known to lose plasticity over the course of training even in relatively simple
learning problems, but the mechanisms driving this phenomenon are still poorly
understood. This paper conducts a systematic empirical analysis into plasticity
loss, with the goal of understanding the phenomenon mechanistically in order to
guide the future development of targeted solutions. We find that loss of
plasticity is deeply connected to changes in the curvature of the loss
landscape, but that it often occurs in the absence of saturated units. Based on
this insight, we identify a number of parameterization and optimization design
choices which enable networks to better preserve plasticity over the course of
training. We validate the utility of these findings on larger-scale RL
benchmarks in the Arcade Learning Environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1&quot;&gt;Clare Lyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikishin_E/0/1/0/all/0/1&quot;&gt;Evgenii Nikishin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pires_B/0/1/0/all/0/1&quot;&gt;Bernardo Avila Pires&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabney_W/0/1/0/all/0/1&quot;&gt;Will Dabney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01584">
<title>Evolutionary Augmentation Policy Optimization for Self-supervised Learning. (arXiv:2303.01584v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01584</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised Learning (SSL) is a machine learning algorithm for
pretraining Deep Neural Networks (DNNs) without requiring manually labeled
data. The central idea of this learning technique is based on an auxiliary
stage aka pretext task in which labeled data are created automatically through
data augmentation and exploited for pretraining the DNN. However, the effect of
each pretext task is not well studied or compared in the literature. In this
paper, we study the contribution of augmentation operators on the performance
of self supervised learning algorithms in a constrained settings. We propose an
evolutionary search method for optimization of data augmentation pipeline in
pretext tasks and measure the impact of augmentation operators in several SOTA
SSL algorithms. By encoding different combination of augmentation operators in
chromosomes we seek the optimal augmentation policies through an evolutionary
optimization mechanism. We further introduce methods for analyzing and
explaining the performance of optimized SSL algorithms. Our results indicate
that our proposed method can find solutions that outperform the accuracy of
classification of SSL algorithms which confirms the influence of augmentation
policy choice on the overall performance of SSL algorithms. We also compare
optimal SSL solutions found by our evolutionary search mechanism and show the
effect of batch size in the pretext task on two visual datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_N/0/1/0/all/0/1&quot;&gt;Noah Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_Z/0/1/0/all/0/1&quot;&gt;Zahra Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matwin_S/0/1/0/all/0/1&quot;&gt;Stan Matwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01590">
<title>Technical report: Graph Neural Networks go Grammatical. (arXiv:2303.01590v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01590</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a framework to formally link a fragment of an algebraic
language to a Graph Neural Network (GNN). It relies on Context Free Grammars
(CFG) to organise algebraic operations into generative rules that can be
translated into a GNN layer model. Since the rules and variables of a CFG
directly derived from a language contain redundancies, a grammar reduction
scheme is presented making tractable the translation into a GNN layer. Applying
this strategy, a grammar compliant with the third-order Weisfeiler-Lehman
(3-WL) test is defined from MATLANG. From this 3-WL CFG, we derive a provably
3-WL GNN model called G$^2$N$^2$. Moreover, this grammatical approach allows us
to provide algebraic formulas to count the cycles of length up to six and
chordal cycles at the edge level, which enlightens the counting power of 3-WL.
Several experiments illustrate that G$^2$N$^2$ efficiently outperforms other
3-WL GNNs on many downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piquenot_J/0/1/0/all/0/1&quot;&gt;Jason Piquenot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moscatelli_A/0/1/0/all/0/1&quot;&gt;Aldo Moscatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1&quot;&gt;Maxime B&amp;#xe9;rar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heroux_P/0/1/0/all/0/1&quot;&gt;Pierre H&amp;#xe9;roux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+raveaux_R/0/1/0/all/0/1&quot;&gt;Romain raveaux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramel_J/0/1/0/all/0/1&quot;&gt;Jean-Yves Ramel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Adam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04630">
<title>Mining the contribution of intensive care clinical course to outcome after traumatic brain injury. (arXiv:2303.04630v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04630</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods to characterise the evolving condition of traumatic brain
injury (TBI) patients in the intensive care unit (ICU) do not capture the
context necessary for individualising treatment. Here, we integrate all
heterogenous data stored in medical records (1,166 pre-ICU and ICU variables)
to model the individualised contribution of clinical course to six-month
functional outcome on the Glasgow Outcome Scale - Extended (GOSE). On a
prospective cohort (n=1,550, 65 centres) of TBI patients, we train recurrent
neural network models to map a token-embedded time series representation of all
variables (including missing values) to an ordinal GOSE prognosis every two
hours. The full range of variables explains up to 52% (95% CI: 50%-54%) of the
ordinal variance in functional outcome. Up to 91% (95% CI: 90%-91%) of this
explanation is derived from pre-ICU and admission information (i.e., static
variables). Information collected in the ICU (i.e., dynamic variables)
increases explanation (by up to 5% [95% CI: 4%-6%]), though not enough to
counter poorer overall performance in longer-stay (&amp;gt;5.75 days) patients.
Highest-contributing variables include physician-based prognoses, CT features,
and markers of neurological function. Whilst static information currently
accounts for the majority of functional outcome explanation after TBI,
data-driven analysis highlights investigative avenues to improve dynamic
characterisation of longer-stay patients. Moreover, our modelling strategy
proves useful for converting large patient records into interpretable time
series with missing data integration and minimal processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyay_S/0/1/0/all/0/1&quot;&gt;Shubhayu Bhattacharyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caruso_P/0/1/0/all/0/1&quot;&gt;Pier Francesco Caruso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dkerlund_C/0/1/0/all/0/1&quot;&gt;Cecilia &amp;#xc5;kerlund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_L/0/1/0/all/0/1&quot;&gt;Lindsay Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1&quot;&gt;Robert D Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_D/0/1/0/all/0/1&quot;&gt;David K Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steyerberg_E/0/1/0/all/0/1&quot;&gt;Ewout W Steyerberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nelson_D/0/1/0/all/0/1&quot;&gt;David W Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ercole_A/0/1/0/all/0/1&quot;&gt;Ari Ercole&lt;/a&gt;, the &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+investigators/participants_C/0/1/0/all/0/1&quot;&gt;CENTER-TBI investigators/participants&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12653">
<title>Robust mmWave Beamforming by Self-Supervised Hybrid Deep Learning. (arXiv:2303.12653v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12653</link>
<description rdf:parseType="Literal">&lt;p&gt;Beamforming with large-scale antenna arrays has been widely used in recent
years, which is acknowledged as an important part in 5G and incoming 6G. Thus,
various techniques are leveraged to improve its performance, e.g., deep
learning, advanced optimization algorithms, etc. Although its performance in
many previous research scenarios with deep learning is quite attractive,
usually it drops rapidly when the environment or dataset is changed. Therefore,
designing effective beamforming network with strong robustness is an open issue
for the intelligent wireless communications. In this paper, we propose a robust
beamforming self-supervised network, and verify it in two kinds of different
datasets with various scenarios. Simulation results show that the proposed
self-supervised network with hybrid learning performs well in both classic
DeepMIMO and new WAIR-D dataset with the strong robustness under the various
environments. Also, we present the principle to explain the rationality of this
kind of hybrid learning, which is instructive to apply with more kinds of
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fenghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bohao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chongwen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexandropoulos_G/0/1/0/all/0/1&quot;&gt;George C.Alexandropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuen_C/0/1/0/all/0/1&quot;&gt;Chau Yuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debbah_M/0/1/0/all/0/1&quot;&gt;Merouane Debbah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14604">
<title>Green Federated Learning. (arXiv:2303.14604v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14604</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of AI is fueled by increasingly large and computationally
intensive machine learning models and datasets. As a consequence, the amount of
compute used in training state-of-the-art models is exponentially increasing
(doubling every 10 months between 2015 and 2022), resulting in a large carbon
footprint. Federated Learning (FL) - a collaborative machine learning technique
for training a centralized model using data of decentralized entities - can
also be resource-intensive and have a significant carbon footprint,
particularly when deployed at scale. Unlike centralized AI that can reliably
tap into renewables at strategically placed data centers, cross-device FL may
leverage as many as hundreds of millions of globally distributed end-user
devices with diverse energy sources. Green AI is a novel and important research
area where carbon footprint is regarded as an evaluation criterion for AI,
alongside accuracy, convergence speed, and other metrics. In this paper, we
propose the concept of Green FL, which involves optimizing FL parameters and
making design choices to minimize carbon emissions consistent with competitive
performance and training time. The contributions of this work are two-fold.
First, we adopt a data-driven approach to quantify the carbon emissions of FL
by directly measuring real-world at-scale FL tasks running on millions of
phones. Second, we present challenges, guidelines, and lessons learned from
studying the trade-off between energy efficiency, performance, and
time-to-train in a production FL system. Our findings offer valuable insights
into how FL can reduce its carbon footprint, and they provide a foundation for
future research in the area of Green AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yousefpour_A/0/1/0/all/0/1&quot;&gt;Ashkan Yousefpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1&quot;&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sayan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stock_P/0/1/0/all/0/1&quot;&gt;Pierre Stock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maeng_K/0/1/0/all/0/1&quot;&gt;Kiwan Maeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruger_S/0/1/0/all/0/1&quot;&gt;Schalk-Willem Kr&amp;#xfc;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1&quot;&gt;Michael Rabbat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Carole-Jean Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mironov_I/0/1/0/all/0/1&quot;&gt;Ilya Mironov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16132">
<title>Transformer and Snowball Graph Convolution Learning for Brain functional network Classification. (arXiv:2303.16132v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16132</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced deep learning methods, especially graph neural networks (GNNs), are
increasingly expected to learn from brain functional network data and predict
brain disorders. In this paper, we proposed a novel Transformer and snowball
encoding networks (TSEN) for brain functional network classification, which
introduced Transformer architecture with graph snowball connection into GNNs
for learning whole-graph representation. TSEN combined graph snowball
connection with graph Transformer by snowball encoding layers, which enhanced
the power to capture multi-scale information and global patterns of brain
functional networks. TSEN also introduced snowball graph convolution as
position embedding in Transformer structure, which was a simple yet effective
method for capturing local patterns naturally. We evaluated the proposed model
by two large-scale brain functional network datasets from autism spectrum
disorder and major depressive disorder respectively, and the results
demonstrated that TSEN outperformed the state-of-the-art GNN models and the
graph-transformer based GNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinlong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yangmin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shoubin Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05527">
<title>Black Box Variational Inference with a Deterministic Objective: Faster, More Accurate, and Even More Black Box. (arXiv:2304.05527v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05527</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic differentiation variational inference (ADVI) offers fast and
easy-to-use posterior approximation in multiple modern probabilistic
programming languages. However, its stochastic optimizer lacks clear
convergence criteria and requires tuning parameters. Moreover, ADVI inherits
the poor posterior uncertainty estimates of mean-field variational Bayes
(MFVB). We introduce ``deterministic ADVI&apos;&apos; (DADVI) to address these issues.
DADVI replaces the intractable MFVB objective with a fixed Monte Carlo
approximation, a technique known in the stochastic optimization literature as
the ``sample average approximation&apos;&apos; (SAA). By optimizing an approximate but
deterministic objective, DADVI can use off-the-shelf second-order optimization,
and, unlike standard mean-field ADVI, is amenable to more accurate posterior
covariances via linear response (LR). In contrast to existing worst-case
theory, we show that, on certain classes of common statistical problems, DADVI
and the SAA can perform well with relatively few samples even in very high
dimensions, though we also show that such favorable results cannot extend to
variational approximations that are too expressive relative to mean-field ADVI.
We show on a variety of real-world problems that DADVI reliably finds good
solutions with default settings (unlike ADVI) and, together with LR
covariances, is typically faster and more accurate than standard ADVI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giordano_R/0/1/0/all/0/1&quot;&gt;Ryan Giordano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingram_M/0/1/0/all/0/1&quot;&gt;Martin Ingram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broderick_T/0/1/0/all/0/1&quot;&gt;Tamara Broderick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10970">
<title>Can GPT-4 Perform Neural Architecture Search?. (arXiv:2304.10970v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10970</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the potential of GPT-4~\cite{gpt4} to perform Neural
Architecture Search (NAS) -- the task of designing effective neural
architectures. Our proposed approach, \textbf{G}PT-4 \textbf{E}nhanced
\textbf{N}eural arch\textbf{I}tect\textbf{U}re \textbf{S}earch (GENIUS),
leverages the generative capabilities of GPT-4 as a black-box optimiser to
quickly navigate the architecture search space, pinpoint promising candidates,
and iteratively refine these candidates to improve performance. We assess
GENIUS across several benchmarks, comparing it with existing state-of-the-art
NAS techniques to illustrate its effectiveness. Rather than targeting
state-of-the-art performance, our objective is to highlight GPT-4&apos;s potential
to assist research on a challenging technical problem through a simple
prompting scheme that requires relatively limited domain
expertise\footnote{Code available at
\href{https://github.com/mingkai-zheng/GENIUS}{https://github.com/mingkai-zheng/GENIUS}.}.
More broadly, we believe our preliminary results point to future research that
harnesses general purpose language models for diverse optimisation tasks. We
also highlight important limitations to our study, and note implications for AI
safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Mingkai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xiu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Shan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00365">
<title>A Transfer Learning Approach to Minimize Reinforcement Learning Risks in Energy Optimization for Smart Buildings. (arXiv:2305.00365v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00365</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy optimization leveraging artificially intelligent algorithms has been
proven effective. However, when buildings are commissioned, there is no
historical data that could be used to train these algorithms. On-line
Reinforcement Learning (RL) algorithms have shown significant promise, but
their deployment carries a significant risk, because as the RL agent initially
explores its action space it could cause significant discomfort to the building
residents. In this paper we present ReLBOT - a new technique that uses transfer
learning in conjunction with deep RL to transfer knowledge from an existing,
optimized and instrumented building, to the newly commissioning smart building,
to reduce the adverse impact of the reinforcement learning agent&apos;s warm-up
period. We demonstrate improvements of up to 6.2 times in the duration, and up
to 132 times in prediction variance, for the reinforcement learning agent&apos;s
warm-up period.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Genkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McArthur_J/0/1/0/all/0/1&quot;&gt;J.J. McArthur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.01666">
<title>BrainNPT: Pre-training of Transformer networks for brain network classification. (arXiv:2305.01666v4 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.01666</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods have advanced quickly in brain imaging analysis over
the past few years, but they are usually restricted by the limited labeled
data. Pre-trained model on unlabeled data has presented promising improvement
in feature learning in many domains, including natural language processing and
computer vision. However, this technique is under-explored in brain network
analysis. In this paper, we focused on pre-training methods with Transformer
networks to leverage existing unlabeled data for brain functional network
classification. First, we proposed a Transformer-based neural network, named as
BrainNPT, for brain functional network classification. The proposed method
leveraged &amp;lt;cls&amp;gt; token as a classification embedding vector for the Transformer
model to effectively capture the representation of brain network. Second, we
proposed a pre-training framework for BrainNPT model to leverage unlabeled
brain network data to learn the structure information of brain networks. The
results of classification experiments demonstrated the BrainNPT model without
pre-training achieved the best performance with the state-of-the-art models,
and the BrainNPT model with pre-training strongly outperformed the
state-of-the-art models. The pre-training BrainNPT model improved 8.75% of
accuracy compared with the model without pre-training. We further compared the
pre-training strategies, analyzed the influence of the parameters of the model,
and interpreted the trained model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinlong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yangmin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shoubin Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07804">
<title>Improving Small Language Models on PubMedQA via Generative Data Augmentation. (arXiv:2305.07804v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07804</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have made remarkable advancements in the field
of natural language processing. However, their increasing size poses challenges
in terms of computational cost. On the other hand, Small Language Models (SLMs)
are known for their efficiency, but they often struggle with limited capacity
and training data, especially in specific domains. In this paper, we introduce
a novel method aimed at improving SLMs in the medical domain using LLM-based
generative data augmentation. The objective of our approach is to develop more
efficient and capable models that are specifically tailored for specialized
applications. Through experiments conducted on the PubMedQA dataset, we
demonstrate the effectiveness of LLMs in refining and diversifying existing
question-answer pairs. This refinement process leads to improved performance in
a significantly smaller model after fine-tuning. Notably, our best SLM, with
under 1.6 billion parameters, outperforms the few-shot GPT-4 on the PubMedQA
dataset. Our code and generated data are publicly available to facilitate
further explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shangdi Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11322">
<title>Knowing When to Stop: Delay-Adaptive Spiking Neural Network Classifiers with Reliability Guarantees. (arXiv:2305.11322v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11322</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) process time-series data via internal
event-driven neural dynamics whose energy consumption depends on the number of
spikes exchanged between neurons over the course of the input presentation.
Typically, decisions are produced after the entire input sequence has been
processed, resulting in latency and energy consumption levels that are fairly
uniform across inputs. However, as explored in recent work, SNNs can produce an
early decision when the SNN model is sufficiently ``confident&apos;&apos;, adapting delay
and energy consumption to the difficulty of each example. Existing techniques
are based on heuristic measures of confidence that do not provide reliability
guarantees, potentially exiting too early. In this paper, we introduce a novel
delay-adaptive SNN-based inference methodology that, wrapping around any
pre-trained SNN classifier, provides guaranteed reliability for the decisions
produced at input-dependent stopping times. The approach, dubbed SpikeCP,
leverages tools from conformal prediction (CP), and it entails minimal
complexity increase as compared to the underlying SNN, requiring only
additional thresholding and counting operations at run time. SpikeCP is also
extended to integrate a CP-aware training phase that targets delay performance.
Variants of CP based on alternative confidence correction schemes, from
Bonferroni to Simes, are explored, and extensive experiments are described
using the MNIST-DVS data set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiechen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangwoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1&quot;&gt;Osvaldo Simeone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning And Evaluation. (arXiv:2305.13399v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently learning visual representations of items is vital for large-scale
recommendations. In this article we compare several pretrained efficient
backbone architectures, both in the convolutional neural network (CNN) and in
the vision transformer (ViT) family. We describe challenges in e-commerce
vision applications at scale and highlight methods to efficiently train,
evaluate, and serve visual representations. We present ablation studies
evaluating visual representations in several downstream tasks. To this end, we
present a novel multilingual text-to-image generative offline evaluation method
for visually similar recommendation systems. Finally, we include online results
from deployed machine learning systems in production on a large scale
e-commerce platform.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19569">
<title>Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19569</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jong Moon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01940">
<title>Sampling binary sparse coding QUBO models using a spiking neuromorphic processor. (arXiv:2306.01940v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01940</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of computing a sparse binary representation of an
image. To be precise, given an image and an overcomplete, non-orthonormal
basis, we aim to find a sparse binary vector indicating the minimal set of
basis vectors that when added together best reconstruct the given input. We
formulate this problem with an $L_2$ loss on the reconstruction error, and an
$L_0$ (or, equivalently, an $L_1$) loss on the binary vector enforcing
sparsity. This yields a so-called Quadratic Unconstrained Binary Optimization
(QUBO) problem, whose solution is generally NP-hard to find. The contribution
of this work is twofold. First, the method of unsupervised and unnormalized
dictionary feature learning for a desired sparsity level to best match the data
is presented. Second, the binary sparse coding problem is then solved on the
Loihi 1 neuromorphic chip by the use of stochastic networks of neurons to
traverse the non-convex energy landscape. The solutions are benchmarked against
the classical heuristic simulated annealing. We demonstrate neuromorphic
computing is suitable for sampling low energy solutions of binary sparse coding
QUBO models, and although Loihi 1 is capable of sampling very sparse solutions
of the QUBO models, there needs to be improvement in the implementation in
order to be competitive with simulated annealing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henke_K/0/1/0/all/0/1&quot;&gt;Kyle Henke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelofske_E/0/1/0/all/0/1&quot;&gt;Elijah Pelofske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_G/0/1/0/all/0/1&quot;&gt;Georg Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1&quot;&gt;Garrett T. Kenyon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05150">
<title>Bayesian Optimization of Expensive Nested Grey-Box Functions. (arXiv:2306.05150v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05150</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of optimizing a grey-box objective function, i.e.,
nested function composed of both black-box and white-box functions. A general
formulation for such grey-box problems is given, which covers the existing
grey-box optimization formulations as special cases. We then design an
optimism-driven algorithm to solve it. Under certain regularity assumptions,
our algorithm achieves similar regret bound as that for the standard black-box
Bayesian optimization algorithm, up to a constant multiplicative term depending
on the Lipschitz constants of the functions considered. We further extend our
method to the constrained case and discuss special cases. For the commonly used
kernel functions, the regret bounds allow us to derive a convergence rate to
the optimal solution. Experimental results show that our grey-box optimization
method empirically improves the speed of finding the global optimal solution
significantly, as compared to the standard black-box optimization algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuning Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svetozarevic_B/0/1/0/all/0/1&quot;&gt;Bratislav Svetozarevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_C/0/1/0/all/0/1&quot;&gt;Colin N. Jones&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10940">
<title>TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting. (arXiv:2306.10940v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10940</link>
<description rdf:parseType="Literal">&lt;p&gt;Wildfires are increasingly exacerbated as a result of climate change,
necessitating advanced proactive measures for effective mitigation. It is
important to forecast wildfires weeks and months in advance to plan forest fuel
management, resource procurement and allocation. To achieve such accurate
long-term forecasts at a global scale, it is crucial to employ models that
account for the Earth system&apos;s inherent spatio-temporal interactions, such as
memory effects and teleconnections. We propose a teleconnection-driven vision
transformer (TeleViT), capable of treating the Earth as one interconnected
system, integrating fine-grained local-scale inputs with global-scale inputs,
such as climate indices and coarse-grained global variables. Through
comprehensive experimentation, we demonstrate the superiority of TeleViT in
accurately predicting global burned area patterns for various forecasting
windows, up to four months in advance. The gain is especially pronounced in
larger forecasting windows, demonstrating the improved ability of deep learning
models that exploit teleconnections to capture Earth system dynamics. Code
available at https://github.com/Orion-Ai-Lab/TeleViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1&quot;&gt;Ioannis Prapas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Ioannis Bountos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondylatos_S/0/1/0/all/0/1&quot;&gt;Spyros Kondylatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1&quot;&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1&quot;&gt;Gustau Camps-Valls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15557">
<title>Simple Steps to Success: Axiomatics of Distance-Based Algorithmic Recourse. (arXiv:2306.15557v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15557</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel data-driven framework for algorithmic recourse that offers
users interventions to change their predicted outcome. Existing approaches to
compute recourse find a set of points that satisfy some desiderata -- e.g. an
intervention in the underlying causal graph, or minimizing a cost function.
Satisfying these criteria, however, requires extensive knowledge of the
underlying model structure, often an unrealistic amount of information in
several domains. We propose a data-driven, computationally efficient approach
to computing algorithmic recourse. We do so by suggesting directions in the
data manifold that users can take to change their predicted outcome. We present
Stepwise Explainable Paths (StEP), an axiomatically justified framework to
compute direction-based algorithmic recourse. We offer a thorough empirical and
theoretical investigation of StEP. StEP offers provable privacy and robustness
guarantees, and outperforms the state-of-the-art on several established
recourse desiderata.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamer_J/0/1/0/all/0/1&quot;&gt;Jenny Hamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valladares_J/0/1/0/all/0/1&quot;&gt;Jake Valladares&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1&quot;&gt;Vignesh Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1&quot;&gt;Yair Zick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15865">
<title>Differentially Private Distributed Estimation and Learning. (arXiv:2306.15865v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15865</link>
<description rdf:parseType="Literal">&lt;p&gt;We study distributed estimation and learning problems in a networked
environment in which agents exchange information to estimate unknown
statistical properties of random variables from their privately observed
samples. By exchanging information about their private observations, the agents
can collectively estimate the unknown quantities, but they also face privacy
risks. The goal of our aggregation schemes is to combine the observed data
efficiently over time and across the network, while accommodating the privacy
needs of the agents and without any coordination beyond their local
neighborhoods. Our algorithms enable the participating agents to estimate a
complete sufficient statistic from private signals that are acquired offline or
online over time, and to preserve the privacy of their signals and network
neighborhoods. This is achieved through linear aggregation schemes with
adjusted randomization schemes that add noise to the exchanged estimates
subject to differential privacy (DP) constraints. In every case, we demonstrate
the efficiency of our algorithms by proving convergence to the estimators of a
hypothetical, omniscient observer that has central access to all of the
signals. We also provide convergence rate analysis and finite-time performance
guarantees and show that the noise that minimizes the convergence time to the
best estimates is the Laplace noise, with parameters corresponding to each
agent&apos;s sensitivity to their signal and network characteristics. Finally, to
supplement and validate our theoretical results, we run experiments on
real-world data from the US Power Grid Network and electric consumption data
from German Households to estimate the average power consumption of power
stations and households under all privacy regimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1&quot;&gt;Marios Papachristou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimian_M/0/1/0/all/0/1&quot;&gt;M. Amin Rahimian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01482">
<title>Nexus sine qua non: Essentially Connected Networks for Traffic Forecasting. (arXiv:2307.01482v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01482</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial-temporal graph neural networks (STGNNs) have become the de facto
models for learning spatiotemporal representations of traffic flow. However,
modern STGNNs often contain superfluous or obscure components, along with
complex techniques, posing significant challenges in terms of complexity and
scalability. Such concerns prompt us to rethink the design of neural
architectures and to identify the key challenges in traffic forecasting as
spatial-temporal contextualization. Here, we present an essentially connected
model based on an efficient message-passing backbone, powered by learnable node
embedding, without any complex sequential techniques such as TCNs, RNNs, and
Transformers. Intriguingly, empirical results demonstrate how a simple and
elegant model with contextualization capability compares favorably w.r.t. the
state-of-the-art with elaborate structures, while being much more interpretable
and computationally efficient for traffic forecasting. We anticipate that our
findings will open new horizons for further research to explore the possibility
of creating simple but effective neural forecasting architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_T/0/1/0/all/0/1&quot;&gt;Tong Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1&quot;&gt;Guoyang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lijun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jian Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03393">
<title>Exploring the Potential of Large Language Models (LLMs) in Learning on Graphs. (arXiv:2307.03393v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning on Graphs has attracted immense attention due to its wide real-world
applications. The most popular pipeline for learning on graphs with textual
node attributes primarily relies on Graph Neural Networks (GNNs), and utilizes
shallow text embedding as initial node representations, which has limitations
in general knowledge and profound semantic understanding. In recent years,
Large Language Models (LLMs) have been proven to possess extensive common
knowledge and powerful semantic comprehension abilities that have
revolutionized existing workflows to handle text data. In this paper, we aim to
explore the potential of LLMs in graph machine learning, especially the node
classification task, and investigate two possible pipelines: LLMs-as-Enhancers
and LLMs-as-Predictors. The former leverages LLMs to enhance nodes&apos; text
attributes with their massive knowledge and then generate predictions through
GNNs. The latter attempts to directly employ LLMs as standalone predictors. We
conduct comprehensive and systematical studies on these two pipelines under
various settings. From comprehensive empirical results, we make original
observations and find new insights that open new possibilities and suggest
promising directions to leverage LLMs for learning on graphs. Our codes and
datasets are available at https://github.com/CurryTang/Graph-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhikai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Haitao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Wei Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiaochi Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08364">
<title>Q(D)O-ES: Population-based Quality (Diversity) Optimisation for Post Hoc Ensemble Selection in AutoML. (arXiv:2307.08364v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08364</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated machine learning (AutoML) systems commonly ensemble models post hoc
to improve predictive performance, typically via greedy ensemble selection
(GES). However, we believe that GES may not always be optimal, as it performs a
simple deterministic greedy search. In this work, we introduce two novel
population-based ensemble selection methods, QO-ES and QDO-ES, and compare them
to GES. While QO-ES optimises solely for predictive performance, QDO-ES also
considers the diversity of ensembles within the population, maintaining a
diverse set of well-performing ensembles during optimisation based on ideas of
quality diversity optimisation. The methods are evaluated using 71
classification datasets from the AutoML benchmark, demonstrating that QO-ES and
QDO-ES often outrank GES, albeit only statistically significant on validation
data. Our results further suggest that diversity can be beneficial for post hoc
ensembling but also increases the risk of overfitting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purucker_L/0/1/0/all/0/1&quot;&gt;Lennart Purucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_L/0/1/0/all/0/1&quot;&gt;Lennart Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastacio_M/0/1/0/all/0/1&quot;&gt;Marie Anastacio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1&quot;&gt;Joeran Beel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoos_H/0/1/0/all/0/1&quot;&gt;Holger Hoos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13055">
<title>MARIO: Model Agnostic Recipe for Improving OOD Generalization of Graph Contrastive Learning. (arXiv:2307.13055v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13055</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate the problem of out-of-distribution (OOD)
generalization for unsupervised learning methods on graph data. This scenario
is particularly challenging because graph neural networks (GNNs) have been
shown to be sensitive to distributional shifts, even when labels are available.
To address this challenge, we propose a \underline{M}odel-\underline{A}gnostic
\underline{R}ecipe for \underline{I}mproving \underline{O}OD generalizability
of unsupervised graph contrastive learning methods, which we refer to as MARIO.
MARIO introduces two principles aimed at developing distributional-shift-robust
graph contrastive methods to overcome the limitations of existing frameworks:
(i) Information Bottleneck (IB) principle for achieving generalizable
representations and (ii) Invariant principle that incorporates adversarial data
augmentation to obtain invariant representations. To the best of our knowledge,
this is the first work that investigates the OOD generalization problem of
graph contrastive learning, with a specific focus on node-level tasks. Through
extensive experiments, we demonstrate that our method achieves state-of-the-art
performance on the OOD test set, while maintaining comparable performance on
the in-distribution test set when compared to existing approaches. The source
code for our method can be found at: https://github.com/ZhuYun97/MARIO
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haizhou Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenshuo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siliang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16039">
<title>Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback. (arXiv:2307.16039v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16039</link>
<description rdf:parseType="Literal">&lt;p&gt;A key technology for the development of large language models (LLMs) involves
instruction tuning that helps align the models&apos; responses with human
expectations to realize impressive learning abilities. Two major approaches for
instruction tuning characterize supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), which are currently applied to produce the
best commercial LLMs (e.g., ChatGPT). To improve the accessibility of LLMs for
research and development efforts, various instruction-tuned open-source LLMs
have also been introduced recently, e.g., Alpaca, Vicuna, to name a few.
However, existing open-source LLMs have only been instruction-tuned for English
and a few popular languages, thus hindering their impacts and accessibility to
many other languages in the world. Among a few very recent work to explore
instruction tuning for LLMs in multiple languages, SFT has been used as the
only approach to instruction-tune LLMs for multiple languages. This has left a
significant gap for fine-tuned LLMs based on RLHF in diverse languages and
raised important questions on how RLHF can boost the performance of
multilingual instruction tuning. To overcome this issue, we present Okapi, the
first system with instruction-tuned LLMs based on RLHF for multiple languages.
Okapi introduces instruction and response-ranked data in 26 diverse languages
to facilitate the experiments and development of future multilingual LLM
research. We also present benchmark datasets to enable the evaluation of
generative LLMs in multiple languages. Our experiments demonstrate the
advantages of RLHF for multilingual instruction over SFT for different base
models and datasets. Our framework and resources are released at
https://github.com/nlp-uoregon/Okapi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1&quot;&gt;Viet Dac Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1&quot;&gt;Chien Van Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1&quot;&gt;Nghia Trung Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thuat Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan A. Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thien Huu Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16648">
<title>LLMs4OL: Large Language Models for Ontology Learning. (arXiv:2307.16648v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16648</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the LLMs4OL approach, which utilizes Large Language Models (LLMs)
for Ontology Learning (OL). LLMs have shown significant advancements in natural
language processing, demonstrating their ability to capture complex language
patterns in different knowledge domains. Our LLMs4OL paradigm investigates the
following hypothesis: \textit{Can LLMs effectively apply their language pattern
capturing capability to OL, which involves automatically extracting and
structuring knowledge from natural language text?} To test this hypothesis, we
conduct a comprehensive evaluation using the zero-shot prompting method. We
evaluate nine different LLM model families for three main OL tasks: term
typing, taxonomy discovery, and extraction of non-taxonomic relations.
Additionally, the evaluations encompass diverse genres of ontological
knowledge, including lexicosemantic knowledge in WordNet, geographical
knowledge in GeoNames, and medical knowledge in UMLS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giglou_H/0/1/0/all/0/1&quot;&gt;Hamed Babaei Giglou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1&quot;&gt;Jennifer D&amp;#x27;Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Auer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00127">
<title>DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms. (arXiv:2308.00127v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00127</link>
<description rdf:parseType="Literal">&lt;p&gt;Datacenters are increasingly becoming heterogeneous, and are starting to
include specialized hardware for networking, video processing, and especially
deep learning. To leverage the heterogeneous compute capability of modern
datacenters, we develop an approach for compiler-level partitioning of deep
neural networks (DNNs) onto multiple interconnected hardware devices. We
present a general framework for heterogeneous DNN compilation, offering
automatic partitioning and device mapping. Our scheduler integrates both an
exact solver, through a mixed integer linear programming (MILP) formulation,
and a modularity-based heuristic for scalability. Furthermore, we propose a
theoretical lower bound formula for the optimal solution, which enables the
assessment of the heuristic solutions&apos; quality. We evaluate our scheduler in
optimizing both conventional DNNs and randomly-wired neural networks, subject
to latency and throughput constraints, on a heterogeneous system comprised of a
CPU and two distinct GPUs. Compared to na\&quot;ively running DNNs on the fastest
GPU, he proposed framework can achieve more than 3$\times$ times lower latency
and up to 2.9$\times$ higher throughput by automatically leveraging both data
and model parallelism to deploy DNNs on our sample heterogeneous server node.
Moreover, our modularity-based &quot;splitting&quot; heuristic improves the solution
runtime up to 395$\times$ without noticeably sacrificing solution quality
compared to an exact MILP solution, and outperforms all other heuristics by
30-60% solution quality. Finally, our case study shows how we can extend our
framework to schedule large language models across multiple heterogeneous
servers by exploiting symmetry in the hardware setup. Our code can be easily
plugged in to existing frameworks, and is available at
https://github.com/abdelfattah-lab/diviml.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghannane_Y/0/1/0/all/0/1&quot;&gt;Yassine Ghannane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelfattah_M/0/1/0/all/0/1&quot;&gt;Mohamed S. Abdelfattah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00436">
<title>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning. (arXiv:2308.00436v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00436</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent progress in large language models (LLMs), especially the invention
of chain-of-thoughts (CoT) prompting, makes it possible to solve reasoning
problems. However, even the strongest LLMs are still struggling with more
complicated problems that require non-linear thinking and multi-step reasoning.
In this work, we explore whether LLMs have the ability to recognize their own
errors, without resorting to external resources. In particular, we investigate
whether they can be used to identify individual errors within a step-by-step
reasoning. To this end, we propose a zero-shot verification scheme to recognize
such errors. We then use this verification scheme to improve question-answering
performance, by using it to perform weighted voting on different generated
answers. We test the method on three math datasets-GSM8K, MathQA, and MATH-and
find that it successfully recognizes errors and, in turn, increases final
predictive performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_N/0/1/0/all/0/1&quot;&gt;Ning Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1&quot;&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>